[
    {
        "title": "Single Source of Truth leveraging Pandera - Flyte Community Meeting | Dec 12, 2023",
        "transcript": "hello everyone happy\nTuesday um I'm sure you guys are as\nexcited as I am since we are near the\nholiday season just few more days to go\num before we start I must say that I am\nsuper\ngrateful for the opportunity to speak at\ntoday's event and meeting all the smart\npeople like you I would like to thank\nNeils for being an aming partner in our\njourney to ramp up Pandera to new\nheights thank NE and um thank you David\nfor arranging this and letting me speak\nhere all right so let's get started\num so here's the agenda for this session\nI'll start with a brief history about my\nbackground and Quantum black which is an\nAI uh an R&D Wing uh for\nmckeny and then I'll jump over how it\nall gets started and um how we\ndiscovered\nPandera then I'll also cover about my\nteam's contribution to Pandera followed\nby some new features we developed um for\nbroader use cases finally Q&A at the\nend all right so about me so my name is\nn motra um I'm a principal engineer at\nQuantum black AI Labs I joined firm in\n2019 and since my day one had been\nclosely working with our clients and\ntheir leadership to help them leverage\nAI to solve their business or\norganizational\nchallenges after some time doing that uh\nfirm gave me an opportunity to join\ntheir R&D ring which is quantum black AI\nlabs in my past life before firm two I\nhad been closely working on product\ndevelopment side so when I when firm\noffered me to join R&D team it was a\nperfect fit for me uh interest and skill\nset\nwise uh before from I had been a lead\narchitect for uh Telecom systems uh more\nspecifically Building Systems the\ncompany I was working for had about 70%\nmarket share globally at the time so I\nfeel good that I was lucky to be part of\nsomething big um in other words your\nTelecom provider might be using our\nsolution to build you so please don't\nhit\nme um I also had been lucky to start a\nnew R&D initiative back in the days to\nbuild a predictive system for our\nclients much before Ai and big data was\na thing can you imagine how hard could\nit\nbe um talking about today my D to-day\nwork is around building in-house um\nproducts and solutions for our firm's\nclients some of the projects uh my CU\napp team have developed in recent years\nare also open source and I I'll cover\nthem a little bit um\nyeah all right so let's talk about\nQuantum black what is quantum black\nQuantum black began its um operations as\na data analytics company focusing on the\ndomain of Formula One\nracing from car designing and testing to\nmeasuring precise locations of the car\non the race track within centim and Pit\nStop duration to\nmilliseconds um those who don't know it\nmakes a huge difference in winning or\nlosing the race with with this data it's\na great study and um I have added QR\ncode for anyone interested to read more\nabout it and also you can google and\nfind um Quantum black uh Formula 1 um\nstudy or something like that\num in 2015 it was acquired by McKenzie\nand has grown rapidly since then\nbuilding a team of thousands of\nTechnologies across the across the globe\ntoday um and the firm is committed to\nshare its work with community and in\nrecent years through Quantum black uh\nour team have released more open-source\nSolutions um including\nkro um\nc v which is I think we released this\nyear um nucleo and ml run and I hope\nthere will be many more coming in future\ntoo so this is about Quantum black and L\nlet's jump gears and talk about how it\nall got started like how do we get\nstarted with Pand and it started with\nwith a single source of Truth what is\nsingle source of Truth so let me explain\nwith an example\num imagine you want to buy Tesla\neveryone wants to buy Tesla nowadays so\nlearn so so to learn more about all\ndifferent cars models um Tesla offers\nyou will visit like you should visit\ntesla.com right in in this case\ntesla.com is the single source of Tru\nfor most up toate information about all\ntheir cars um including their pricing\nwhat discounts they are offering the\ncars feature ranges performance\neverything you want to know about the\ncar um is there on the\ntesla.com although there might be some\nother platforms providing same\ninformation by Pur of copy paste from\ntesla.com but if you want most up toate\nand accurate information about anything\nyou should go to the source of it and in\nthis case it will be\ntesla.com right now if you apply the\nsame analogy to a machine learning\napplication the single source of truth\nwill be the base unit of any model which\nis its\ndata um so at Quantum black we wanted to\nbuild a system which can aim to be the\nsingle source of Truth for any data\nrelated\nproject which almost every other data\nproject is\ntoday so in my mind when I was working\non this in my mind such a system must\nhave three\nrequirements first uh centralized data\ncatalog with schema information so I\nwant it to be centralized at one place\neach to\nfind second is the ability to\nautonomously validate not just schema\nIntegrity but also the data\nitself and third and most important was\neasy to plug and play with minimal\nefforts to existing code bases because\nwe have ton of already existing projects\nat Quantum black and I didn't want them\nto be left laptop so I want them to be\nuh able to use what you are building by\neasily plugging into\nit so initially we plan to build it in\nhouse but our Venture led us to Pandera\nwhich is an open source framework as you\nall know and and it's designed for\nschema and data\nvalidation so when I discovered this I\nwas super excited to learn that it is\nexisting and we might not have to build\nit uh from scratch so I immedi is are\ndigging into its codee base to evaluate\nwhether it fits our\nrequirements um unfortunately it was\nlacking na pice spark support at the\ntime and that's the key for us like most\nof our work in the data pipeline world\nis based on the pice\nPark so I must have this feature on pent\nbefore I could use it for our inhouse\nproject so I remember reaching out to\nNeils um asking if he could build\nsomething to support pis Park workflow\nin in Pand and I needed it like\nyesterday so I reach out to him um do do\nyou have this in plan are you working on\nit and and he immediately responded back\nand he asked me oh uh would you like to\ncontribute to it and I was like oh\nman\num so in\nbut but I was super excited like um okay\nso this is an open source he doesn't\nhave the plan right now but maybe this\nwill be something we can do so as in any\nconsulting firm here at Quon black and\nMcKenzie I had to run the proposal to\nour legal and compliance team before we\ncould\nengage and once they approved we started\nworking on it immediately and literally\nwe put the rocket engines to Pand um in\nin my opinion by adding by far\nsupport\num so my plan actually evoled uh during\nthis process\nto um let's first extend\nPandera\nand and then we can bring it to plug it\ninto a gr solution so the first step was\ninstead of building something of our own\nwe need to go and enhance Pandera and\nalso uh contribute back to the community\nand then we will plug it into our\nsolution and uh the cool thing is that\nthe firm is super excited about it and\nand they they agreed to this\napproach um after we build this uh I'll\ntalk about what we did there in a bit\nbut after we extended it it was much\neasier for us to just plug it into our\nsolution um and and we are not only\nextending P penda for Pisa but we also\nbuild in the process many other\nutilities on top of it which uh were\nneeded for our solution so some of them\nare contributed back to the community\nbut some were very specific to our use\ncase so we built it in a house and I'll\ncover some of them in a\nbit now with the help of some decorators\nand wrappers we could easily integrate\nit into our in-house products and we\nadded all those uh side utilities to it\nto to to get the work done we needed for\nour use cases now today there are like\nmultiple projects within the firm which\nare either have adopted uh Pandera and\nour solution uh or they are working on\nit and as Neil know there are many\npeople also working still working\nenhancing few things um on the panda\nside and pushing into the community\ntoday so we are\nactively um um actively engaged with\npanda even today\num so while while we are talking about\npend let's also talk a little bit about\nwhat what we actually built\nthere um so this is an example\nof U how the schema for a pisar data\nframe can be defined using Pandera and\nbpark as you'll realize the those who\nhave used Pandera they realize that the\nlook and feel is very consistent with\nother pend versions like uh like if\nyou're working with pandas on the on the\npanda side and we intentionally kept it\nlike so because one of the goal was like\nPandera supports multiple multiple\nFrameworks and one of the goal was to\nkeep the user experience consistent\nacross all\nFrameworks and I think we did a good job\nthere we we tried to be as consistent as\npossible there are today I think there\nare some extra features on the bbar side\nwhich are not on the other Frameworks\nbut eventually they will also be adopted\nin other\nplaces so here what you see is a simple\nschema definition of a data frame with\nonly three Fields um and each field is\ntype annotated and in this case the type\nis a by Spar data\ntype um the field function is as you all\nknow it's a panda function to define the\nfields and it it comes with other\nwhistles and row I'll just um talk a\nlittle bit more about that in a\nbit um this is the same example but here\nI have two two more fields in the uh end\ndescription and meta and these are just\nto highlight that in p park Pandera\nversion we are also supporting complex\ndata types uh which is pretty common in\nbpark World um so essentially if your\nuse case is requiring complex data types\nwe have the support for\nit um so so far we have looked at about\nlike how the schema is defined but where\nthe data validation comes in so for data\nvalidations we use something called\nchecks it's same concept as Pam we we\ninherited it and we enhance it to work\nwith the ppar data frames so for example\nhere in the ID column the first column\nin the schema you'll see something\ncalled uh inside um a field there's an\nargument\nGT equal to 5 which basically highlights\nthat the this is this is a check data\nlevel check which basically tells that\nID column should be greater than five um\nand if it is not it is not complying\nwith the constraint we have on the data\nlevel for this similarly the other field\nuh product name must start with the\nletter B so there are many checks which\nare already\nsupported uh and you can build new\ncustom checks for your use\ncases um so that flexibility already was\nthere in in\nPandera all right so once you have all\nthese schema definition done if you want\nto put everything all together like this\nis this is the full working example this\nis how it look like so in our case we\nfirst Define the schema same five Fields\nI I I talked in the previous\nslide but to\num to to to simulate this uh\nvalidation I have some the data\nintentionally corrupted values in it to\nto make sure that there is some error in\nthe end for example in the first row\nalthough we have a validation that value\nshould be greater than five but the\nfirst value here is five so it will fail\nand the second row is not starting the\nproduct name is not starting with the\nletter b instead of butter it is cutter\nso like so like just to simulate the\nerror now in order to define a schema\nlevel validation\nI also have to Define scheme in ppar too\nso to enforce like on the input data\nframe before running through Pand ppar I\nhave the schema in ppar defin so this is\ncompletely optional it's not needed in\nthe real world but just for simulating\nthis I have to add it and one thing that\nwhich is not obvious from this example\nexample is that for the meta field in\nPandera schema by default allows null\nvalues as we didn't mention any the rule\nfor it in B Spar schema but I have said\nfalse uh here which means the null is\nnot allowed for this field this is just\nto simulate uh a schema level error and\nI'll show you how the error report looks\nlike in a bit so once you have this you\ncan just create your data frame and run\nit through the through the validate\nfunction passing your data frame and it\nwill generate an\noutput um this is how the output looks\nlike\nso um so initially the I think in\nprevious versions of the panda the\noutput was displayed on the screen but\nwe changed that for a production like\napplications um to generate some kind of\nReport with ppar you will get an report\nin Python dictionary format which can be\nconverted to Json easily and then it can\nbe plugged into like uh some\nvisualization tools like grafana or\nsomething um we have built something in\nhouse for it in other case where we are\nproviding these\nreports um to be consumed by the\ndownstream\napplications now if you look at the\nreport it has two sections one is the\ndata section the other is the schema\nsection\nand for data section you can see like it\nis highlighting all those errors which I\njust explained that uh there is an issue\nwith the ID column there's an issue with\nthe product name um and for the schema\nit is uh intelligent enough to um\nhighlight that there is a null value\nallowed but uh it is not allowed in like\nthe pice Park like it's when I enforce\nit it was not allowed but by default it\nis allowed so there is something wrong\nwith the scheme and it it CAU that\nerror\nnow um in our in-house product\nintegration when we were working on\nsingle source of Truth project we also\nversion controlled this report so what\nhappens is like when we run the pipeline\npipeline consumes the data validates it\nand then give it to the uh machine\nlearning and training model uh we\ngenerate this report and and we have a\noption\nto um fail the pipeline when we find the\nissue or we generate the report and save\nit but we continue training the model so\nI think that's the default behavior in\nin in our uh use case so these reports\num are version controlled in our case\nwhich means we can maintain a Time\nseries of the error reports and we can\nvisually display it or um or they can be\nconsumed for whatever like automation or\nsomething like that now they are in our\ncases in our product they are offered as\noutof the Box functionality all the ml\nproducts we we build here so it is very\ntightly uh integrated now in all those\nproducts\num now let's talk about some of the\nutility functions we built on top of\nthis to to make it\nmore suitable for our use cases and some\nof the I think the ones which I will be\nsharing now will be will are they are\navailable in the open source version of\nit\nso all right so first one was like onof\nswitch which is pretty common so for\napplication that rely on Bice Park\nhaving an onof switch is an important\nfeature that can make a significant\ndifference in terms of flexibility and\nrisk management things those who have\nworked on the big data application they\nknow the pain uh specifically an honor\nswitch allow them to disable data\nvalidation um or scheme of validation in\nproduction without requiring code\nchanges this is especially important for\nthe Big Data pipelines where performance\nis critical and time is of the essence\nthey don't want to run the validation\nevery time or throughout the day they\njust want like backend end of the day\nprocess something like that so those\nkind of challenges come in when you are\nworking with the real production\nevaluations so I think this is one of\nthe most important features we have to\nhave in Pandera and again this is\navailable for anyone to adopt uh\ntoday um now in in many cases data\nvalidation can be of a significant\namount uh in processing time which can\nimpact the overall performance of the\npipeline so it is good to have this\ncontrol um we buil this feature for our\nown requirement in single source of to\nproject but we also thought it will be\ncritical critical for anyone adopting\nPandera for their real world use cases\nso it is part of Pandera bpar today like\nI\nsaid all right um next one was we we got\nmore deeper into pend now in addition to\non of switch where you can just turn off\nthe whole validation part for the for\nthe\npenda now what if you want to do the\nvalidation but we don't want to do like\nall kinds of validation like schema and\nthe data validation both which is the\ndefault setting where if you define your\npipeline you define your schema you\ndefine your validations and you run it\nit will generate the error report like I\nshowed you if there are any schema level\nissues and the data level issues it will\nlook at both of them but for some cases\nlike people don't want to do that uh in\nin real world for example sometimes you\njust only need a schema level validation\nis good enough for your to use cases\nsometimes you need the data level\nvalidation but you don't care about the\nschema you know the schema is right and\nyou don't want to spend any processing\non it uh every time the pipeline is\nrunning so there are all different kind\nof variations now to give that kind of\ncontrol to the user we build this\nfeature which is uh a Pandera validation\ndepth you can control the depth of your\nvalidation whether you want schema level\ndata level or both both is the um\ndefault I\nthink all right so this was another\nthing\num this third feature\nwas was kind of my favorite um miror\ndata at the column and data frame levels\nso we introduce a new feature in Pandera\nthat allows users to store add addtional\nmetadata or the context information at\nthe field and schema level this feature\nis designed to allow users to embed\ncontextual information in their schema\ndefinitions while they are defining\nwhich can be leveraged by other\napplications so think of this way um\nlike I\nsaid um so once you have your a schema\ndefinition done for the project you have\ndata validations done for the project\nyou feed that the data into your\nmodel um and everything is working good\nnow when you have to ship it to the\nusers or the the client like in our case\nwe keep enhancing our product we have\nreleases and everything one of the\nchallenge we had in in our side was um\nwe had to maintain a how to guide how to\nuse guide where we educate our client's\nteam\nto uh to follow the steps they need in\norder to use that product and for any\ndata product they have to have a data in\ncertain format in certain layout with\nsudden schema for the product to work\nproperly so we had to maintain this\ndocumentation and and we had\nlike honestly no developer wants to\nmaintain the documents uh that's a pain\nso and when you are releasing often it\nbecomes difficult to keep do\ndocumentation and code in\nsake so um with this particular feature\nwhat we did was or the reason for this\nfeature was like one of the thing we\nwant to make sure that\nalways the steps to get the data or the\ndata request are consistent with the\nwith the code so so we used to like in\nour how to guide there was a data\nrequest section where we ask our clients\nyou need all these data you need all\nthese tables and when you plug this in\nyou will get this kind of outut now that\nfield the the list of those tables and\nthe fields can be autogenerated if you\nhave the contextual information about\nthe schema itself in your in in your\nproduct and this is where we use the\nmetadata information we added here so as\nyou can see it's a very basic example\nhere we Define like a metadata on the\nfield level ID there is a use case and\nthere are like two dummy use cases\nretail pricing and consumer Behavior\nwhich indicates that this particular\nfield is used by these two use cases so\nif I'm packaging this product for retail\npricing\nclient uh this will automatically be\npart of the data request uh document and\nthey will know that they need this so\nthe so developer doesn't have to\nremember it and go and update the\ndocument it will be automatically\ncreated we we created all those util\nfunctions uh in in our product which\nlooks at the schema and then generate\nthe documentation needed for\nit um similarly for like it's not just a\nfield level it can be also at the data\nframe level and that's what the metadata\nat the end um\nindicates\num right so we generate this in our\nworld and and send it to the\nclients um and here is an alha function\nwe added in Panda to get this\ninformation out of viewer schema now the\nnow one of the challenges when we were\nworking on this um we had an\nexisting existing product which had\nmultiple prod multiple data inputs\nand one of the daunting task was like\nokay so we are bringing this Pandera\nthing now we need to build the schema\nfor for all of our data frames which is\na timec consuming process so we build an\nin-house script for it like today it is\nautomated um you just plug it into it\nand it will there's a script that will\nrun uh that will look at all your data\nuh inputs all your data frames and it\nwill generate this schema information\nfor it um users still have to like in\nthe current version we're working on it\nbut in the current version users still\nhave to go and uh\num um add any data level validations\nthey want for their use case and also\nthe metadata so metadata is something\nthat uh you need some kind of expertise\nto know before you can uh automate this\nthing so those those are the things we\nwe have done on our side and and this\nfunction here um going back to this uh\nget metadata will extract the schema\nlevel um including all fields in it um\nfor for anyone to consume it and use it\nin their St so this is pretty\nstraightforward okay\num I think we are in the final slide so\nnothing is possible without a great team\nso here um with my team we were able to\ndeliver this in I think couple of weeks\nor so and then we were also working on\nside for our use case\nso uh we B\num Midnight Oil uh a couple of days\nworked uh late hours but it was all\nworth it and I'm super\nexcited um to be part of this journey\nand again thanks to Neil and team for uh\nmaking it happen um anyone who wants to\nknow more about uh Quantum black and\nwhat are our open source Solutions we I\nhave some QR codes here just feel free\nto use them and also for panda\ndocumentation the second QR code is uh\ngoing to point you to that um yep so\nthat's it uh I'm happy to take any\nquestions\nI just want to say really quick that um\nyeah this was\na very momentous sort of collaboration\nfor the project because up until our\ncollaboration pend P Pandera had just a\nbunch of assumptions right that we had\nto break out of so for\nexample um for the most part it operated\non in-memory data and simply that\nassumption leads to a bunch of design\ndecisions that did not quite fit uh big\ndata type data frame Library so this\nreally shook us out of our kind of local\nminimum of uh you know utility as a as a\ndata validation tool and um so that's\none dimension the other big Dimension is\nwe really had to clean up the kind of\nextensibility story of Pandera making it\nyou know streamlining it and making it\npossible to add your own back end within\nlike a month or two or less if you're\nyou know working on it like crazy um but\nyeah I was really I'm really proud\nof um the fact that we got this out the\ndoor in like I think a month or two so\num and then obviously we tested it and\nthere was a little bit of lag time there\nbut the core of the the back end was\nbuilt very very quickly so yeah kud\nkudos to nud and\nteam um I guess I did have a question so\num you know we we we send each other\nemails back and forth U somewhat but I\nI'm\ncurious what\num yeah I'm curious like who who has\nadopted this I don't know how much you\ncan share but like how what has the\nadoption story looked like and are there\nany blockers that we can help with\nyeah\nso um first of all thanks Neils uh it\nlike we did it pretty quickly\nbut it was because the Pender itself was\nbuilt in a way that we could easily\nenhance it so goodu to you and team\num so talking about what other other\nadop\nadoptions um that's the funny thing with\nConsulting lot of the things we are not\nsupposed to share uh those are propriety\nI hope you understand but at a very high\nlevel I can see that there is a good\namount of traction within the\nfarm and uh I I'm not sure if that went\nthrough but there was some talk about\nlike some live um implementation with\nsome of the clients too and I think\nthere there are a couple of guys they\nare also engaging you\nuh directly there they're working on\nsome enhancements they are also working\non some other projects of their own\nwithin the\nlab um so they're they're enhancing and\nbuilding it for their own use cases uh\nwithin the lab so that there is\ndefinitely a good amount of traction\nwithin the form and there are a lot of\npeople who reach out to me even today\nasking for all these questions about\nintegration and how they can leverage it\nso\nyeah I have a question for you um how\nmany downloads we have now U did you do\nyou have the\nnumber uh I have not actually checked\nrecently it\nis um what does it look like I did I did\nsort of notice an\nuptick right like a few months ago weeks\nago but yeah I generally I generally\nstay away from those\nmetrics yeah that's nice and any any\nstories that you can share apart from us\nlike any\nother um group of people or the other\nthe companies who benefited from this\nchange that you can\nshare uh I need to I actually need to do\nsome detective work but there have been\nissues and PRS touching the ppar code\nbase so um yeah there's there's\ndefinitely\nat least like four or five independent\nfolks um maybe they're the same team I\ndon't know but they there's definitely\nfolks uh using it now and you\nknow um some of the things aren't\nsupported like pantic using spice Park\nSQL schemas and pantic models but yeah\nthose we\ncan um have Community contributions for\nokay\nnice\nthank you any other question\ncomment there was interest from Netflix\nin this right what happened do you know\nNetflix wanted to uh I I need to keep\npoking them um yeah I I poked them like\ntwice already maybe maybe you can do\nthat as well yeah I I can uh NE great\nwork by the way my name is SK then uh\nthank you for presenting I will connect\nyou guys with Netflix uh Holden Car she\nwas interested in this so might be very\ninteresting nice to meet you K\nfinally um yeah super excited about the\nNetflix uh um interest like definitely\nwant to know what they are using it\nfor"
    },
    {
        "title": "Flyte Community Roundtable | Flyte Community Meeting - November 28, 2023",
        "transcript": "and yes um so we are going to have a\nflight Community round table today\nconsider this uh laidback space to\ndiscuss about flight be use cases or\npain points we encourage open-ended\ndiscussions so please feel free to ask\nyour questions and join the conversation\nso today we have Chris grass guy arit\nJake Dodd Dan Butler Dennis schwest off\nand Blake Jackson joining us a big thank\nyou to you all for agreeing to join the\ndiscussion\nI'm pretty excited for this so without\nany further Ado let's get started so\nChris um could you let us know about\nyourself the company you work at and\nwhat you\ndo yeah hi uh my name is Chris grass I\nwork at MCG Health um we're based out of\nSeattle um we I work on a data platform\nteam where we're trying to spin up um\nkind of a data warehouse solution for\nthe entire company um part of that is\nserving data scientists and allowing\nthem to run experiments on our data and\npart of it is an ETL platform uh\nsolution that we're trying to develop\nand we're hoping flight can help us with\nuh both those\nprojects yeah thanks for the\nintroduction and next um guy would you\nlike to introduce yourself as\nwell uh I'm sure uh I'm my name is guy I\nwork uh for a small startup called\ndimbo the startup deals with\na data collection from from the web uh I\nwas uh looking to\nusing I was looking into using flight\nfor uh the workflow orchestration\ncapabilities\num we will have uh as part of our our\nproduct um different workflows we're\ngoing to want to run depending on on on\nthe kind of data collection we're\ndoing\num I as far as I could understand it's\nnot like the original intent of flight\nuh we we might do machine learning but\nthat's not the main uh purpose uh I want\nto use flight for\num I haven't we haven't started actually\ndeveloping and and using it yet um but\nso far it looks\ngood so you are currently using flight\nfor data pipelines right no no so we're\nnot using flight yet I'm looking into\nusing it uh mostly for workflow\norchestration\nokay and not specifically for data\npipelines\nno thank you and Dan um could you\nintroduce yourself as well sure um I'm\nDan Butler I'm a bioinformatics Analyst\nat the sulk Institute in San Diego um\nand I'm also the data management lead\nfor a project called the Suk harnessing\nplants initiative um so this is a this\nis a project that involves um\nphenotyping and genotyping plants and um\ndoing correlational analysis could call\nit um so right now I'm in the process of\ntesting flight for our for our needs um\nwe have a bunch of analysts spread among\nuh spread among a few different research\ngroups here who have developed various\npipelines um that are mostly run\nmanually right now and so this was an\neffort to um kind of centralize\nour workflow orchestration um and so I'm\nI've implemented a sort of initial\nversion of this system with flight and\nhave some learnings and thought it would\nbe cool to hear what everyone else is\ndoing\nawesome H\nJake hello so I'm Jake I'm a machine\nlearning engineer for a London based\nstart passway um we kind of focus on\npredictive but traffic for retail\ncompanies so having a small data team\nkind of focused on\nproductionizing a lot of the data\nscience notebooks um and we kind of\nreached the capabilities of what airf\ncan offer us and we're kind of hoping\nfor something that can provide more of a\ncuetes native solution so we're\ncurrently undergoing a PC with\nflight all right thank you and uh Chris\nI think you already introduced right\nBlake yeah I'm Blake Jackson um I work\nfor a small software company called RVA\nenergy we're a virtual energy shop um uh\nthe shop was using a spring batch um but\nI come from back I was at Spotify\npreviously um we've you know been using\nflight there um and so yeah we're kind\nof looking to switch out some of the\nthings we're doing with uh spring batch\nor some of the new work we're doing to\nstart using flight um we we are in\nkubernetes so it it sort of you know\naligns nicely with what we've got\nalready and then the experiences there\nfrom you know my\npast right uh could you talk about the\nuse cases within your company that led\nyou to use flight yeah I mean it's\nbasically you know uh like data\npipelines for for right now um and some\nyou know I guess workflow orchestration\num you know uh yeah I mean you know\nwe're we're we're pretty early right now\nin the ingesting data phase\nso thank you and\nDennis yes hi my name is Dennis I'm\nworking in quantpie and uh we working in\ntransfor II area and building a solution\nthat\nhelps other company to verify if their\nmodel comply uh with some governance\nrules governance\nlaws uh and uh to this we also test a\nmodel uh and uh using\ndifferent data modification different by\nso data transformation and here uh we\nwould like to use flight for\norchestration of all this pipeline so\nto uh to\ncompute test results for one and another\ntest you also Ethan um could you\nintroduce\nyourself hey how's it going my name is\nEthan Brown I'm from uh Domino data labs\nand we are currently in the process of\nevaluating uh Pipeline and orchestration\ntools for integration into our platform\nso fl's one of the tool chains that's\ncome across kind of uh my my radar and\nwe're kind of kicking the tires right\nnow and doing a little bit of a\nPC right so what are the use cases\nwithin your\ncompany uh so we provide software um so\nwe're a software vendor so you know we\nhave customers that are installed on AWS\non Azure on gcp and on on Prem\nkubernetes clusters and so um you know\nwe have an execution engine within our\nproduct but we're looking for something\nthat that provides a little bit more of\nthe orchestration cap capabilities um\nthat a tool like flight\ndoes all right thank you and Dan could\nyou talk about the use cases within your\ncompany that led you to use flly\nyeah sure um so\nthe the main two types of data pipelines\nwe have right now are for analyzing\ngenomic data and for analyzing image\ndata and running deep learning pipelines\nuh on images and um and then finally we\nhave some Downstream pipelines that\ncombine those two sources of data and\nright now each of these different\ncomponents is basically run by different\nanalysts in our team um and increasingly\nit's kind of become a lot of work to\njust sort of coordinate data flowing\nfrom um one team to another and uh so\nthe idea behind using flight for this\nwas to allow these kind of heterogeneous\nworkflows to um be used together on a on\nan ongoing\nbasis so how big is the\ndata um we have like millions of images\nand um right now sort\nof hundreds to thousands of\ngenomes all right okay thank you and\nJake so we do a lot of Time series m\nmodels um and our current approach is\nkind of running\nthem by doer containers running on BMS\nbut we've had some\nwhere some of the more complex models\nthey there's a failure and they're left\nrunning when you run there's way\nof Mark a failure um so kind of a main\nmotivation to go towards\ncuber\nresources all right thank you and\nguy so what's your use case\nuh you you are\nmuted\nSor damn it thank you um basically we\nare\num our input to to the to the product is\nuh um a request uh to to do some uh data\ncollection with a certain parameters\nand our our\ntechnology\num which is certain steps we need to\ntake in order to collect this data is uh\nis going to be um a combination of of\ndifferent steps depending on the\nparameters of the data\ncollection\num\nso basically we need a good uh workflow\nuh a flexible workflow F protction\nframework um that allows uh like good\nuh um a decent scale uh flexibility in\nterms of uh using our own\nimages um and and the F flexibility to\nto connect to easily connect uh\ndifferent uh steps along the\nway\num okay got it so how has the experience\nbeen so\nfar so so far I'm\nI'm I'm pretty much uh digging through\nthe through the documentation um I have\nbeen I have been able to uh deploy an\nAWS uh cluster using the I think uh\nDavid's\nuh flight the hard way\ntutorial it was it was difficult it was\nindeed the hard way\num it um I eventually managed to to to\nhave a flight cluster running I managed\nto submit a workflow but then it failed\nimmediately because some permission to\nthe S3\nbucket uh which uh which at that point I\nput it put it put it aside because uh uh\nmy main job my main role is not devops\nand I I was hoping that uh at some point\nthe devops will join in and help with\nwith\npermissions um so that was hard um got\nalmost uh to\nthe uh to the finish line but didn't uh\nso now I'm actually playing around with\nthe\nsandbox just to get the feel of the\ncapabilities and and and the tutorials\nare pretty good uh I have some questions\nthat that uh that\n[Music]\nare questions that that um that pops\nup um that some are answered in in other\nplaces some are\nnot uh but so far\num it seems like it's it does Suits our\nneed uh our needs uh uh it seems like\npretty robust I'm still trying to get a\nfeel of the the whole monitoring uh\nstory um for example I was trying to\nafter running some workflows get to the\nexecutions themselves and the tasks from\nthe workflow using a flight remote uh\nobject and I couldn't uh maybe I missed\nsomething and I was going to bring it up\nin the in the community\nbut yeah please feel free to post your\nquestions and ask the community Channel\nand we'll make sure that they are\nanswered yeah\num uh but yeah I mean it's it seems\nsomething we could use I definitely see\nthat there's\na there's a not negligible uh ramp up\ntime both in terms of the capabilities\nand the\ndeployment\num that's an observation not necessarily\na complaint because I don't know so far\nit's like\nit's basically free and with a good\ncommunity support and a lot of\nplugins uh and a lot of things are\nhandled behind the scenes for you it\nseems like a robust product so but it is\nit is\na something to\nnote\num that's it for now I don't know unless\nyou have\nquestions thank you\nsure Chris uh do you want to talk about\nthe use cases within your company that\nled you to use\nflight yeah we're also in the um pretty\nearly stages um we have a flight binary\ndeployed an eks um using AWS resources\nS3 and things like that um I work with\nTerren who also um added some support\nfor Stow um for Azure for Azure for Stow\num so we ALS have it deployed in an AKs\ncluster using Azure blob storage um\nbeyond that we're at the pretty early\nstages of actually using it for for full\npipelines um we our eks instance does\nhave a simple workflow that um ingests\nsome data from a blob store does some\nbasic Transformations and then stores it\nagain but that was basically P to prove\nthat we can have it deployed in eks and\nactually do something with data um\nbeyond that we haven't um implemented it\nas part part of any of our production\npipelines but we hope to in the in the\nnear\nfuture so what uh type of pipelines are\nyou experimenting with\ncurrently right now they're basic ETL\ntype things um we expect to ingest a\nlarge amount of data from disper ources\nand um want to be able to package that\nup with just minimal Transformations um\nbasically just dat metadata about the um\ncollection of messages and then store\nthem in long-term storage and then from\nthere have a secondary pipeline that\ndoes some more interesting stuff with it\nso um like I said we're we're in the\nreally early stages of that process and\nwe're not entirely sure if um this will\nrun in eks or AKs or both um but uh at\nthis point we've proven that we can\ndeploy to both environments which was\nour first\nstep all right thank you and Dennis do\nyou want to talk about the use\ncases um yeah I can tell uh we still\nhave not been started using flight so\nfar but our cases is that um basically\nto test a model uh we provide a lot of\ndifferent so perturbations fire of a\nmodel and different so feature extractor\nfor example of a model which can be\nalso quite\nsophisticated uh block and then uh we\nhave so so\npredefined pipeline where we can plug in\nthese different perturbant and uh\nfeature extractor to get some value and\nanalyze it to provide some number and\nhere uh uh basically we buil a pipeline\nuh this direct recycle graph by ourself\nuh so because we need to put some blocks\ninside optimize it uh and we want to use\nflight to deploy it to run it on some\nCloud not on a wo\nmachine so are you currently testing\nyour pipelines on the demo cluster uh no\nnot now we not start to\nintegrate F yet uh but\nuh all right thank\nyou and uh BL do you want to talk about\nuh your you know how has the experience\nbeen so far so how are you finding\nFlight yeah um so for the most part it's\nbeen pretty nice um you know we ran into\none kind of weird uh well so so I guess\nI'll I'll there's a couple couple things\nI'll mention um so one uh doing the you\nknow we Helm to deploy and um doing the\nupgrade from like the 1 n to\n110 uh we were just starting to look\ninto the agent and then the agent sort\nof changed how it was deployed and I\nguess it needs a new Helm template to\ndeploy so that kind of threw us off um\nand it's paused us on you know looking\nat the agent stuff just because you know\nwe didn't have time to to kind of figure\nout how to get that deployed\num let's see another thing that's that's\nbeen interesting so we are working with\na contractor and um there's some\nsensitive data that we have and there's\nno ACLS that we can uh like use inside a\nflight so what we've had to do is stand\nup a separate flight uh instance um\nthat's kind of isolating the data over\nthere uh so any pipelines or workflows\nthat are created that have the sensitive\ndata we are you know kind of keeping in\nin its own instance as opposed to using\nsome type of like\nACL model um so that's something that\nI'm I'm definitely curious to know how\nothers are are dealing with that\num but yeah I mean overall it's it's\nagain it's been it's been a nice\nexperience all right thank\nyou\num Dan do you want to talk about your\nexperience\nyeah sure um\nso I um I basically set up this flight\ninstance on our on premisis high\nperformance Computing system um and it's\ndefinitely been a challenge for me I\nwould say it was uh you know flight is\none of several orchestration tools that\nI was interested in trying and I was\nhoping to kind of get something up and\nrunning quickly um and then and then\nmove on to comparing it to other things\nbut it took quite a while to get um to\nget it working for our workflows uh the\nI think the pain points have been we\ndidn't have a kubernetes cluster up and\nrunning to start with so even just um\ngetting more familiar with kubernetes\nwas a a bit of a barrier um another\nissue is that we uh we because we have\nvery heterogeneous pipelines um having\ngood support for raw container tasks is\nimportant um because we have a lot of\nthings that aren't in python or they you\nknow have very like specific system\nrequirements um and so I sort of uh I\nguess hit some hit some challenges with\nwith getting things running smoothly\nwith container tasks um\nand yeah also moving raw files between\ntasks uh I think it's sort of from the\nsound of it there's a lot of flight is\noften used for like ETL tasks where the\nyou might just be moving a couple of\nfiles between tasks like CSV files or\nother tabular data um and I think flight\nflight has been great for that uh when\nit comes to moving like millions of\nimages between tasks that's like\nsomething that is has been a little bit\nof a challenge I think um and yeah\nthat's kind of that's kind of where I'm\nat right\nnow would be great to just dig in on\nthat real fast when you say uh moving\nbetween tasks are you talking about how\nneed to basically download and you know\nthe data into a pod before you can start\nrunning computation on it or is it\nsomething\nelse yep that's what I'm talking about\njust like kind of best practices for\nmoving um a bunch of just a ton of\nindividual files image files in this\ncase into a into a pod before executing\nthe primary\ncontainer yeah awesome\nyeah\nall right um Jak so how has the\nexperience been so far for\nyou so this kind of the first experience\nI've had with deploying anything on\nkubernetes um so I've been able to\ndeploy on GK\nfollowing the sort of terraform scripts\nthat I think David had to like\nbinary um but I kind of stumbled more\nwith like setting up the domain stuff um\nbecause that's not an area I had any\nknowledge with um and then we kind of\nwaited a week whilst we were trying to\nset up the authorization layer on it um\nto connect like our workspace accounts\num which I think from the Guide online\nwe kind of struggled to map it up um\nbefore we have like a support company we\nkind of were able to figure out what was\nmissing um so we've kind of just got to\nthe stage where yesterday we finished\nwith we have it running there's a domain\nwe haven't got like moving any work\nyet thank you and Ethan\num uh yeah so I'm basically at the point\nof just doing installation PC kind of\nstuff um obviously I'm not running any\nreal pipelines at this point I'm more\ninterested in kind of what the\ninfrastructure and architecture looks\nlike um how that would kind of fit with\nour distribution um I do have a lot of\nkubernetes experience um so that that\npart isn't too challenging um my\ncommentary on the architecture would be\nthat uh there's a little bit in my\nopinion there's a little bit too much of\nthe plumbing is kind of exposed in terms\nof um the ways that clusters need to be\nconnected um and the way that AWS kind\nof resources need to be configured for\ninstance um I think in an Ideal World\nthose sorts of details wouldn't be\nexposed and like multicluster setup I\nshould I should mention I'm talking\nabout multicluster in particular here um\nmulticluster setup I think would be um\nagnostic to the environments where it's\nrunning so in other words I would expect\nthat maybe there's an agent that could\nrun inside of azure but the main flight\ninstance is hosted uh in AWS for\ninstance right um so in my idealized\nworld those sorts of um Cloud specific\nand kubernetes specific details wouldn't\nwouldn't leak through the architecture\nlike that\num just just to give you a point of\ncontrast uh I have been using uh prefect\nalso I've set that up and done P's with\nthat their architecture I think Nails it\na little bit better for me it's it's set\nup a little bit more uh like Ci agents\nwould be set up in a platform like Azure\ndevops or travisci or Circle C Etc uh\nwhere you install an agent the agent\nthen communicates back up to the\ncentralized server and picks up the work\nthat it needs um and doesn't necessarily\nneed any specific details about\num you know topology or environment\nwhere those things are running there's\njust a communication protocol where\nthose two things are trusted by each\nother um and the reason that's that's\nbetter generally from a security\nperspective is that then you don't need\nto reach into client clusters from\nparent clusters like typically you don't\nyou don't want to set up like that you'd\nwant something where where the the you\nknow the agent clusters only access the\ncentralized clusters there's only an\nIngress point there so that's one thing\nthat I'm kind of like struggling with a\nlittle bit in terms of like that that\ndesign um I think we can make it work\nbut it's probably not ideal um that said\na lot of that the rest of the feature\nset of flight looks great I think kind\nof nails it in a lot of other ways um\nbut that would be my one you know just\nbased on kind of the initial work and\ndiscovery that I've done so far would be\nmy one comment about\nthings um does anyone want to chime in\nyeah maybe I'm totally off base and\nthere's some things that I missed to\nsince I'm some Net News so so please uh\nyeah feel free to tell me I'm\nwrong yeah I think it makes a lot of\nsense Yeah my two sense yeah I\nwas it's curious because these days I\nwas thinking on this on the on the\nmultiple multicluster patterns and\nprobably having some form of operator or\nagent that will handle for example\ncredentials distribution to the\ndifferent data planes and and really\ncentralize everything because yeah as\nyou said\na lot of stuff is exposed in a\nmulticluster deployment right now for\nflight so yeah definitely makes a ton\nsense\nokay I'm just wondering um Ethan what if\nif prefect is working for you actually I\ndidn't I didn't try a prefect but why\nwhy are\nyou uh trying flight in the sense like\nuh\noh we're we're evaluating a number of\ntools right now so um you know prefect\nis great at the multicluster support but\nthere's a lot of other things that it\ndoesn't do that flight does do um so you\nknow like everything in software it's\nall about trade-offs and pros and cons\nand finding the best fit for your\nparticular environment in use case right\nso um you know there's a lot of things I\nthink that are great about flight and\nthat flight I think is ahead of other\nplatforms on so I just wanted to make\ncomments about that kind of multicluster\nstuff hey uh Ethan hi this Isam um I\nwanted to ask a couple of questions if\nthat's okay about your setup uh so for\nuh for your prefect trial you are using\ntheir managed Cloud um no app or you're\nusing this just the open source yeah\njust the open source version right and\nso we have a multicluster setup where\nI've kind of deployed the agent there\nand set up worker pools and worker\ncues I see um you yeah there's I think a\nlot\nof features uh that are available only\nin the cloud program yeah look at that\ntoo um the managed version of flight um\nwhere we at Union cell um is set up the\nway you describing uh so you you do\ninstall yeah I don't know if you are\nlooked at that the docs for that or not\nuh but the\none of the things we can enforce there\nis is authentication we can authorize\nThe Operators we know we who is talking\nto the control plane and what um\nworkflows they can pull to run and so on\nuh which you yeah without enforcing\nauthentication you can't really um\nguarantee uh in uh just the open source\nuh flight setup um but but yeah I think\nI think it is I agree with you it is the\nright uh set up to pull um then to push\nand expose the cluster end points and um\nhardcode these credentials and all of\nthis um\nyeah so but yeah I I'll be happy to talk\nmore about that that any of that if\nyou're open to like exploring the you\nknow managed version um but if if it\nfits the bill and fits your needs or not\nI'm not\nsure yeah I think we're sticking with\nopen source at the moment but I I will\nthat thank you appreciate\nit um I think if uh if\nyou if you get past I guess that point\nand you like you know start happily\nusing uh flights and you would like to\ncontribute back that portion we'll be\nhappy to also work with you on that um\nlike you know re architecting the\ncommunication protocol between\nthe propeller the worker pool and the uh\ncontrol plane yeah yeah fair enough\nthat's that's probably an area where we\ncould contribute but I don't commit to\nit just yet but sure certainly an area\nwhere you know we have some expertise\nand be able to help with yeah yeah I\nthink as you're probably just trying to\nget something working and fits what\nyou're trying to do uh is definitely the\ntop priority uh and all of these details\ncan be worked out later\ncool\nall right thank\nyou um Dennis so how has the experience\nbeen so far for\nyou uh well it was nice but uh as I said\nwe haven't used it so much uh we just\ntried to with demo claster um it was\ngood but\nI have a bit of doubt that we are going\nto going to so basically inside our\nframework we already have task uh so\ntask entity and with flight we need\nbasically\ncreate uh task let's say run time so we\ncreate a graph run time and the question\nis if it's if why it supports it or it's\nnot a best practice Sor would you mind\nfilling it up a bit I appreciate it uh\nwith coffee thank\nyou thank you John and your\ncoffee sorry Dennis so what was your\nquestion uh basically we uh want\nto uh create uh duck so uh\npipeline uh on run runtime so it's not\nsome pre-built graph when we decorate\ntask F function with task decorator but\nbut we already have some graph and want\nto deploy this graph which was built uh\nin runtime uh to flight and if it's uh\ndon't recommend it to do and uh flight\nworks good only with pred Define task or\nmaybe\nsome any uh advices how to do\nthat\nsorry can can you elaborate more Denise\nI didn't quite get the what that like\nwhat the dag you're trying to build look\nlike so basically uh we use uh we create\na test and test is a s pipeline where we\ncan uh provide us plugin different uh\nlet's say perber of a image for example\nokay and it can be different and we\nfor example uh if we have several per\ntuber we can build several\ntests uh with several test results and\nit's not uh something that we know uh\nbefore running our framework it's only\nsomething that we can uh get in the\nmiddle of work that we want to build not\none graph but actually three\ngraphs uh and also executed with flight\nuh to execute on different\nthey not because it's might be this all\nthis peration might be\nuh uh computation expensive and so on\nyep yeah um have you looked at Dynamic\nworkflows uh think should I saw but I I\nhaven't investigate so deep Prof yet she\ntries to keep the the the syntax pretty\nmuch the same so you can write an at\nwork at Dynamic um on a on a function um\nand then inside that function you can\nuse the inputs to uh build other\ndags um and then return them and they\nwill uh like that task will run just to\ngenerate the dags um and then they will\nrun the same way sort statically defined\nworkflow would have run so get\ndistributed on multiple nodes and\neverything um I think that's that sounds\nlike what you're trying to do I there's\nanother piece depending on how if you\nknow that the dag beforehand like if you\nif you know what it would look like um\nyou can you just need the at Dynamic but\nif you don't and you're like you know\ndepending on conditions you will like\ncall this task or that task uh and\nformulate the dag um imperatively is\nthere's another feature that you can use\nin conjunction with that called\nimperative\nworkflow um where you can build nodes\nand add inputs and so on um uh in\ncode uh I will a couple of examples here\nthat might explain\noh uh David thank you you are already\nahead of us okay so this yeah David sent\na link on the dynamic workflow\nuh and this is another link on the\nimperative workflow so take a look at\nboth of these um I think should help you\ndo what you're trying to do we do thank\nyou all right\num Chris so um could you talk about your\nexperience yeah um like I said we're\nwe're fairly um early in the process um\nwe we don't have it in a production\nenvironment yet and and we don't have it\num working on um any any real data yet\num getting it up in eks uh the flight\nbinary was uh relatively straightforward\nfor Terrance he has a lot of kubernetes\nexperience and um a lot of orchestration\nplatform experience so um that was\npretty easy for him uh on the AKs side\num obviously not everything was\nsupported um initially so that was that\ntook a little bit of time uh for me\npersonally I didn't have any kuber 9es\nexperience so trying to understand the\ndifference between Cube CTL and flight\nCTL took me you know a few days um but\noverall it's been a really positive\nexperience um we really appreciate the\ncommunity support and the quick\nresponses in uh slack uh that has been\nexcellent um it's the first open source\nproject I've kind of contributed to or\nbeen even really read about uh so it's\nbeen a pleasant experience for\nme all right thank you um so are there\nany pain points that the team should be\naware of says to\neveryone maybe I didn't uh get to it in\nin my uh reading of the\ndocumentation I I came from uh\ndeveloping in spark for the past two\nlike years or\ntwo\num it's not entirely clear to me\nimplementation wise uh for example if I\ngenerate a large list and then I pass it\nto another\nuh task or or using\nmap H like how is this handled by by\nflight I\nmean maybe I want to stream the list\nthat I at some point I saw that you can\nyield from a task if I'm not\nmistaken uh and then you pass the\niterator but in the next task it shows\nup as a list so is this being\nmaterialized before the task like\nimplementation wise it's it's not\nentirely clear to me um and that becomes\nimportant\nwhen when you have larger objects\nand like uh higher scale of\nthings so is serialization and\ndeserialization of data not clear to you\nis that concept not clear to you no\nSo that obviously serialization of data\nis is\nis is not too\ncomplicated I'm I'm more I'll try to\nrephrase um for example if if a task\nreturns an iterator and then I pass that\niterator\ncan\nI like the next can the next task start\nworking on the results of the previous\ntask uh in a streaming kind of\nway we don't support yeah heyan please\ngo\nahead yeah I was gonna say exactly that\nuh we do not support um streaming lits\nis what we have been calling it um I\nthink we have a um an RFC for this if\nI'm not mistaken uh but this is not\nimplemented uh we were looking for uh\nsomebody to partner with uh who has a\nuse case for it um if that is a thing\nyou're interested in we'll be happy to\num work with you on that uh but that's\nthe the concept is exactly what your\ndescribing you you would return\nsomething that you know we can call it\nan iterator I guess uh we're calling it\nstreaming literal um and you can\nimmediately start consuming that in in a\nsubsequent task and they would both\npretty much start at the same time um\nand as data is produced get streamed\nover to the processing to the next task\nto process it um\nthe uh yeah the only there's no\nstreaming today so the only thing you\ncan do is either return the list but if\nit's too large then you can offload it\nto a file um so you can return a flight\nfile instead um or if it's there's a\nstructure to it you can return a\num structured data set um and then use\nit in the subsequent task but yeah that\nwould the sequ task you have to wait for\nall all the data to be generated before\nit can start\nrunning yeah um let's say I I do offload\nto a file\nthere's still the issue of if you want\nto map it later then then you can't it's\na single single\nobject correct um the yeah um like the\neverything I will give you is really\npcks for through this but this is not a\ngreat uh way to you\ncan yeah um so you can start a map on\nlike indexes uh and the past the same\nfile to all of them um and they can just\nuh these workers can read like the file\nand index into it based on the the index\ninput let's that's maybe to low level\nbut basically a flight file um if\nI it won't automatically load the file\nthe whole file into the Pod like only if\nI read it correct um and it streams as\nwell well so it doesn't like it will\nstart immediately giving you bytes as\nyou read it uh um even before like the\nwhole file is downloaded on the Pod yeah\nand quick question about the structur\ndata sets uh is it anything like data\nframes in spark in the sense that if I\nread a specific column it will only read\nlet's say I store stored a parquette\nfile and it can read only that portion\nof the file correct\nyes okay so the the back is a Pary uh\nit's this is just a um like synthetic\nsugar for flight kit to use it so you\ncan express the structure of the data\nset and so on uh but in the back it's\nexactly as you said it is stored as\nparquet and if you only read one column\nit will only read that portion and so on\nawesome\nyeah\nother\nquestion um this is a little bit related\nthematically to uh guys discussion there\nbut um basically my question is if I\nneed to pass in um sort\nof just call it like half a million\nimages into a task um or or run a task\non on a half a million images is there\num kind of like a like a template that\nthat you guys have for how to run a task\nlike that so like um in this case you\nknow it seemed like the flight file\nabstraction is not really designed to be\nused for such a long list of files um I\nmay maybe I got the wrong impression\nfrom the docs but it seemed like that's\nnot really the\nintention so there is also a flight\ndirectory um right so you can represent\nlet's say a prefix in S3 where the half\nmillion files are uh and you can pass\nthat around and you can use like OS\ndirectory um functions you can walk that\ntree uh and you can like you know index\nand find the files you need or the batch\nof files you want to process um per task\nand then only read those um so like will\nbe all metadata otherwise are just\npassing names of files\naround um is that what you're looking\nfor or how how or that one task will\nneed to process all half a million\nimages\nI didn't quite understand your response\nbut but yeah I think I think we're on\nthe right path so for example let's say\num we have a machine we have like a deep\nlearning model that needs to be run on\nthat on each of these images um in\nbatches of like say a thousand or\nsomething like that is there like a\nrecommended way of doing something like\nthat um yeah so I would do something\nsimilar uh to what we were just talking\nabout like you would send the those\nbatch um like maybe ranges um uh\nconstruct a list of those ranges uh so\nfrom zero to 100 100 so on into a list\nand then run a map task on that and pass\nthe flight directory as the like a\nsingle input to all of them the same\ninput to all of them um so then each one\nof them will start up it will get um\nlet's say three inputs right the start\nthe end and the flight directory um as\ninput and then it can read that section\nof the directory um um I don't know how\ndo you index into like is it are there\nnumed or are the names of the files like\nwhichever way I guess you you index into\num or you structure the files file names\num it can use these ranges to index into\ndirectory did I get that\nright I think yeah I think I understand\nalthough in this case like flight\ndirectory is that that's primarily used\nto that will automatically download like\na remote directory to the to the Pod if\nif you uh just call like download on it\nyes it will download everything from\nthat directory uh but you don't have to\ndownload everything you can walk it and\nthis will only like transfer metadata\nyeah got it okay\nyeah I Haven just an observation I\nwonder if I'm if I'm correct I\nthink it's less of a use case for flight\nto work\nwith\num such large numbers of elements I mean\nthat's that's already in the uh\nborderline um Big Data\npipeline in a sense and I don't think\nflight is meant for it correct me if I'm\nwrong\num it's\nI I'm not sure how do you draw that\ndistinction um but this is a very common\nscenario we customers run into uh\nrunning deep learning models on huge dat\ndata sets it's just like you know\nrunning in batch and repeated processes\nacross uh a\nlarge data set uh is that what you mean\nwhen you say uh big data comput so maybe\nit is me like maybe it's less common\nlike I come from from from\na big data in my previous company I work\nwith a lot of data so we used the spark\nto process like you know gigs G gigs of\ndata um so like half a million of files\nis something that spark can uh can\nprocess uh\neasily um yeah and so there\nare maybe when machine learning is\ninvolved is is is more the advantage of\nflight in the all the integration and\nthen the fact you can actually write uh\nregular code it's less Spar is more for\noptimized SQL queries and stuff like\nthat yeah okay so I I think I got uh the\ndistinction and you know I assume that\nyou can run spark also a spark one of\nthose tasks in in Flight can be a spark\num task and it will automatically spin\nup a spark cluster for you um yeah and\nyeah and run data uh processing on it um\nbut we have seen But as I think you are\nyou have correctly pointed out that uh\nmachine learning running machine\nlearning models is slightly different\nthan uh what you would use spark for\nlike you're not you don't really have a\npipeline a spark pipeline to optimize\nand try to efficiently pass data around\num for comput uh in those cases right\nthere are very\ndistinct uh data sets in that sense you\nwant to run the same compute on each\nbatch of data um and what we saw in a\nlot of cases people who started with\nspark because of familiarity because of\nthe other um problems they had uh that\nneeded spark uh and started using spark\nfor this and then switched over to a\nmuch simpler model where it just it's\njust a map job on a lot of data um so a\nlot of improvement in in per in in\nruntime and compute usage um for the\nsame reason right because spark doesn't\nbenefit you and there's a lot of\noverhead um involved shuffling data and\nso on um but yeah\nit's say depends I guess on what you're\ntrying to do what the computer you're\ntrying to do on the big\ndata yeah got\nit um does anyone have any more topics\nlike to\ndiscuss um this is also uh relevant to\nour discussion with flight directory so\nI just wrote this in the chat but in our\nuse case we're mainly we were mainly\nhoping to deal with raw containers and\nso it seems like the support for raw\ncontainers is just a little bit uneven\nand that more of the development has\nbeen on the pure python task is that\nfair to say that is absolutely fair to\nsay uh I I would if you don't mind uh\nfiling an issue on that um that is\nsomething we would definitely be happy\nto look into um okay so are you running\nraw containers because there's no python\nin the container it's all like some\nnative binaries\nrunning well it's funny\num in the in the fullness of time there\nwill be other pipelines I'm running that\nare not pure python but this one\nactually is python it's just that it has\na very specific set of requirements that\nI couldn't figure out how to get running\nin a flight python task very specific\nlike system requirements um so or system\ndependencies I should say so um I was\nonly able to get it running in a\ncontainer that I built myself and so I\nguess another route would be to try to\nbuild that container with flight kit but\nI also wasn't able to get that working\nso um yeah I kind of was stuck with\nbasically a raw container that I built\nbut that didn't know anything about\nflight and then just getting data into\nthat container was a bit awkward so yep\nyeah I think there's a lot more support\nuh for like primitive types passing and\nso on\nthan um for fancier types um you can uh\nthe the so you can yeah you can\ndefinitely write that code like the the\nyou know you can pass the let's say a\nthree prefix in as a string into that\ncontainer uh and do the walk um with FS\nspec uh yourself which is essentially\nwhat flight directory is doing um but\nthat's not a great answer we we should\ndefinitely support flight directory\nbetter in in raw container tasks um\nhopefully be good news to learn as well\nthat we are also in par working on the\nflight kit dependency issue uh\nsimplifying that so that you h less\nconflicts and it's easier to uh get it\ninstalled in you know alongside your\nother\ndependencies cool yeah perfect yeah I\nfound that um that just the process of\nbuilding a container with flight kit and\nmy own dependencies for some reason\nwasn't straightforward there aren't a\nton of examples like there is the um\nlike the CLI or something will generate\na template for me but it just didn't\neven you know the generated template\ndidn't uh that that Docker image that\nwas built from it didn't work and I I\ndon't know why but at any rate um yeah\nglad to hear that there's more ongoing\nwork in that\ndirection yeah we will definitely look\ninto that\num the template the generated template\nthat should absolutely work out of the\nbox should work yeah yes okay um yeah\nwe'll look into that uh if you can send\nus maybe on slack uh the the command you\nuse to generate the from templates um\nyeah okay we can try to no thank you"
    },
    {
        "title": "Developing an LLM app with Flyte Agents - Community Meeting | Nov. 14, 2023",
        "transcript": "my name is Eric and let me share my\nscreen first so I can you see my screen\nhere yep uh so\nlike okay okay nice okay so let me turn\nup the so okay so hello everyone my name\nis Eric Chen and I am a FL contributor\nwith four months experience and have\nabout 40 po request to flight so let's\nstart it and today's agenda I'm not\ngoing to sh about sh about the lesson I\nlearned from flights but I'm going to\nshare about how the synchronous plugin\nWorks in flights and the synchronous\nagent working place I think this is\nreally really important for data\nscientist and machine learning Engineers\nto to to collaborate with flight because\nwith if if they can be really really\nfamiliar with how to De develop their\nown plugin in flights then flights can\nbe super powerful and through and\nbecause of that agent is written in\nPython and in the flit which is the the\nthe flight Pon SDK so so they can they\nonly know need to learn how Pyon works\nand can PR it locally so I think this is\na super powerful feature and I'm pretty\nexcited to share with you and later I\nwill share Tre text overview and how the\narchitecture combine all of them and I\nbring Ser use case to you so you can\nhave more maybe more so you can know how\npowerful it is okay so let's recap what\nplugin is so so let's recap the plugin\nfirst so FL propeller is the component\nin charge of executing node text so\nnormally\nthe so normally the life cycle of a task\nin like let me turn on the the life\ncycle of\nthe of a task in popular will be create\na part and execute the code in the part\nand delet the part which will consume a\nlots of containers resource so flight\npluging is a thing that reduce the tax\nlife cycle so for example we plugin we\ncan save the container results in the\ntest execution it can reduce to like the\nlife cycle will be like this you just\nhave to execute plugin in F propeller\nand when execute plugins in F propeller\nwe can maybe in maybe like for example\nthis is the Sparky example you will run\nURS in the node or maybe we can\ncommunicate with the external service\nlike Google Beery or data Bri spark or\nAmazon Sage maker which is super\npowerful and you don't you don't need to\ncreate a part and communicate you need\nto you don't need to create a part and\nexecute\na and communicate with the exteral\nservice in the P you can just do it in a\npopular which is fter yes so this is uh\nthe benefit of using flly plugins and we\nhave a we have a\n[Music]\nrouting we have a routing mechanic to\nfind the correct pluging to execute a\nspecific X type because for example like\nthis this kind you have to specify spark\nand\nand\nin which spe\nwhich text type for spark but if you if\nyou specify the agent service or specify\nthe datab Bri you will you will go this\nway to communicate with external service\nbut if you change the key to maybe spark\nyou will communicate in this way so we\nwill have a routing mechanis to config\nthe config map and if you want to to\nlearn more about outl parking words AVU\nhas has has introduced it like like two\nweeks ago you can Google for this\nYouTube video and is a super great video\nokay so let me recap a syn plugging a\nsynchronous pluging so before this PR\nrequest be before synchronous plugin\nimplemented we have only asynchronous\nplugins in flight and asynchronous\nplugin are for test which for running\nloan for example running query job a\nsynchronous plugin interface has three\nfunctions which\nis\nuh which is create get create get and\ndelete so take data bricks for example\nwhen running a data Bri spark jobs\nthrough a plugin we will create the job\nuse a create function and get a job ID\nand uh we will continuously use the job\nID to get the status of the data Bri job\nand wait for it until it comes to a\nterminal State and the terminal State\ncan be like seceded or Val and the\nasynchronous plugins are re B for these\nlong running jop cases like B query\nAmazon s maker or\nthe so Reno is our main character s\nsynchronous plugin and synchronous\nplugins are for the synchronous API and\nwhich is uh compar for compared to\nasynchronous plugins which might running\nlonger synchronous plugin is for job\nrunning shorter and for\nexample um the you can retrieve some\nmetadata from a backend service such as\nSQL or you want to get a message\nresponse from CHP openi or even you want\nto send a message to SL channel from SL\nAPI and synchronous pluging are really\nBor for this cases and and compared to a\nsynchronous pluging have three function\nsynchronous pluging have only one\nfunction and it's just a do do request\nto you achieve this\nmechanics and let's go through the sync\nagent and this this slide is going to\ntalk about the benefits of the\nsynchronous agent and this is the\narchitecture of syn the synchronous\nagent so before um first propeller will\ncommunicate with external Service uh\nlike like this like this like this the F\nwill directly communicate with the\nexternal service and now we will\nintroduce a agent server so now f f\npropeller will Comm unate with the agent\nserver and the agent server will\ncommunicate with the external service\nand let me explain the two main benefits\nto use\nagent so the first is that the five prop\nare all written by Golden and which is\nwhich is not easy for data scientist and\nmachine learning engineer who develop\nplugin because uh because you need to\nunderstand understand how five propeller\narchitecture works but right now if we\nuse the agent server the agent framework\nyou don't need to understand how the how\nfive prop works but you can still\ndevelop the plugin you want so the so\nand is all written in tyon written FL it\nso the first the first benefit of it is\nthat is written Pon and is far more\neasier to develop and you don't need to\nunderstand how\n[Music]\narchitecture how how it\nworks and and the second is that uh if\nyou use the agent framework you can\nreduce the overhead of executing plug-in\ntext and the the overhead is transferred\nto the agent server and this is really\nreally\nimportant and so before B has\nto has to s the test and monitor the\nstatus of the job so right now five\npropeller has only need to monitor the\nstatus of the test task and which is\nwhich can consume less resource and I\nthink is is it really really really\nhealthy for five propeller and for more\ndetails you can use\ngrafana open an open source tool to\ncompare the resource usage between use\nthe agent server and not use the agent\nserver and as my knowledge Spotify and\nis also the the spot Spotify is a big\nfan of both a sync agent and sync agent\nmaybe someday we can invite them to talk\nabout more use case about about\nagent next\nslide okay so oh here is an example\nabout CH GPT pluging so you have to\nDefine\na name and this this name can be however\nyou want and uh you have to specify two\narguments in the topic it's really\nsimple the first is the open air\norganization if you have any experience\nabout open air API you will know what it\nis and the next is about the config of\nthe trt and you can see the config\nthrough this URL is a documentation\nwritten by open Ai and and yes and after\nyou define the Char task you can simply\njust just input the input it in the text\nyou can simply just run it in the\nworkflow and later I will show you how\npowerful it is so just have to kind of\nhave an idea okay so before the demo let\nme let me talk about the architecture\nagain so the whole process of using the\nagent pluging and and especially the\nsynchronous agent plugin and take CHP\nfor example will be like you can you can\nregister your code to the fly cluster\nand run it by a fly console or just\nsimply use py round Das Das remote to\ntrigger the code after you trigger a\ncode you just you just the flly of mean\nwell will tell the flight propeller to\nexecute the code L FL and the F\npropeller and and when running a Char\njob the F propeller will communicated\nwith the agent J PC server which is\nwritten by FL and written by Pyon and\nand you will and the the agent J P\nserver will communicate with the Ching\nservice so this is just whole\narchitecture and yes let's go through\nour first use case so the first use case\nis that of is the flight uh we have\nlatest release summary let me open the\nworkflow so here is the workall and I'm\nand I have already create a a St channel\nfor demo you can see right now my time\nis 1 1 a.m so later the the message will\nsend it at 1:00 a.m. so okay so you have\nthree three inputs uh which one is the\nowner one\nowner of the gab reple what is the owner\nof let me just open it\nso so the owner means the flight org\nhere and the redo means the the rep code\nflight here okay and the\nchannel the the channel\nmeans later I I'm going to send a\nmessage to this channel called demo and\nlet me run the war for and expand the\ncode okay so this is the code and this\nis the worklow so oh basically I\njust I just I just do two things in in\nthe the task here so first is that I I\nuse the GitHub API to get a latest\nrelease it will return the string\nhere here it will return the\nthe text here okay\nand later later uh later I will I will\nmake my own prompt to to to to generate\nthe the message to send to the slake and\nI think the prom is important because\nfor slake we the the the response have\nto\nfix within 4,000 characters because slay\ncannot you can SL send\nover 4,000 characters in one time and in\nslate so you have to have the try to to\ngenerate a shorter summary okay and and\nafter you after the\ntrp uh give give me a response I just\npause it to the Slate which is pretty\nsimple yeah I think it's it's a seeed so\nyou can see the the workflow ised now\nit's super fast like\njust\num uh like 30 SEC or 40 seconds\nand here here\nis oh no here here is the latest summary\nis the same as my time so this is this\nis one of the the use case okay so let\nme go through an another use case and\nyou can okay so this is okay so the next\nis that we can we can get the latest uh\ntrend from median so for example I want\nto know the median the the weekly Trend\nabout ml Ops from medium so I can I can\nI can create one more workflow to do it\nfor me so this time I'm going to show\nyou how to\nex no the the agent and the workflow in\nlocally so let me\njust so I will I\nwill but this time I will I will send\nmessage to my to my Twitter account let\nme\njust execute enough and I will\nexpand so the over is simply\njust three task one is scale weekly\narticle from medium yes and I just use\nthe selenium selenium and the Bea\nbeautiful tell me do this after get\narticle again I I have to for this time\nwe are we are sending a message to the\nTwitter and the the limits of the of one\ntw\nis 200 280 characters for tweets so you\nhave to tell the CHT so that CHT can\nhelp you and\nafter it you just you just get a get a\nsummary from CHP and just send a tweet\nto the Twitter yes okay it is still oh\nit's already finished so yes uh let's\ncheck my demo Twitter\naccount okay this is my\nand so so in the\nso in in this workflow I choose I chose\nthe flights tag to as my example which\nis maybe Aon can take a look a bit and\nlike like like this but but he didn't he\ndidn't update for for long time I know\nbut uh yes so you will talk about what\nfly is happening these days uh this\nthese articles yes but but if you but if\nyou choose a tag are more po which is\nupdate almost every day you will really\nlike one day ago five day ago six day\nago 5 hours ago five days ago it can\nreally give you the the weekly summary\nof it okay so let me go through next\nnext example which is super super cool\nand the third use case is uh about the\nYouTube latest summary so so\nso for\nexample uh let me let me run through a\ncode first\num it's like this\nso so you can you can choose for\nexample you can choose any URL you want\nfor example for me is is flight my my\nfavorite okay so so\nso you just choose the fights and and we\nwill I will go to scrap the latest uh\nvideo from flight and give you the\nsummary so so for example let's just\ngive it and I will go to Just Launch it\nyes and is running give you some timeing\nand when running I will expend the\nlogic so this is a little bit\ncomplicated because I have do some the\ndynamic workflow so let's go through\nworkflow this time I just do do two\nthings I will I will specify the YouTube\nthe YouTube channel I want the summary I\nwant and where's my here so I'll will\nget a chunk first so in this in this\ntext I just do I just get the the latest\nvideo ID which which is this for example\nthe the latest video ID means the ID\nhere you have to get I want to get the\nID here and if I have the ID I can use\nthe YouTube transcript API to get a\ntranscript of it and then later I will I\nwill TR CH transcript uh the reason why\nI need to split it is because uh there\nthere's a limit there there there has a\ninput limit in the CHT and I don't want\nto exist the limit so I just I just\nsimply use one 10,000 characters as a as\nas a chunk to split it and after that I\nwill go to my I will pass it to my\nDynamic wflow so I will first pause a\nmessage and like like here okay you can\nsee here so let tell you that this is\nthe latest video summary let's check out\nit and then and then I'm am trying to I\nI will first save all\nthe I will first summary all the chunk\nfirst so so and the summary message will\nhave a list to to store it and after\nthat for all summary message in the\nsummary message list uh I will I will\ntry to conest them I will try to conest\nmy message and the summary message so if\nit is too long I will send a I will send\nthe summary first about the the\nmessage and if is if not exceed the\n100 15,000 character I will save it I\nwill I will still compare through the\nmessage and and I\nwill and I will then one more summary at\nthe end\nI think I think you might you might\nthink the dynamic sub flow is a little\nbit complicated if you if if you you you\nif this case is not wring workflow it\nmight be far more far more easier to\nunderstand the code but yes but but but\nthrough through this conditional War you\ncan you can run it in remotely so I\nthink this is the tradeoff and and yes\nyou can see that\nis is run now\n[Music]\nand yes is yes run not yes through\nthrough the through and it's almost\nfinished okay so maybe we can we can\nwait\nit okay I think I think is finished\nright yes it is finished so so basically\nuh if you if you want to want to listen\nto this video uh this commun maybe as\npodcast you need to spend like 46\nminutes but if you just want to know\nwhat happens you can you can just take\nfive minutes to see is there any any\ntopic you are interesting and and\nbecause the the summary is kind of have\nit's like a Time series like if you are\ninteresting in this this section so you\ncan you can properly know that uh\nthe you can properly know that the the\nmessage will be like in here just the\nportion you can you can catch by by\nyourself so which can save your time in\nmy opinion yes so this is the F you\nsummary and there is a bonus bonus uh\nbonus example which is Cav Su airflow\nairflow sample so actually FL right now\nis integrated with with airflow so with\nwith airl you have lots of operator can\nchoose let me just give you some Cher to\nsee how how how amazing how fantastic it\nis so again here is a Dem message and\nlater you will it will send a message to\nmy stel call hello this is uh this I\nmaybe m a agent fing demo okay here yes\nso and and I can prove that is really\nring flight\nbecause because through FL flight uh let\nme\nquickly\nso I am\nusing\nlet me let me modify the code in the\nairflow and you can just when I execu it\nagain in uh I know you you might be a\nlittle bit confused maybe later Kevin\nwill explain how it works because you\nyou didn't import anything from airflow\nplugins but it's it just use it but it's\nkind of magic maybe heav will explain\ndater but when when you exam it yes like\nwhen you execute it you can you can run\nair flow of the Opera inight and it\nreally go through this place so it's\nreally cool okay and I forgot to I I\nforgot to explain the the mechanism\nabout how it works locally about the\nChar agent so let me let me explain it\nbriefly now so CH Inc a class called\nexternal API test and it iner a class\ncalled a Sy a synchronous agent executor\nand in here we have wrote about a du\nfunction to to do to to trigger the\nfunction to for the synchronous task and\nthrough this uh through this class uh\ntheir stist and machine learning\nEngineers they can develop agent in\nlocally so this is far more efficient\nyes so so you you can ask more question\nlater okay so let me go back to the\nslide\nand and I think the use case are are\nthis is just some use case but there are\nlots of use case for example like\ninternal system as the source because I\nthink in very very much very many big\ncompanies they have lots of\nnotifications for their for for their\nworkers and\nand you can you can manage your internal\nsystem with fles and with the charv\nsummary is I think you will be really\npowerful like YouTube gab releas gab\nweekly past uh medium Hi News and stand\nto any any communication no channels you\nwant so I think it's really powerful and\nnext\none and actually I think like flight has\na have a bus called fly Bots and in some\nof the documentation like fly snakes\nthey have they will embed some video\nlike for example this one is fly see\nhere intro I'm thinking like can we can\nwe integrate the TRD summary with fly BS\nfor for example uh you can you can just\njust um you can just pack check for\nevery every video in the flight\ndocumentation is L is it provide a\nsummary if not we can trigger the\nworkflow or trigger the charity task and\nand embed embed the the summary in the\ndocumentation and create a progress for\nreviewer so I think if is possible and\nyou will be super helpful for beginner\nfrom like like me because for me I'm I\nhave only four four months experience\nwith flight and and most of the time I\nwant to guess the I want to be better\nhave better understand if I I have to I\nhave to spend lots of time to to listen\nto the video to to listen to watch the\nvideo but if I have just a summary and\nmaybe it will be more\nefficient yes so this is my today's talk\nthank you very\nmuch that's amazing thank you Eric and\nthank you especially because it's really\nlate for you or early I don't know\nthanks for your effort there are a\ncouple of question\nquestions uh first one when\nyou um yeah when you configure the\nconnection with the open AI API if you\ndon't set the or the organization so you\nneed to put the API key on the config oh\nyes yes let me let let me showare my\nscreen\nso yes of course uh for let me\nokay you can you see my\nscreen yeah uh okay so so so we will use\nthe open a AI module and F the secret\nkey in here the API key in here and the\nfunction here called get agent secret\nwill be you have\nto uh\nyou have to set the API key let me just\nshow\nyou okay so you you have to you have to\nedit the the secet in fly agent then you\ncan use\nopen and I will I will I will write the\nD later so yes so next\nquestion cool thank you yeah by the way\nthere's a block post coming up right oh\nyes yes oh let me show you so so to help\nuh data scientist and machine learning\nengineer can develop paring more\nefficiently I actually I have written a\nblock and which will be passed in a fly\nblock and everyone can go follow it and\nlike this one and I I will go through\nall the details and try to try to give\nyou the necessary knowledge you to that\nyou can develop the agent plugin more\neasily and you don't need to understand\nthe complicated five prop architectures\nyes I will I will try to do it yes I'm\ntrying to do it and maybe we'll release\nit in yes in maybe in this months yes F\nthe block yes thank you I think the\nquestion on I think the question on\nSecrets might have to do with like uh\nsecurity so like it does does Agents\nintegrate with flights support for like\na cloud providers or like an external\nSecrets\nmanager Kevin I need does no I can I can\nhelp answer yes uh so the secrets are\nthe same as the secret quest in\ntasks um and you need to have those\nSecrets available in your underlying\nsecret manager eventually in kubernetes\nand that's how they get mounted so it's\ncompletely secure there is no you're not\nhashing it storing it fite doesn't store\nit anywhere it's all going through your\num your preferred secret\nmanager yeah I had a question yeah um I\nhad a question hrew firstly fantastic\npresentation that was there was like a\nbullet train going through like 15\ndifferent things Jumping All Around in\nthe code and just amazing it's crazy in\nfor months um the amount of\ncontributions and and the love you have\ngiven to the community absolutely\ndeserves an Applause so thank you for\nyour hard work and thank you for you\nknow being here um I do I did see a lot\nof cool stuff in there I think it needs\nto be talked about I I I think some of\nthe things that people have not realized\nthat open a I plugin is a plugin that\nAndrew completely developed on his own\nbut what that does is you can now send\nmessages to open AI get a response or\nsimply replace it like literally with\none line the native llm uh that is\nrunning on a GPU no somewhere and not\nrunning as a service only spun up on\ndemand and turn off and this is useful\nfor many use cases like you know if I'm\nif I'm getting a YouTube summary I don't\ncare about one second response time I'm\nokay waiting a few minutes and I'll get\na slack but so it's a fantastic example\nin which you could drop in llama too and\njust get it for almost if you have a GPU\nresource lying around almost get it get\nit for very very few\ndollar um another thing I saw tweet and\nuh\nFlack uh code that you had written in a\ntask I think you should make those into\nagents those are perfect do agents like\nuh no\nproblem about before yes yeah yeah\nyou'll go much you'll run much faster\nand so yeah otherwise um just like a lot\nof fun stuff also use pip flight run\nmore and give it more feedback but thank\nyou okay thank you K\nokay so so so I need to answer more\nquestion yeah there also you sneaked in\nsneaked in showing a flow running on\nflight agents that is that was a very\nsneak\npreview yes I think Kevin do you want to\ntalk about it I think it's really really\namazing and actually a can not run in\nthe locally it can run it remotely so\nit's really\npowerful it\nyes yeah so so idea behind is is that we\ncompile the a FL operator to fight tax\nso we around almost any kind of a\noperator in Flight agent so it's very\nuseful like some of the yeah there\nthere's like thousand of f operator and\nthen in fly we only have like 50 or 60\nuh pluging so if for example like one of\nthe our customer use data Flor data prod\non gcp and then we don't support this\nkind of plugging so they can just use\nairflow their flow operator and fly and\njust run it they don't need to change\nthey don't need to write their own\npluging anymore\nyes nice okay so is is there any\nquestion I can\nanswer let\nme no it seems like that's it any other\nquestions yeah with that thanks so much\nEric Andrew we're doing this for all the\ntime you spend on the building\nintegration making F\ncontributions I will keep building\ntogether and it was great to learn from\nyou awesome thank you I I I learn lot I\nlearn a lot from havu he helped me go\nthrough the process from Zero to Hero so\nI really I'm really grateful for this\nguide and yes he really spent lots of\ntime to help me so yes\nokay so this is my demo and and you you\ncan get the all the all the\nexample\nin here here is here is the all the\nprojects and I have record all the all\nthe videos of it and it it's almost a a\nblock post yes and I I will read Rock PA\nabout it then thank you everyone for\nsure yeah feel free to add this link to\nnotes\ntheend it's great and yeah thank you to\nKevin for his con constant\nmentorship awesome all right uh yeah\nnext I was Devin I don't know if we are\nkind of short and time I don't know if\nfive minutes is you now for\nyou yeah um I can I guess quickly just\nintroduce it I think it actually um it's\nsuper relevant to what hu er um you were\ntalking about so I think at least just\nstarting the conversation could be cool\num it's kind of like an extension\nessentially of the work you've been\ndoing cool let me share my screen I\nactually have a really funny um thing\nthat I just I guess coincidence small\nworld so um so I guess this is dosu um\nknow what we're working on is using lm's\nCode understanding to um do other things\nbesides coding that we do as Engineers\nso\num kind of in the ethos of what H was\npresenting about is like how do we do\neverything that we need to do as an\nengineer um now that we like have\nsomething that can understand code how\ndo we automate a lot of the other\nprocesses so things like issue labeling\ntriaging bugs helping with\ndocumentation um all the stuff that uh I\nguess we're working on on dosu and a\nreally fun fact that came up in that\nlast present presentation is um when\nEric was showing the airflow task\noperator um I saw Kevin\nGitHub handle and so um the when we\nfirst launched dosu with Lang chain in\nJuly like immediately this is like we\nlaunched very early um and I didn't\nthink anyone would install the bot um\nbut actually Kevin I don't know if you\nremember this you were the first person\nto install the bot um and we didn't have\na weight list at the time and it\nbasically just took things like\neverything down because you have like a\nthousand\nForks so it's really funny um I guess\nSmall\nWorld um but uh yeah um so I guess what\nI want to go through here is just a\nreally quick demo of kind of what um\ndosu is so I touched on kind of the\nfunctionality I'll just show show some\nexamples um essentially um the current\nproduct is a GitHub um app um that helps\ndo what I just described so label issues\ntriage bugs and um help with other kind\nof tasks so um I think the kind of an\nexample here is um you know we work with\na bunch of Open Source projects um\nreally kind of popular ones are like\nlength chain and LL index within the ml\necosystem um but here's an example um I\nguess from two weeks ago where you know\nsomeone was uh created like ran to an\nissue um and so um you'll see here this\nis dosu B is dosu in action so you know\nfirst it greets users so it actually\nremembers its interactions with users\nover time which is kind of fun um and\nthen uh you know it helped um him this\nperson uh debug the issue so um I know\nwe're short in time so we won't go into\ntoo many details um but one thing that\nwe do that's different with do like\nother kind of support Bots is that it's\nactually code based so like here's a\nreally good example of um dosu using\nboth a previous issue which was related\nto this bug um and the code itself in\norder to suggest a\nsolution um and then you know the user\nsaw this uh and it was actually the\ncorrect solution they made a PR um and\nfixed it um so that's kind of the ethos\nthat um we're trying to follow with dosu\nis like it's about empowering users and\ncontributors communities um so like not\nyou know necessarily autogenerating the\nPR but like how do we get more people\nable to contribute to um and be part of\nlike the Community for example um so uh\nthat's one example and then just you\nknow like I guess do who does other\nthings here's an example I'll do quickly\nof um you know it also does autol\nlabeling you'll see here um but here's a\ngood conversation where actually you\nknow dos is struggling um and so it goes\nback and forth with the user um and you\nknow at some point you know it realizes\nit can't do it anymore um like it's like\nexhausted it's knowledge based and\nactually knows to like l Loop in a\nmaintainer so it finds the relevant\nmaintainer and pings them um and so\nhere's just like a really cool example\nof kind of dosu working together with\nthe Lang chain maintainer to um push\nthis issue forward so helping highlight\nthings that um you know you as a\nmaintainer of community should be\nfocused on and that can't be like\nanswered purely through\nautomation\num and yeah um I guess I'll stop there\nbecause I know we only have two minutes\nleft um but uh\nany any questions\ncomments um this is fantastic I had a\nquick question uh how do you prevent two\nthings like one is bot fatigue and\nsecondly um in case of it leading people\ndown the wrong path like how do you\nmaintain that quality right yeah really\ngood questions on the bot fatigue side\num we do I guess there's two things one\nis like a lot of it is like\num teaching people about dosu so you'll\nsee here um you know we explicitly call\nout that you know if someone hasn't\ninteracted with a dosu before we'll say\nyou know it introduces itself which\nhelps just gauge expectations like you\nknow it is here for while they wait for\na human maintainer it is not a\nreplacement for um and you know\nsimilarly like kicking out to a\nmaintainer when it can't resolve an\nissue um and we also do um like intent\nanalysis to see like if users actually\nwant help sometimes you know user has\neverything um you know knows what the\nsolution is and it's more for tracking\npurposes so trying to gauge that um and\nthen for going down the wrong path um\nyou know there's no Silver Bullet we do\nour best um to like reduce\nhallucinations in kind of like the way\nwe approach um breaking down the problem\num but you know at at some point um you\nknow like it might not get everything\nperfect um and and that's why we think\nit's like important to like Loop in\nmaintainers or even just highlight um to\nmaintainers wh which issues require um\nhuman\ninput thank you that Mak sense yeah yeah\nthank you de I also have some questions\nthat probably will need to P up later on\nwith Edward and the\nteam and uh because it looks fantastic\nthank you DAV for joining and thanks\neveryone for joining\nwe are at time hope to see you in the\nnext\none\ngoodbye than thanks everyone byebye\nthanks everyone\ngoodbye"
    },
    {
        "title": "Flyte Contributor's Meetup - November 9, 2023",
        "transcript": "um oh welcome everyone to flights\ncontributors Meetup today is November\n9th and um a couple of reminders first\none meeting is being recorded and it\nwill be available uh on the flight\nYouTube channel under the contributors\nMeetup\nplaylist and also um as every\nCommunication channel for this project\nit falls under Linux foundation's code\nof\nconduct uh okay I'm again here the link\nof the\nagenda\nnotes should be this one should\nwork um all right let's kick this off as\nusual you can add yourself to attend\nthis list totally\noptional um right let's welcome new bers\nwho's joining for the first time let's\nstart with Daniel Dan farl great to have\nyou here man hi great to be here awesome\nwill you mind briefly introducing\nyourself uh yeah\nI'm uh Danny frell uh or Daniel doesn't\nreally matter I guess uh and I work for\nCyrus\nbiotechnologies and um I'm a scientist\nat there at that company and I've been\nworking on using flight to sort of\norchestrate all of our complicated\nprotein design\npipelines\nand uh yeah that's that's\nme yeah that's great you've been really\nhelpful in the community so yeah thank\nyou so much for this and um yeah I don't\nknow has been the journey so far with\nfly we have a number of users in the\nBiotech Industry but I'm curious how's\nbeen your journey so far the highlights\nand low lights adop team\nflight uh it's been I mean it's been\nreally good so far I mean\njust relatively easy to set up to just a\nfew little like roadblocks here and\nthere but none of them were really that\ncrazy I think probably\nthe biggest ones that we're looking\nforward to would be the GPU like you\nknow using more\nGPU uh like what I want to say like just\nbetter GPU interfaces but I mean we have\nway ways around that anyway and then\nalso the docker plugin that I uh have\nthe pr up for those are probably some of\nthe bigger things and then other than\nthat though it's been much better than\nwriting yaml for all of the workflows\nthat I sorry about that all of the all\nof the yaml that I was writing before\nfor so\nuh yeah so this is I mean you know the\nbar is really low in that regard but\nstill it's been very nice to work with\nflight instead\nso awesome thanks for sharing Daniel all\nright also Nikki sorry to call you out\nbut I think it's your first time joining\nthis meeting and and I feel like you\nplay a mayor role here so will you mind\nintroducing yourself briely yes um yeah\nthis this is the first one of these I've\nattended um I just started at Union as a\ntech Rider uh for flights specifically\nso uh we'll be working on flight docks\num started working on some of the kind\nof the onboarding getting started\ndocumentation but the goal is to really\nrevamp all of the docks um so yeah nice\nto meet\neverybody awesome thank you Nikki anyone\nelse um for the joining for the first\ntime I don't think\nso uh cool welcome everyone again uh so\nlet's move to New rfcs I don't think\nthere are new rfcs so we can move\nquickly to the next section unless\nthere's any objection I don't see any\nnew\nRFC uh what we see here is uh oh well\nworking group updates is there any\nupdate on\nthe the only official working group we\nhave so far\nthe config over\nright I mean last time we\ndiscussed we made some progress on V\ndecorator yeah the initial PR is already\nmerged and we working on the following\nImprovement oh no that's different from\nthe\nhold of the conf\nover I will let AI take\nnote H sounds better all right yeah\nthank\nyou\num Baron cool all\nright I guess we can move to next item\nnow this is the long one new ideas in\nthe incubator um if you're not familiar\nthe incubator is a section in the GitHub\ndiscussions in the flight repo where you\ncan um gauge the interest in the\ncommunity and potentially having new\nrfc's ideas that that could be a new new\nRFC and if you're not sure uh that's the\ncase uh well the the incubator is the\nplace to kind of Flesh row version of\nyour IDE and get feedback we have a\nnumber of this ideas there and I guess\nit's time to revisit this I I the\nfreedom of moving app uh this one\nbecause it's related to a contribution\nfrom\nDaniel I wanted to let it to the\nend first one build images on the flly\nworld\nregistering\num yeah I think this is already achieved\nby IM back is\nit was also my understanding I feel um\nit's like the the thre is on March and\nfor NFD that's implemented and there is\nthis RC that we talked about just now\nthat implements the same for Docker so I\nthink it's interesting to talk about the\npr that adds this for Docker but I feel\nthat the RC incubator threat at this\npoint can be closed what are your\nthoughts\nagreed cool\nthank you all right and yeah there's a\nrelated PR from\nDaniel developing a Docker plugin so he\nhad a number of\nquestions trying to get feedback so I\ndon't know if you um Daniel do you\nwanted to elaborate a little\nbit\nuh let me see let me see if I or you\nwant to share the screen no yeah I'll\njust just talk and I guess I'm really\njust sort of trying to get some ideas of\nyou know what's been thought about and\nwhat's been discussed before in terms of\nimage spec and so I think I did see\nearlier that there was\nsome discussions about having a Docker\nplugin um but then it looked like ivd\nhad been used instead and I didn't\nreally know you know why that was done\nor just the reasoning behind it and uh I\nmean I've liked envd overall so far just\nfor little things but when it comes to\nthe\nvery uh bloated academic workflows that\nI'm used to using you know everything's\njust been always done in Docker so um\nyou know converting that and translating\nthat over would be sort of confusing so\nthat's why I thought I would just make\nthis plugin and so one of\nthe concerns was that and I think I\ndidn't write it here but if you have\nhave you know a\ndynamic workflow currently and\nthen essentially when that Dynamic\nworkflow is actually run on the flight\nexecutor then it's uh then it it tries\nto build everything itself um like\nanything that needs to register and all\nthat stuff which you know makes sense\njust because of overrides and everything\nbut it sort of complicates everything\nbecause then you need to add in all of\nthe credentials and\nyou know you're going to have to push\nthat all the all that to the server as\nwell and so things just get really\ncomplicated so I was trying\nto think if there was you know a good\nway that you\ncan we might be able to integrate like\nthis Docker Builder but\num\nuh in a way that you know we can avoid\nhaving to push credentials and all these\nother things um and so I think in the pr\nthere's like some discussions and then I\nalso have\nsome uh like notes just written\nthroughout the thing in the read me and\nso on but\nessentially one of the problems that's\nhappening and\num I can't remember who I was messaging\non this PR uh earlier but\nuh is that if you have a workflow\ncurrently like your whole workflow\neverything is versioned based on the\npython code that's that's imported and\nso that's great and everything and it\nworks you know for envd perfectly\nbecause uh anytime you want to change\nsomething you're actually changing the\npython code but if you're actually\nimporting a file such as you know\nthrough a Docker file or something that\nis currently incompatible with the way\nthat workflows are run and\nregistered uh through flight and so\nbecause changes in the docker file are\nnot propagated to the version hash\nthat's um used to define the\nworkflows uh you can't technically do\nthings the way that I've implemented\nthem and I wasn't sure if people thought\nabout that or if there had been any\ndiscussions early about sort of like\ncallbacks to you know either hash more\nor to Hash parts of task separately I\njust wanted to get some feedback on that\nbefore I like dove in and started\nplanning some things\nso uh hi Daniel uh yeah we have some Fe\nback so yeah first of before doing this\nwe have uh talk about this internally\nyeah we want to do this internally yeah\nso thank you so for the hash stuff like\nuh I think you can uh read a do file and\nthen hash this file and then save it\nsomewhere in the image back right\nokay yeah I I could I just\nwas we we can specify the requirement.\nTT so we hash the requirement. TT so\nevery time you change it reement we will\nreu the image back as\nwell okay so then if I use the source\nand\nthen like if I hash the docker context\nas well as the docker file then that\nmight be one way of doing it should be\ngood okay cool and and then the issue in\nDynamic tax is that for now when you use\nimage SP in Dynamic Tex um flight key\nwill not check the check if image exist\nand it it just th build image in the\ndynamic text that's a that's a bu in\n1.10 we will fix it in the next release\nbecause when you uh before before before\nyou reach the RO flow we will build an\nimage for your text inside di Dynamic\nright so when you run the dynamic text\nin the on the cluster we should not\nbuild image\nimage already exists in in the do home\nsomewhere right so we should not build\nit yeah so I I will fix that and then I\nwill give it to you okay yeah and then\nso I guess on\nthat sort of uh on that train of thought\nlike if you have like currently if\nyou're if you have you know your\nenvironment locally or wherever you're\nrunning these things from so if I have\nmy Docker contacts that doesn't\nnecessarily get pushed you know or you\nknow tarred\nup to the actual task so then there\nneeds so I think I might be able to just\nuse copy all but it might need to be\nmore complicated than copy all uh does\nthat make sense I'm sorry if I'm not\nexplaining the workflow code\nitself yeah so like right now when you\nso if\nI'm like say I have you know this is the\nlocation and my docker file and this is\nyou know all the files that you know\nmaybe I want to import them into the\ndocker image when I'm building it and so\non if you push you know\nif if I'm sending my workflow to you\nknow a remote then everything gets\npackaged up and then run there but when\nit does that it's checking you know\nwhere's the context directory and then\nit's checking in that that Docker build\ncontext directory and then it's checking\nin the docker build context directory\nlike what are the files in here and\neverything and then it's building the\nhash based on that so the hash on the\nremote will be different than the the\nhash locally because it's not copying\nall the files with uh the associated\nlike python files does that make\nsense yeah yeah I get you I know that\nthat prob like um what what hash do we\nneed to calculate in the cluster I don't\nget this part\nlike the hash like on the cluster or\nsorry so there's like the hash that\nversion that's you know associated with\na workflow or task is associated with\nlike all of the associated you know\npython code and files that are like\nimported and run I believe I can't\nremember the exact\nfunction um and so then that's like when\nthe\nwhen dot when image spec is uh checking\nto see like hey does this uh image exist\nor\num like already exists it's\nrecalculating that hash on the remote to\ncheck and see if it exists my my my\npoint is I feel like there should be an\nif statement somewhere there that checks\nare we currently registering locally or\nare we already in execution mode on the\ncluster in the letter case it shouldn't\neven check whether the image exists\nanymore because at that point it should\nalready exist right yeah so in the\nplugin I have like a sort of you could\nconsider it like a a hack in a way like\nbasically if the um I don't know there's\nsome variable that has like an\nunderscore in IMG that and like\nbasically if this variable is set to\nanything then don't build but you know\nthis on the downside of that like if\nyou're actually using I don't really\nknow if people are using flight to you\nknow do like Ci or CD or anything but\nyou know if they're actually going to be\nyou know building Docker images remotely\nthen that would sort\nof cut that out does that make sense but\nbuilding Docker images on kubernetes at\nleast it's is since they since container\nD is standard it's not easy\ndoable and it's not in the scope of\nflight does I feel yeah yeah I know I\nmean I I wouldn't do it but I I don't\nknow what everybody else is doing\nso I I just didn't want to make a\ndecision without discussing it\nfurther we discussed this in before as\nwell like we want to uh have a pipeline\nto run some TX and then some of the par\ncan build image but it require to round\nDocker in cont in Docker so it's yeah as\nFabio say it's hard to do but we will\nnot to do it now but\nit's we we we have thinking this before\nyeah we want to do this at some point\nyeah so\nthen I guess\nif I think like one option is to either\njust you know someh have somehow have\nlike some un like either some\npre-built some way to like pre-build and\npush things um just on the actual like\ncomputer that you're doing like P flight\nrun on and then but then the problem\nwith that is that there's like\nno\num what I want to say yeah I think like\nlike you can't do with overrides for\nenvironment variables and stuff because\nthat requires that the docker files are\nlike the docker things are rebuilt\num\nso you can environment variables by\nsetting them on on the Pod I think you\ndon't need to put them into the\nimage yeah I think I feel like I thought\nthat but then it didn't work that way I\ncan't remember I don't know um I can\ndouble\ncheck but I guess that would be pushing\nmore\nof you know the hand ing of the\nenvironment\nto uh\nlike I I can't remember what actually\nruns everything flight\ncontroller oh because I think in envd\nand in and maybe is it true in envd uh\nthat the environment variables are set\nat build time\nyeah build yeah but is it set at build\ntime or is it set at\num like it's said at build time and at\nexecution time like so with overrides\ndoesn't trigger a rebuild\nor I think for now like we should force\npeople to build the image locally before\nthey register the wflow and then uh in\nDynamic no matter in dynamic or in\nregular text we should not build the\nimage that I think that's an important\npoint because right now the philosophy\nis that flight does not care where the\nimage comes\nfrom um and I feel it shouldn't change\nbecause of a\nplugin\nphilosophy yeah I mean that makes that\nmakes sense to me I just want to make\nsure is this related to I remember\nthere's I don't know if this been fixed\nbut I remember at some point when you\nregister a\ndynamic task workflow uh we don't\nactually know what's called inside of it\nso we\ncan't unlike a static workflow we don't\nknow like what we need to register\nnecessarily to make that Dynamic\nworkflow actually work at\nruntime um so this this seems somewhat\nrelated because like I call task one\nthat I've imported from somewhere\nelse like at registration\ntime I guess if that task is\nin imported in like the module you're\ntrying to\nregister you can just assume hey let's\njust build everything that's being\nimported in here like that's build the\nimages associated with\ntasks that are imported or defined in\nthe\nmodule um there also an issue with eager\nworkflows it's like you don't you don't\nknow what's what it's going to need\nbeforehand\nso um the work around there is just you\njust have to register\neverything before even trying to run it\num so yeah I'm not not sure how\nwe suffer this in this like specific\nkind of like Dynamic workflows with\nDocker Docker build stuff like do we\njust force people\nto yeah I don't know we forc everyone to\njust like build build and push those\nimages with pipelight\nbuild yeah I don't know\nwhat all right thank you any other\ncomment you I guess yeah I guess go\nahead I guess the only other\nthing that I was wondering about is so\nright now image spec has you know a\nbunch of\ndefined arguments that you pass to it\nand so then when\nI uh wrote this plugin I wanted to add\nmy own\narguments uh to pass to the image and so\nI mean I don't really know what the team\nreally\nhas you\nknow like looked into the future and and\nsaw like you know how extravagant are\nthese image spec plugins going to get\nbut uh you know like right now I have to\nchange the flight kit uh code and then I\nhave to you\nknow it's not like super easy to develop\nI guess and so I didn't know if there\nwas any thoughts on like maybe having\njust like\na a nit dictionary or something I mean I\nknow that isn't really ideal because you\nlose typing and everything because of\nthat but um like as the way it is right\nnow like I have to like developing the\nplugin requires also modifying the\nactual fly kit module\nso I don't know if like the problem that\nI'm having right now if anybody thinks\nthat that's like a this is probably just\na one-off thing or if anybody thinks\nthat it might just be so Daniel so your\nproblem is that like when you try to add\na new argument in the image bag you also\nneed to change the empty plug-in right\nright so then I need to build an image\nto use as the like executor whatever\nimage on\nthe light\nserver okay because then it's sorry no\ngo ahead go ahead sorry right because\nit's like saying it's like okay here's\ndo I want to like pass the argument\ndocker file but that argument doesn't\nexist in whatever\nso okay uh I haven't looked at your PR\nyet uh let me think more about it can we\nthink ofline later\ntoday sure yeah yeah no problem yeah\nthank you and I I'm not like expecting\nthings to be done super fast I just want\nto talk about it so\nyeah yeah\nvalum I just wrot it in the um okay in\nthe chat you you in the pr you wrote one\nsentence um which was that um having the\nshared image back kind of defeats the\npurpose of the plugin system for the\nBuilder um and I feel that's true I feel\nlike the plugin system where you can\nregister the Builder you you should also\nbe able to give it um like the fitting\nimage back for the Builder\nclass um I brought it in a PR\ntoo yeah I guess\nis there like any documents on like if\nI'm developing\nthis like is there sort of like a\nmore\num like I guess like iterative quick way\nto be like rebuilding and packaging\nflight kit and then you know having that\nbe the image that the remote workflow is\nrun on I guess you guys have like a\nreally fast way of doing that\nor\nbecause I've just been like building\nDocker images and pushing them and then\nsetting the dash dash image flight run\nyou can copy the flight kit folder into\nthe image and set the python PA path\ncorrectly so that when you do import\nflight kit it chooses that one that you\ncopy it into the image over or just fast\nregistered into the image over the one\nthat is in the side\npackages so if you put it into the when\nyou do p Dr or P register and then your\nflight folder is Sim linked into the\nroot of that of the tball and it's\ncopied into the bucket and then you just\nneed the right python path and then you\ncan basically develop flight kit while\nfast registering your workflow and you\nstill got updates from\nFL okay okay I'll try that that sounds\nmuch faster what I've been doing\nso thank\nyou awesome thank you all and yeah give\nthe conversation going in the pr all\nright uh time to move to the next\none disabling catching when running\nlocally um here is a comment from Fabio\nbasically probably could be just a PR\nnot an\nRFC any objection\ncomment around to implement this um\nreally looking forward to it because we\ndid run into a few issues um like quite\nsome um and and so this is also kind of\ncompany priority for us too get this\nin right\nCo sorry there's a bunch of noise here\num in\nconclusion uh okay to make it a and\nprobably not anfc\nor I don't think we need an RC for this\ncool\nuh cool thank you thank you Bernard\nalso cool uh streaming\nliterals yes with\nRFC to out ux it's a question and not\nnot\nstatement\nokay comments on this\nidea you be an RFC or not that's the\noverarching question in this\nsection I can say that people have asked\nme whether this works for flight in the\npast and one of my friends also posted\nthat he who asked me about this wrote\nhere in the discussion that he'd love\nthis feature um for me the question\nwould be whether you need feedback on\nhow should look like or whether you have\na clear idea how it should look like\nokay any other comment feedback\nplease so it it FS into an RFC or\nnot probably yes\nright yeah I think so okay\ncool cool so let's addit this and uh\nGuess The Next Step will be\nto craft a new RFC for this will be an\nRFC thank you I don't know who's taking\nnot is it you\nFabi yeah yeah like I'm updating\nthe thank you all right see Baron still\nhere no seems like he's not and here is\nthe\num vs code\ndecorator oh yeah the the vs code\ndecorator\nso uh we have merged the first version\nof it into master\nso uh everyone can use it right now I\nthink\nokay so I guess no RFC required\nand yeah this is totally isolated from\nthe the FL it's a plugin so I think no\nRFC required\ncool yeah\nor if you requir I would have one\nquestion because I I briefly saw this\nthe other day and read through it I\nwonder what the the use case is because\nI feel like I'm missing that use case in\nthe sense that I I feel like this could\nbe useful but I don't know\nhow so this is like if you want\nto uh interactively debug your task so\nyou probably will create a task and if\nyou just debug it locally you might not\nhave the environment in the remote so\nhere you can do you just add a vs Cod\ndecorator and you run it on the\nremote then it will create a vs code\nserver in the remote and you just you\ncan uh open your browser and connect to\nthe vs code server so you can debug\nremotely with the vs code server okay\nvery cool I'll try that out thank\nyou\nyeah question thank\nyouy right any any thoughts on um like\nwriting a Blog about this Baron I think\nI think it would be really interesting\nto Once once we get it like baked a bit\nit would be super cool to show people\nhow this is\ndone okay sure sure we definitely will\ndo that than yeah thank you oh sorry\nTroy I I looked up without seeing it was\nyou and I know Byron was talking about\nthis so I called you Byron sorry about\nthat no yeah no problem I like Byron had\nthis idea first and he asked me whether\nI want\nto uh collaborate on these yeah so this\nis uh the idea from Byron yeah super\ncool uh but no I was really trying to do\ntwo things at once and got confused this\ngreat no no problem thank\ncool next one uh is plugin out or um\nlooking for\nfeedback on on their\nimplementation\nso yeah probably for the question if\nthis is an RFC or not yeah probably is\nnot the better place but in general\nprobably we can I can continue the\ndiscussion with him and in the\ncontribute\nChannel and everybody can jump in and\ngive\nfeedback um you\nagree there you go so\nme thank you f all right uh cool next up\nGPU type\nselection yeah get different ideas how\nthe ux should be and the comment is that\nit should be an\nRFC um that's just my suggestion has any\nI think g g or ye sorry that I probably\nbuted your name was working on it uh\ndoes anyone know what the status\nhere yeah we we actually um pretty close\nto merging this in fight kit like the\nchanges in in the back end already done\nlike you'll be able to select different\nGPU accelerators\num okay there will be like a blog post\nor something like this is a big big\nfeature then that's not take a look like\nthe the flight K PR shows like how how\nyou're\nthinking like um once no new gpus come\non board like how you can actually\nselect them like that would be my\nquestion can I not use the next Nvidia\nGPU with flight kit one9 because it\nwasn't in Flight kit one9 that that\nwould be a question yeah no we thought\nabout like how to you know we not locked\nto a fixed set of gpus there's Escape T\nokay good then I think it can be closed\nhere I'll let you add the notes in raise\ncondition okay uh save resol execution\npiring flight\nadmin where are you um there you\ngo should be an RFC or an\nissue your\nthoughts\nokay any objection against proposing\nKatrina to make this an issue instead of\na\nRFC right let's do it so I'll reach out\nto her to ask open a new issue for\nthis\nright\nthere you\ngo\nyes distributed\nlocks yeah now\ndevelopment\nsince July\nAugust interesting idea but it should\nhave a\nchampion I mean we don't have\nexpiration dates established for for\nentries in incubator as we have for\nexample for working groups but um yeah I\nmean if if there's no Champion for\nthis uh what we can do is to cast a\nwider net and see if the original althor\nor someone else in the community wants\nto Champion this or then we can decide\nif it's time to close you\nagree uh a quick question David on this\nand I guess process just in general on\nthese um it's really great to have this\naround and have you know continual\nupvotes for people that are interested\nin this if we close this are we\ntransitioning it to an issue so that we\ncan still kind of maintain some kind of\nview on what what community interest is\nI think there's a lot of people that are\nperiodically interested in this but at\nthis time there's not like a clear path\nforward or you know somebody to do the\nwork um but but it is a very important\ncomponent that I think we will'll\ncontinue to get interest\nin yeah I need to dig a little deep on\nwhat's the behavior close discussions if\nit's still browsable that can we can\nreopen I'm not sure uh but yeah you're\nright I mean the idea is not to lose\nthis\nforever\num and and also indeed the idea of rfcs\nis not not that the authors should\nimplement this uh so yeah probably\nhaving a champion for this is not\nnecessarily for them to make it happen\nuntil the end but at least to continue\nthe discussion so yeah I I will dig a\nlittle deep what would be the best\nmechanism to retain this IP it's very\nimportant you're\nright thank\nyou okay\nso add that to the\ncomments uh not sure yeah we'll let's\nsay that will reach out to the original\nOutdoors as the first step to see what\nthey want to do with this\nright cool all right yeah run flight\nexcept except from the Jupiter notebooks\nis very\ninteresting\num yeah some comments\nthere so what are your thoughts\non making this idea an RFC or not\num I don't I feel like this could be an\nissue um there's nothing that I can\nthink of right here that\nrequire sort of like multi component\nchanges it's like pretty much flight\nkit maybe flight IDL but I don't think\nso um\nyeah and I think that yeah so I guess\nfor that I would maybe say this is an\nissue\num I don't know how relevant it is to\ndiscuss details but that's my\nthought oh thank you\nNeils um all\nright\num where are you\nSt oh next one uh feedback mechanism\nfrom task but to propeller use pot\nlocks I wrot that's it's important I can\nI can I can jump in this one and try to\nmake an RC understand it's a sensitive\ntopic because it will require like or it\nmight require architectural changes but\nI'm asked Often by users like why can't\nI just add this like this information\nhere from my from my run to the UI\nimmediately\nlike people would like to go from a\nrunning task to for example one TB link\nor So currently they can't um which is\nwhy I feel this having this\nconnection uh this communication channel\nfrom a running po to to propeller in one\nway or another would enable us to do\ncool things um\nwhich is why I think we should discuss\nthis\none I actually uh weird weirdly um found\nanother project that's doing this exact\nsame thing with with kubernetes is using\nusing a sidecar container to you know\nuse logs to um send information back\nthat that we might be able to use as\nlike an imp implementation guideline if\nthis is the path we choose to go um but\nyeah if if you're willing to Champion\nthis Fabio like I'm happy to help help\nwhere I can um and certainly go from\nthere awesome okay\nthank great all right next one introduce\ntriggers for\nworkflows okay it has some activity\nthere so what are your\ntalks yeah the comment there from fa is\nif Neils Could Be A Champion maybe maybe\na question there are triggers now via\nthe airflow integration right would the\nairflow integration allow you to do that\nalready here or would are these kind of\ntriggers uh not able to register one\nanother workflow R for example because\nthat's what it's what is described\nhere\num yeah I guess when I wrote this out\ndidn't have a\nparticular strong opinion on a solution\nin mind just the ability\nto um Express soft dependencies\nbetween the dags or I guess the graphs\nthat we have today that are mostly\nstatic um feel like the airflow\nintegration\nwould basically support this use case um\nI know with artifacts we have some\ninteresting things in mind but\num yeah\nI I think maybe this is worth like an\ninvestigation to\nsee you know how to use the airflow\noperators um kind of enable what's\nexpressed in\nthis so\nyeah and just do this one open\nmaybe uh he just one question so are we\nwant to do do we want to leas the Warlow\nor test execution event and then trigger\nthe another Downstream Warlow is that\nwhat we want to\ndo um yeah I need to look at the RFC\nagain it's been it's been some time um I\ndon't even remember I feel like the only\nuse case I can remember is is like\n[Music]\num like say I have a workflow that\nproduces a model and then I'm using that\nmodel in a scheduled batch inference\njob\num yeah so that I mean that that was one\nuse case of like using just the\nlatest model produced by that\nworkflow um and I guess that's an\nartifact I I feel like that is now\nfulfilled by artifact so it it's kind of\nlike conflated the different use cases\nin that that incubator ticket so um yeah\nmaybe I can go in and kind of revise\nit thank\nyou\nall right thank you uh next one\ncustomizing login\nlinks this one from\nfaon it's it's very close\nto um the one about the communication\nchannel from the task pod\nto um to propeller and I feel if we had\nthis communication Channel this year\nwould be probably not worse than RC then\nit would be would just be like hey we\nwant to send another piece of\ninformation from the running task to\npropeller so maybe when since I said\nthat I'll um I'll spear had the other\none uh maybe we can just for now um not\nworry about this one and once we have\nthis communication Channel then just\nimplement this one as like create an\nissue for and implement it and just use\nthis one here now for one reason why the\nother one is\nneeded\nokay any\nobjection\nno cool so I guess we it will remain\nopen until we\n[Music]\nconfirm yeah is that correct\nfa um yeah we can just leave it open um\nwhen writing the IRC for the\ncommunication channel one I'll I'll put\nthis one here in AR mentation as one use\ncase what this would be needed for and\nthen I lost both of them got\nit thank\nyou all right notification system yeah\nimprove\nthe slack\nintegration so yeah I\nthink in may we already decided that\nthis should be an RFC and reach out to\ndifferent channels\nto uh the\nauthor uh with no response\nso I guess next step will be to cast a w\nnet and see if someone else in the\ncommunity wants to\nChampion there was already a PR that\nimplemented this at least for gcp with\npop sub and I think also with red is um\nis there the last thing I saw that it\nwas closed because of the mono repo and\ntransfer to the new the new flight monor\nrepo um is there a plan to continue this\nPR is it on\nhold I linked it\nhere who was the original author\nfa um I don't know can you can you click\non it uh David can you go back and the\nother tab I link the I link the pr\nthere wait let me\nseconds okay so you are obviously the\ncurrent author because you opened it and\nthe original\none oh no that's the sorry Network\nInterruption I am sorry last thing was\nhere flight web hooks probably you\nalready answered this but do we want to\nuh or do we plan to continue this\nPR I think so\nright uh yeah I think yeah for now we\nare working on some other stuff like if\nanyone can help like we can continue on\nthis PR what if the we so we would want\num we would want to use the feature what\nwhat kind of help um would you say is\nneeded because then we can think about\nwhether we can whether we can help on\nthis uh yeah for now I I added a way\nhook in the fly admin so people can\nconfig the way and point in the admin\nconfig map uh I think we should also add\nan other stop in Flight key\nso and also\nideal we should Define the which you\nthink more about the ux identify the\nsome API in\nlike and if you want to help like we can\nsync offline yeah let Sy let syn I\nsyn all right thank you uh yeah we are\nrunning short of time but next one spark\nresource config from machine config\nokay what are your thoughts on making\nthis one oh I'm not sharing on making\nthis one in RFC or not and if so\nprobably will need a champion I can\nreach out I think per do is\nJames uh or was\nJames what are your thoughts on this\nidea I think that that was Evan\nEV yeah\nright\nso yeah in general this one should be\nonc picture request or we should close\nthis any\ncomment\nyeah probably need to figure out what to\ndo with this kind of ideas where where\nthe author probably is not engage\nanymore and um I guess the next logical\nstep will be to again reach out to the\nbroader community and see if one someone\nwants\nto um take the next step\nbut the overarching question is if this\none should be an RFC or\nnot\nor it needs more information\nwhatever it\nfits so yeah\nprobably I'll investigate a bit more\nwhat to do with this idea and we'll let\nyou\nknow yeah yeah true cool uh add notes to\nexecutions yeah the question is if this\nis already covered by the system tax and\nmetata\nRFC so the we have the the system tags\nand metadata in the back and that\nthey're not in flight consolle and I\nthink what\nthe um what the user who who wrote this\nwould want to have is the ability to do\nthat in Flight console so maybe this can\nbe transformed into an issue that this\nshould be added to flight controlle as\nit supported already in the C and in the\nback\nend B are you talking about execution\ntags so in I think it was if it was it\nin 110 it was just recently right you\ncan you can do p flight run minus minus\ntag and then just give some tag to the\nexecution yeah exactly I think there was\nalso the ability it also gives you the\nability to add\nmetadata or comment I haven't tried it\nyet I'm not sure exactly how it was\nnamed um and back when we discussed the\nRFC the idea was to bring it first to to\nthe clis and uh basically make it work\nin in the back ends and then in the in\nthe next step to allow users to add and\nmodify and delete tags and metadata also\nin the flight console and I feel this is\nexactly what this uh this discussion\nhere is\nsuggesting there already an\nissue yeah this seems like a great uh FL\nconsole type of request can be this use\ncase can be\nserviced by these execution tags exactly\nyeah um so this is already the issue for\nit\n[Music]\num all right thank you so already\ncovered by an\nissue\num\nhere right uh\nallow users to inject Secrets via P\nflight\nrun be\ninteresting yeah this\nis we're working on this on Union Cloud\nside I don't know I know we generally\ndon't discuss those things here but yeah\nI just want to call that\nout\nokay so yeah for the arching goal\nprobably it won't be an RFC right yeah\nwe can probably clst this um okay unless\nyeah\num me add this\nhere okay only\ntwo uh left infiny running\ntasks\num yeah yeah this is another oldie\num that earlier earlier this year yeah\nthis basically just\nlike like tasks as end points you can\nhit um and so that kind of breaks\nthe the opinion that we have about how\nFL flight tasks work\nso probably not an open source\nthing got it thank\nyou and finally improving launch plan\nux um I think many of these let's\nsee this has kind of been addressed I\nthink kaan's been on a a rampage kind of\nlike making flylight run and highlight C\nbetter um so I think this this is\naddressed pretty\nmuch yeah\n[Music]\nrights\noh great thank you all for your patience\nwe needed to do this it was long overdue\nand uh yeah thank you Fabi for taking\ntime we\nare um out of time so thank you all for\njoining and hope to see you in next one\nit was great to see\nyou bye thanks everyone have a good\nevening day\nbye"
    },
    {
        "title": "The Flyte journey at Cleo AI - Flyte Community Meeting - Oct. 31,2023",
        "transcript": "thank you very much uh to the whole\nflight Community Aid as well for\ninviting me to uh come tell our very\nshort experience so far with fight and\nthe excitement that we have uh for for\nthe project um so my name is Jose abarro\nI um I um joined Cleo just uh not even\ntwo months ago\nso I'm very fresh to flight and Cleo I\njoined as an mlops engineer uh as part\nas the platform team and I also run uh\nmlops Community meetups in Bristol\nUK um the platform team at Cleo as like\nany platform team is uh the goal is to\nprovide services and tool to make\nEngineers more efficient in our case um\nwe're working closely with the data\nscience team so that's uh why uh um the\nenvelops um in particular in this 10 one\nof our goals is to enable data science\nteams to iterate faster on machine\nlearning\nmodels and um what it means for us to\nwill be uh will be to iterate faster\nanml models is that\nby uh iterating faster on them we can\nlaunch new features quickly into\nproduction and therefore trying to find\nwhat's valuable for our users in the\nproduct and what is bringing uh business\nvalue uh quicker instead of having a\nlong iteration to be able to deploy a\nnew a new model or to retrain an\nexisting model in\nproduction um another valuable of\nlike being able to to iterate faster on\nmodels we need need to somehow have\nfunctionality around to to have metadata\nabout the training jobs that were done\nin the past and what data was needed to\ntrain that so like having tools to be\nable to store that type of\nmetadata it's important for us in in in\nin the context of this goal and\nalso it will mean to lower the barrier\nof a Cleo so that if we make it easy to\nretrain existing models in production\nthat means that\nno uh we don't need the a specific\nexpert that created that model to train\nthat model to to redeploy it that means\nthat other data science can retrain and\ndeploy models that are existing in\nproduction and potentially people who\nare outside the data science teams as\nwell like some like PMS potentially can\nhave a look and and see a model that\nhaven't been retrained in a while and\nmaybe they want to piig a new uh retrain\nPipeline with more um upto-date data and\nsee what's the evaluation like with uh\nwithin the tool or maybe with other\ntools that have been piped into into the\ninto the training pipeline to evaluate\nthe\nmodels um so to give you a little bit of\ncontext about Cleo's product so Cleo is\na it's a fin tech product it's the it's\ncurrently available in the US it's a\nit's Pioneer in the conversational UI\ntype of product so we nowadays very used\nto conversational AI type of like\nproducts everywhere but Cleo has been\ndoing this type of product for like the\nlast six years and with the release and\nthe Advan advances of like the latest uh\nconversational AI type of models we are\nbeing able to enrich our coverent\nproduct and and enrich the experience\nfor for our users um our mission is to\nempower people to build a life beyond\npaycheck so especially helping young\npeople who are like leaving paycheck to\npaycheck and helping them in the journey\nto to become um feeling better about\nmoney uh some of the products that that\nwe we have is a salary Advance\nbudget um you can something that is very\npopular within the product is being able\nto be roasted by Cleo so uh think about\nCleo like a friend uh who you can\nbasically talk about money and is uh\nconscious about your expenses and about\nyour income and then he can um help you\nout like or pointing out that you are\nspending too much money on certain\ntakeaways or\nUber um so coming back to the goal for\nthe platform team um we want to enable\ndata sign teams to itate faster on ML\nmodels um so the current way of\ntraining at uh Cleo is mostly through\nlocal training so like data\nscientist um train models using the the\nlaptops uh fortunately everything is\nverion control which is uh is already\ngood but\num this is very good because it has\nallow us to like go very fast uh it's in\nthe value of the of the organization to\nmake it happen let our speed go very\nvery fast but um since we have a number\nof successful ml models out there and um\nthat are bringing Direct business value\nuh to the organization and that coupled\nwith uh an expansion of the data science\ntheme so we're GNA start bringing more\nand more data science te we need ways\nand standardize the way we train uh our\nmodels in a way that is more reliable\nmore\nreproducible and um in um iterating over\nthem\nfaster so some of the things that I find\nabout like local training is that\nusually okay so you have all your\ntraining script version control but it's\nusually not a single script you have\nmaybe some scripts to like run some\nqueries Fe process some data build the\ndata sets then you have another script\nfor the actual training\nuh of your model then you have a\ndifferent uh script for evaluating maybe\nthere are a few more things coming up\nand and um a question usually that comes\nup if like someone else look into the um\ntraining scripts from like other data\nscientist is like okay so you have like\nfive six uh scripts here there's no\ndocumentation like in which order do I\nrun these scripts what type of\nparameters do I need um another ual um\nissue is that the dependencies man\ndependency management like data\nscientists uh have like local\nenvironment set up with like lots of\ndependencies with uh\nno um reflection on it on the on the\ntraining scripts and that happens quite\noften so like even if you get the order\nright for the scripts there are maybe\nsome like dependencies man dependencies\nissues like okay so you have the\ndependencies but you don't know exactly\nwhat the version is and the script is\nfailing somehow you don't understand you\nalways have to come back to the owner of\nthose training scripts um the original\nuh data science who create them um\nanother common issue that I I I notice\nis that certain task require certain\npermissions and and some of those things\nare again not documented so like to run\na specific query on a table in a data\nwarehouse you need a specific permission\nthat not everyone has and and that\nwasn't documented anywhere and again you\nhave to rely on the\num uh\noriginal\num owner of those of those scripts and\nthen training results artifacts like\nwhere is the is the data set that was\ncreated store somewhere the evaluation\nresults are St are stored somewhere\nwhere Where is the actual uh model for\nlike trainings uh that were unsuccessful\num if you want to run some like tests or\nsomething um so yeah those are like\ncommon problems so what we want to get\nis to somewhere like this and these are\nlike a couple of examples that I did\nwith flight like something where you can\nhave everything end to endend defined uh\nin a very pythonic way with everything\ndependencies management uh DS for like\nthe corrector the um dependencies\nmanagement uh correctly said um you\ndon't need to ask what type of resources\nyou need to uh have for each step\nbecause everything is\ndefined\num\nso some of the things that we loving um\nabout flight is that it has a very easy\npython API to build the Dax so that is\nreally helpful for like data science\nteams that are very pythonic Centric so\nthey don't want to or they shouldn't\nhave to like learn all the kubernetes\nunderlying complexities and and learn\njust another thing they should be\nspending the time in what they are best\nright which is just creating the best ml\nmodels out there so having something\nthat is very pythonic helps a lot\nbecause it's already um you don't need\nto you don't need to teach them or they\ndon't need to learn a lot of like new\nstuff to be able to to build um a flight\nwork work\nflow um then custom images are really\nimportant um like every single task that\nwe run they're going to need some Custom\nImage and uh we find very useful the\nimage spec and they L when we when we\nshowcase flag to them one of the things\nthat stand out it was the image spec\nbecause they didn't have to build like\nDocker images themselves anymore they\njust have to define the dependencies\nand and then uh flight will like build\nthat image and package it up with the\ntask um then as a platform engineer it's\nvery important for us to be able to\nprovide comp resources per task because\num sometimes like you want individual\nset of compute resources uh that are\nvery expensive like you want to train on\na GPU but then the pre-processing of the\ndata all the queries that you need to do\nto prepare the data set they can\nprobably run on a more like cheaper\ninstance and be enabled and being super\neasy for them to say this needs CPU this\nis just a regular CPU and memory type of\ntask it helps a lot to keep the cost\nunder\ncontrol um then um being able to pass\npermissions and get through service\naccount I haven't found and again maybe\nI see a few things that are not\nparticular uh um uh true but I haven't\nfound a way to provide the service\naccount through the task without having\nto use the poort spec and again one of\nthe benefits of using flight for from my\npoint of view is for the data scientist\nnot to have to understand what like the\nPO spec and the\ncuetes um API is so it would be great if\nwe could like say like service\naccounting this task will like use\nsomething else maybe that's possible I\nhaven't found it um then being able to\ntrack like what artifact um was produced\nby what workflow and uh vice versa as\nwell so having that um lineage of\nartifacts against the um against the\nworkflows then being able to have like\nVersion Control in the workflow and\nenvironment Separation by this designed\nit's it's great um and then caching\nfunctionality Dex is is some like\ncashing we already use inex um we\nhaven't used it yet but it looks very\nextensible and it's very very uh\ninteresting for\nus um so that's what I have to say then\nI have a\nlittle not like demo showcase that I\nlike to show you this is just an example\nthat I did uh it was quite exciting for\nme to work on it\num and it was uh quite an\nimportant thing for us to Showcase to\nthe data scientist because\num it's uh quite important for us to\nhave hyper parameter student within\nflight or in general for training uh\nmodels for a particular model that we we\nhave um but the rest of the models they\nnot using hyper parameters unit and it's\nnot because it's not useful for them\nit's just because it's it's just a an\nextra extra task that they need to do uh\nin order to try to like fine-tune that\nmodel uh if they training luckily they\nhave to like script everything El\neverything so it's just a extra work for\nthem to do and we want to provide that\nand Abstract some of the functionality\nthat I'm going to show you now to the\ndata scientist and add them as like a\nlibrary Tas TKS so that they can just\nrun hyper parameters tuning super easy\nthrough flight with um with optuna and\nand pipe all the results into MF which\nare another two open source um to um uh\nprojects uh so it looks like there so we\nhave um the flight workflow and using\nthe powerful Dynamic workflows uh\nparallelize some um tuning uh pars task\num that uses of tuna to run some hyper\nparameters uh space search and then it\nall the results are uh pushed into ml\nflow so um I'm GNA show you uh I'm gonna\nclick go for the um workflow and then\nwhile it's running then I'll show you a\nlittle bit the code um that produces\nthis so I have like two parameters one\nis uh the number of Trials so how\nmany uh training jobs we want to run per\nuh concurrency and I'm going to put like\nfive concurrency jobs um so that will\ncreate an ofuna study and then it will\nlaunch the dynamic uh workflow and in\nthe meantime I will show you um the code\nfor it which is incredibly\nsimple um can everyone see my vs\ncode yeah um so the workflow definition\nis super easy so the parameters as you\nsaw is number trials and concurrency I\ncreate the optuna study that I can show\nyou in a minute uh that's a regular task\nthen I call the parallel training jobs\nwhich is the uh Dynamic workflow with\nthe result of the study um the\nexperiment for pushing everything into\nmflow and then the parameters that were\ncalled um so create of tuna study is\njust going to create the mlflow uh\nexperiment and create the tuna um a\nstudy which optuna is the library for\nhyperparameter sets just an open source\num tool um then the parallel training\njobs is the um Dynamic\nworkflow which will Loop um through the\nnumber of uh uh concurrency parameter\nand then call the tune pams task and\nthen upend the um the result of into\nbest else um the tune params is just a a\nmock um training job so what is this\ndoing it defines the search space for\nthe parameters and then just randomize\nthe accuracy so it's not doing any\ntraining but we're going to provide this\nas a as a library for for our data\nscientist so that they can uh basically\njust implement the um the train code and\nthen everything else will be will be\ndone automatically for them and then\nReturns the best trial for each one and\nthen I have like some like simple get\nbest trial that iterates over all the\nbest trials and then get the best of the\nbest and then one for like printing out\nthe\ninformation and um about ml flow so\nofuna allows you to add a call pack and\nit's the one that we using to push the\nexperiment um the MLF flow experiment so\nthat every single trial can push the\nresults of it so yeah so um this work\nthank you to the demo\ngos um so as I said it creates the um\noptuna study and then he parallelized\nthe tune prams each of these uh have run\nfive mock of the training job and then\nres uh return the best um um trial and\nthen the result is passed the list of\nthese results is passed to the guest pet\ntrial\nwhich is passed to the registered trial\nand hopefully um this one has like\npushed the best trials well I I can\naccess the the locks but uh we can\nactually see that the results should\nhave been pushed uh into ml flow now uh\nthis is the uh\nmlflow um uh view for for my experiments\nthat I run with flight and in ofuna and\num and yeah um they can have a look into\nthe results here and then and maybe they\nwant to select uh an experiment\ndifferent than what was the best of the\nbest because of something and then every\nsingle parameter used in that training\nit was\npushed um the metrics results and the\ntax and everything but the important bet\nfor us it was that it was so simple to\nparallelize all these jobs using Dynamic\nworkflows with\nflight that it will allow us to abstract\nall this information\num into into into the tasks that we will\nprovide in a library for them to just um\nbe a they will have to just to implement\nthe training code\nbasically um right uh come back I don't\nhave anything else so if you have any\nquestions um feel free to\nask I have many many questions so\nI um it's if I be slightly unfair I'll\nbe kind because my questions are going\nto be more like asking flight developers\nreally more questions than you questions\nyou're kind of like a proxy for those\nquestions um to course Rich has a lot of\nuh code written in go and also\neverything else you can possibly name\nand Ruby and CP and everything you could\npossibly think of um how easy is it is\nit to integrate uh flight with\nmicroservices built into languages do\nyou just write python code that does API\ncalls to those services or is there a\nmore direct way of doing\nthat there is a hey my name is kaan I'm\none of the earlier creators of like no\nlonger cod as much but um the there is\nan agent framework and I'm sure there\nare people here who can demo it or show\nit um there is a\ndocumentation uh the idea is flight back\nend itself is plugable so instead of\ncreating containers or pods when you\nwant to invoke an API that's a direct uh\nintegration into the back end and you\ncan manage it at the back end without uh\nhaving to go\nthrough uh now you can of course write\npython code to whatever you feel like\nyou want to call apis you can now there\nare problems with that like in some\ncases we have some users who have done\nuh places where they invoke a endpoint\nbut that's a long running thing in that\nremote end point and let's say the Pod\ndies or you terminate the task and it's\nvery hard to uh kill that long running\nthe job and let's take an example let's\nsay it's a sagemaker training job or\nsomething that's running for hours that\nwould be hard right but with agents all\nof that life cycle is handled uh it's a\nvery very simple python API I think\nsomebody shared and then the second one\nover there is it all runs locally as\nwell because agents are implemented in\nany language there PC based but the\ndefault protocol that we have\nimplemented is in Python so if you're\nwriting your in Python you can run\neverything locally and then hit a button\nand it goes to remote so did you say it\nwas Json\nRPC uh\ngrpc jrpc okay yeah it doesn't matter\nlike that's the internal protocol from a\nperspective it looks as if you're just\nusing python object cool\nunderc that's great thank\nyou yeah uh I think there is somebody or\none of the core contributors has added a\nI don't want to advertise this a lot but\nan airflow agent that takes airflow\noperators and runs it in line and makes\nthem very efficient but you can\nbasically see those examples and write\nyour own and they're pretty Tri to\nWR I basically mentioned this meeting\nkind of off hand Discovery because I'm\nwatching LinkedIn and I mentioned it to\nmy manager who's operating from America\nand he started getting a bit excited so\nit could very well become flight users\nwe'll see we'll\nsee uh Jose I had a quick question\nthanks for the presentation um have you\nlooked into pruning uh I've looked into\nhyper parameter tuning and uh for things\nwhere you want to preemptively kill a\nrun you need something stateful um\nobviously you can roll your own like\nrdbms and connect to that in the fly\ntasks is that something you're looking\ninto or it is it not like in your use\ncase yet we we have done it um it's uh\nit's fairly simple it's completely uh\nyou just create um create a database and\nthen when you create the study you pass\nthat database so that every single task\nthat is running in parallel it's pushing\nthe results into the central database\nand then ofuna is able to see that one\nof the trials is a bit unpromising and\nthen prunes that automatically before\nthat we were running everything in in\nparallel and then we were running\nisolated trials but then we added the\ndatabase and now it's like the whole\nparallel job is like orchestrated\ntogether and uh and like the best\nresults of one trial in one task is\nhelping other tasks to search in the\nright space and pruning some examp some\nfils that are not um um very\npromising it's as simple as having a\ndatabase and that database setup was\nsimple enough there there isn't you\ndon't yeah see a need like for a native\nthing and flight for\nthat no no no it's uh at least for for\noptuna it's a it's um it has a support\nfor postgress uh database which is what\nwe we created a small postr database in\nAWS which is the cloud where we are and\nuh and we just added some code to like\nget the um URL with the secrets and\neverything um but yeah super super\nsimple okay awesome\nthanks um I had a question on that great\npresentation Jose thank you for sharing\num one my question I also put it I don't\nknow it's also for needs and you maybe\nuh is it possible because you already\nhave\nmlflow integration in there is it\npossible to just render that as a\nstatic uh the\nthe the exploration of your hyper search\nspace right the nice preeg graph that\nyou get in mlflow you could render that\nas part of flight de and you may not\neven have to jump uh onto MLF flow for\nthe ones that have already completed yes\non on real time mlow is useful right as\nyou're watching and observing I don't\nknow how many people do that really uh\nnobody really does it right at the end\nof you look like this yeah I know we all\nwant real time but who watches\nexperiments in real time as it's\nhappening um but uh flight decks is\nsomething you don't even need a database\nyou could just dump it into a into\nflight deck renderer and it shows up\ndirectly in the UI I don't know if you\nhave explored it and if you found\nwhether you know about it uh and maybe\nNeil's also want to join in I I I\ndefinitely knew about it I know that I I\nhad a look into the examples and how is\nis to create renderers there um I have\ndone it I have done an example where I\nwas evaluating things in within the task\ninstead of like pushing things into\nmlflow and um and then I was like\ncreating the um confusion metrics images\nand then uh I was thinking like oh it\nwould be cool if we have everything in a\nsingle place within flight but the issue\nwith that is that we we we always going\nto need for now like mlflow for like\nlike ml flow is great for like tracking\neverything automatically if you're using\nlike tensorflow P some of the like\nregular type of like models it tracks\neverything like very like it does the AO\nLing thing that tracks a lot of the\nstuff including like pushing the\nartifacts automatically to A3 as well\nyeah so so and then and then we\nintending we we are intending to use\nmodel registry as well at some\npoint so having ml flow that is not\ngoing to go away but I agree that like\nrendering some of the information within\nflight it's it's pretty it's veryy\ninteresting to have everything in a\nsingle place that yeah yeah yeah the\nproblem with the current mlflow plugin\nis it renders the the training traces\nonly for one task yeah um most people\ncare about the experiment so we'll need\nto somehow figure that out um yeah it\nwas my\nfirst uh way to go with the mlflow\nplugin but then I discovered that I\ncouldn't pass the like the experiment\nrun across to like have like um like\ndifferent task doing things in the same\nexperiment and or even in the same\nexperiment run\nsometimes yeah just to call out the\nmachine learning mental health handbook\nshould have don't constantly refresh\nyour experiment runs because it's it's\nnot not not\ngood\nyeah what do you mean you want a real\ntime watch as\nthe slowly goes\ndown I had more of a generic question\nfor you there Jose um just speak if you\ncould speak a bit more about your\nexperience with kubernetes and your sort\nof maybe a bit your pain in the past and\nexactly how this has helped you\nalleviate your pain uh because I know\nyou've dealt with a lot of Jor notebook\nstuff in the past and random bits of\npython and I think bits of Ruby and you\nknow hacks upon hacks upon hacks and\nhow's how do your current experience now\ncompare to what you used to deal\nwith again my experiences is still very\nshort um but for this time we chose not\nto tackle any of the de Dev experience\ninto like they like trying to help with\nthe like\nlocal um experimentation like the the\nthe experimentation so like Jupiter\nnotebooks and stuff so um what we're\ntrying to help is like once you once you\nfigure out your like experiment maybe\nyou have work in a Jupiter notebook\ntrying to figure out your training code\nand everything then you build the uh uh\ntraining pipeline\num\nbut some things that I didn't expect is\nhow easy is to run the flight workflows\nlocally as well so I'm starting to think\nthat maybe maybe the need for Jupiter\nhave to be able to that to do that\nexperimentation it goes away uh by just\nrunning the pipelines locally over and\nover until you until you have something\nlike that is canly like more or less\nlike what you want and then you just\nbasically hit remote and then package\nyour your workflow and then register\nyour workflow in the development and\nthen you start doing things uh\nremotely\nyeah on my end I've got kind of a dual\nneed for better machine learning tools\nfree open source ones because I'm both\ndoing cor search and also the nonprofit\nD analysis that I run and both are now\ndoing machine learning workflows um so\nit's I'm probably inevitably going to\nuse flight for one if not for cor search\nfor also dense analysis um and the way\nyou're describing there with instead of\nusing J note books using some flight\nstuff locally will probably be exactly\nwhat I end up doing because my I my my\nfriend slir fellow director George um\nbasically just use Vim because you know\nyou know us use Vim so VI is kind of AR\nfor\nNotebook yeah just to add to what Jose\nsaid that was our goal to that you write\ncode you test it LO you even unit test\nit like one day we hope be parts of the\ncode that people will unit test or\ngenerate fake data send it through right\nlike do all of that locally and when\nyou're happy hit a button and run it\nremote that that's the goal that's the\nholy gra that we are going for with\neverything now it's uh it's almost close\nto realization but there are like there\nare problems right like you may not have\naccess to data but this is where\nmovement of data between the remote and\nlocal is also the thing that we're\nconstantly working on so that you know\nlet's say if you have a database query\nthat results in a data frame which is\nfrom Snowflake or whatever you can\nmaterialize that remote but access it\nlocal through a secure API only\nephemerally right you're not going to\nstore it and then throw it away so that\nthat would be the holy\nso say I really appreciate that because\num probably the most difficult thing to\ndo you set up your kubernetes cluster\nand you run away with have all this\nautomation of scaling things and you\nbuildt this entire ecosystem all these\nsurfaces and you can't run it\nlocally is the problem you get with giv\nany I really appreciate any kind of\ntooling that makes it easier to run\nthings locally it's just common issue\nwith the instrumentation and\norchestration yeah and hopefully Jose\nthinks that you can run most of things\nlocally I think you're the one of the\nlatest users so you should be able to\ntell yeah it's one of the things that I\npoint out to the team like like oh my\nGod I don't have to like hit remote\nevery time to to figure out that I wrote\nthe name of the variable wrong and I\ncould like fix all those little issues\nlocally before I just go and try to get\na GPU out of\nAWS I also mention type checking in\nPython helps you with that issue quite a\nlot yeah that's one of the things\nthat um flight comes with right um\nhaving to having to have like strongly\ntype\ncode you\nalmost dismay it forces you but I think\nit's the right approach over the long\nterm I'm a huge fan of um pyite for T\nchecking which is a Microsoft's open\nsource Checker mypie is also potentially\ngood depending on what you're doing but\nPirate's been my preferred option for a\nwhile now works quite\nwell Jose thanks for the presentation it\nwas really really awesome um I'm curious\nhow you like could you talk a bit about\nhow you operationalized the platform for\nusers like you had mentioned using image\nspec you had mentioned um like having\ndata scientists like Implement only the\nyou know training logic uh would just be\ncurious to hear how how how does that\nprocess work like do they create a PR\nand then you run an action that creates\na flight workflow like just curious how\nthat works we we still figuring out uh\nwhat our process is going to look like\nwe are in the on on the process of\nmigrating our first training script that\nis producing a a model that is in\nproduction to flight and we're doing\nthat just manually for now with them\nhelping them on the journey uh we\nhaven't instrumented or automated any of\nthis process we haven't figure out if we\nwant to keep all our workflows in a\nmonor repo or if we want to\nlike have everything like separated or\nhow like all of that we we still we we\nwant to push this thing as far as\npossible and see where like where are\nthe pain points where are the things\nthat we need to focus on and then like\nall that like hopefully by the time that\nwe have this example our idea of like\nhow we want this to work it's is a lot\nclear and and we don't fall into a\nprocess that\num uh it's not it's not good for us but\nin terms of like um instead of in in\nterms of the abstractions the way I'm\nthinking at the moment we have this like\num library that we provide uh from the\nplatform team called like Cleo data and\nfrom CLE data we can add like um we we\nhave like abstraction for data\nscientists to run like Humanes jobs like\nsuper parallel that they can like point\nto like the training or the inference\ncode that they want and then like a cuen\njob is created like in parallel like\naggregat results blah blah blah so we we\nwere thinking about like like things\nlike the tasks that I showed you like\ncreate\noptuna experiment create mlflow\nexperiment um\nthe maybe the dynamic workflow that\ngenerates the or that is using the tune\nparam stuff like put it in the library\nand then just let uh the data scientist\nto do like from clear data import\nhyperparameter task and that comes with\nlike all that so they they don't need to\nknow they just need to know that they\nneed to create this like training code\nthat they need to pass to the task and\nthat is basically lunch and everything\nso trying to simplify as much as\npossible to them so yeah if they don't\nneed to know what a dynamic workflow is\nfor us is is better I mean they probably\nwill figure out at some point if they\ncontinue using um flight uh but that's\nour like motto like trying to abstract\nas much as possible yeah that's really\ninteresting thanks for the Insight\nbecause we're always looking for like\nwhat's the kind of optimal or what in\nreality happens when you give a platform\nteam something like flight like how much\nhow many iterations does it go through\nbefore it touches the end user and we've\nseen you know virtually untouched to\npeople building you know uh image\nbuilding service and otherwise untouched\nto like going all the way to the other\nside and you know kind of fully\nabstracting the uh the workflow\norchestration part so interesting to\nhear I think it depends on the it\ndepends on the organization in my\nprevious role I got to the point where\nlike the machine learning teams they\nwere like super experienced with\nkubernetes and that was because of the\ndecisions that I made and the team made\nin the platform team to like add\nservices and help them in the Journey of\nlike learning the kubernetes size of\nthings and and because our idea was that\nyou build it you operate it like you\nhave to like you own it completely and\nthat means that you need to go into the\nkubernetes cluster and figure out if\nsomething is wrong um whereas in Cleo is\nslightly different uh it's more like\neverything is abstracted for the data\nscientist which which is great I mean\neverything seems to be working fine and\nthat means that the data scientists can\nfocus in exactly what they need to um\nthey don't know anything about\nkubernetes they don't have access to\nkubernetes clust they just know that\nthey have to like implement this code in\nthis monor repo if they want a new\nservice and then they have to have like\na a jaml file with some values that will\nresult in their service having more or\nless memory or more or less CPU or maybe\na GPU and and everything behind the\nscenes is completely abstracted we build\nthe docker images uh using like a\ntemplate that we have plus the code that\nthey have and install the dependencies\nthat they given and build the H release\nand apply that H release everything is\ncompletely abstracted so it depends on\nthe on the organization and and and the\nculture of the company I guess and yeah\nI've been in both both ends I'm Tor at\nthe moment I like the clear away but I'm\npretty proud of what I got in the\nprevious role with the ml team knowing\nwhat a toleration\nwas it's very interesting because I I\ntend to think of uh these things like\ndifferent spheres of knowledge that you\nrespect like a data scientist uh thinks\nit a very particular way and acts a\nparticular way and Engineers are think\nand act in different ways and uh some\npeople can do both um I don't think it\nshould be required that should people\nshould be able to do both things it's\nthe same way that you can have clear\nseparation between quality assurance and\ndevelopment or separation product\nmanagement and you know y y these are\nlike separate disciplines some people\ncan learn both but often you'll find\npeople have like specific\ndiscipline yeah that's\nI mention that because it course the\nchallenge is quickly becoming one of\nmany many many challenges just one of\nthem is uh Bridging the Gap between an\nengineering team that has very little\nmachine learning knowledge and data\nscientists and some experts in between\nwho know a little bit here and there\nit's sort of starting to bring into the\nthe company a bit more integration\nbetween data science and like active\nengineering work\nthis is sort of I\ninterested yeah um thank you again Jose\nI I had a question um I mean so far in\nthe past two months you've been able to\nmake a lot of progress so that's great\nuh but I was curious what's what's been\nyour number one struggle with flight I\nmean what's been your probably main\nroadblock during the process maybe the\nanswer will change uh with the\ntime but\nnot I think we we have hit a few rocks\nbut the community has been great is\nsuper responsive every time I have a\nquestion I go to the slack community and\nthen I got an answer in less than five\nhours because of the time difference I\nwill say that some of the things that I\nhaven't figured out yet\num it's um as I as I have so we use\ncustom images uh with images spec and uh\nwhen I try to use that with uh Dynamic\nworkflows it tries to rebuild the image\nremotely because it evaluates like uh\nlike maybe again everything that I say\ntake it with a pinch of salt because it\nmight be that I'm doing the wrong thing\nbecause I'm still very early in the\nprocess but I yeah I was trying to like\npush this like Dynamic workflow which\nuses a custom image and then when he\ntried to run in the remote it was like\ncomplaining because he didn't have the\nflight key M MD uh dependency because it\nwas in my opinion it was trying to\nrebuild the image that I mean already\nexisted because I build it builds when\nyou push it but I don't know what was\nhappening there um I basically changed\nthat to the name of the image that I\nknew I buil and then I move\non but I I need to revisit that maybe\nI'm doing something wrong with that um\none of the things that\nwe we have spoted is that the schedu\nschedu it's a single\nreplica and uh we would love to have a\nscheduler that we can make highly\navailable with multiple\nreplicas\num otherwise um I'm not sure if we going\nto be able to rely on flight to a\nschedule training jobs\num um we can we can launch workflows\nlocally or we can hit the UI and and\nretrain stuff through the UI but I'm not\nsure and again maybe this is wrong um\nbut having theu as something that is\nhighly available and we have like\nmultiple replicas and we know that if\nsomething happen to one board from the\nschu is going to be running somewhere\nelse uh it will be super important for\nus um is this the Quran schedule that\nyou're talking about yeah yeah so that\nis you shouldn't need it will\nautomatically if one replica dies the\nother one will come up but one can\nhandle hundreds of thousands it's just\nit's it's it's all go routine it's like\njust in it's just generating schedules\nand firing request it doesn't do it it's\na very very process so um it's not like\nit's not reading bag or workflows it's\nnot not reading it's not it just looks\nat a crown it creates an inmemory timer\npool and boom boom just it can fire\nhundreds you have tried so yeah I don't\nthink that's your problem the problem\nwill just\nbe uh that the downstream may not be\nable to handle as fast as fire so that's\nwhere you need capabilities in your\nsystem to have the you know to\nscale great to know by the way it's a\nthat would have been one of the issues\nwe had to deal with so it's good to know\nyeah the entire back end is written in\ngo so even flight propeller that\nprobably you're running single replica\nbut that's you can scale it out you can\nsh it out to multiple but most of our\nusers even at like Spotify scale or\nsomething running 880,000 50,000\nworkflows in an hour or every\nminute one go process is enough because\nit's iio bound mostly it's not really\nCPU it's like 20 25% CPU utilization uh\num it's good to know it's good to\nknow\num so yeah I mean\num that's that's all really um being\nable to select like service account\ndirectly on the task would be would be\ninteresting and and nice for us but um\nbut that's like something that we can\nprovide as well as a liary like as the\nservice account and then we build the P\nspec for it it's super simple but having\nthat like exposed within the task would\nbe nice but yeah I mean so far so\ngood great I think the dynamic image spe\nmight just be a bug I think we should\njust make sure that is the case it might\nbe some sort of we don't know if you can\nshare an example somebody in the\ncommunity volunteered to try and find\nand fix it so yeah definitely\nshare I will I will try to build a\nsimple example and then see if I I still\nsee the same error because this was like\ntwo weeks ago so I might did something\nwrong and then I just like I was trying\nto go fast and then I just uh copy the\nname of the image and and the tag and\nthen I use that instead um so yeah maybe\nI did something wrong I don't\nknow it may be a bug I feel but\nyeah all right um if there are no more\nquestions thanks for the presentation\nreally appreciate you joining the flight\nCommunity to share your\nexperience and yeah that's all for today\nthe next Community sync is on November\n14th if you wouldd like to speak please\nadd your name to the table present at\nthe top of the agenda document or you\ncan just let us now thank you so much\nfor your time and participation see you\nall at the next Community\nsyn thank you everyone thank you for\nmuch\nwas great thank you\nbye"
    },
    {
        "title": "Power your ML workflows with Bacalhau Computing | Flyte Community Meeting - Oct 23, 2023",
        "transcript": "to be back I'll be your host today and\num well a couple of reminders first this\nmeeting is been recorded and it will be\nposted to flight's YouTube channel and\nuh second thing is uh just a reminder\nthat this in this meeting we should\nabide by Phil Linux foundation's code of\nconduct which is basically be nice to\neach other as usual cool let's get\nstarted um all right um\npasting here the link to the agenda in\nthe zoom chat and I'll share my\nscreen feel free to add yourself to\nattend this list including your\naffiliation uh it helps with notes and\nall that\nstuff uh cool it's totally optional but\nit will be great all right um anyone\njoining for the first time I think I see\nuh I know Troy is this your first yes\nI'm in the first time cool great welcome\nwould you mind introducing\nyourself uh sure\nso so I'm a student from UC San Diego\nI'm pursuing my master's degree in uh\nkber SS and um I know Kevin Kevin Sue\nand um I have been working on flight for\na while while but uh in the summer I had\na summer internship so I don't I didn't\nhave I didn't really have a lot a lot of\ntime but um I I have finished that so\nright now I will have more time to\ncontribute to flight and I'm really\nhappy to join the community and I'm I'm\nreally interested in different topics so\nif you have anything that I can help\nthat I'm I'm more than happy to to\ncontribute to the\ncommunity thank you that's great I think\nyou were participating on Hover Fest\nright sorry you are you participating on\nhackover Fest on the issues we have open\nfor hober Fest or\nnot\nwhat what sorry can say issue again yeah\nI will have an entire section on October\nFest is oh\ninitiative I don't know if you're aware\nof you're already participating in\nit oh I I I don't think so well I I I I\ndon't know what's\nthat no problem I'll introduce it\nbriefly here during the meeting and I\nwill sync with you after the meeting to\nkind of guide you through the process\nit's an interesting Initiative for for\ndifferent open source project okay thank\nyou so much first time contribution\nopportunity so yeah it's great that\nyou're\nhere okay thank you yeah I'm I'm curious\nhow how are you finding flight so far um\nI mean in terms of the learning curve is\nit too hard to\nlearn what are those aspects of life\nthat you find more\nchallenging I think it will not be too\nhard to learn um originally I chose a\nlot\nanother uh open source project uh it's\ncalled Unicorn it's a like a scheduler\nof commes and I found that really hard\nso and I saw Byron Byron from LinkedIn\nuh share a post about flight so I I I\nthink so that time I thought why why not\njoin the community and try to contribute\nand I found this community\nlike uh more support\nso it's not it it will not be really\nhard to to make progress because\neveryone will Happ you\nyeah awesome thank you Troy for your\nfeedback all right and I think besides\nTroy we have familiar faces here and\nwe'll have I'll introduce David later on\ncool all right uh we'll go to next\nsection\num news from the\necosystem first one is the state of eii\nreport this is produced by the I think\nit's a b BC firm uh they have really\ninteresting\nresearch it was good to find for me that\nit was not a state of llms but more\nGeneral AI of course llms are are big um\ntopic here but more generally LM Ai and\nuh to me one of the interesting findings\non the research area was um some\npredictions that probably LMS will\nexhaust uh the stock of human generated\ncontent\nby somewhat\n2025\n2030 up to\n2060 and uh it's unclear what are the\nimplications of training LM some\nsynthetic syn synthetic\ndata uh and it's it's impressing and how\nhow many years we spend creating content\nand posting stuff in the internet and\nnow LMS are are starving for for Content\nso yeah interesting times you can check\nout the entire report in the\nsite uh all right\nalso yeah also no problem a uh\ninteresting visual resource if you are\nlike myself just getting started with\nLNS and trying to to understand all\nthese terms and stuff uh this is a\nreally interesting resource I I read the\nentire thing a couple of times and and I\nI understand a bit better um some of the\nbasic uh topics and the role of the\nTransformers model in this revolution so\na recommended\nresource um right a really interesting\ntake on the stateless terraform yeah I\nme uh the tldr is that terraform needs\nsome form of state and uh kind of the\nupdated a proposal from the author here\nis to use uh G as the back end uh to\nhandle state for\nterraform uh what's interesting to me is\nto see uh hopefully we'll see more of\nthis discussion happening in the open in\nthe open tofu uh terraform Fork now\nhosted by Linux foundation so hopefully\nsome of\nthese\num foundational aspects of theform a\nprojects will be discussed\nthere or that's my\naspiration cool and and finally protect\nAI they open source uh number of tools\nto kind of help with the to secure AI ml\nworkloads uh two of them are are\nbasically scanning tools uh scanning for\nJupiter notebooks scanning for for ML\nmodels and the third is uh it's a group\nof tools to try\nto um yeah to avoid or mitigate prompt\ninjection attacks on\nLMS um\ninteresting there's again a lot to this\nfront because one thing is scanning and\ncomparing a guess a against a known\ndatabase of vulnerabilities one\ndifferent thing is to really uh sign and\nprovide provent Provence and attestation\nfor the\nentire um pipeline that's a whole\ndifferent thing but this is an\ninteresting move towards\nu a good end uh right yeah state of AI\nhas wave and protect AI cool that's true\nthank you okay then\nright any comment around stuff that is\nhappening out there in the AI ecosystem\nat\nleast no cool right let's move to next\none uh some Community project updates\nfirst\none uh team at Spotify had a really cool\npresentation a couple weeks ago at the\ncrunch conference uh I don't think the\nrecording is out yet\nand I'm not sure it will be out but if\nit's out we will make sure to post it uh\nIn the Event Channel and let everyone\nknow but uh son Ericson from Spotify was\nthere sharing how they're using fly to\npower a NextGen platform and running 20K\nplus workflow so talking about\nscale uh so this is really interesting\num and I hope again that recording is\nout soon but\num yeah it's it's it's the kind of stuff\nthat that will be really beneficial for\nthe project out there that you are\nprobably not at a major conference all\ndays but even here uh to see some of you\nall sharing what you've learned and your\nexperience so far with flight so this a\ngood\nexample cool also Sita on this uh this\nsame theme or or on creating uh\nresources for individuals just trying to\nlearn about LMS samita created a very\ncomprehensive guide on starting from the\nbasics uh what are LMS and Etc and and\nuh finally how you can uh train them and\nhow you can handle hallucination and\nfine tuning uh in production for example\nusing\nflight right so this is interesting very\ninteresting article and she spend a lot\nof time putting this in place so we hope\nyou find it useful and um keep the\nfeedback coming anything you you like to\nsee in the blog just let us know and uh\nwe're happy to work on\nit uh I'm excited about this one uh\nfinally we managed to convince Kevin to\ngo out and be on a flight school episode\nuh if you're not aware flight school is\na used to be a viwe we l um live stream\nnow it's a monthly live stream so we can\naccommodate even better content for you\nH and it's mainly focused on education\nwe spent a full 60 minutes just diving\ndeep on some specific aspect of the\nflight platform and uh we ran a survey\nand the the main topic the community\nwanted to see is the the process to\nwrite a flight plugin using the\nthe agent\nframework so who who's better than Kevin\nhimself to present this so I hope you\nall can join ask questions and um yeah\nbe there it's going to be really\ninteresting this is happening I believe\nit's October 26 so feel uh please make\nsure that you reserve your seat by uh\nadding your info\nhere all right h hctor Fest again this\nfor some of you if you're not aware this\nis an initiative that was born in\ndigital ocean uh several years ago it's\na way to create or to kind of have a\nmonth dedicated to firsttime open source\ncontributions and and try to help folks\nwith with again becoming open source\ncontributors or or helping with with\nspecific stuff open source projects for\nflight um I I was curious to see what's\nthe behavior so far it's it's been a\ncouple of weeks uh since uh since this\nstarted and and I want to check what was\nthe performance compared compared with\nthe the first two weeks of October last\nyear so the good news is that there's a\n61% increase in the number of code\ncontributions which is awesome uh by a\nsimilar number of contributors um this\nis all nonunion Affiliated contributions\nto be clear so um a similar number of\ncontributors producing even more uh code\ncontributions means a\n36% increase in productivity let's say\nso it could mean many things it could\nmean that we are creating a more diverse\nset of contribution\nopportunities uh that we are becoming\nslightly better\non um qualifying something as a good\nfirst issue uh but we can we can be even\nbetter at that but this is a this is\ngood I mean good numbers here uh there\nare right now 42 open issues labeled as\nhob Fest 14 of them are\nunassigned I will share a note on this\nand but 33 of them are assigned but not\nlinked PRS yet I mean there there's not\na sign of progress I mean at least right\nnow right so you can check the the query\nthere uh but kind of their\nrecommendation is that um well I will\nshare a couple of recommendations in a\nbit uh there are two block Post in\nreview because it's not only code uh we\nare trying to promote the idea of hey if\nyou're just becoming aware of flight and\nat the same time you're trying to\nimprove the code base probably that's\ntoo hard you can split the effort you\ncan uh collect your learnings in the\nform of a blog post or a screen share\ndemo Etc of what you've learned so far\nabout flight and that also\ncounts and right now we have two blog\nposts in review uh for from folks uh\noutside in the community uh sharing what\nthey've learned so far which is really\ninteresting and this is\nthe let's say the\num yeah yeah kind of the amount of\ncontributions I forgot the name uh in\nthe in the first place we have no other\nthan Mr future outlier Eric Chen with 16\nPRS and hongin also other and some other\num individuals in the community some of\nthem firsttime contributors So yeah\nthank you all for your effort uh but uh\nby collecting this data I I saw some\npatterns and I I I will share a more\ndetail note on this but kind of the\nguidelines right now are in terms of PRS\nfirst come first Serv will be the\napproach because there's a there are a\nlot of issues from Hector Fest that\nsomeone says Hey Please assign this to\nme and then one week two weeks with with\nno progress no questions nothing and uh\nit kind of prevents because other folks\nthink okay this is already a assign I I\nI won't work on this so right now kind\nof the approach is don't wait uh on an\nissue to be assign to you just feel free\nto work on this uh and uh if you submit\na PR well the first valid PR will be\naccepted that's\nit uh it's not a competition it's a\nmatter of of getting also things done\nand uh also please link your PR to the\nissue using the GitHub keywords it's not\nenough to just paste the link to the is\nthat's it you need to use one of these\nkeywords for GitHub to actually link the\npr to the issue and uh with this we can\nrun reports and really know uh what\nissues have shown some sign of progress\nand there's a because there's a PR link\nto\nit right um okay yeah I will share more\ndetails in this later today in the um\nOctober F\nChannel any comment around October fast\nso\nfar\noh\ncool all right so without further Ado\nlet's go to the main show guest\npresentation uh it's great to have here\nMr David aric\nhe has a long story on um prominent open\nsource projects like gretes qlow and\nother\ncommunities um I like your hat David\nreally very I like EV so that's a very\nuh convenience\nyeah um yeah welcome D I have to say I\nI'm sorry I haven't participated more\nlike there are so many sweet links in in\nthe community Channel like uh I'm like I\nI totally forgot that the state of AI\nreport was out and that's like my\nfavorite day of the year is to go\nthrough that uh every year but then I\nwas reading I was I was off reading the\nthe terraform stateless thing which was\nreally fascinating um sorry I it just\nthis is a terrific terrific Community\ncall um I just want to compliment you um\nuh I'm sorry please I I I didn't mean to\njump in if you you had something more to\ncover all right it's feel free to jump\nin I mean okay welcome and please thank\nyou so much and probably share with us\nsomething that you yeah yeah yeah of\ncourse I'll I'll I'll give a brief uh\noverview on on what um B Yao is and and\nuh again so let me let me say I I am um\nuh longtime open source Community person\num interestingly I saw a flight uh\nbefore it was called flight it was like\njust still like an internal lift only\nthing uh and uh it was when we were\ngetting Cube flow going and I was like\nwow this is so great we got to figure\nout an integration and so on uh and then\nyou went off and built this amazing\nplatform so really really exciting for\nlike all the progress\nhere um my background as you mentioned\nis um I was the first non-f founding PM\nfor kubernetes I led that for um a bunch\nof years when I was at Google uh I\nstarted Cube flow while I was also at\nGoogle um and uh then I went off to do\nwork at Microsoft in the office of\nmachine learning strategy and or excuse\nme the um doing open source machine\nlearning strategy out of the office of\nthe\nCTO um and uh then I started this brand\nnew platform uh I'm a huge fan and I\nrespect communities super deeply so like\nplease take this as just guy who's a big\nfan coming in to chat uh like and\nideally you know my dream is that we\ncould figure out how 1 plus 1 equals 3\nbut uh you know truly it's just like a\npleasure to to meet and and support\nother open source\nprojects um\nso let me dive in and give you a little\nbit of what I'm working on now and why I\nam here talking to you at all uh can you\nsee my screen in presentation mode yes\nsir okay great so the you know as I\nmentioned I I had previously worked on\nkubernetes kubernetes still is an\namazing platform I'm really pleased with\nyou know how how broadly how much it's\nchanged Computing um it's really really\nexciting and so useful but it is uh it\nalso has boundaries it's it's very good\nin centralized clusters particularly\nthings inside of single zones uh there\nmany other Solutions out there that that\nare more domain specific things like\nspark and hoop and so on um but uh one\nbig gap that those didn't don't fil is\nthe issue of data um they're they're\nvery intentionally designed to focus on\ncompute and how to move your computer\naround distribute it orchestrate it and\nso on but there's it they really kind of\nleave the well okay you need to figure\nout where your data problems are and you\nneed to figure out how to get our\ncompute jobs to that data in order to\nrun with it and um in uh February of\nlast year um we were you know myself and\nand a couple other people were looking\nat it and we're like geeez you know\nwouldn't it be nice to have an\norchestration system that focused on\ndata first and and really relied on or\nreally made it much easier to work in\nnon-centralized waves and by that I mean\ncross Zone cross region cross Cloud on\nPrem Edge iot and so on again this is\nnot a replacement at all for those\ncentralized systems this is hey we\nshould have an augmented system where we\nhave different systems handling\ndifferent components uh and like I said\nthe the problem that we're leaning into\nhere is that enterprises spend too much\non data they spend too much money moving\nit around storing unnecessary things uh\ntoo much time uh in in moving it like uh\nultimately you're gated by the speed of\nlight and bandwidth and other things\nlike that uh let alone if the networks\nare unreliable and then finally too much\nrisk some data cannot be moved it's in a\nregular ated environment it's in a\nregulated zone or region or whatever it\nmight be and you can't move the data at\nall so the idea is we came up with this\nproject called B yaa uh B ya uh lowers\nyour data cost uh because you don't have\nto move as much of it it can reuse\nexisting resources it can process your\ndata faster because it's distributed\nacross all these many devices um uh and\nthen of course it enhances security uh\nbecause you you know the the moment you\nhave to move the data off then you're\nopening yourself up to new a surface\narea and our our tagline is your\nEnterprises operate worldwide your\ninfrastructure should too um expansa\njust like uh Union is the commercial\nbacker or one of the commercial people\nmanaging flight back uh expans is the\nthe commercial backer um for backo uh\nthe back out Coast I should say is uh\nyou know all totally open source Apache\n2 MIT licens uh the trademark and the\nbinary are not open source but the\nsource absolutely is and you should feel\nfree dive in go poke around uh we're a\nbrand new you know project and uh you\nknow have a lot of like would love love\nyour contributions so uh people often\nask why we call it backa in the first\nplace and please dive in for any\nquestions I can talk a lot here but I\nI'll try and be quick uh the year was\n2021 we were in we had this question\nlike you know can um uh can we build a\ndistributed computation system\num that is you know works specifically\non distributed data um and we we happen\nto be in Portugal uh and we kept\nabbreviating it as compute over data and\nit turns out that the uh word for uh we\nkept abbreviating it as Cod and it turns\nout the word for COD in Portuguese is\nbakoo uh so that's why we call it this\nvery strange you know hard to pronounce\nname hard to pronounce for\nnon-portuguese speakers anyway uh and uh\nI've said it about a 100 million times\nand I still probably get it wrong uh you\nknow Portuguese speakers are probably\nstill mad at me um you know the the big\nthree areas that we want to address with\nBayo is what you see here um I'll let me\ndive into each one the first is you know\ntransforming your data before you move\nit um before Bayo you might have a node\nthat's split somewhere in the world that\nnode could have some amount of data on\nit um and you have your central compute\nclusters over here which is fine um\nagain we are not a replacement for them\nuh but more often than not in order for\nyou to process any of this data the\nfirst thing you have to do is move the\ndata into these Central locations dump\nit in a bucket dump it in your Lake and\nso on uh of course uh this often comes\nwith a raft of problems uh bandwidth\nsecurity caching syncing so on and so\nforth uh and even worse it's not just\none machine moving a small amount of\ndata it's machines moving a whole bunch\nof data uh you know in different volumes\nso um you know this is not ideal um uh\nideally you would treat the data where\nit is before you move it uh and that's\nwhere things like Bayo come in hey come\non sorry my boys are angry at each other\nfor some reason so what you can do is\nwith Bay Yao is um instead you deploy\nyour agent to the nodes where you'd like\nto do some data processing then you run\nyour initial data processing jobs on\nthat itself that could be filtering\ntransforming fine-tuning whatever it\nmight be and then you move only these\nsubsets back and by doing it that way\nyou you develop a much more efficient\nhey boys come\non um uh you develop a much more\nefficient ETL data processing pipeline\nbecause For Better or Worse back reduces\nthe challenges that you saw there again\ndoesn't eliminate them and doesn't\neliminate the need for a central compute\ncluster uh it just changes the way you\ninteract um the second is if you have to\nuh execute over an unreliable Network so\nhere we have our data scientist and she\nhas a global deployment she has cross\nCloud compute she has devices sitting\nall over the world on Planes Trains cars\nships uh Vehicles you name it uh and she\ncan obviously interact with those in a\npretty straightforward way today but you\nknow as you know once you get into any\nkind of like networking that crosses a\nsingle zone and even often inside a\nsingle zone you could have outages you\ncould have conflicts you can have all\nsorts of things and the truth is that\nmost systems today are not built for\nthat you often have to build a\ndistributed queuing system a declarative\nrollout system things like that uh our\ngoal with Bayo is to provide all of that\ninfrastructure for her so all she has to\ndo is her job to B Yao and describe\nwhere she'd like this thing to run and\nfrom that point forward you could just\nthe B Yao will wait until these things\nappear and then she can go forward and\nexecute them on them uh directly so\nthat's um that we we help with executing\nover these unreliable networks again um\nuh and when when when I say this I mean\ntruly anything where you're beginning to\ncross a latency boundary that's where\nyou're going to start having um you know\nuh um you know un reli or reliability\nissues not because anyone's not\ndelivering you top quality but because\nthat is the nature of\nnetworking uh and then finally using\ndata and\nisolation um uh you know before back you\nmight have a centralized orchestration\nsystem uh where you know this this team\nis developing you know foundational\nmodels and so on and then they want to\ndeploy them into their individual zones\nregions whatever unfortunately as you\nknow the zones often can't talk to each\nother because have a firewell but really\nfrom a regulatory perspective this\nCenter of Excellence which is outside\nthese orgs often can't talk to it either\nuh this could be you know Hippa socks uh\ngdpr you name it um you know folks with\nyou know touching data today often are\npresented with a number of different\nsecurity issues uh with bakaya what you\ndo is you simply set up a a Bayo cluster\nthat spans these various regions and\nthen you deploy via that network but\nthen you can show audita that nothing\nhad ever moved back that all the work\ntook place inside these organizations\nand on and from that you um are able to\nrun the jobs in place and get a pure\naudit log um uh this could be used for\nthings you know just trivial things like\ndoing log vending or things like that or\nmore sophisticated things we have an\nexample uh doing uh Federated learning\nusing pedals for example as\nwell um these are a few of the overall\nuse cases we're seeing the first two\nhere are probably the biggest ones that\nwe're seeing people lean in log\nManagement edml training or inference um\nbut we also see people using it for\ndistributed data warehouses Fleet\nManagement just doing queries on\nmachines with drivers logs things like\nthat um if you have nodes with uh\ngeographically distributed information\nfor example S3 buckets spread all over\nthe place uh that could be an\nopportunity as well uh and there's lots\nmore stuff that we uh do um every day\nthe our community comes forward and says\noh have you thought about doing it this\nway I'm like wow that is a great idea uh\nsomeone just last week talked to us\nabout doing things on satellites for\nexample um with this network okay so um\nuh that's the high level you might ask\nwhy I'm here talking to uh flight folks\nuh just to give you a layout of the\noverall architecture here um you have\nyou know a user they submit into the\nback of Yao network of via CLI or API or\nhosted dashboard or the UI that goes to\na logical uh not a physical Global API\nuh endpoint and then that Network\nbetween a series of local schedulers\ndecide who get the job that job gets\ndeployed to a node which bids into that\nand then that job actually gets executed\npretty straightforward stuff and not\ndifferent than most um uh Global\nschedulers you can see our um you know\nour history with centralized schedulers\nlike kubernetes on the team but this\nconcept of these local schedulers that\nhave authority that then pass that off\nto the node that's where we really start\nto get kind of unique value okay so like\nI said um that's the overall\narchitecture what we're seeing and what\nwe hope that we can partner with flight\nand and other orchestrators on is\nsomething like this where you might have\na back Network that understands where uh\njobs need to be scheduled according to\nthe data that they have or other\ncriteria now we are not a workflow\nengine right we need an external\nworkflow engine to drive us and we're\nnot going to go and try and reimplement\nhow to executes on dags so what our\nthoughts are is you our data scientist\ntruly you know other than knowing that\nthere exists a back ofo Network ideally\nshe has very little to do with any\nindividual um uh inter interaction with\nback out instead she submits her job to\nflight and flight says oh this is a\nbacky out job and and I'll show you how\nwe do that in just a second um you\ndeploy the backo network with the\nscheduling constructs and then back a y\nwill find the the node where the uh that\nis the right fit for that particular\nstep runs it on that location and then\npasses that back to step one uh step one\nthen passes it on to step two step two\ndoesn't need to contact a biao network\nyou may contact a different uh\norchestration system or something like\nthat um but that goes forward and then\nwe move on to step three and step three\nsays oh you know I this is also a bako\njob it hands that to bako bako says oh\nthat's good but this is a separate node\nthat needs to execute on this\ninformation it finds it schedules it\nruns it and passes it back and then it\ngoes to step three and then our data\nscientist is Happy uh but all she had to\ndo was interact with flight in order to\nmake this work does this all make sense\nI know that was a lot of talking uh let\nme show you just really quickly what\nthis looks like um uh we have a blog\npost where we talk about this uh we just\nannounced uh this overall plugin you\ncould see this in our uh video here but\nthe code is really straightforward uh\ncan you see my screen\nhere y yeah okay great so the code is\nreally straightforward it's not super\npythonic we apologize we know that it's\ngoing to be better we promise but you\ncan see here that how easy it is so with\nwithin your overall flight workload uh\nyou can see here's the uh plugin that\nyou use uh you define what the Taps uh\ntypes and taskar and then inside that\nyou use native uh these are back of Yao\nconstructs what engine you run on the\npublisher spec uh the specific dot you\nknow dot version of Doc or Docker image\nyou use and so on um and this is a\nreally simple one if we click through\nhere our code is all published uh again\nwe are big open source folks\nuh and I don't know why that linked off\nto that we need to fix that\nURL but if we click through to our\nflight uh you can see this here um uh oh\nwhere is it in\nexamples my apologies here\nuh oh here an example sorry about that\nso um here are two example ones here's\njust a simple hello world that you could\nsee earlier uh this is with you you saw\nearlier that we had a bunch of defaults\nthis is with if you define all of the\ndifferent features that you would want\num but pretty straightforward you could\nsay Docker you could say wasm you can uh\nuh dictate like exactly what labels\nyou'd like to deploy to so on and so\nforth uh but if you'd like to do\nsomething more complicated like chain\njobs together where the output of one\njob uh is now an input to the second job\nyou can see that here here's how you uh\ndo two tasks uh executed in serial where\nyou do task one and then task two uh um\nand again very straightforward uh we\nknow that this is not pythonic I am a\nbig python fan we will get there um but\nuh generally speaking you can do this\nright now and already begin to fold back\nof tasks into your flight workloads so\nthat's the broad Strokes uh I did a lot\nof talking there uh who has questions or\nthoughts this is great um I had a\nquestion\nI'll leave this up so that if people\nwant to join you can jump jump through\nplease go ahead sorry yeah\nso\ncomputation you know when you do\nmultiple sets of computation one after\nthe other you have to move the data\nbetween\nsometimes uh what is the underlying\nfabric to move that data so let's say a\ncompetition a runs on\nsome as you showed in the corporate\nexample\nA1 and then the computation 2 runs on E2\nwithout allowing the data but\ncomputation 2 wants to use the results\nof a which may not fit into which may be\nterabytes how does that happen yeah so\ngreat question it largely is up to your\noverall architecture what I will say is\nthat some people say okay well I'm gonna\nyou know put the intermediate stuff into\nan S3 bucket that's perfectly acceptable\num some people could make the data\navailable via URL we support that\nnatively so you could say Okay backo job\nuh or backo task two um pull from this\nURL and we can do that in a job itself\nthe native way we do it is by using ipfs\nto automatically move data from place to\nplace uh again you don't have to think\nabout it job two you could say I want to\nschedule you to here but the data is not\ngoing to be present there I want you to\nmove that data to you know the second\njob and will take care of the moving for\nyou as long as you've set up a private\nNetwork now um and by setting up a\nprivate Network I only mean that the\nnodes can see each other uh which is a\nbig caveat some people don't want nodes\nto be able to see each other and in that\ncase you're going to have to figure out\na different way to move the data between\neach other but the default is that if\nyou set it up and any node in any Zone\nuh can see each other we can handle\nmoving that data for you does that make\nsense got it yeah so basically it is a a\npluggable data Fabric and one the\nrecommended one is ipfs I actually\ndidn't know much about ipfs I'm learning\nso sorry uninitiated here uh yeah no no\nworries yeah that's exactly right so\nipfs does the charting does the movement\nthings like that by default it it really\nis is up to you we have a native S3\nreader writer we as I mentioned support\nURLs uh we also have an open pluggable\nstorage spec um the reality is is that\nevery scenario can be different some\npeople want data to move some people\ndon't want data to move some people want\njust the results to move all of these\nare available to you um and if you have\na particular scenario in mind that we\ndon't support please come talk to us\nbecause we'd love to be able to support\nit but like I said by default just\nimagine there are magic Gremlins behind\nthe scenes that will make sure your data\nis where you want it to be when you want\nwhen you tell it to execute a job in a\ncertain\nlocation uh and we take care of that but\nagain that is the default it is not\nrequired what other questions\nanything hi David uh Neil's from Union\nAI just uh wanted to ask what your\nexperience was uh writing that plug-in\nwas it easy hard well so it's\ninteresting of yeah um uh you know it's\ninteresting I I spoke to the\ncontributors uh meeting last week uh\nbecause our experience it was good but\nwe're like hey shouldn't there be a\nsimpler way to do this like you know we\ncan't be the only people looking to do\nthis and so the truth is we just weren't\naware\nof how rich the agent framework was um\nand and now we're going to go back and\ndo some work again in the spirit of\nmaking our our SDK more pythonic we also\nwant to do this so um really like I I\nwill say um uh I I think our experience\nwas generally quite good I'm happy to\nput you in contact with the developer\nwho specifically worked on it but I\ndon't think we had any you know um uh\nparticular issues we were really\npleased awesome yeah for the agent stuff\nyeah we're happy to set up a channel\njust to you know have have a channel\ntheir communication so uh Kevin and\nothers can help you out with the um the\nagent I love it\niteration you know I mean I I don't uh I\ndefinitely don't want to turn this into\na business development or other meeting\nbut look you know what I said earlier\nwas was genuine and and real like we are\nwell aware that that we will only\nsucceed if we go and and develop very\nclear Partnerships with other open\nsource projects and other projects in\nthe world who solve the problems that we\ndon't um and so for us the more that we\ncan highlight flight as a solution for\ncustomers that we're talking to either\nour customers or your customers where\nwe're like hey you know oh you have this\ncomplex workflow that you want to use\nsome Bak but you also want to use these\nother things here's a wonderful\nstraightforward way to look at at your\noverall solution your overall problem\nwith a a joint uh solution and the more\nthat we can highlight that the happier\nwe\nare cool awesome cool thanks\nDavid I uh is there any other\nquestion well in the meantime I do have\none I I noticed that the architecture is\nbasically a\nGlobal API endpoint I believe and there\nare kind of local schedulers that yep oh\nsorry let me this here yep I'm just\ncurious let's say that the the back end\nfor one specific environment is is cetes\nitself so and and also you have flight\nso potentially you will have three\nschedulers yeah yeah yeah absolutely so\nyeah I don't know how has been your\nexperience on these kind of multi\nscheduling environments well so that's a\nvery interesting point um uh we have had\nlimited experience to date and the\nreason is is because in our one one we\njust announced a new feature here um\nwhich you can come and read about and uh\nactually uh we had a uh office hours a\nfew weeks ago that talked about it was\nthe wrong blog post uh\nhere um so the feature you're going to\nwant is called a pluggable executor and\nwhat this allows for is any arbitrary\nengine just anything that's a binary on\nthat node itself you can then execute\nyou can run a script you could run you\nknow whatever Cube control flight\ncontrol like take your pick um so we\nhaven't um uh dug into that too deeply\nyet um um but that's not because we're\nnot really excited about this because\nyou're exactly right our theory is that\nthese aren't going to be just nodes this\nis going to be a kubernetes endpoint or\na SAS endpoint or a flight endpoint or\nwhatever it might be and this will\nschedule to a local scheduler inside a\nsingle zone and then inside that zone\nyou know we're going to communicate back\nand forth with whatever's local uh and\nthat could be like I said a trivial VM\nuh or it could be a very rich you know\norchestration system um either one works\nfor us and we would love to you know\nexplore um use cases and things like\nthat I'm sure that you know uh uh you've\nseen many customers who have you know 10\ndifferent flight deployments based on\nthe region like wouldn't it be nice to\nhave a way to layer over the top of\nthose so someone could schedule just\nbased on a um a label um you know and\nthat's something we very very easily\ncould\ndo okay thank you\nDAV any other\nquestion okay well like I said please\ncome join us um uh you know we we would\nlove to um you know make this richer\ndeeper you know I can't stress enough uh\nhow many um uh workflow use cases we\nkeep hear it popping up uh which is why\nwe we had to go and work on this um uh\nyou know we work we've we've uh\ndeveloped two different workflow plugins\nand are probably gonna have more uh\nwe're gonna we have a kubernetes crd in\nthe pipeline uh that that people are\nworking on and other things like that we\nthink there are lots of ways of doing\nthis uh the easiest way to get in touch\nwith us is uh on on slack you can go to\nour GitHub repo and so on uh but uh\nthank you so much for the time and and\nopportunity to come speak and uh please\nlet me know how I can do I'm a huge open\nsource fan and and want to help\neveryone thank you David thank you for\ncoming yeah it's really interesting\nwork all right um great I don't see\nanything else in the agenda is there any\nother question topic some of you want to\ndiscuss yeah Troy I\nknow yeah that's\nclaps\ngood all right with that thank you all\nfor joining and hope to see you in the\nnext one have a good\nday thanks everyone bye bye thank you so\nmuch thank you thank\nyou"
    },
    {
        "title": "Running pipelines on spot compute with Flyte and MMCloud - Flyte Community Meeting - Oct 3, 2023",
        "transcript": "um really great to be presenting to such\na vibrant uh open source Community um\nand I'm really excited about some of the\nthings that uh that we're doing uh in\nPartnership my name is Jen sha I'm the\num I'm the cloud go to market and\nPartnerships leader for mge and I'll be\none of your presenters today uh I'll\npass it over to H so that he can\nintroduce himself as\nwell hi uh my name is CH so uh I'm a\nsenior solution engineer working for uh\nwe rich and I you know uh we work very\nclosely with this community and uh\ndeveloper this uh new uh m m c you know\nplin and really love the job and want to\nsee this uh take off\nthanks yeah we have several uh members\nuh in this uh meeting as well so Frank\nYT and heren can uh pass to you know\nFrank Maybe\nhi everyone H this is Franco I'm the\nmanager of support at manver and I'm\nvery glad to participate and uh yeah\nthank\nyou and uh this is YT L I'm the product\nmanager from M uh based in California um\nexciting to join this community and I'll\npass over to Halen oh hi my name is\nHelen and I'm working in mver as a\nsoftware engineer intern so I'll be\ncontinuing doing this fly plugin um yes\nnice to meet\n[Music]\neveryone great um David should I kick\nthings\noff I would say yes sorry yeah for\nsure okay great so um what we what we\nprepared today is a a few quick slides\njust to give everyone the background um\non our company um our product and how\nit's uh integrating with flight and then\num we'll keep the second half of the\npresentation um as a demon as a\ndemonstration uh segment where we have\nboth a recording as well as um some live\nuh show and tell that that we prepared\nand uh please feel free to um ask\nquestions and um yeah just chime in uh\nat any given point if you've got um\nthings that you want to double click on\nwhen make this as as interactive as\npossible so so with that um I just want\nto start with um basically uh you know\nwhy why do we exist and what are some of\nthe problems that we're aiming to\naddress here at mverge and the first is\nreally um a skill shortage which is uh\nmultifaceted when it comes to uh cloud\ncomputing um there's all these different\nuh tools and Technologies to learn um\nthat that can be both um used in the\ncloud as well as on Prem and one of the\nthings that we find is that um most uh\nEnterprise and also um especially we see\nthis in the uh Research Institute and\nhigher ed uh organizations is there's\njust a general lack of deep uh Cloud\nengineering and Cloud devops skills and\nthat's one of the areas that um that\nwe're focused on as a company and\nbecause of that um uh because that kind\nof lack of uh deep Cloud skills\num as people move to the cloud for all\nthe different benefits that it brings um\nit's also really really easy to um both\nuh be sub-optimized and also to\noverspend so then on the right um what\nwe're also trying to address is one um\nat the same time helping uh you know\ndevelopers researchers data scientists\nbe able to take advantage of uh Cloud\ncompute resources more easily and at the\nsame time help them really quickly take\nadvantage of ways to be cost optimized\nand also performance optimized when\nthey're starting to use the cloud and\nthe way that we do all of that is\nthrough software that we've built that\nhelps to automate different parts of\nusing the cloud and one of the really\nnice um intersections that we've\ndiscovered uh as a product and as a\ncompany is that um one of the areas we\ncan help the most is to help um automate\nas an extension of uh workflow managers\nuh like flight and so you can think\nabout memory machine Cloud which is the\nname of our product as a tool and also a\nautomation platform in the cloud for um\ncontainerized\napplications um that you're trying to\nrun and um one of the things that we\ndistinctly um help you do uh very\nquickly is to start using using spot\ncompute in different parts of your\npipelines your workflows and even just\nindividual\napplications um and we all know that\nwhen you start using the cloud there's\nseveral hundred different Computing\nchoices just you know just at the\ncompute layer alone each Cloud offers in\nthe hundreds of compute you know sizes U\nmachine types and um you know kind of\nfit for purpose like compute optimize\nversus memory optimized versus General\ncompute you have so many choices and one\nof the things that we also do is to help\nyou quickly take advantage of the right\num compute uh options um on the clouds\nand so we started on\nAWS um we also now support gcp and then\nin Asia uh we support Ali cloud and by\nCloud so um who's our typical end users\nuh these would include uh developers\ndata scientists and also um we found a\nparticularly strong um Niche with\nresearchers both in the um university\nhigher ed um independent Research\nInstitute as well as in in some of the\nbiofarma spaces and um the kind of the\ntwo to three killer use cases is\nautomating uh data pipelines which many\ncases for us means just a chain of\ndifferent uh batch Computing steps or\njobs that can get that can sometimes\ntake um hours to even days because of\njust how much data or how\ncomputationally intensive they are and\nalso um what we um are really great for\nas well is uh interactive Computing\napplications so even just something as\nas simple as a Jupiter or our studio\ninstance or um different programs that\nyou've written in python or r that you\nwant to run on a regular basis or on a\ncontinuously running\nbasis um so as an extension now with our\nplugin uh for flight um we offer a set\nof uh capabilities that you know before\nuh you would have had to essentially\nbuild your own stack to go and do uh the\nfirst is we have something called float\nwhich is a a way to automate the spin up\nand spin down provisioning\nde-provisioning and monitoring of the\ncloud infrastructure so you can think of\nfloat as something that goes out and\nusing AWS as an example it quickly goes\nand grabs all the compute that you need\nfor a particular\nflight workload uh that you want to run\nand then it monitors those workloads and\ncompletes them and then gives those\nresources back to the uh cloud provider\num and it can do that across a range of\nuh cloud computing options and it can do\nthat across different pricing um uh\ntiers as well so for example on demand\ncompute versus spot\ncompute um once we are provisioning and\nmonitoring those workloads we can do two\ntwo uh really actually three really\ninteresting things um the first of which\nis we can spin up uh spot spot compute\nand we can monitor and automatically\ncheckpoint recover and migrate those\nworkloads um if and when the cloud\nprovider is taking that spot VM back and\nwe all know that um one of the benefits\nof using spot compute is that you can\nsave up to in some in some cases 80 to\n90% uh versus the on demand price for\nthe exact same machine but then the\ndrawback with each of the cloud\nproviders is that you get between um\nanywhere from 30 seconds as a pertains\nto Google cloud and Azure and and ads is\na little bit more generous they give you\ntwo whole minutes to basically take care\nof your workload before they are able to\njust um take it away even if you're not\ndone and so spot Surfer is a really nice\nway to not have to start your batch jobs\nor rebuild your environments um in the\nevent that the cloud provider takes the\nVM away before your workload is complete\num we have a way of doing this so that\nthe memory state of the workload is\ncaptured and then automatically um move\nuh to another uh machine that we're\nspinning up for you with the float\ncapability and then wave riter is a way\nfor um especially for uh users who are\nbringing workloads from an on- premise\nenvironment into the cloud to really\nquickly find the right set of uh cloud\ncomputing\nVMS um so that you're not overspending\nby over-provisioning and that you're not\num underprovision and then you know run\ninto performance issues or even just um\nyou know all altogether run out of\nmemory and kind of you know fail a\nworkload and have to restart because of\nthat so Wave Rider takes the same um\ncheckpoint restore migration service and\nit repurposes it so that it's um it's\nfocused on moving your workload to the\nright Siz VM either larger or smaller\nand then wave riter is basically a way\nto um continuously monitor all of those\ndifferent uh flight um tasks that are\nbeing executed by the uh memory machine\ncloud\nservice so um I'll pause there just to\nsee if there's any questions before um\nbefore moving into our\nintegration okay um not sure if I yeah\nnot sure if there was any questions but\nI'll I'll keep just feel free to chime\nin if uh if you do I might have this is\nmar I'm with the Union I just have one\nquestion for for getting onto the memory\nmachine Cloud uh is there is there a\nfree tier to to try this stuff and for\npeople watching this yes yeah there's a\nthere's a free tier um that we offer uh\nand we'll be updating the the details\nthis month but essentially how it works\nis you get 500 free uh core hours uh\neach and every month and you get all of\nthese features so you don't have to um\nyou know upgrade to get something like\nspot Surfer or Wave Rider you get to see\nthe entire platform um and you get to um\nyou know run run different uh fairly uh\na fairly uh good amount of work um\nbefore you kind of hit that um 500 uh\ncore hours\nthreshold and\num yeah and then we're we're also you\nknow obviously um interested in working\nwith different uh folks who may need to\ntest uh larger workloads as well on the\nplatform fantastic we we provide a free\ntrial as well of the of the other\ntiers okay um great so how exactly does\nit work with flight um for this part of\nthe presentation I'm going to transition\nit over to we uh and feel free to feel\nfree to take it away yeah thanks J so\nyeah the uh memory machine Cloud uh\npling for flight uh as Jin said allows\nthe flight users to uh configure mm\nCloud as the computer environment for\nfor the execution of flight workflows\nright so the with mm clouds as plugin uh\nplugin at the uh computer environment\nthe flight user can choose uh leverage\nuh mm Cloud features uh uh for example\nchoose uh R the worklow on on demand or\non spot instances also can choose like a\nwave Watcher to uh check uh how big the\nresources required to run a work throw\nto avoid over provisioning or under\nprovisioning right so how does it work\nso basically thanks to the uh you know\nbeautiful framework uh EST established\nby fright so we develop a agent called\nmm Crow agent that allows the user to uh\nsubmit a job on mm Cloud St flight like\nuh uh use the commands like a p flight\nwrong so the mm Cloud agent we call MM\nCloud task so the task is just the S\nsimple form of the job running on the mm\nCloud so the mm Cloud clask will uh take\nthe configuration from the mm Cloud\nconfig which configs how the mm Cloud\nclask can be executed on the mm cloud\nso and we have some like uh extra uh\noptions to allow the users you know to\nconfig some like a migration policy a\nWim policy or the resource requirements\nlike a Max CPUs max memory these kind of\nthings so uh eventually uh the task will\ngo to a Flo submit uh like we uh just\ntalked about this will submit job to the\nmm CLS and we have a lot of uh uh\noptions to can specify uh the the job\nhow how this will be wrong so uh with\nthat said uh we are going to run a demo\nhere so what I'm going to do here we uh\nrecord a demo uh in a short like a very\nshort video uh we are going to play LS\nthen then I can run a a live demo and\nthen we show you how this works and\nwe'll show you how the Ops uh our mm uh\nCloud uh can help to provide insight to\nthe jobs also can uh you know uh just\nadjust the the resource utilization uh\nbased on our features so let me see uh\ncan I be can I be like share something\nhere yeah I should be able to share I\njust took the slides down okay so hold\nup hi all I'm going to quickly\ndemonstrate the MERS memory machine can\nyou guys hear the sound or everything\nokay yeah yeah we can hear the sound uh\nand then we the the screen I see is um\nis on the left is the memory machine\nCloud uh op Center login screen and then\non the right uh there's a there's\nanother side so it's a like a two-sided\nscreen is that what you need uh yeah so\nyou did you see that uh uh video\nhere no in Cloud plug-in for flight\nfirst can can you see that the video no\nyes oh yes okay I think that's I think I\nthink the two screens is what Jing was\nsaying is the video that you're showing\nmaybe yeah yeah yeah yeah you want to\nset up memory machine Cloud also known\nas m Cloud as well as a flight hi all\nI'm going to quickly demonstrate the\nmverge memory machine Cloud plug-in for\nflight first you want to set up memory\nmachine Cloud also known as mm Cloud as\nwell as a flight deployment instructions\nfor how to set up mm Cloud can be found\nat docs. mge.com before we log into MM\nCloud we can quickly grab the float CLI\nbinary which we will need later for our\nagent deployment we can interact\ndirectly with the op Center via the web\nCLI but this is not useful for\nflight we can also get the float binary\nafter logging\nin here we can see that we have no\nactive\njobs to use mm cloud with flight install\nflight kit plugins mm Cloud using pip I\nalready have it\ninstalled also copy the flow binary\nmentioned earlier to your agent\ndeployment I have already done this this\nI am using a sandblock deployment so I\nwill be running the agent service\nlocally this is the workflow we will be\nsubmitting to\nflight simply specify task config in\neach task\ndecorator it is based on the example\nworkflow from flights getting started\npage we can register and run the\nworkflow using pight run registering the\nworkflow can also be done using pight\nregister or P flight package and flight\nCTL\nregister we can specify the image to use\nin the packaging or registration\ncommand alternatively we can specify the\ncontainer image for each task image spec\ncan be used to build images\nrefresh the jobs tab in op center to see\nour submitted tasks let's wait a bit for\na task to\ncomplete click on the job to see its\ndetails here we can see the script that\nwas submitted to the op Center we can\nalso find the standard error and the\nstandard output of the job\nhere\nadditionally there's a log of events so\nthat you can know what's going on when\nyour task is running wait for our\nworkflow to\nfinish we can look at the S3 bucket\ncontents to verify that our workflow has\ncompleted\nsuccessfully okay uh that's it so that's\nthe demo video and now I'm going to uh\nshow you guys live demo to how to do\nthat so give me one seconds let me share\nmy\nscreen okay uh hold on how do I\nget\nI oh can you guys see my\nscreen yeah it this is one so uh yeah\nthis is actually the uh demo the example\nwe took from Fright uh the website uh\nwhich is like uh I think uh get the\nclassic Y data set use the SK Len under\nuh in this uh workflow there's three\nsteps the first step is get the data uh\nsecond uh step is process data and the\nlast one is like building the training\nmodel right so this is the example we\nrunning so we need uh basically uh we\nstart the\nuh flight like a agent service on the\nlocal Port which are already started and\nwe also need uh uh you know the apply\nthe uh configuration uh about the mm\nCloud uh adjust uh usern name password\nand also the some options uh like a CPU\nuh uh you uh requirements memory\nrequirement all these kind of things in\nthat you know the mm Cloud configuration\nso basically I already start that so\neverything start so to run the job we\njust use\npy uh run like choose what image you\nwant to draun and then P python example\nfiles and how to run that so with that\nrunning okay the job is running\nsuccessfully and now we go back to you\nknow basically our uh mm\nclouds to check that you can see the job\nis passed to our mm cloud and uh one the\nfirst step is uh started and we are\ninitializing the job here so basically\nif you go to uh fright project you know\nthe the dashboard you can see there's a\nnew project created which is running\nthis example and uh uh you can see the\nfirst uh first job is running the first\nnote uh and if you check the process you\ncan see there this is a uh workflow the\nthree steps get data process data and\nthe uh chaining model right so basically\nnow is uh the first step is draun and\nyou can see uh you can\ncheck inputs outputs uh from here uh it\ntakes some time uh for us to get you\nknow the instance uh from\nAWS uh or any like a uh cloud service\nprovider uh so that's take about two or\nthree minutes uh to get the first step\ndown uh for our op Cent you can check uh\nthe logs to see how this uh you know the\nthe the job is the job status right so\nyou you can see we pick up the spot\ninstance from bch of the uh the\ncandidates so if you go to like\nuh uh uh if you uh before this it take\nsome time so actually I uh this morning\nI already wrun uh this uh demo uh you\nknow the successfully so you can\nactually check the result from the\nprevious runs so you can uh uh find this\nuh from the right side as well you can\nsee this is the the second one it's the\nwrong I uh completed this morning you\ncan check the time is\n8:41 this morning and then you can see\nfrom here is 841 so this is the first\njob and you can see this job is running\non a spot instance because is very small\njob so it's running a very I just SK a\nvery small like CPU memory so it's micro\nyou know instance right it's on a spot\ninstance so and from the W watch here\nyou can see this one you can see a very\nlike how many memory is used uh how many\nlike CPU how the CPU is used and the\nnetwork if you have like a external\ndiscs uh we are show the performance and\nthe utilization of discs as well so and\nuh uh you know this can be wrong on the\nspot instance and uh if uh doing in a\nspot instance running and if there's a\ndeclaim happens then we can uh use our\nlike uh technology to float like job to\nanother spot instance right so similaris\nif the spot instance is not available at\nthe moment then you have the choice to\nchoose to run us on demand as well to\nmake sure the job is\ncomplete so to sub me the job uh talk\nabout something in the uh job here so\nyou can see uh uh this is same thing as\nwe Le at from command l so command is\nbuild in with our agent but uh I just\nwant to quickly show you guys uh what\nkind of like option can pass to a job\nright so you have like a you can choose\nthe image here which is we talk about\nyou can either from like a public uh\nrepository or you put in your uh private\nuh repository and you can uh choose like\nuh you uh you build the image yourself\nor you can choose something you can get\nuh from uh like in a community so then\nyou you can put a n here and the job\nscript uh uh or you know uh something\nthen uh good thing about here you can\nsee for the instances you can put like\nCPU or memory type or instance type you\ncan choose the specific instance to run\nyour job right so you can defy your\nstorage uh configuration and also you\ncan uh we talk about this like uh you\ncan choose uh spot first or spot only or\nyou choose the running on the only match\nright so another thing gin talk about is\nwave writing uh we call that migration\nso can you can enable uh to do the\nmigration manually or automatically\nright manually basically I'll show you\nan example a little bit later how you\ncan do that manually to make sure your\nuh resource allocation is a uh defined\nuh well for the jobs right so here is\nlike a if you enable auto uh migration\nthen it can based on CPU uh utilization\nlike a like up bound uh like 90% if the\nCPU usage is reached 90% then you can\nFloats or low bounds like if you like\nthe CPU utilization is very low then you\ncan float to uh low lower uh instance to\nsave the cost right so also you can\nbased on the memory but is memory usage\nand uh you can choose how do you want to\nmigrate like every time you jump like\n50% or 100% so this is a good feature uh\nJin talk about with uh the uh wave\nwriting right so uh now let's go back to\nthe job here so you can\n[Music]\nsee the first one the first step is\nalready succeeded so you can come to\nhere you can see the first one is\nsucceeded and the second one is just\nstarted so if you go to uh my uh you\nknow the AWS account you can see uh a\nec2 instance is started for the second\nsecond step right so you can click here\nyou can see like uh the information here\nand also uh for the first job is\ncompleted you can go to this uh uh uh\nsomething like a here you can see uh the\njob it's like\na this is the like a some like a results\nfile is created here right so and also\nuh while the job is running so uh I can\nquickly show you something uh like a\nspot server feature uh let me trying to\nfind a job for you guys to\nsee uh how we can quickly show you\nlet me search a\njob so you can see this job is showing\nlike how sport Surfer uh Works uh in mm\nCloud uh this one actually is R like uh\nyou know the I'm not sure if we have\nsome like U bioscientist here this use\nlike a star uh index the application so\nwhen you trying to run like uh uh star\ninex application uh we starting the job\non spot instance right so then uh sport\ninstance get requin we float to another\nsport instances and then uh it get uh\nreclaim again and finally uh we we throw\nto on demand and make the job completed\nso this is like a if you check the job\nevents here so if you\nsearch for spot you can see clearly\nthere are two spot in inter Interruption\nhappened and first time uh is happened\non the spot instance and the float to a\nspot instance and uh uh not not very\nsoon and we get the second spot\nInterruption and now now we flow to the\non demand but the job itself you can see\nis a complete successfully there's no\nissue even for this kind of like a you\nknow uh large instance here like a 16\ncore 128 gig bytes instance the job\nstill FL successfully so the job is\nrunning very well this is a uh how you\nknow uh our spot surf you know works\nright so another thing uh I want to show\nyou guys is like uh we can see\nuh maybe we can check for how the wave\nwriting\nuh is there a space on theend at the end\nhuh oh maybe space oh oh yeah yeah yeah\nI see\ns so you can see this job right is the\non uh uh we learning on the uh Cal\nworkflow like you can see uh at the\nbeginning uh the the actually the memory\nuh required for the job is very low so\nwe start with the small instances right\nso when go to the second job and the\nthat process requires much larger like a\nmemory and so we go to uh allocate a\nlarge instance for large job and the\nsame for the uh thir third process you\ncan see uh because different process\nthey require different resources so uh\nif you use for the without mm CS if you\nwant to run a job uh success\nsuccessfully uh from the beginning to\nthe end you either you will over\nprovision your uh uh resources from the\njob right so basically you have to take\nthe uh L largest requirement and the\nlocated the instance uh largest instance\nto complete the whole job that's will be\nvery costy and uh very expens bive and\nobviously it's over provision it and uh\nyou spend not NE unnecessary unnecessary\nYou Know M for that or if you AR\nprovision instance you use like low\nmemory you might when you go to the\nlarge instances the large job uh you\nmight hit the a of memory ero and the\njob will fail so but with mm Cloud\nobviously we can allocate different\nresources for different jobs and you can\nsee this is kind of like very\nsuccessfully WR first job only require\ntwo core and the 4GB right memory for\nthe job but second one is like 32 core\nand 64 GB and the third job requires 48\ncore and 966 GB so like a you know like\nI said without mm card you probably you\nhave to uh Pro provision a 48 core\n96gb the watching machine to complete\nthe whole job which is obviously uh you\nknow uh very expensive so C we save the\nmoney we also save the money to run this\non spot instance so that's a good\nexample to show this uh\nthing uh so I think the job is complete\nhere so let's go back to uh our demo and\nyou can\nsee\nuh so you can see uh all three steps\nexcuse me uh completed successfully and\nif you go to the graph here you can see\neverything like a get data process data\nand the train model or ended\nsuccessfully so and uh like I said you\ncan uh go to your S3 package to get your\nresults so I think uh uh I'm going to\nstop here and uh uh this is the demo I'm\ndoing I did and I you know Jin I passed\nback to you to see what else do we have\nmost ni sure I sure I see a um I see a\nraised hand so maybe um if there's\nanyone who wants to ask a question Let's\nuh let's take those\nyeah K yeah hey um the great demo a\nquick question so do you allow reuse of\na machine or are you bringing up\nmachines just\ndynamically just like dinner yeah we\nbring up the instance for the job when\nthe job is completed right we just get\nrid of the instance so uh but you you\nyou have no choice right if you want to\nrun that uh uh like uh keep using the\nsame machine then probably you you can\nlike oh actually we have the option to\nsuspend the machine so then you can\nreuse that let me quickly show you I\nforget that that's good okay all right\nlet me let me get off the screen share\nagain yeah let's quickly show you uh\nkayen so uh we have the this really good\nfeature and I think I I have to show\nthis one second so uh I have another job\nring on a different uh uh off center\nthis is actually off center only on UC\ngcp so with not only support AWS we\nsupport the gcp and we support some\ncloud in China cloud service provider\nlike Al Cloud so this is a multiple uh\nclouds uh platform we actually uh we\nhope in the future we can you know uh\nutilize the resource uh across different\nclouds but not yet so here I have like a\njob Starling this is my job free job you\nsee here this is actually the jupyter\nserver so I suspended that job for uh\nsome time you know then if I want to uh\nhere is the jupit server uh we bring up\nuh in the a in the\nAWS uh uh in the gcp sorry right so here\nso I suspended this job so that means if\nyou don't want use the instances uh in\nthe like for example overnight over\nweekend you want to like uh people take\nrest and don't use the duy the server\nthen you can start spending that job and\nuh basically without spending money\nhowever when you come back to work on M\nyou can assume that server and everybody\ncan start to use that so we there's two\nways there's one way we have this uh\nbuild in uh stuff here so we can\nuh lock into our Cloud uh\nCloud op Center and then you see the\nstatus is suspended you can resume the\njob from here you can also go back to\nthis job uh list and you can resume job\nfrom here as well either way it's okay\nso you if you like you resume this job\nbasically Kon is not used at the spot\ninstances we create a new spot instance\nto continue our job but our technology\nwill keep the state of that instances so\nthe the the old state will be similarly\nmigrate to the new instance in that way\nyou don't lost uh lose the previous job\nand you can continue you know running\nthe the server so which is a a really\ngood feature uh\nhere so you can see it's resuming and\nit's waiting for\nsometime it this will take a couple\nminutes so but but it's just to you know\nshow you guys how this work yeah like I\njust had a quick question about the\nintegration point with flight like so\nwhen you when you started that job uh it\nyou started it off really small and then\nit increased the resources as it went um\ndoes your agent take the like resolved\nparameters that the user specifies in\nlike a flight decorator that says I want\nthis much memory and this much CPU and\nthen like I noticed you had a list of\ndifferent instance types ranging from\nlike a C5 to like a you know a t3a micro\nor something uh is that like\npredetermined or do you guys like\nrespect the resource request from flight\nwhen you generate that short list of\npotential instances to put the job on\nyeah there are two ways uh to do that as\nI said right a manual way there's a\nautomate way right so if like a the\nusually if you know uh how much uh you\nknow the resources are required to run\nyour job you can pre defy that you can\npass that parameter use some like a Das\nC is for CPU dasm for memory you can\npass those parameters to your job so\nwhen we launch the instance in the cloud\nwe will uh launch the instance based on\nyour parameters this is one way the\nsecond way is like we can enable auto\nMigration by Auto migration we will\ndetect the real utilization of the\nresources if the result like the\ninstance is too small to run your job\nbut we can automatically to large\ninstance so yeah cool I have a couple of\nquestions but sorry we don't have a hand\nto raise here uh in the meeting\nroom um the how does security work like\nhow do you uh run with the I don't know\nthat's a service account or IM AMR if\nyou're running on ews how is that picked\nand like how do how do users Define that\nwho has access to pass on that role to\nyou know the job that\nruns we B we run based on I right uh I\nthink a friend maybe can help me yeah\nyou can Define Security Group and uh the\nrules and we obey all the rules defined\nfor the am Cloud instance\nand when it spin up a worker node it\nwill inherent the the policy defined um\nby the for the mm cloud and and you can\nspecify like which um availability Zone\nand which subnet want to um spin up the\nworker note so those all within the AWS\num uh\ndefinitions so when you start the job\nlike when the agent in the flight agent\nstarts the job for a task it should pass\nin U all the\nsecurity like um identities it need is\nthat the idea I don't know if it's\npassing that or not but that's the\nidea um so there yeah there are some\nparameters yeah you can specify uh but\nmostly it will\ninherent the security group we defined\nfor the uh M Cloud instance so oh so\nthere's just one role for the cloud\ninstance that all the jobs will run as\nby default that's the model um yeah by\ndefault it is uh and also depend on what\nkind of a uh job you you'll be running\nlike if it's like our studio if it's are\nlike some interactive the um uh tasks\nthen you can specify additional Security\nGroup um and to allow those you know\nsecurity firewall rules to be applied on\nthe new worker node so there are like a\ntoo okay so there's some configuration\nin the Operation Center we say if this\nif the job type is this then apply these\nsecurity groups okay I see yeah we also\nwe are consider to enhance that uh even\nbased on like a different uh like\nworkload and you know the the op Center\nthough separate this two but that's we\nprobably will do that in the future I\nsee we have this concept of the admin uh\nuh group managers and individual users\nuh um that can all access what you see\nhere which is called the op Center and\nthe memory machine op Center um would\nrun inside the uh the flight users uh in\nthis case AWS account within their VPC\nand then all the worker nodes that are\nspun up also uh you know reside within\nthis this\nnetwork um and hopefully that helps to\nkind of you know um uh better describe\nyou know where we're running and also\nthe security considerations based on\nthat yeah yeah\num go ahead yeah and then and then you\nknow the the interaction point with the\nflight agent is through the um the um uh\nthrough the CLI uh or you know or\ntechnically we have we have an API\ninterface as well so from there you know\nwe can launch um the different uh worker\nnotes based on the different VM policies\num and parameters that are\nspecified I\nsee um cool uh couple more questions\nsorry um the I think it will be cool I\ndon't think the agents support that yet\nuh if we specify if if like in the\nflight UI there's a link that takes you\nto the job in mm Cloud um and vice versa\num there is capability in flight but I\ndon't think agents uh allow you to\ncustomize that just yet maybe something\nwe can work on\ntogether I I noticed you were matching\non like time stamp to find a job that\nwas launched for a task uh and on a you\nknow large deployment that will be\nrather hard oh so links will be nice um\ncan you I don't know how how propriatary\nlast question sorry I'll uh I leave the\nmic after um the can you speak more\nabout how the the tech behind the like\ncapturing memory and moving it work like\nwhere do you store the state or is that\nall\nproprietary\nuh you mean you mean uh how to we do\nthis like uh uh uh protein or you know\nlike\na the migration yeah piece like how does\nthat work how does that take behind like\nbehind the scenes kind of thing how does\nit work oh we we have something called\nApple capsule uh we have some like\nunique technolog so basically we are\ntake the snapshot of the memory and keep\nthe system state of the job and trying\nto when you throw in then we kind of\nlike a not only like the uh storage\npiece also the memory piece to make sure\nthe state is kind of like it's is the\nsame and it is stored in like S3 or\nsomething when you suspend the job uh we\nactually create the uh local dat warant\nand we stall that state in uh we we save\nin the local War yeah if you go to like\nuh the uh any instance you can see we\ncreate a exture like a warant we call\nFlo data so we put the the state there\nsorry where is that stored in in the\ncase of AWS it would be like an EBS\nvolume that the yeah ABS yes that is\nautomatically attached to all worker\nnodes um so they can they can spin up\nany of the existing suspended jobs okay\nthat's right that's right yeah I see\nokay thank\nyou oh there's one more question from\nyou jeeve g hey sorry yeah thanks um my\nquestion was more around uh you know\nyour your ability to support things like\ngang scheduling and stuff even know\npeople are considering spinning up like\nRay clusters or you know elastic like\ntorch clusters and whatnot is that is\nthat currently a supported use case or\non the road map or\nsomething uh excuse me is what what the\ncluster I did not get get get you last\nyou mean uh uh youas cetes from the\nthat's no no so like if if I want to run\na job and the job is a ray job or you\nknow some you know like some distributed\ntraining job for instance yeah and I\nwant something like you know five\ninstances or five nodes to be up before\nI can run my job yes like that typically\nyou know requires things like gang\nscheduling and stuff is that something\nthat like mm Cloud currently supports or\nis it just like a single node for job\ntype situation uh we don't do it by\nourselves right we live to the like I I\nassume that's for example the workflow\nmanager right I leave this to Fright or\nnext flow or you know this kind of\nthings we don't take care we kind of\nlike a bunch job uh launcher so we\nwhatever the manager pass the job to us\nwe launch in the cloud and make sure\nit's running on we can run on spot\ninstances or on demands we make sure the\njob is completed\nsuccessfully regarding the sequential\nyou know this kind of order of the jobs\nwe leave this to you know we we don't\nmanage that by\nourselves\ngot it thank you\nyeah maybe maybe real\nquick oh okay I just wanted to say just\none thing adding to uh que's response\nwas we have a project with Ray um it's\nfocused on uh solving a slightly\ndifferent set of issues um but\nessentially you know it would uh to work\nuh where you can launch like a ray\ncluster and then have some of these\nbenefits of memory machine Cloud I think\nit requires a little bit of\ndisassembling some of the layers that we\nbuilt um which we we do have a uh a\nversion of that running for kubernetes\nso um it's not it's not as tight of a\nit's not this tightly you know uh more\nopinionated um way of using us but it is\nsomething that we can support um so we\njust encourage people to reach out if if\nthat's of\nInterest got it got it and so so I think\nthat false and you know the same answer\nwould be for work workflow preemption on\na ray cluster that you leave this up to\nthe orchestrator to figure out what's\nhigher priority lower priority is that\ncorrect\nyes so we would have to you know this\nmight this would be like a maybe it's a\nseparate project that we we look at\nworking with the ray Community um to\nenable Okay\nthanks that's fine you can yeah I also\nhad a question I I see that for every\ntask users had to Define an mm Cloud\nconfig task config uh what's that\nexactly what do they need to put in the\nconfig uh let me show you quickly show\nyou some like uh uh share some\nconfiguration here oh so yeah DAV some\nof it is also for flight to identify\nthat this job has to be run on MM Cloud\nright yeah FL has some need of\nidentification exactly so you you need a\nlike a for example you need a this is\nthe one example fire in the yamama fire\nso you need to enable the agent service\nright so what task mm CR task we call is\nAgent service and here in the plugins\nyou define the agent service like what's\nyou know what's the end point obviously\nuh we are running in the Local Host so\nwe use this like a 127.0.0.1\niio uh uh object storage build in you\ncan use that uh uh for the simple job\nbut for the large job you probably you\nwant to uh use your own uh\nS3 uh like uh you know the storage like\nyou you see here we we config some like\na uh storage uh there\nyeah so this is a uh configuration you\nneed to pass to your uh ml coud uh uh we\nwe have another thing we can config uh I\nthink that's with Dev we can pass some\nlike CPU requirement or you know the the\nmemory requirement but in this example\nwe did not use\nthat thank you\nyeah right awesome I think you have any\nother content to share\num so I just want to say that um the\nslides that we presented uh along with\nprobably the last slide which has some\nsome useful links for people um we're\nhappy to share that out uh you know\nwe'll take your lead David on the best\nway to send it out um maybe we can email\nit to like all the a all the registered\nattendees or some other list surf that\nyou have but this is really this is\nreally the last slide so we just wanted\nto um highlight the um the architecture\nof how we work um once flight has sent\nus uh tasks um and then you know if you\nwant to get started if you want to read\nmore uh get access to the docs these are\nthese are the\nlinks oh that was great thank you so\nmuch and sorry for overwhelming you with\nquestions that's great that's good yeah\nquestion\nyeah awesome all right yeah we are Pas\ntime anything else any other\nquestion thanks for having us on this\nthis time no thanks again ye and the\nrest of the members team it was great\nand we hope to keep collaborating and\nthank you all for joining yeah thanks\neverybody thank you thanks everyone yeah\nyeah yeah bye okay\nbye"
    },
    {
        "title": "Flyte Contributors Meetup - Sep.28, 2023",
        "transcript": "oh welcome everyone the flights\ncontributor Meetup\ntoday September 28th and\num well we have a kind of short agenda\nfor today the meeting's been recorded\nand it falls under Linux Foundation\nSchool of conduct\nso um I'll show you my screen\nuh\nright\ngreat uh yeah I think this list totally\noptional and we know each other but just\nfor the record anyone this is last\nweek's\nyou should make a copy\noh wait you already did sorry yeah\nI I need to push the notes and remove\nthis from here\num yeah\ncool so I don't see anyone joining For\nthe First Time cool uh all right new\nrfcs I didn't see any\nexactly new RFC\nbut I saw that the simplify retry\nBehavior one\nuh has some activity apparently\nfrom the point of view Fabio and and\nthe authors I can remember\nsorry uh it's done implementation is\ndone uh it's open for review ready for\nreview\nso asked\nuh for a review there and uh see if we\ncan move this to done at least we\nconsider this accepted for sure\nnow the question is if the\nimplementation is done well\nuh probably after the reviews\nis there any comment around this one\nthat's looking good I I noticed that um\nfive years handled he handled the uh the\ncomment that that Dan made\npre-bumped about this because this is uh\nit area of confusion so and what have\nyou have you seen any of the stuff\naround\num some folks using like\nlabels and node selectors and this\npotentially conflicting or clashing with\nlike the fourth retry or the whatever\nthe last retry after you switch from\nspot instances and it switches to an\non-demand\nwe\nwe it's maybe a special case that we can\ntalk about later but I think some folks\nhave are using these kind of manual\nlabels to map certain jobs to certain\ninstance types and you could have a\nsituation where there's like a taint on\nan on-demand instance and everything is\nspot by default which might break this\nkind of nice functionality we have where\non the last retry you switch from spot\nto On Demand\num probably not not in scope for this\nconversation but maybe something\nrelevant we could just look at this and\nsee how it if if it extends or if it's\nthis situation\nit is just\num tangentially related like this is\nmore about like flight's notion of what\nwe consider a user a retry versus\num\na system retry\nbut yes definitely but I'll I'll make\none comment if we have people out there\nwho are like\nwe're aware of this this distinction\nthat we made for like the last attempt\nlike at that point\nwe should assume that they know what\nthey're doing yeah fair enough\nand yeah awesome cool and I'm just out\nof a point of curiosity like do we\nsurface the system retry budget\nin like any of the logging or anything\nas a result if\nif not we could like maybe uh\nadd a follow-up request or something\nthat says uh expose the system level\nretry budget when when one is made\nversus like a user level retry\nyeah part of this this RFC John is to\nlike to clarify the behavior there but\nlike it saw a bunch of studies now you\nknow we don't really\nhelp the user understand where the\nbudget is being spent\nand this is what this is about this RFC\n[Music]\nthank you uh all right next up in the\nworking group updates I saw the the for\nthe configurable right working group\nthat once recent update is\nbasically Keaton's proposal for the\nexample and uh it's stealing\nconsideration for the LinkedIn team\nto implement this\nuh that's that's the information I have\nis there any other update around\nwork in this working group\num maybe I can can speak this yeah I\ndon't have anything more uh so lots of\ndude I reference the same parent said\ngiven the the amount of work this is\nLinkedIn is reconsidering whether they\nare they're implementing this\num I think it would be nice to know if\nthey're not because then we can uh value\nmyself could get back on on track with\nthis we kind of they did took over\num and we figured their more engineers\nand that's them but if they're not doing\nit I think it would still be worth\num implementing potentially unless\nsomeone there's any objections\nI think this makes sense um you are in\nthe call do you know any any news from\nByron like I know that he was\nexperiment with this alongside that\nother PR to like make data classes\nbetter\num\nI don't think he's been working on this\nactively\ndo you know\nyeah last time I talked to Pirate he\nsaid like he is very busy he's called\nher but he will try to work on the in\nthe like early next year hopefully\nthe um\nyeah Byron was started working on the\nthe data class that the mallowable path\nthing instead so which was not an RFC\nbut I I\num\nyeah it kind of fell off\nuh yeah\nall right cool thank you I'm not sure\nhow to proceed with this like I I know\nthat like\nthe\nlike the problems with the current\nproposal that you know Byron brought up\nlike I don't think we\nwe updated the text of the RFC to handle\nthem\nyet\nit also feels a bit like we're back in\nthe\nthe design Sage a little um especially\nwith this document which is\num\ncertainly not\nyeah prior to the RF Series so\num but yeah it seems like we\nyou pick it\nat the drawing board a little\nagreed\nbut the\nI think the problem now is that I don't\nthink\nfirewood\nhave time to you know update\ndrfc\nand at the same time like we we wanted\nto make progress on this so I I don't\nknow maybe it might be worse I'm just\nlike\nsyncing with him\nyeah oh sounds good I can I can check in\nwith him and\num if not I think Fabian myself can\ncan take this forward and see how much\nthis differs and whether we need to\nchange the Roc and then take care uh\nyeah I should have a bit more time\nalso this next quarter to implement\nstuff again it's going to be busy lately\nbut now in winter it's usually a bit\neasier\ncool thank you\nthank you Bernard\nall right any ideas in the incubator I\ndidn't see anything so\nyou can skip this one unless you want to\ndiscuss anything here\nwait there was one thing\nwas it you who suggested that we turn\none of the bugs into this narashi\ndiscussion\nmy misery memory things\nyeah probably I suggest that for the GPU\nmachine type\nthing but\nuh yeah\nI guess yeah we need to check again\nbecause there are others are\nuh good candidates example this one\num\nyeah I don't know if we want to\ngo through this ideas probably\nasynchronously and explore which of them\nare good candidates for rfcs because\nthey're a number of good ideas yeah I\nlet's think offline David I think that's\na good idea to like do one pass on this\njust\nright and uh well okay in the open mic\nor questions section status of the\nmonolith or I was going to put the model\nreport there Bernard but thanks for you\nknow\nprompting me so um\nthe current status is we have a Beta\nrelease\nthat was built off the monorail and\ntoday\nI am posting\num\ngreen API in each of the components that\nwill teach\num the\ncreators of PRS that are still open how\nto transform those PRS into like the\nmonorigo priors\nand that guide will work\nregardless you know because\num my plan is a week from now we're just\ngonna Mark all the component uh\nrepossessed redone\nit's trying to be a good citizen there\nbut that's the plan like\num\nyou can start already using it\nif you feel like it okay today that was\nthat was my question because I I rebased\nmy Fork the other day and and you saw a\nbunch of stuff and I saw a bunch of\nstuff popping up uh and I wasn't sure\nlike I wanted to quickly wanted to debug\nsomething and so just reverted back to\nan old commit where where things weren't\nthere\num but if it's if you say it's ready for\ntrying a yeah\nokay now and if you find anything just\nlike paying me on slack like the\nexpectations that it's ready for prime\ntime okay cool thank you\ngreat\nthank you Eduardo\nall right\num anything else you'd like to discuss\nhere\nquestions\nno\nright well that was quick\nuh I would I would have one question\nsorry um we're currently\nhaving an issue with propeller and and\nwith some like notes not filling where\nthey should be failing\num and I've raised an issue and\neverything just to be so this is that\nwe're having on record but I'm happy to\ndebug it\num Dan is also pointed me to places\nwhere where things might be failing\nbut I think I have a more general\nquestion of\ndoes anyone have a good strategy of\ndebugging propeller especially since all\nthe things happen at the same time and\nfor example\nworkflows are evaluated all the time and\nso it's like if you put a breakpoint\nsomewhere it might be that you hit this\nbreakpoint 200 times before the error\npops up\num and especially for exploratory\nbecause I don't have a very good sense\nof all the internals of propeller it's a\nbit hard to\nto get into the right spot so I'm\nwondering if someone has a any tips or\ntechniques that that I could use to\nput there or do you just need to know\nroughly where it is and then\nstart looking\nthat would be ideal huh but Bernard if\nyou have a more\nor a specific question I would\num I would tag Dan you know on like a\nlike a maybe asking for you know initial\npointers or something but oh no yes I I\nI've asked in he is he's been great and\npointed me to the right thing just like\ntwo hours ago it says I haven't had a\nchance to check\num okay okay I was mostly wondering now\nthat I'm you know going off in a\ncompletely wrong direction there's an\neasy way to\ncertificate okay\nyeah the joys of debugging concurrent\ncode you know it's uh it's hard\nyeah let us know how we can we can help\nthis yep\nsometimes bugs are very hard\nyeah\nit's just a complex very complex system\n[Music]\num\nwith lots of lots of knobs and lots of\nlines of code\nthank you very much\nuh all right any other question\ntopic\nno\nwell with that thank you all for your\nhelp thank you for joining and yeah hope\nto see you in the next one\nall right thank you\nbye"
    },
    {
        "title": "Flyte Contributors Meetup - Sep 14, 2023",
        "transcript": "hi David\nhey Dennis\nhow are you\ngreat fine thanks how about you\ndoing well\nthanks for joining and it's a bit late\nfor you\nhey news welcome\nhow's it going\nwell\noh hi guys\nhey Fabio\num you're welcome\nand welcome everyone to the flights\ncontrollers Meetup\nuh it's great to have you all here\nso um yeah regular reminders the\nmeeting's been recorded it will be\nposted to vlad's YouTube channel\nand as with every other Communication\nchannel for this project it falls under\nLinux foundations\ncode of conduct\ngood so let's get started I'm sharing\nagain here the link\nthe agenda notes um\nso far seems like we are kind of short\non agenda so we can cut it early if it's\nneeded\nall right anyone joining for the first\ntime I don't think so I think I know all\nof you\ncool\num new rfcs oh yeah one small thing that\nprobably you will notice once you open\nup the agenda document\nuh now there's an archive for all\nmeeting notes\nuh it's right there reference in the\ndocument in the community repo so after\nevery meeting\nwe'll be uploading updating this archive\nfile and the hacking the you'll only\nfind\nnotes for the last session right so we\ndon't want to have this gigantic hacking\nthe\nfile that anyone can edit it's still\nopen for anyone to edit but we're\nlet's say preserving the archive on the\nrepo cool\num right new RFC is I didn't see any net\nnew RFC uh the simplified retry Behavior\none that wondering if we are in a good\nposition to move this to final command\nperiod or what's the status here\nwhy do you all think any objection to\nmove this proposal to final current\nperiod\nlet's speak a little bit we've got\nGeneral consensus here I think there's a\nfew minor problems uh not minor problems\nbut just like a\nthe only issue I know like Fabio and\nDennis you guys been working uh\nfantastic tests on this and all that I\nmerged I don't know if you saw this\nmorning I merged the interruptible\nbehavior into the pr\num did this started writing up unit\ntests have it basically figured out I\nthink the only thing we need to decide\non right now is right now with the flag\nfor the retry cause ignoring the retry\ncause is set we are using the\ninterruptible failure threshold as a\ncomplementary number\num which I know we had discussed and so\nwhether we just expose that as a\nseparate configuration variable or um I\nhad thought of like if it's negative you\nknow treating it a certain way\num it on my side that's the only point\nof contention right now otherwise I\nthink this is fantastic I think we can\nwe can get emerged relatively soon once\nwe come to some consensus on that\nif we introduce a new variable it would\ndo the same thing just have a different\nname that may be more fitting for the\ninterval right of the negative number\nyeah I don't love introducing another\nvariable I didn't know if just you know\nmaking interruptible Theory to threshold\nnegative one means the last one\num\nwhat do you think\nyeah I think the same like as long as\nit's well documented\num because so far like for some of the\nvalues it was sometimes a bit hard to\nfigure it out like we then dark the\nrather read the source code but I guess\nlike if it's well documented all of that\nis fine yeah I don't think we need\nanother variable\nperfect yeah no I agree I didn't love\nanother variable so I think uh uh maybe\nI can play around with making that\nchange and I'm happy to move this to a\nfinal comment period um and probably\nprobably shortly after if that's uh\nagreeable\nyeah I mean that sounds very good\nperfect and again I just appreciate all\nthe work you guys have been doing on\nthis\num as Shepherds of it I think uh it's a\nlot of people are going to be very happy\nwe appreciate you being our champion\nhere because understand that in the\nbeginning\num but that's an annoying RFC right like\nwe appreciate that you jump on it and\nyeah I helped us push it through\nawesome thank you all\ncool uh yeah not no new RFC so far I\ndon't know if any other comment around\nrsc's\nyeah David I have one quick one\num for the eager mode RFC\nuh the pr for that was merged\num I do have a little follow-up PR with\na like a bonus feature\num\nbut I think we can uh move this to\nI don't know whatever the next\nproposal right\nyeah\nuh well the last status was\nimplementation phase\num\nso has anything changed or\nyeah so the pr is merged and the flight\nsnacks example is up so I think we can\nclose that out\nokay cool\nawesome yeah let me see what\nwe have here probably we need then\nanother different status and yeah close\nimplemented\nor something like that I say yeah\nbecause this this one score like the\nwhole kind of life cycle of\nimplementation but yeah it's not clear\nwhen when it really finished so yeah\nI'll make sure that we have a status for\nthis one\num and yeah thank you again okay cool\ngreat all right uh cool around working\ngroups updates well right now the config\noverrides working group is there any\nupdate\nbesides what we saw on the previous\nsession\nI don't think there is\num I think we need to take a decision at\none point how we would treat this um\nbefore I see there's this working\ndocument\num with the guys from LinkedIn\num I think we should decide at one point\nwhether that's what we're going to\nimplement\num\nmaybe we can ping\num Byron to trying this time\nfor sure\nright thank you Fabi\nokay uh any ideas in the incubator is\nthere anything new there I don't think\nso right well there is more and more\nuh\nquestions on the community\nEtc folks who probably don't mention\nthis directly but they want to have\nsomething like this GPU matching type\nselection so\num yeah I think that\nwith enough comments here probably it's\ntime to consider creating an RFC out of\nthis discussion\nso any objection any comment around this\none\ndo I think this makes sense\noutlooking\num you see like how we can proceed like\nbut yeah\nlet me add some notes Here\n[Music]\nI think what we would need for this our\nseason overview how you select different\nGPU Types on different Cloud providers\nwhether it was basically tames and node\nselectors on all Cloud providers because\nthen there could be a mapping that we\ncreate in a platform config where we the\nplatform maintainers they create a map\nfrom GPU name can be whatever and they\njust list iterations and mode selectors\nwhatever is needed for their platform\nand then people can select these names\nin the task decorator that have been\nconfigured on the platform side I think\nif that works on all Cloud providers\nthen that would be pretty good ux\num but I can only confirm that it will\nwork on gcp this way\num\nbut that's what we're already doing\ninternally we have something very\nsimilar\num so it would be cool if people that\nare familiar with AWS and Azure\nEtc they can confirm that it works like\nthat there too\num\nnothing\nthank you yeah it takes a village\nto make it happen\nthank you value\nall right next up Open Mic mono repo\nyeah I'm excited about this one Eduardo\nwell before you go uh I'm gonna update\nthe ticket today with the example\nI already listed you all right yes thank\nyou\nbut yeah I'm on repo uh let's talk about\nthis\num I've been chipping away at this this\nproblem for a little bit and\num this PR is\nit basically does a lot of stuff it's a\nreally long yard they're like a bunch of\nstacked PR's on this\num I try to do the\ntry to explain what this thing's doing I\ndon't think reviewing this PR is\ngoing to be the post\ntrivial thing like GitHub has problem\neven loading this so\num\nsuffice it to say the idea is that uh we\nmove the we're going to move at least in\nthis you know initial phase of the mono\nrepo all the back end components to the\nflight repo\num this will take into consideration you\nknow the permissions David I know that\nyou did some work to like to set up the\npermissions\nper image repo like we're gonna have to\ntranslate that into you know owners\nfiles in the model repo but like the\ntransition is one to one it works the\nsame way or very similarly\num I'm going to provide a script to help\npeople who have pending PRS to move to\nthe model repo\nand my expectation is that in two weeks\ntime\nwe Mark the components as read-only\nand we move to the monopole you know so\nlike changes to fry propeller plugins uh\nStudio live admin data catalog all the\nback-end\num stuff will happen now in the\nhonorable\nit should be mostly a transparent change\nlike I don't expect any change in the in\nthe regular like Dev workflow you know\nyou go to the\num\nthe like imagine you're working on\nnon-propeller like now you you'll be\nable to just check out the flight repo\ngo to the flight flight propeller\num subdirectory and just like do your\nusual thing nothing really changes\ntax expectation so we're gonna get there\npretty soon okay and um we're in\ndiscussions to make\nto move even more\num\nto move other repos to the mono repo but\nthat is still TVD so for now we're gonna\nstabilize the back end you know make\nsure that we can release using the\nmonorail so\nI hope that the next flight released 110\nwill already happen through the monorail\nyeah any questions\nare people unhappy with with us moving\nto the modern repo\namazing so um yeah\none action item I'll provide the script\n[Music]\nshould be pretty simple and\nyeah\nmono repoise flight\nthank you everyone yeah I I know you\nalready mentioned this but I'm curious\nif if someone is assigned as a reviewer\nhow to review a PR like this and what is\nthe expectation there on reviews\nyeah so\num yesterday I had a\nI actually we I had a a guided review\nwith a bunch of people here at the union\num he was there hate them\num Kevin\nJeeves is also here so uh\nI I don't I do not expect anyone from\nthe community really to review this\nreally\num my plan is to\nget this in provide the the support you\nknow scripts and tooling and\num\njust have an announcement you know that\nlike hey if you have any any painting\nPR's in any component run the script\nthis is going to move your PR to the\nflight repo\nand you you pick up the work exactly\nwhere you were where you stopped\ngot it\ngot it thank you any question\nthank you can I ask my question yeah\nKevin has a question so when should we\nstart opening for requests to fly Ripple\nso\nafter this after the sphere is merged\nwe're gonna have this time where\num\nI think the tldr you know the answer for\nyour question really is after the we do\nthe the 110 release\nup to this point\nwe like I was I was manually like\nchoosing which\num gig Xiao of each component was\nsupposed being taken but\num let's make this official after the\n110 release like I expected\nPRS will have to be open in the flight\nlevel and we'll mark the existing\nreboots read on me right and yeah we'll\nmark the yes six yeah\nin the script it will keep working you\nknow like you had like a like a PR that\nwas open for like a few months and you\nwanted to move to the to the flight trip\nor like the script is still going to\nwork\nbarring some conflicts but you know\nthat's a separate issue\nany more questions\nthank you\nall right uh next up the\nproxy integration PR\ncan we talked about it a bunch last time\num he and heitam have reviewed it in the\nmeantime and also proved the one in\nFlight kit and on the on the weekend\nI've implemented the same for flight\ncontrol I need to tie it up a bit\num but then just open a PR for that and\nthen from my side it's done we will we\nrelease a beta or something that we can\ntest a bit or\nwe do this\nI think we'll need to do as much as we\nhave at least gcp and AWS\nso we'll just test through all the flows\nthat we can\nand ask some other community members as\nwell\nwhich flows can you test\nuh just to make sure that the existing\ncodes continue to work\njust as a sanity truck\num\nfor for like the the components that are\naffected so not\num I guess mainly\num\nmainly just flight cuddle and click it\nokay sounds good opinion that they are\ndone yeah thanks thanks for the time\nreviewing this one yeah\nawesome\ncool uh all right anything else\nthere's anything else on the agenda but\nany other common question\nno\nokay so yeah we can call it early thank\nyou all for joining and yeah hope to see\nyou in the next one\nall right thank you\nbut they buy"
    },
    {
        "title": "Flyte Community Roundtable | Flyte Community Meeting - September 19, 2023",
        "transcript": "uh so we are gonna have a flight\nCommunity round table today consider\nthis as a laid back space to discuss\nabout flight be flights use cases or\npain points we encourage open-ended\ndiscussions so please feel free to ask\nyour questions and join the conversation\ntoday we have near youngster Prasad\nanirudh sridha guy Harrell Erickson and\nNan Quinn joining us a big thank you to\nyou all for agreeing to join the\ndiscussion I am pretty excited for this\nso without any further Ado let's let's\nget started\ncould you let us know about yourself the\ncompany you work at and what you do\nyeah\num all right thanks for for having me\nand\num I am the head of AI at a company\ncalled Sounder I've just been there for\na couple months now and\num fairly small company uh\nhave a handful of machine learning or AI\nmodels but not a lot of orchestration\nI've used airflow a lot in the past\num and I'm looking at at flight as a\npotential uh maybe better use case uh\nsince the orchestration world has\nevolved to\ngive us better tools on a machine\nlearning side\nall right thank you so could you briefly\ntalk about the use cases within your\ncompany that led you to use flight\nwell we're not using flight uh yet I'm\nuh just kind of getting started with\nplaying with it\num so I would say right now we have a\nlot of ad hoc\nuh again like no real orchestration so a\nlot of ad hoc uh\ntraining or retraining of models it's a\nsmall company where there's a bunch of\nmodels that have been trained maybe once\nor twice but we're starting to need to\ninvest a little bit more in making it\neasier to retrain and deploy models so\nin our case those the model I'll mention\none of the types of things that we're\ntrying to to predict\nuh so Sounder works with uh podcasts\num and we're trying to make it possible\nfor advertisers to know whether a\npodcast is suitable for their brand\num and so there's some suitability\nguidelines that are out there and those\nare the things that we're trying to make\npredictions on based on uh the audio\nfile and the transcript that we generate\nfrom that so things around like hate\nspeech or\nprofanity or uh you know other types of\ncategories of of brand suitability and\nso basically the models that we train\nare based on things that we have\ninternal annotators uh labeling and so\nretraining those models is becoming more\nand more important and it's time\nintensive today\nyeah okay thank you for the introduction\nI'll get back to you again and next\nPrasad could you unmute and introduce\nyourself so which use cases are you\nusing flight for\num\nyeah all right\nuh so who else do we have\nit's Nan nanqueen yeah could you unmute\nand introduce yourself now\nyeah\num yeah my myself is munching and um\nthanks for having me I'm at a company\ncalled protopia AI\num we are\num we are providing solutions for\num data security data privacy when using\num third-party machine learning or AI\nproducts like chat tpg and um yeah we\nI'm I'm managing the infrastructure\nplatform for the model training and\ninference at protopia\nwe use flight mostly for\num data processing and model training\num for both vision and NLP models\num\nyeah flights is great uh as we were able\nto\num you know\nsplit our\nlarge training scripts into smaller\npieces smaller\ntasks and having\nyou know being able to have cash caching\nfor each task is uh\nis a great is is a great resource\nsaver\nif we you know need to rerun a workflow\nat some point\num\nyeah\nthank you thanks for the introduction\nand then we have fun so Han uh you have\nmade significant contributions to flight\nso could you please introduce yourself\nand share what inspired you to\ncontribute to flight\nokay\nso actually I am just 21 years old and\nI'm going\num uh is my right now it's my fourth\nyear in my University and actually my my\nI I said I have set a goal like\n900 days ago my goal is to become the\nGoogle intern in Taiwan and and I didn't\ndid it because of the laid off and\ntaiwan's tag is also be impacted and my\nmy situation is like I have three\ninternships before in Taiwan and right\nnow most of taiwan's internship can't\nlet can't uh let me learn too much since\nand so I decided to join open source\nbecause open source can let me learn\nmore things than other internships yes\nbecause in taiwan's internships most of\ncompanies don't let in turn to do\nsomething too important or yes and we\ndon't have we don't have too much chance\nto deal with large-scale system but\nlet's but I think if I want to be a\nGrace engineer I have to learn this\nstuff and I just met Kevin and he just\nsaid that he can guide me so I just\njoined here\nawesome thank you\nall right\num so each of you comes from various\nbackgrounds and engages and different\nkinds of work so with many workflow\norchestration tools out there how are\nyou finding flight\nuh would any of you like to speak about\nthat\nme\nsure\nhow to find the price or actually\nactually uh I just saw there so Kevin is\na PMC of avachi submarine and actually I\njust uh searched uh search open source\non YouTube about like Taiwanese is doing\nopen source I just found that Kevin is\nguiding uh some students in Taiwan about\navashi submarine so I joined the average\nsalary meeting and Kevin said you can do\nflights yes yes and and actually I\nuh my goal to influence is to learn\nhow how the backend works but not about\nreally uh ml mL of sports and I just\ncome in and learn back it and just to\nlearn no then know how ml Ops works but\nit initially let's let's not like go yes\nit's kind of a bonus for me yes and it's\ncrazy\nokay thank you\num so nearly install could you talk\nabout your experience with flight\nyeah again so I'm I'm coming it's very\nvery much a newcomer and so my\nbackground is uh as an applied\nmathematician and\num I'm having to wear a lot more hats in\nmy current role uh including trying to\nuh you know put this infrastructure into\nplace where we can\norchestrate and where I came from last\nuh had a great team of of Engineers\naround me where uh kind of from the\nground up we built airflow and this is\nprobably going back to like\n2017 2018 so\num\nthere is like a huge amount of\ninfrastructure that went into supporting\nairflow\num and I kind of see with with flight\num\nthe benefits of\nuh having kind of out of the box being\nable to partner with different\nkubernetes in the cloud clusters is kind\nof my hope uh so I'm maybe I'm talking\nmore aspirationally and theoretically\nthan in practice yet\num\nmy last\nrole we had you know a devops team that\nmaintained a kubernetes cluster uh where\nany could get deployed and and so now\nI'm kind of coming at it from the\nperspective of how can I get something\nsort of bootstrapped\num that boasts both Let's uh\nmy team orchestrate\num workflows data science workflows\nwithout needing to uh know the the\ndetails of of everything on the\nkubernetes side\num and also is simple enough where we\ndon't need to manage our own kubernetes\ncluster and\num and hopefully also makes it easier to\nkind of like and more uh more natural to\nto carry data around with us from task\nto task within a workflow\nso are you still testing things out on\nthe demo cluster\nso right now I've uh I just came back\nfrom vacation so I haven't done anything\nfor the last week but um what I'm what\nI'm kind of like and I this is perfectly\ntimed to hear that there's this uh\npresentation that David is going to do\nin a couple days very much\num good timing because uh I'm still kind\nof working through the\ngetting things running in in the cloud\non AWS so\num I've been you know doing a lot of\nlocal testing getting things\num\nuh just from a demo perspective uh\nunderstood\num and then I think the next step would\nbe to to try and run things in the cloud\nwe're kind of when I say like we're\nearly like\nagain there's no orchestration so\neverything is kind of uh manual here so\nit's kind of an interesting\num scenario where we're both having to\nto build the tasks themselves and\num just want to make sure that we're\nwe're starting on good fitting and and\nthat we're\nyou know we're kind of picking an\norchestration tool before we have\nthe things to orchestrate per se\ngot it and have you used airflow to run\nmachine learning pipelines or have you\nalso try to run data Pipelines\nuh both uh in my in my previous role so\num yeah some of them were like NLP\nmodels uh but a lot of it was data\npipelining\ngot it thank you yeah\num yeah and nancoins so how has it been\nworking with flight for you\num yeah so I started doing ml Ops using\nkubeflow as orchestration tool for\nmanaging\num\nuh training jobs data processing jobs\nEtc\num I mean kubeflow is great but it's not\nthe learning curve is kind of\num a little bit difficult for for um for\num you know\num\na typical data scientist without\nexperience or knowledge of things like\nkubernetes\nand yeah when I joined\num protopia we we have\num uh we had very unique\num requirements regarding what\num what we regarding the regarding the\nbasically the mlops platform we we want\nto use and we want to deploy on to the\nclouds and yeah I value evaluated a few\nthings like flight skip flow\num and I think flight is great in terms\nof Simplicity uh to pick up for any for\nyou know for data scientists to to pick\nup and the UI is awesome\num\nyeah we had some you know\num\nquestions problems running in different\nkind of jobs workflows and flights but\nand the\nthe community is really great I got a\nlot of help from the from the slack\nchannels\num both for both you know deploying\num deployments with you know special\nuh configurations and also\ntroubleshooting our\ntroubleshooting the the issues we had\nwhen um you know deploying and running\nthe workflows\nso are any of your ml Engineers or data\nscientists using flight how's their\nexperience with it\ncan you repeat the question\nyeah are any of your ml Engineers or\ndata scientists using flight\num\nam I using applied as a data scientist\nor my my team\nin your team are there any data\nscientists or you know animal Engineers\nso how are they finding flight\nhow yeah they\nthere is a little bit\num\nyou know there are a few things to learn\nbefore they can write correct and\nefficient workflows and tasks I\num basically I created a few wrappers\naround flight objects\nfor according to our typical usage your\nuse cases and created a few different\nexample workflows example tasks for them\nto as a reference so\num so far Things Are\nI would say so far the things are moving\npretty well for them\nwas there anything sorry to jump in I\nwas curious was there anything you\nlearned from building those tools for\nyour U-verse uh that you that we could\ntake as like a feature request on the\nflight side or was it more you know\ntuning it to your specific\nthe way you access data or like common\ntools that you use\num\nmost of the things are just very\nspecific to our\nuse cases\nI guess\nif\num\nI guess one thing I find might be useful\nis to\num\nmaybe enable\num all the\nall the things you can do with uh with\nflight control in the python API for\nexample if I want to\ncreate\nor create a project\nI I cannot do it with python API I I\nneed to do it with file control\nsometimes it's\num\nyou know it's not very\nconvenience\num\nand yeah on AWS they are they are yeah\nwe had issues getting\num\ngetting getting resources from from like\ngetting resources like uh a100 gpus\nthese days\nand um\nyeah I I I think it there could be some\nimprovements in terms of\num\ntimeout when you know some of the\nresources are available but not all of\nthem are available things like that\nawesome thanks maybe I'll ping you on\nthe community so we could we could chat\na bit more about those uh scheduling uh\nfun stuff around scheduling\nyeah\nthanks\nyeah I I know I had a question a couple\nof questions so for the three of you or\nanyone else here in the cold uh the\nfirst one will be we already heard you\nwill you will have different levels of\nfamiliarity with the project which is\nperfect\nuh so in your impression in your\nimpressions so far or your experience so\nfar\nwhat are those things that you would\nlike to see the project improving not\nalways share a couple of them\nbut those things that you struggle with\nor that in general your wish list for a\nplatform and work for orchestrator\nwhat kind of stuff that you like to see\nin flight\nso I don't know if my question is clear\nand if who wants to start\num I'll just mainly I've been working\nwith the documentation and I think\num the\nthis may sound maybe it's familiar to\nyou guys but it may sound uh\nstrange uh I kind of found myself\ngetting lost in the documentation where\nI I was unsure like how I was navigating\nit at times\num and I found like I think there's part\nof the issue is that there's a lot of\nways to do to do things the same thing\nand so\nit's like wait a second wasn't there it\nwasn't I looking at this somewhere else\nand they were suggesting a different way\nso\num so yeah that was one thing where\num\nuh I found myself kind of like\nnot really sure uh where I was at times\nas I was going through tutorials or\ndocumentation\ndefinitely near I'm with you thank you\nwell\nokay so I I I've seen flights\ndocumentation is better than lots of\nOpen Source foundation and better than\nsome companies I have been before but I\nthink is if I want to be a contributor\nas a student\nuh I think is is you still need to go\nthrough more details and maybe I will\nwork on it later yes because I think uh\nas a student or maybe I don't have that\nmuch or work experience sometimes I will\nstill need to ask lots of questions in\nsnakes or ask Kevin that you can help me\ngo through the Lamborghini and after I\nknow how to contribute then I don't need\nto ask that much question but I think\nthe beginning is still there's there are\nstill some place I think maybe we can\nhave something like a knowledge graph\nthat I can have have a deeper\nunderstanding to flights yes not\nknowledge group maybe it's like\nhard base\nthere is a\nmaybe this kind of\nthis kind of uh software\nno no software to make the knowledgeable\nyes\nokay\nthank you yeah definitely that's it\nthat's a common theme in\nyeah there are some of the ideas around\nhow to how to fix this but probably in\nmany of them we will need\nthe help of the community\nSo yeah thank you and I know you already\nhave contributed back to the\ncontributing guide which is really\ninteresting\nthank you thank you\nokay who else\nalso give some feedback\nright all right and my next question was\nuh I mean last week I was listening to a\npodcast episode it was an interview in\nthe ml apps Community podcast\nand uh there was this sentence that it's\nstill resonating in my mind it was like\nyeah the the ml in data and generally AI\necosystem has a lot of tools is not sure\nof tools there are a lot of tools\nFrameworks out there it's very\nfragmented uh so it will be great if\nthere will be some abstraction layer on\ntop of many of these tools and\nFrameworks that will let data scientists\nDefine what they want\nfocus on their code and forget about\neverything else\num it seems like the kind of the\nobservation from the guest was that\nthere's now such a thing right now\nuh is it\ntoo far of a stretch to think that that\nflight at least in theory is designed to\nbe that kind of obstruction plasma and\nwe at least I mean let's see this is a\nquestion for for well for a three guests\nbut in general to everyone else here\num am I right or too optimistic to think\nthat flight could become that\nabstraction layer at some point\nyeah the answer could be now\nI mean I think I think that's a good\na good goal uh I think it would be great\nI I do Wonder\num myself sometimes I get lost in\nthinking about what's the right\ncombination of tools that is kind of\nlike the the MVP of like a a data\norganization\num and I think\nyou probably don't want to be too many\nthings at once on on one hand you know\nif they're good tools for certain things\nbut um\nbehave\nthe fewer tools that it kind of takes to\nbe kind of that uh that structure\num\nI think the better but\nbut yeah it's it's a it's a good uh\nthought experiment\nJunior\nyeah absolutely it's a good goal and\nalmost impossible one but\nright who else\nyou'll think\nor in general what will be that we\nshould histor the requirements that you\nwill take in front for an abstraction\nthe abstraction layer that AI needs\nwhere else then and you have any\nthoughts around this\nasking I'm sorry I I was multitasking\ncan you repeat the question\nuh it's yeah probably it's a too\ncomplicated question but but in general\nI was asking if if you also\nfeel that a the AI ecosystem is too\nfragmented in terms of many tools many\nFrameworks and there's a neutral\nabstraction layer so a kind of the\nquestion would be if if you see that\nprobably flight has some of the elements\nthat will have become that extraction\nlayer uh if not what will be your\nrequirements for a true abstraction\nlayer to handle the the chaos in the AI\necosystem\nyeah yeah I totally agree that\num\nthe the\ntools the library is available out there\nfor any for ML and ml Ops are kind of\ngrowing and fragmented\num I think\num\nthen one a nice thing with flights I\nreally like is the integration with all\nof the of with those tools like Ray uh\num flow\ntraining job\nyeah I do hope to see more you know\nplugins Integrations with other tools\nthat are\nthat can make\num make things seem easier to to\nconfigure to configure and to to run\nbasically\nyeah I could chime in a little bit at\nleast from what we're seeing at a member\nthere's a lot of new hardware coming out\nto the market like data center Hardware\nwhich is enabling some new new\narchitectures and the faster we can kind\nof Upstream the Simplicity of accessing\ndifferent types of Hardware especially\nlike like larger memory systems\num\nit'll probably make it easier for like\nAI developers as well to like optimize\nhow their models run and also like bring\ndown the cost of you know a production\nenvironment as well\nso it's still early but um we're really\nexcited to buy some of the things that\nwe're seeing\nyeah that's great I mean that opens a\nwhole new chapter on this conversation\nbecause I my background is\ninfrastructure mainly and sometimes I\nfeel that right now the bottleneck for\nAI is on Hardware\nand it's mainly infrastructure and\nHardware\nuh as as some others mentioned the for\nexample the lack of availability of gpus\nand Cloud providers and\nstuff so probably if if there are\nvendors out there or projects that help\nmoving execution to other\nlayers in the hardware not only gpus but\nyeah probably what members is doing uh\nusing memory uh by the way they they\nwill be an episode\nI think it's next meeting that members\nwill be presenting here it's it's great\nto have Gina here uh but yeah I feel\nlike there there are projects popping\nout I saw last week other projects that\na project that is trying to also uh move\ninference and all these kind of\nworkloads to CPUs and not rely on gpus\nso\nyeah it's it's funny to see all these\ncrazy Innovation happening on the\nhardware space\nyeah I think that's another chapter the\nwhole uh what can CPUs actually do where\nyou don't actually need the GPU but\nwe're seeing some of that on the\nbioinformatics side of things too if you\ncan make your your algorithm if you can\nmake your algorithms\num work a certain way like we're seeing\nsome of the Next Generation sequencing\num workloads you can actually run them\nreally well on CPUs so looking for ways\nto yeah use CPUs more since they're more\navailable\nwe sell something\nthank you Jing\nyou're right um\nis there any other question yeah\ndoes anyone have any more questions\nI guess like from the flight\nyeah go ahead\nyeah go ahead everyone\noh yeah cool\num\nnow you can see my face\nyeah just on the flight side we\ngenerally see it appeals to platform\nEngineers first because it meets a list\nof requirements that they have for a\nsystem\nI'd love to understand like just more\nabout the the next jump to like the user\nthe person kind of like writing the code\nto\nof the tasks and I guess like\nthere's different flavors of it and I'm\ntrying to figure out what's like the\nprototypical pattern like is it\ntypically like a platform engineer will\nactually set up the structure of the\nworkflow\num and if so like is that really a\nplatform engineer or is that like also\nkind of a fusion between the uh a\nworkflow author and uh and a platform\nCreator\num whereas like I guess like what what\nare the users what what role where is\nthe gap or where is the line drawn\ntypically between like the ml engineer\nwriting flight code and the platform\nengineer taking care of the platform or\nor is it like pretty blurry\nyeah I can talk about it from my Express\num because I am basically managing the\nplatform so I'm the\nml Ops engineer here and\num like I said I created example example\nworkflows example tasks and our data\nscientists and ml Engineers are the\nactual tasks and workflow writer\nto create\nto create Underground workflows on the\nplatform\nyeah I'll say um\non our side I guess it's it's different\nin the sense that we're small we have\nuh\nyeah a very small sort of technical team\nand a lot of it is the platform so to\nspeak is focused on just\num kind of like\nuh the ingestion of podcasts right like\nthe the uh\nyou know keeping various things running\nthat our customers use but less so like\nthe ml op side uh and so I think like\nthere isn't any distinction between\nthose two\num at our size and so we kind of have to\nhave a tool that is both accessible\num to uh to machine learning uh sort of\ndata scientists\num\nand and and one that you know they can\nthen sort of operate with\num but but yeah I think yeah\nthat's that's great context and I was\nkind of I group users into kind of two\ncategories and this can be wrong but\nlike one of them is like a software\nengineer that taught themselves machine\nlearning and the other is like a\nscientist or a statistician they taught\nthemselves to write scripting and code\nand I guess like my hunch is that flight\ntends to appeal to the former more than\nthe latter but we would love to like\nlearn how to appeal to kind of everyone\nso I guess like\num from your just recent comment it\nsounded like you were working with kind\nof more like the scientist statistician\ntypes and you're looking to build a\nplatform for them and so I'd love to\nknow like are there gaps around like are\nyou going to build like a Jupiter Hub\nlike notebook system to kind of do an\nextra layer of abstraction to make it\nsimpler\num just curious about how that how\nthat's how you're thinking of doing that\nyeah I mean I I'm myself and more of the\nlatter\num I've been sort of in Tech long enough\nwhere I've uh through osmosis\nuh learned enough coding where I have I\nhope better practices than\num most just fresh statisticians but um\nuh\nI don't think that I know I've seen a\nlot of the the examples where people are\nusing Jupiter and I think that that's\ngreat and I think that is typically how\num\npeople are doing kind of experimentation\nuh and when I say people it's really\nlike uh two other people besides myself\num but the the training that's that's\nhappening is very ad hoc and so it's\nit's you know run something on uh and a\nlot of it is like leftover where we've\nhad like some turnover and so people who\nare before us uh you know we're running\nthings in Google cloud and so there's\nlike\num\nthere's this kind of like a\npassed on like lore of how do you\nactually how did this model actually get\ntrained and so\num I actually I don't think that using\nJupiter is what I'd like to see happen I\nthink I just want to see\nthat we have the ability to reproduce\nlike the scientists in me wants to be\nable to reproduce\num the steps for making a model and\nbeing able to trace what\nuh\nyeah what was the the data that was used\nfor his training data and evaluation\ndata and\nmake sure that we're actually able to\nspeak you know clearly about what we\nhave and don't have\nawesome thanks that's great context\nthank you\nsure\nI actually had a question maybe again\nanother beginner question but uh\nwe are mainly an AWS shop but we do have\nkind of like\nmaybe a preference for running certain\nthings on Google Cloud just from a cost\nperspective with gpus and just just\ncurious if that's like a common thing\nthat people in the flight Community\num uh are working with or or if there's\nlike a if this is like a solved issue of\nlike how people have set things up for\nthat\nyou would do want to answer that\nyeah I mean I I don't I didn't have data\nto see you as the percentage of users\nrunning on AWS or gcp or Russian but uh\nbut we see an important uh\nfraction of the community running gcp\num we're working on some resources to\nautomate the entire thing\num and and there's an there's an old\nguide out there that helps you at least\nmanually to prepare the environment\num but yeah I mean right now they're\ncertainly there are users running apply\nthem gcp\num so yes it's totally\nfeasible\nto run it there I don't know if what you\nmean is running workloads on both\nhaving\nyeah so like I think for example our\ntypical again I wouldn't call it a\nworkflow but I would say like manual\nworkflow would be to train on gcp and\nthen Deploy on AWS and so like I guess I\ndon't know what the consequences are if\nthere's if that's a typical pattern and\nif so like\nuh you know setting up orchestrators on\nyou know both platforms or like what\nwould be the kind of uh\nuh sort of setup that would would let\none accomplish something like that\nI I can try to answer that near I have a\ncouple questions actually um when you\nsay deploy what exactly do you mean\nuh basically having the so that I mean\nthat's a it's an open question as well\nso uh the the model hosting is is\nhappening in in AWS so the model file uh\nis in S3\num and right now it's not a standalone\nkind of microservice it actually just\ngets uh\nspun up uh ad hoc uh by the essentially\nthe core engineering team when we're\nprocessing like new podcasts that come\nin or something like that\num but that could change in the future\nas well\nwould you it sounds like maybe this is\nsort of offline like would you is batch\noffline like uh inference or whatever\npart of like things that you're\nconsidering\nand is that is that why you're asking\nabout maybe deploying like flight or\nactually using flight in AWS as well I'm\njust trying to understand that part\num I think the I kind of want I so\nthere's a few things that are missing\nfrom what we do as so we have basically\ntwo main engineering teams one is kind\nof like the AI research team and the\nother one is the core I guess you could\ncall platform team but does it really\nhave like an ml Ops flavor to it\num and so what I when I think of\norchestration I'm I'm thinking about\nuh you know training uh evaluation and\nthen deployment of the model eventually\num and deployment meaning like\neither uh\njust putting it into the the Target\nlocation where the core team is\nexpecting it or potentially\num\ndeploying a service that hosts the model\nas an endpoint\num\nso today that kind of end location is in\nthe AWS uh S3\nand also like\nthe uh\nyeah so I guess maybe it's\nit's something where\num there are other workflows that maybe\naren't machine learning that I think\nflight would benefit us uh as well to do\nand\nwe just have everything that we do is\nmostly in AWS so I think it would\nI guess it's kind of like the odd one\nout is\ntraining is usually something that we've\ndone in Google Cloud again doesn't\nnecessarily have to be but what I've\nheard is from a cost perspective uh like\na100s are cheaper and so that's the\npreference but uh\nyeah hopefully that yeah no that makes\nsense\num I I would say like there's def\nthere's at least two deployment models\nthat could work right I think one is\nyou can just put a control plane in gcp\nand another control plane in AWS so\ncontrol plane data plane and gcp control\nplane data plane AWS uh they'll have two\nseparate endpoints basically\num and then there is a you know a\nboundary right and the boundary could be\nthat uh a model archive from Google\nCloud gets you know pushed to an S3\nbucket uh on AWS and then there's like\nyou know they could be like Watchers or\nwhatever there that you can use to like\nautomatically deploy these these model\narchives once they drop an S3 uh bucket\nnotifications and whatnot the other\nmodel and I think I you know for a long\ntime I hadn't seen this out in the wild\nbut I think it's like starting to come\nup uh you know quite a bit today\num is this single control plane\nmulti-data plane model where you might\nhave a single data a single control\nplane either on Google cloud or I\nimagine it'll probably be an AWS for you\nguys\num and then a data plane cluster that\nlives on gcp and on\nuh and another one that lives on AWS and\nyou can choose like you know where\nworkflows would schedule so you can say\nlike your training workflows would\nschedule on you know your gcp data plane\nand the rest of the stuff can like work\non the AWS data plane right\num and then you know there's like\nthere's a whole bunch of like complexity\naround things like auth and everything\nbut uh there have definitely been teams\nlike I think I've seen in the open\nsource flight slack that have actually\ndone this successfully\num they've done the multi-data plan\nsingle control plane successfully I'm\nnot sure that they've done you know the\nmulti-cloud but I don't imagine it'll be\nlike all that different\num you know from like a glue standpoint\nonce you figure out how to like actually\ndeploy a successful data plane on the on\nthe different clubs\nyeah I think I think you you're I think\nthe the heart of my my question is\nis there like a\na flight uh way of of kind of handling\nthat and I think\nthe answers may be like\nit's it's just there isn't like a\nnecessarily a right way to a preferred\nway to to do it it's it's maybe kind of\nlike\num\nnot solved or not not uh I shouldn't say\nthat's all it's not uh\nthere's not like a wrong way uh yeah I\nthink that's it it comes down to what\nyou guys want and like the isolation\nboundaries that you want to set in your\nengineering team right so like at my\nprevious company we actually didn't we\nran a separate cluster entirely for like\ndevelopment uh and then you know for for\nproduction and then for research we also\nran a separate cluster for like Dev\nresearch and production research and\nstuff like that so we had like lots of\nyou know boundaries but I've seen other\ncompanies where like they have a single\nlike control plane that supports like\nyou know like development research\ndevelopment research production as like\nseparate uh projects and domains in\nFlight uh and I believe that's what Lyft\nwas doing which is why like you see\nthese like abstractions around like\nprojects and domains in Flight right and\nso I think I think it comes down to like\nhow you know how you want to Define like\nyour project boundaries and how you want\nto like establish sort of isolation\nbetween your uh different projects and\nlike the different sort of Scopes that\nyou that you're working on but I I but I\nthink it would be like yeah it would be\nuh\nyou know up to your infrastructure team\nor like your architecture\nmakes sense like yeah\nwell if you have more questions about\nthis I'd love to follow up on them yeah\nI'll definitely take you out if if uh if\nthey come up\nall right\num thanks everyone for your time and\nparticipation and yeah if we if there\nare no more questions then yeah uh the\nnext Community saying is on October 3rd\nI'll see you all then thank you very\nmuch\nthank you\nthank you thank you very much thanks\neveryone"
    },
    {
        "title": "Flyte Contributors Meetup - Aug 31, 2023",
        "transcript": "all right let's start okay welcome\neveryone to the lights contributor\nMeetup\nI'm glad to see you all here\nuh today August 31 and a couple of\nreminders the meeting has been recorded\nand it will be posted to flights YouTube\nchannel and this meeting falls under\nLinux foundation's code of conduct\num all right\nlet's get started so I'm sharing again\nhere link to the\nnotes agenda\nfeel free to add yourself to attend this\nlist\nmaybe this one is better\nmore specific\nso today is\num session and uh someone joining for\nthe first time\nI don't think so\nright let's move to the next item I will\nshare my screen real quick here\nand introduce new rfc's as far as I can\ntell they are not net new rfcs\ntoday\num some many of them are in the\nimplementation phase uh thank you\nand uh there's one in review recently\nFabio and Dennis\nrun uh let's say an experiment\nwith this one any comment besides what's\nin the pure\num yeah I can maybe get some context\nabout the experiments\num so that that is a colleague of mine\num he and I wrote this proposal and\nwe and Dan talked about it a few weeks\nago and then had a good idea how to\nimplement it with a rather minimal\nchange in propeller\nand had the feature Branch or\nimplemented that idea and we we just\nrendered in our staging cluster and\ntried different scenarios how could\nbasically compare compare the old logic\nexisting logic to the New Logic for\ndifferent scenarios to test basically\nwhether this implementation does what we\nhope to achieve with the RFC and we\nfound that it basically does in almost\nall cases and we made once more PR\nto adapt the one case where it didn't\nwhich is that in the end\n[Music]\num\nfor the last retry we switched to a\nnon-preemtable instance always\num\nand we we made it we made a small PR\ninto the one from then\nand we've been writing on slack about it\nwith them\nand he says that there will be some\ndiscussions with Union customers in the\nweek in the next week or the one after\nhe wants to make sure that also for them\nthis solves the problem\num and if yes then I think at least we\nare very happy with the implementation\num then do you want to add anything did\nyou talk to Eduardo about it\num no didn't have a chance to have a\nconversation yet Eduardo was actually\nout this week\num so hopefully right when he gets back\nwe'll dive into that but I I think\nthat's a fantastic synopsis of where\nwe're at here like the changes are are\nvery small to achieve this I think\nthere's a lot of you um a lot of Open\nSource users that have had a lot of\nquestions on how retries work\num\nand this fixes it very succinctly I\nthink\nno maybe maybe one more sentence about\nit\num the main problems that our our users\nhad with three tries is that\num\nso there's there's system failures and\nthere's a budget for those and their\nuser failures there's a budget for those\nand the different plugins they sometimes\nhandle certain things differently they\ncount them towards a different budget\nfor example the python function task it\ncounts a preemption as a system failure\nbecause we can see from the events of\nthe Pod that was preempted\nthe kubeflow plugin for example doesn't\nbecause the the coupon jobs they don't\nshow any sign of a preemption you would\nhave to look at the underlying parts\nso for a cube flow distributed training\njob the preemption counter the user\nerror it counts against the retry budget\ndefined in the task decorator whereas\nfor a normal part it would count against\nthe system retract budget in one case we\nswitched to a to a regular instance\nafter um for in the end in the other\ncase we don't which has led to a\nsituation where our Engineers just don't\nuse bot instances anymore because\nthey're super confused they never know\nuh what will happen now they said like\nsometime in the task decorator and then\nsome other number of retries comes out\nbecause sometimes it's a system retry\nand\num the idea is to not remove this\ndifferentiation between system failures\nand um user failures in the in in Flight\nkit and propeller because it's really\neverywhere but there's one function\nwhere we check whether a job is eligible\nfor a retry and there just have a switch\nin the in the platform config where we\nwere a new new Behavior could be\nactivated where we just we will be\nbasically add the two budgets together\nand just check like is there any budget\nleft still\nand that still means that in propeller\nwheel or in plug-in Shredder we'll still\ncheck for the python function task\nwhether it was a preemption we still\ndon't do it for for example Q flow\ndistributed training jobs but in the end\nit doesn't have any difference in the in\nthe user facing way at least because it\neverything counts against the number of\nretracts set in a task decorator\num\nand that makes it simpler for users\nbecause that's the only number that they\nneed to be aware of\nand for example it also allows them to\nsay hey we have this training here the\ntraining is expected to run for four\ndays\nI don't mind if it gets preempted 10\ntimes whereas for another job you might\nnot want that because 10 would be like\nway too much\num so yeah I think let's wait what comes\nout of the discussion with your\ncustomers\num\nand jumper is what they say I'm curious\nwhat Eduardo says\num I think no rush to get it emerged but\nlet's try to get it right but at least\nthe experiments we ran were I think we\nwere pretty happy with what we saw\nno\nI have a question for you this is pretty\ncool I would love to have these people\nin a dock um like in the flight docs\nonce once it's the picture is out\nbecause I think it will verify\nexactly what would happen in What\nscenario so I think it's great\num if you scroll down uh David maybe\nyeah the this that the search from the\nbottom where it used to do read two\nretries and now it will do one uh\nit it says system retards are not\ncounting against the users the user\ndefines unified retry budget\nwhat\num what how did it why did it do two\nbefore is that did we uh have a\nthere's a platform remember that part\nokay there's that there's a value in the\nplatform side that's two by default okay\nand all system retries count against\nthat\num I see and then we get questions from\nour users like I said five here why is\nit two\nhey quick quick question when you in the\nmiddle two columns when you say two\nretries is that two executions or two\nretries I'm sorry if you covered this\nalready I'm a bit late\num\nyeah to be 100 sure I'd have to check\nagain I think it's like not executions\nbut actually retries\num\nbut it's kind of this one-off error I\nneed to check I'm pretty sure it's\nretries but I need to choices this is\neverywhere in the code too about like\nretries versus attempts so two retrys\nwould be three attempts right you'd have\nthe original one and then I would be\ntried once and twice would be three\nattempts to retries I believe right\nFabio\nyeah yeah we will we can check again and\nlike one-off errors are the only errors\nthat are super annoying um and then the\nthe last case\num\nyeah if you scroll all the way down\nDavid\num that but interruptible\nso you're changing that behavior from\nswitching to non-interruptible as the\nlast one\ninterruptible\nSo currently the um So currently for\num So currently it's implemented this\nway when you set interruptible it was to\ntrue\num then for for the last system retrying\nthat you have we switch to\nnon-interruptible to guarantee\ncompletion we don't do that for the user\nfailures\num\nbut since with kind of this V2 version\nwhich by the way is not the default\nright you have to activate it in the\nplatform config do not make it breaking\nchange but in the future since\neverything comes against this number of\nretries that the user defines we would\nwant that\num in the end it always switches do not\npreemptable instance because it's the\nkind of the last free trial right\ncurrently it's only for system retries\nand with the like the two Behavior it's\nfor the budget that the user defines but\nit's the only budget in the end\num oh sorry I was looking at the columns\nwrong the new code is what will switch\nto non-interruptable\num I see or user errors\nuser error exactly yeah yeah for for\nsystem failure the the existing\nimplementation also switches but only\nfor system failures\nthis is one point of contention right\nnow that we're trying to iron out uh\nvery very clearly is that like the\ninteroperable failure threshold and\npropeller configuration right now is a\npositive number like it's just zero one\ntwo three and it's meant to play with\nlike system failure configuration which\nis also propeller configurable so if you\nspent 10 system failures you want that\nthreshold to be nine so that the last\none is if we do this weird combination\nwhere you know users can now it's retry\nthe number of times the test having that\nzero to n number doesn't really make a\nlot of sense what we want is a you know\nnegative number we want the last one\nand so we're you know does it make sense\nto\nyou know introduce a different\nconfiguration value for that or how to\ndo that\num that's one one point of contention on\nthis but but I think you know saying the\nlast one will be I don't know I'd like a\nconfiguration value for but I don't want\nto make this more complicated than it\nalready is yeah so then I also had a\ndiscussion about it last week\num I think it would be cool to not have\na hard coded to one because sometimes I\nthink ml code is typically garbage right\nso sometimes it crushes for other\nreasons\num so having a number that configures\nthis is I think core I don't think the\nuser they think it can be for us it\ncould be a platform site conflict I'm\nnot sure about others but for us if\nthere's like a negative number that we\ncan set in the value on values file that\nwould be perfect right\nnot sure like I'm interested with also\nyour customers say about this maybe they\nwant it in the operator\nwhich of course would work for us it's\njust more\nwon't work I guess\nokay any other comment\nno I would say well let's leave it in\nthe let's leave the RC is still in the\nkind of the discussing stage right\num then we'll talk to Eduardo talk to\nUnion customers and I think we can wait\nfor that and then maybe have another\ndiscussion\nto see whether something new comes from\nthat\nthat sounds great and then after that we\nare we are pushing full force on this\nand getting into it's been open for uh\nquite quite some time now so I'm excited\nto get this closed up\nI think in our end we've seen very\nsimilar issues probably probably to the\nones you've seen um\nI've actually changed a doc once after\nthe discussion in the flights like and I\nthink it's actually wrong now because I\nthink even in this like a few people\nwere in that message a few people who\nwere confused about like what retry\nbudgets consequences were\num and whether you have to set retries\nfor interruptible tasks and whatnot also\ngiven that gcp continues to break their\napis and then the\nautomatical\num\nwhat is it called the function that\ndemystifies whether it was the\nPathfinder yeah domestic fat failure\nthat that is\nvery therapy it doesn't seem as stable\num and I've recently chatted with them\nand apparently it is not stable that's\nthat's\nwhat they they say so I think given all\nof that um the new Behavior\nwould be could be really nice yeah\nI've also read The Roc and it's a degree\nyeah\nawesome thank you very much thank you\nall right uh okay next up in the agenda\nworking groups updates and like there's\na discussion on the third iteration of\nThe Proposal\nconflict our rights I mean\nright so any comment about this status\nso this working group\nwork\nnope I think the relevant people are not\nhere\num\nproposal comes from the LinkedIn guys\nand I think he\nwas part of like took part in the\ndiscussions\nright\nokay cool\nokay next up in the incubator ideas in\nthe incubator I didn't see anyone okay\nyeah anything exactly new\num\nyeah a couple of recent comments but not\nanything\nreally new there\nright\num any comments so far any other topic\nregarding proposals\nrxis we can move to the next item or\nwise\nokay next one will be\nOpen Mic section\nat least one topic I have one uh\nprobably this one\nwe need to announce this broadly the\ncommunity but I'm sharing here this\naudience\nthere's a new bot\nout there in the um like repo in this\ncase looking for all issues\num\nlabeled in as steel if if nine months or\nmore of\nnot not any activity and providing\nwarning and automatically closing after\nseven days of\ninactivity so\njust be aware of this uh yeah we have a\nbunch of\nold issues open out there and um if you\nhave any\nuh you probably will see a note like\nthis from the boat\nand you can come in here this is your\nissue you can come in here and\nand ask if you need you need to keep it\nopen otherwise we'll be closed\njust to make sure that we have relevant\nissues opened\nright\nand next up\nyeah the identity aware proxy\ninitiative from Fabio which is great\nyeah like if I'm not going to talk about\nit last time but I think different\npeople were there\num Sherlock like\na few sentence of context again or\nyeah I think so I don't know if\num okay I'll just say a few words about\nit so gcp identity where proxy is a\nmanaged service by Google\num it allows you to\nhave\na login basically at the load balance\nrange so requests that are not\nauthenticated cannot even pass through\nthe load balancer and can never reach\nthe back end very easy to configure\nTheory\nand because of that\nseveral companies that I know on that\nhave worked at but also others have an\ninternal security policy that whatever\ntooling is deployed needs to have\nidentity where products in front of it\num because log into sleep well right no\nunauthenticated request you can ever\nreach the back end so it doesn't matter\nwhether the back end is implemented well\nor not well because\nwe don't have to trust it right because\nwe kind of trust that Google Cloud\nimplements this properly and then we\njust put this at the load balancer and\nthen nothing unauthenticated you can\naverage the back end\num\nso we have a security policy to use that\nprevious company the two and some\nadditions like also said that they kind\nof need integration with that\num and like workarounds are typically\nthat people port forward flight admin to\nregister and run workflows\num or we make flight admin available in\nthe VPC and then only register from from\nVMS in the same VPC or some people use\num use VPN like connect the VPC with the\nVPN and then after being a VPN to\nregister\num but all because basically\nslight flight control and flight kits\nremote they can't talk to flight admin\nthrough through identity by proxy and\nand\nbasically this PR\nATT attach it or it adds two things the\nfirst thing is that\num\nan external command can be executed to\ngenerate a token for identity by proxy\nand then we add it to the proxy\nauthorization header and then\nflight control and the remote that can\ntalk to flight admin and the the\nidentity where proxy just make sure that\nthis proxy authorization header contains\na valid token and then the rest of the\nauthentication flows just as it is now\nso it's like a second gatekeeper at the\nload balancer that makes sure that\nnothing that is not authenticated with\nGoogle can talk to any backends and with\nthat\num\nyeah for like us for us this would help\na lot because then we can register from\nnotebooks currently we can't because of\nkind of the security policy and the guys\nfrom free known they have the same\nsecurity policy they also want this\nintegration\num in this PR if that's the other cycle\nPR it adds this to flag kit um I'll\nstill do the same for flag control\nbasically that is proxy authorization\nheaders can be sent\num and the pr\nincludes a plugin that introduces a\ncommand to generate these tokens as an\nexternal command reuses the existing\nmechanism and it also contains\ndocumentation how to deploy the stuff on\ngcp because it yeah the deployment is a\nbit complicated because gcp requires\nencryption between the load balancer and\ndeep backend for HTTP 2 services\num but there's a readme in the plugin\nthe document\nyeah I'm happy that python you are in\nthe call because he said that you should\nlook over this\num do you have any questions of how does\nhow this is integrated\nyeah I don't think I saw this before so\nI'm still catching up uh but a couple of\nquestions uh in this setup do you enable\nauthentication in Flight admin or do you\nnot you do okay\num so does it does the proxy forward\nsome token to admin or how does that\nwork but there are two that's a good\nquestion there are two ways to do it\nif you send an authorization if you send\nan ID token as the authorization header\nto the ear pain\nit will validate it it will strip it and\nreplace it with a special token in an X\nGoogle jwts version header so it\nreplaces with another token that's in\nanother header and one could implement\nit in the back end Theory to interpret\nthis token\nin practice it's implemented in a very\nnot nice way from Google\nbecause to validate the audience of the\ntoken you need to know the circle the\nso-called backend service ID but you\nonly get that after deploying so you\nkind of have to deploy get the ID inject\nthe environment environment variable and\nthen restart that would be the idea to\ndo it in the background and that gives\nme a lot of grief to kind of have to\nrestart things to deploy them\nthe alternative would be to that flight\ncontrol and remote send two tokens to\nsend proxy authorization header and an\nauthorization header the proxy\nauthorization header is for the identity\nweb proxy the authorization headers for\nflight\nso the clients they do the exact same\nauthentication for the slide as they do\nnow but they just sent the second header\nto be led in the gate at the load\nbalancer so yes we have\num we have authentication Ableton flight\nthen there's kind of a double login but\nyou don't really notice it because\num we get the security from the identity\nby proxy so we set the lifetime of the\nrefresh token for flight to a week so\nkind of once a week maybe you see like a\nsecond login but you don't see it in\npractice\num\nyeah let me see\num so is that\nlike is ideally this you would use that\num\nsecondary token like is that the\nintended\num\narchitectural I guess of designing your\nservice that you only trust the IEP\ntoken\num and and then everything goes through\nthat new a client only authenticates\nonce with the proxy okay\num\nand how will the and then authentication\nbetween like ad propaganda and admin\nthat happens with like I guess one\nfactor authentication because it's not\ngoing through the proxy in this case yes\nlike it's now okay\nthey're nothing changes\nokay\nuh okay cool yeah so thank you for doing\nthis\num I'm happy to take a look at the pr\nyeah\num\nin the flight CTL want to will end up\nbeing apparently it's implemented for\nfor flight control\num sorry for a flight kit that's\nimplemented already and I do another PR\nI haven't started working on it but I'll\ndo that where I do it for flight City\nerror and um so this PR contains the\nlogic in in Flight kit the auth helper\nis a flight kit to add the second header\nand it contains the plugin to generate\nthis ID token for identity where proxy\nand I assume that for flag control we\ncan use the same external command\nwritten in Python that gives us the\ntoken and we just have to like attach it\nto the requests yeah\num I think for for the other the\nexisting authentication we try to make\neverything driven by the server uh like\neven the fact that you need\nauthentication or not is a is a server\nresponse\num\nthis sounds like it will be more\nproactive\nuh that's like the client once\nconfigured with this will go get a token\nand attach it on every request and maybe\nthat's\nbecause I just got a little bit down\num David I think there should be like a\nline here\nwe'll have it here can you click on the\ntracking issue\num I think there I have the line it's um\nyou need to configure one thing on the\nclient side and that is one line in the\nyou are there here so the way that I\nedit it right now but I'm happy to\ndiscuss this I add this proxy command\nhere in analogy to the command in the\nplatform config and that's that has to\nbe set on the client side\nuh in order to add these proxy\nauthorization headers\num but that will also only be invoked oh\nsorry go ahead\num so this flight IP Command is also\nadded in the pr that's a plugin\num what I like about it doing it this\nway is that in if there's another\nmanaged service or some proxy in this\nkind of in a like a local cluster uh you\ncan use the exact the exact same\nmechanism we just need to have another\ncommand that generates a token\num like gcp identityware proxy is not\nthe only like the proxy authorization\nheader was not invented but then right\nlike others use this tool\nso if if there is ever like another\nproxy that people want to use they can\nuse the same thing just have to have\nanother token\ndoes this have dependency on Google\nlibraries or how does how does that\ncommand work\nthe command so the command no it let me\nthink it does but we could probably even\nremove them so in there's the\nauthorization client in slide clip that\ndoes a Pixi authentication flow right\num I refactored the authorization client\na tiny bit basically I just make it\nconfigurable in the Constructor which uh\nwhich data you want to send with the\nrequest and then I you reuse it to do a\nnormal or two flow with with\naccounts.google.com\nso it's very similar it's a standard os2\nflow and it uses logic that exists in\nFlight kit for it we have one dependency\nto a Google library but it's only in\nthis external command it's not added to\nflag head to flight kit because we want\nto retrieve a client secret from a\nsecret in secret manager in Google\num but it's there it's not adding\nrequirements to flaget it's only adding\nthem to this plugin\nlike sorry I'm stepping back a bit I\nthink I are using this the the way you\ngenerate tokens for the iup\nphotos standard to walk to pixie pretty\ncool it's not pixie\n[Music]\nthere is a user involved there is a\nbrowser that pops up and so on okay\num I see so does it have to be\nimplemented that way like did we\nokay okay I probably don't want to keep\nthe meeting I will take a look at the pr\nand we can uh probably discuss\neither offline or in the next one okay\nsounds good cool thank you\nawesome thank you both\nokay any other question coming about\nthis VR\nthem\nright seems like that's it in terms of\nagenda nothing else well thank you all\nfor joining it was great to see you and\nI hope to see you next one\nthank you bye\n[Music]"
    },
    {
        "title": "The Flyte journey at Cradle - Flyte Community Meeting - 09/05/2023",
        "transcript": "yeah cool\num\ngreat so uh yeah we're a fight or we're\ncradle your flight uh and we're here to\ntalk about uh sort of uh how we're using\nflight today uh you know as a co-founder\nI'm obligated to do my little like five\nminute introduction of who we are and\nwhat we do\num so\num I'll try to blast through that this\nis our leadership team\num so we uh bring together sort of the\nbest in biology and machine learning so\nuh I'm previously a Google brain an\naccelerated Sciences\num Steph who are is our CEO was\nleadership at research and machine\nintelligence we have two excellent\nbioengineers on our founding team\num from zymergen and DSM and um design\nfrom uh Uber and and booking.com\nso uh what's our mission so we're on a\nmission to make programming biology easy\nand we're starting with a really narrow\nuse case which is improving proteins and\nenzymes\num so really specifically we want to\ntake some of the Cutting Edge ML and\nmake sure that it gets in the hands of\nthe scientists who need it\num\num some of you might know uh proteins\nare incredibly powerful but hard to\nengineer and design I like to say uh\nbasically everything interesting that's\ndone by biology is done by proteins\num so that gives you a sense of sort of\nhow much you can accomplish with them\num\nand most experiments in this area fail\nto meet their design goals and spend a\nton of money doing that so\num\nyou know tens hundreds of Cycles\nthousands of candidates uh\ntons of money burned in that process and\nsuccess rates in the one to ten percent\nrange and these are also incredibly long\nCycles so\num what we really want to do is\num bring some ml particularly for\npractitioners who maybe don't actually\nhave the background to do protein or\nenzyme engineering themselves they're\nmore sort of bioengineers at the genomic\nlevel and they're they're stymied in\ntheir current project by their inability\nto do to do protein engineering\num\num so the idea is uh you know take a\nprotein select a set of properties that\nyou want to improve\num and and generate the next round to\ntest in your lab\num and so this is we really want to be\nwith clients from the beginning to the\nend of their optimization process so we\nwant to be there across multiple rounds\num and we want to improve with it with\neach experimental round\num and this is one of the places where\nwe think\num we actually have uh a lot to offer as\nsort of machine learning as a discipline\num because there is\num you know there's a lot of spooky\naction within proteins and understanding\nhow the data that you've generated at\nGreat expense uh sort of informs that\ndecision\num\nis is pretty tricky to do but the models\nseem to do a pretty good job\num\nyeah this is basically a recapitulation\nof that um the idea is like selecting\nregions and mutations to make it those\nregions which would result in specific\nproperty improvements\num so here you see this is like uh\nmock-up of the first version actually I\nthink this might be live now\num\num basically what you put in is uh you\nknow spreadsheets with your data and\nwhat you get out is a spreadsheet with\nuh with the recommended sequences so\nit's a very simple tool um it's really\naimed at scientists who don't have\nnecessarily the the data science\num or ml expertise to use these tools as\nthey're currently present\num\nso so this actually kind of gets into a\nlittle bit what makes sort of our our\nusage of well we think we don't know we\ndon't know everyone who's using flight\nbut makes our usage of flight a little\nbit unique is that we're actually in\nin our production Loop there exists\ntraining and metrics and everything\nbecause customers are bringing new data\nto us and that needs to result in new\nmodels for them\num and um and so we have kind of like\nthis concept of like we're we're\ntraining in the loop and so the the way\nwe've decided to architect that is\nactually to have you know our production\nfront-end call out to blessed versions\nof flight workflows\num that mirror what we've been doing in\nin research\num\num sort of that are the the latest and\ngreatest and then get the results from\nthose um goes back so also feel free to\ninterrupt me and I'm going to go at a\npretty good clip because I have a lot of\nslides but uh feel free to interrupt me\nat any point in time\num if you have questions\num\nso so again the the idea is essentially\nwe give you sequences you know you're an\nexpert in your assay in in you know your\nbuilding of sequences and and getting\nthem to express in uh you know organisms\num and then you take that back and we\nact as an assistive technology for you\nin deciding what the next set of\nsequences that you should test are to\nget to your your sort of end goal faster\num\nso you know not you know this is a\nparticular Challenge and that we're\ndoing training is as part of our\nproduction but we have also lots of\nother things we have you know we're in\nthe bioinformatics world so we have lots\nof weird Legacy tools\num lots of machine types because of that\nyou know we've got you know a machine\nthat needs to always stay up that has\nlike 800 gigabytes of RAM and you know\ncertain jobs need to run on that\num and we also have a large and growing\nteam that's that's actively developing\nworkflow so I think we're at like 20\nsomething I'm looking at someone else\nbecause I can never keep track of with\nit and I think we have a like a nine or\nten people who are actively\num sort of contributing to this and and\nyou see we have a pretty robust set of\nworkflows across a number of tasks and\nmost of them appear to be running\nalthough and it was like I think\nprobably those are mostly tests but uh\nso\num yeah\num yeah so uh the later part of this\npresentation I think I'm going to talk a\nlot about sort of where we struggled and\nwhat we'd like to see improved but I\nthought I'd spend a little bit of time\num you know uh giving some positive\nenergy so so I wanna I wanna cover the\nstuff that we we really love and we've\ngotten a lot out of\num flight and I'm sure a lot of this is\nyou've heard before and so for us when\nwe were doing the evaluation of workflow\nsystems uh these first three Providence\nreproducibility and caching were\nabsolutely like table Stakes for us it's\nbasically like why do you have a\nworkflow engine if you don't provide\nthose three\num and\num and we really like how flight\nprovides these things\num so in terms of like providing the\nreproducibility not just through the\ndocker images but also through you know\nactually storing the configuration of\nthe workflows in the database you know\ngiving things immutable IDs all of that\nproviding the Providence by providing\nthe inputs and outputs um you know\nreally explicitly\num and you know all the type information\nassociated with that and then the\ncaching is just you know like a huge\nTime Saver and also reduces our our\ncompute Bill a lot since it'd be pretty\ndifficult to run write logic which\nresulted in stuff never being rerun on\naccident the way we're designing the\nsystem so\nwe we lean on cashing pretty hard\num and the UI is great for getting an\noverview of that um there's a little\nasterisk there in that we've we've run\ninto enough bugs in starting workflows\nfrom the UI\num that that's sometimes a challenge\num\nbut that leads pretty nicely into uh\nit's great to have a python client and\neven if there weren't you know\noccasionally challenges in starting\nthings from the UI\num we would probably still be mostly\nstarting workflows from notebooks using\nflight remote\num it's just a really nice way to\ninteract with the system\num\nuh yeah we love the local development\nand testing being easy and I think one\nof the big things that's been\num that I we weren't necessarily looking\nfor when we went um and tested out some\nworkflow systems but has been really big\nfor us is that we feel like the way the\nthe flight SDK is designed does a pretty\ngood job of enforcing good development\npractices\num in Python you know it particularly\nfor newer devs or maybe devs coming from\na research context where you know code\nstructuring is not not as critical\num it really helps uh you know people\nthink in a stateless way think you know\nlike make sure they're taking advantage\nof the the python\num first class features like like type\nhinting and and\num you know separating out redundant\ncode and stuff so\nthat's actually I think been a pretty\nbig impact for us yeah here at the top\nyou see a quote from our tour which is I\nthink pretty representative of how most\nof our most of our teams feels it's\nreally sped up our our development quite\na lot\num\nEli this is awesome\num I'm John I'm a product manager on the\nunion side I said a quick question on on\nthe first bullet around Providence like\nthe flight will save its intermediate\ndata sets in your uh in your blob store\nuh how do you how do you um kind of\ntrack the the artifacts on your side\nlike do you kind of store a link to an\nS3 bucket or like what what's the yeah\nright now this is very subject to change\nbut we have weights and biases which is\nlike a training monitoring thing I don't\nknow if you guys are familiar so we\nactually store models and data sets and\nartifacts and stuff on waste and biases\nwith a reference artifact that points\nback to the GCS location that was\ncreated by flight so we never re-upload\nit to weights and biases but right now\nin terms of like the metadata around\nProvidence or like the visibility we're\nlying on weights and biases for that\nsome of that is like you need kind of\ndifferent like you need different kinds\nof Providence for the like the model\ntraining versus like the you know I ran\nthis workflow and It produced this\nresult\num\num\nwe talk a little bit later about like\none of the things we'd like to see more\nof is like tagging and filtering by tags\num\nthe other note I would have on the\nflight it'd be great if there was an\noption to have like human readable\nsemantic uh blob paths\num\nuh I think that would provide a lot more\na lot more clarity where like you\nwouldn't necessarily have to go search\nlike what is this blog path it would\nhave a little bit more metadata encoded\nin the actual path\num I don't know do other people have\nthoughts on the the provenance stuff\num\nI think I think for us there's like\nthere's so much stuff that we do that's\nsort of outside the context of what a\nmore specialized tool like weights and\nbiases would keep track of that it's\nreally important to have this kind of\nlike top level like real source of Truth\nprovenance thing\num and like obviously it would be great\nif you know there were more of the ml\nspecific tooling\num or like you know does other kinds of\nprovenance\num but\nlike half-assed Providence isn't really\nworth anything because you you break the\nchain and so the fact that we do a lot\nof stuff that doesn't fit naturally into\nweights and biases I think um we really\nconsider like the source of Truth here\num awesome thanks\num cool\nuh yeah so so this results in some some\nmore complicated workflows\num\nparticularly like our team's extensive\nusage of this uh you see this has like\ntwo sub workflows and then those are\nboth composed together in a larger\nworkflow\num and we have a bunch of workflows that\nthat look like this\num\nuh it's great that this works uh this\ndoes start to get into sort of like our\nfirst challenge with flight\nwhich is essentially like how do we\nencourage good code practices when we're\ncomposing stuff\num so like\nat least from my perspective when I see\nlike a dag like that that has this nice\nkind of hierarchical like maybe I would\nwant to repeat one of these units or\nsomething or like map over one of those\nworkflows\num I'd like to be able to build in some\ncode isolation that mirrors the the way\nthe execution is structured and\num and the way that it feels natural for\nme to do that is have these sub\nworkflows be packages which are depended\non by sort of the the the code that's\ncomposing them\num\nand um and in particular here as the the\nsort of the highest level workflow grows\nyou don't want to have to necessarily\nrebuild and re-register every sub\nworkflow every time you change something\nin one of the others\num\num and so we've made the choice to to\noperate in within a multi-package python\nmonorepo which is you know not something\nthat is I would say typical in in Python\nyet but I think it's growing in\npopularity and you can see that as some\nof the the popular python packaging\ntools like poetry which is what we use\num add support for this modality\num and\num this next bit I'm going to talk about\nis kind of like the challenge we've had\nin overlaying like a monorepo\nmulti-package approach on sort of the\nflight Concepts\nso so the the first thing we we want is\nessentially that\nI don't want to have to\num depend I want to be able to have\nseparate dependencies\num and like ideally a completely\nseparate python you know environment but\nbetween development of these sub\nworkflows right so like\num if if package a depends on package B\nyou know it's not like I'm directly\ndepending on the dependencies of that\npackage right there is this sense of\nindirection which gives you a little bit\nmore flexibility in versions and stuff\nand we kind of wanted to mirror that in\nthe way that we compose workflows\num and so what we've done is we've\nactually wrapped the task decorator\num so that you when you're importing\nbetween packages you import via remote\nreference for a flight remote\num so in this example if you have\npackage a and you have cast and you have\nthis food task and package a you have\npackage B and you have a workflow that\nincludes Foo as a task right that Foo is\na remote reference and say you had you\nknow\nsay you had a package which had finicky\nversions and you were running into\nDiamond dependency problems in the\nDeclaration of Foo because it's a remote\nreference you aren't going to have to\ninteract with that package when you\ndefine bar\num\ndoes that make sense\nso uh we I can get into like this is the\nat a very high level how the\nimplementation works which is basically\nwe check if uh if you're registering\nright if if not it's you know a\npass-through to the original decorator\num and if you're registering\num we get essentially our is my current\nobject\nin the in the registration path\nessentially so if you think of a\nregistration as scope to a a sub tree in\nthe file tree which which roughly\ncorresponds to a package then we can say\nuh is this object in the the being\nregistered subtree and if it's not the\neverything in the subtree gets\nregistered with that R object as a\nruntime reference and this allows us to\nactually decouple the packages of of sub\nworkflows which has been nice\nhey Eli this is Kate and hi uh nice to\nmeet you\num so isn't this the same as reference\ntest it's just that you're an automated\nthe creation of reference tests is that\nif anything we're literally using\nreference tasks yeah the difference here\nis that we're like yeah\nat a moment yeah importing something\nwe're deciding whether to give you an\nactual task or a reference task the same\ngoes to work clothes as well yeah\noh how are you deciding that that's\nbased on\nyou decide based on whether you're\ncurrently registering that package or is\nis your object is that function that\nyou're importing in the package that\nyou're currently registering or is it\nsomewhere outside if it's outside so can\nyou do the right yeah so this is a\nreferencing this enforces a little of a\nof an object boundary uh or it enforces\na little bit of an implementation\nboundary because you literally can't\nimport\nuh a workflow from outside your package\nas a non-reference\nyeah make sense\nbut but when you import I saw in your\nprevious statement in your previous\nSlide the import was still loading the\nmodule right or maybe I'm not yeah it's\nloading the module but because this\nhappens in The Decorator the mod right\nlike if you're ready your module is\nliterally just a dictionary of reference\ntasks if you're if you're importing the\nmodule outside of your package also the\nthe way for this to work you need to\nhave local Imports for for tasks right\nso otherwise you would get the not uh\ndoesn't like you'd get your your\ndependencies would not be there for for\nthe reference um for the reference\nbut only yeah if you have import\nstatements those are still loaded right\nso that still may cause a problem the\nimport statements are loaded but they're\nyou load the when you're loading the the\nyou know a module is just an object\nright it's it ends up being an object of\nbecause of how The Decorator is defined\nit ends up being an object of reference\ntasks\num so like if uh if a package has a\nspecific dependency that like yeah yeah\nso this allows you to avoid large\ndependencies that are imported inside of\nthe task so that's kind of okay right so\nlike if I if I have an import that will\nonly ever be run inside of the wrapped\ntask logic that logic is avoided by\ndoing the reference task right\nyeah but I thought that you should\nalways be the case but we do a split\nbetween the like\nImports that are needed or sorry the\ndependencies that are needed to compile\nthe workflows and tasks and the\ndependencies that are needed to run the\nlogic within the tasks yeah and then the\nthe\nformer are just imported within the the\nfunctions that Define the tasks so\nthey're not\num causing any any issues with the with\nthe referencing\nyeah this is this is very similar to\nwe've had internal discussions of image\nspec and the same same issues come up\nhere of compile time dependencies versus\nruntime dependencies which\ntoday you have to kind of Reason about\nyourself as a flight user yeah so this\nis just an attempt to I think remove\nsome of that reasoning from our from our\nDev team\num yeah that seems to seems to work\npretty well for us at least we stopped\nwe we got a lot we get a lot fewer\nquestions about like why is this\ndependency not here\num I don't need it for this package etc\netc\num\ncool\num yeah so so separate python\nenvironments smaller Docker images\num no change to Local development\num\npropagating version changes bottom up is\ncumbersome that's a little bit like is\nthat a feature or a bug uh that's a\nlittle bit like this isolation like you\nhave to do a separate rep uh\nregistration for each package in your\nmono repo right which is\nyou know if one registration updates all\nthe artifacts for all the packages in\nyour mono repo do you have a\nmulti-package model repo or do you\nreally just have one giant package that\nkind of looks like a bunch of smaller\npackages you know there's a little bit\nof a of a if there's no actual like\ndeploy boundary is there actually a\nversion boundary right\num\num introducing uh need for reproducible\nversions across the dependency tree that\nis like quite annoying we have a little\nbit of a hacky thing of like a versions\nJson file that tells you which versions\nof the reference you should fetch for\nthe given package\num this is a place where more tagging\nwould be great if you could tag versions\nwith like Branch tags for example and\nyou you know checked out a git branch\nand that automatically meant all your\nyour reference artifacts were fetched\naccording to that that tag\num something like that would be awesome\nso that we didn't really find anything\nthat didn't involve checking in another\nfile which was ended up being slightly\nhacky here\num\nthis is a little bit getting into like\nhow much do you want to step into the\nterritory of a package manager as flight\nuh is is kind of a I understand there\nmay be some trepidation in moving into\nthat space\num so\num yeah\ncool\num okay uh sort of the next thing that\nthat's missing and I think I've had a\ndiscussion with a a few of you about\nthis is manually composing executions so\nwhat does this mean so like let's say\nI'm in this large uh this large workflow\nwhere I have sub workflows which are\ndependent on Upstream sub workflows\niterating on that is slightly annoying\nright because I need to if I want to\nrely on the cash in the iteration of\nlike a downstream workflow I need to\nlike make a change in the downstream and\nthen re-register all of the the entire\nworkflow like together in order to get\nit to run\num\nso it's slightly easier if I can kind of\nlike grab the outputs of the Upstream\nintermediate workflows and then just\nrerun that that sub layer you know this\nis kind of like I'm just grabbing the\noutputs uh if I just want to test one\nfunction at a time you know I kind of\nwant to mock out everything else\num so that's one use case\num the other use case is we found is is\nhacking our way around Dynamic uh which\nis another one of those components that\nwe sometimes struggle with um in in\nhaving edge cases\num so this is essentially you know like\nDynamic is really what it's doing is\nrunning code that generates a workflow\nas a task so this is kind of us moving\nthat process just into our notebook\num where we're able to queue up a bunch\nof workflows by depending on the outputs\nof you know unfinished workflows\num the final thing is like if I'm if I\nwant to prevent or like I want to\ndiscourage users from just like copying\nand pasting raw outputs when they're\nmoving from one workflow to another\num\nthe the reason I want to do that is that\nthat breaks your Providence chain right\nyou're no you no longer know where that\nraw output came from right so I just\nwant to make it easier for users to say\nuse the output of this workflow as the\ninput to this workflow right\num and uh and that way you preserve\nprominence information because the the\nworkflow that that output came from is\nactually part of the input to the next\nworkflow\num and so we did two things here um\nseemed to work pretty well I think this\nis still in a branch I need to merge it\nbut uh the tests all work and everything\nseems to be working well\num uh first we just made flight workflow\nexecution\nflight serializable uh it's basically a\ndata class uh it's pretty easy to do\num so you can just pass fight for flight\nworkflow execution objects around your\nyour um\nyour uh workflow\num\nand uh the next thing we did is I I we\njust made some syntactic sugar for like\nan output ref which just encapsulates\nthe concept of a workflow and an output\nname along with some some type metadata\nthat allows you to not need to specify\nlike the type that you're decoding the\noutput has\num so you see the Syntax for that in in\ntwo tasks here\num\num\nyeah you'll also notice we have a little\nconvenience I we I don't think we have\ntime to get into it today but we have a\nlittle convenience pod template Library\num which we use to sort of um\ncompositionally uh like compose uh\npartial pod templates\num which we found to be the the easiest\nway to use that functionality by far so\nuses flight remote here just means we're\ninjecting the secrets necessary to\nauthenticate to the flight remote into\nthe task\nuh yeah and the the test kind of shows\nyou the usage from the the client side\nany questions on that\ncool yeah some of it you could\ncontribute Upstream yes I think we would\nlove to uh we're we are like always a\nlittle oversubscribed but uh but I think\nyeah that's several of these um\num these we would love to contribute\num\nuh and we have a we have a bunch of just\nlike kind of trying to smooth the flight\nlearning curve stuff that I think would\nactually probably be easier to\ncontribute than this like the Pod\ntemplate library and\num stuff like that\num\num that that isn't listed here but at\nsome point we should get around to\nsurfacing to the broader Community\num so there's I think two things that\nwould make this work even better\num one we from a security perspective we\nfind it a little bit sketched to provide\nfull flight remote position like two two\ntasks\num\nuh so we need more granular permissions\non flight remote we think at the very\nleast it'd be nice to have separate read\nand write promotions so we could you\ncould fetch you know workflow outputs\nand you could fetch already to find\nartifacts and not have access to the\ncreate verbs\num\num\nfinally this like this is another\ntagging thing like fetching the correct\nexecution is kind of annoying like\ngetting the ID\num so this is another great I mean you\nthere are already tags for executions\nnow I saw there's no list verb\nassociated with that that might be uh\nyet\num so I don't know if that's coming but\nthat's what we really love um is if like\nwe could say not only like this is an\noutput ref of type int but like\nI want to fetch the latest thing with\nthe tag you know whatever and get that\noutput and feed it into this workflow\nlike that would make this really\num really fly from from the perspective\nof of I think our internal users\num cool uh okay\nsome more housekeeping things we need uh\ndeleting data is kind of big for us we\ndeal with uh you know farmer companies\nare notoriously paranoid about their\ndata and so we need to be able to\npromise them that we can delete all of\nit\num\num yeah\num\nand yeah so this is basically like we\ncan delete stuff in the bucket obviously\num it just kind of makes it disappear in\na way that's not super transparent to\nflight\num but we can't really delete stuff\nthat's that's stored in the table\nso right now we have two workarounds\nlike basically we can either keep all of\na customers data in a separate flight\ndeployment and just nuke that deployment\nif the customer decides to do uh uh take\nout or\num or we can make sure that all of the\nsensitive artifacts are stored in files\nso this is we've done stuff like instead\nof a list of sequences we use like a\nnumpy array or like a list of strings we\nuse like an Umpire array of strings\nbecause then it gets stored as a blob\nand we can delete the blob\num which is definitely not intended\nusage\num but it does get us around this\nproblem so so this would be really nice\num\nand I think uh\nthe other sort of large thing\num is better mapping and streaming uh\nthis actually ties back into the\ndiscussion we're having before about\num\num\nthe some of the model startup Time stuff\num like the most common use Case by far\nfor us for mapping is wanting to do uh\nlike bash inference over a large number\nof examples with like expensive\ninferences\num but it doesn't really make sense to\ndo in like a regular map because every\nsingle map item has to correspond to a\npod\nif the Pod startup is downloading the\nmodel you're now you're downloading like\na you know whatever 10 gigabyte model or\n20 gigabyte model for every map shower\nthat just doesn't make any sense so\nobviously we have to do like batches\num and that's fine it's like a little\nbit of extra boilerplate everywhere we\nuse map tests which is kind of annoying\nbut um the bigger problem is that it\nbreaks caching because you're caching\nbased on batches and not caching based\non items so something that functions a\nlittle bit more here like uh you know\ntraditional mapreduce\num where you you have maybe or like uh\nsomething like Apache beam where you\nhave like more static shards\num\nand\num\nand then you're caching on like a per\nitem uh basis the other way you could go\nof course is you could go like full\nstreaming which would solve this problem\num sort of implicitly if like there were\nthere were stream Primitives that were\ninputs and outputs to tasks then\nobviously we could just sort of batch\nstream\num\num that way uh for places yeah can I ask\nyou a couple questions in this uh one is\nyou could do\nmanually you could batch right like and\nin a map task and you prefer not to do\nthat manually batch\nno like without using a q primitive just\nlike use a like a list of lists really\nor a list of data frames yeah and then\nwe do that that's one of the workarounds\nfor for batch batching the issue there\nis then you're caching on your\nso yeah so that is one of the reasons\nwhy the caching is the hard part over\nhere that's why we've been not doing it\num though array notes\nso we have done it one of the goals is\nto and that's the new replacement of\nmatch map tasks underneath one of the\ngoals is to have uh sticky pods\nuh and and do something around it so\nhappy to connect and let's you know I'd\nlove to understand and see how we can\ncollaborate yeah another part of the\nstreams actually there is an internal\nref I had written like maybe a year ago\nI am supposed to put it out at some\npoint\num stream literals is what we call it uh\nso it's uh the problem is the default\nimplementation of stream literals is\ngoing to be hard for most people to just\nlike yeah I want to just get it working\nright but uh would love to share it with\nyou yeah yeah\nno that'd be I'd be really interested to\nsee how that works\num so the workaround that we've done\nhere which we use some places\num where it's like really critical\num is uh we just defined a q primitive\nthat we can pass around workflows um\nthat just is backed by a redis queue\num and then we've tried to make this\nfeel as first class in-flight as we can\nuh so like there's there's checkpointing\nwhich goes to like like act Q items and\nstuff\num\num and uh\num you know that it uses flight uh uses\nthe flight type system to deserialize\nand serialized items onto the queue\num stuff like that\num\nit's a it feels a little bit heavyweight\nfor most of the things we wanted to do\nand it'd be nice if it were uh it didn't\nfeel that way but\num\nyeah serviceable\num\ncool\nokay uh the last and this is like a much\nmuch broader topic but this is this is\nsomething we find ourselves wanting\nquite a lot which is just more broadly\nlike higher order functions\num so if I have a workflow\num and really all I want to do is is\nswap out some internal component in that\nworkflow that requires me to re-register\nyou know however many nested levels of\nenclosing workflow\num are there just to get that Swap and\nand that's fine for one swap but if you\nhave to do that with multiple sub\ncomponents to have multiple valid you\nknow tasks or workflows to to be that\ninner logic you get a combinatorial\nexplosion problem pretty quickly in\nterms of the number of registered\nworkflows\num\nobviously like you know viewing dags as\nlike a purely functional Logic the the\nway to deal with this is currying and uh\nwhat that requires is you know some\nconcept of being able to pass functions\nin as inputs and outputs\num which is\na bit of work\num we we think there's like most of the\nstuff for this is in place\num but the the key missing feature that\nis really the Lynch plan for this is\nlike support for\nfor VAR args like support for this is an\narbitrary set of args and I'm able to\nforward everything that is Unbound that\nwas an input to this workflow to an\ninner task of my choice\nright\num because without VAR args there's\nreally nothing here but like an enum for\nlike fetching a task which is you know\nwe could do it's kind of obnoxious and\ngenerally doesn't solve our problem\nbecause it's generally not you want to\nswap out something with the exact same\nsignature it's like you have a family of\nthings that all like meet a protocol and\nthen they also have some additional\nconfiguration and if you don't have that\njust moves your problem up one level\nright then you somehow have to Curry all\nthose things into the exact same\nsignature\num\num so on and so forth um so that's\ngetting a little bit more into the like\num\nobviously longer term more stuff\num like larger things that that we like\nbut I think this is something we do find\nourselves\nstarting to hit the beginning of this\ncombinatorial explosion problem and\nmaybe to give you a concrete example of\nwhere that shows up this actually also\ngoes back to this building models into\nDocker image uh that we were saying is\nlike we frequently find ourselves with\nkind of like a model agnostic pipeline\nexcept for this core sub workflow right\nand this core sub workflow is\nincompatible between two model types\nbecause for example they have\nincompatible Docker images like they\njust have dependency trees that we can't\nfit together so we can't make a Docker\nimage that fits all of our models\ntogether in one python package it's just\nlike python dependency hell prevents\nthis from happening\num and so what we'd love to do is we'd\nlove to have you know one task one\nworkflow that's like this is the\ninference task for this model right it\nhas associated with it the docker image\nthat works\num you know it wraps the the model in a\ncommon interface and we can swap it in\nand out of a workflow and that that\nworks great except that if you make that\narchitectural Choice across many\ncomponents of your system you get this\ncombinatorial explosion problem\nyeah we used to call it interface tasks\nand this was actually four years ago we\nhad come\nand there is some if you found some\nhints up there uh passing functions as a\nfirst class the task templates are\ninherently possible and serialized that\nwas one of the reasons why they are and\ndynamic is essentially doing that so we\ntalked about it but we've never talked\nabout making arbitrary interfaces\nbecause that breaks combined time\nguarantees uh and so our arcs become\nextremely complicated and then and then\nwhat is the the caching is where the\nnuances lie yeah so it does throw a\nwrench in there I'd be interested in\ntalking about how you would uh solve\nthis problem using task templates\nbecause I think\nuh I was probably just gonna hack\nanother you know custom type serializer\nwhere I serialize the workflow in here\nuh which is probably not the right\nchoice but uh yeah yeah but happy to\nhappy to share like how we think uh how\nI think we can solve this problem\num and but but Karim might still be you\nknow the absolute requirement of\ninterfaces might still throw a wrench\ninto this like your potential usage but\nbut I would love to explore more like\nyou know yeah I mean there there is an\nintermediate here right which is like\ndoesn't support the varrogs but it's it\nit's at least nicer to use than like an\nenum switch remote reference thing yep\noh that's completely doable yeah yeah\nnot even I I would make it as an input I\nthink your computer is done yeah all\nright go ahead yeah\num cool so I think that's that's our\nlast slide uh we have\num\nI think\nyou know more stuff we we could share uh\nwe ran out of time on the the\npreparation side as always like to start\nup life just over subscribed but um I'm\nlooking forward to sharing sort of more\nwith you guys um in the future so\nforeign\nany more sort of uh default\nquestions or like or larger questions\nsort of not default questions\num I think this presentation was great\nlike it gave a lot of tips uh can you\ntell us some more about\num I saw the one quote but in overall\nlike why how did you learn about flight\nhow did you end up using it and yeah\nyeah what was the aha moment for you so\nlike I think from nine or ten months ago\nor something and and I embarked on what\nwas supposed to be a very short\nevaluation period for workflow platform\nso like we knew we needed a workflow\nplatform we had just like spaghetti code\nall over the place and uh you know just\nlike wait we way too much uh random\nswitch statements and\num and um\na lifetime ago I worked on tfx at Google\nactually\num\num which was of course Dead on Arrival\nbecause it was one of those those\nprojects that was like developed\ninternally and then surfaced and didn't\nmeet anyone's external needs um so I was\naware of kuplo because that was kind of\nin parallel developed in open source\num so I think that's where we started\nour evaluation journey and then we\nlooked at we we felt like it was making\na little bit too many choices we didn't\nreally feel like the SDK was very high\nquality\num and we looked at like oh could we\njust build something ourselves on top of\nArgo\nrealized the way Argo was doing caching\nAKA not at all\num didn't really meet our needs\num and we're just like looking for other\nstuff out there I don't I don't remember\nwho it was who uh suggested flight\num\nnow remember we also talked to to some\npeople who are using workflow tooling\nand then just collecting yeah what other\npeople were using but so this result was\nalso there we also worked in the airflow\nso that like I think by the time we\nfound flight we were literally like a\nmonth and a half into our evaluation\nlike and already like this is the last\nthing we're trying writing writing our\ndemo pipeline in different workflow\nengines over and over and over again uh\nuh and like wanting to like gouge our\neyes out with screwdrivers um and I\nthink\num then I think we got to flight and\neverything just worked which was really\nnice and the SDK felt high quality\ndifficult thing with like with\ndeployment at that point yes yeah and\nthen once we've managed to deploy it\neverything else was uh yeah and yeah I\nguess also felt like the SDK was really\nhigh quality which which was super\nimportant to us because like we know we\nknow we're going to be in Python you\nknow we're doing ml stuff we're\nbasically like hard locked to python\num\num like having some language agnostic\ntool was not super important to us\num\nsupport in the invest yeah yeah the\nslack Community was was also huge I\nthink in the early days we asked some\nsome quick questions and got really fast\ngreat replies\num\num so that was that was definitely a big\nbig influencing Factory too yeah we\nactually have a slide deck somewhere\nthat we presented to our team internally\nwith like all our evaluation of all the\nworkflow tools and the the pluses and\neverything we could send that out if\nwe'd have to make sure we strip all the\nconfidential stuff out but uh\num but yeah can you believe every\ncompany that like uh you know I talked\nto a lot of company almost everybody has\nthat\nnobody ships it out and they asked me to\nship it out and I'm like I I cannot ship\nit out this is not going to be fair I I\ncannot compare it but you know this is\nreally great\ntoo harsh on the bad tools you're like\nwas my frustration coming out in this\nslide deck too much\num\nyeah\num yes I think that's that's kind of uh\nthe Journey of how we got there and then\nI I will say I think there was a pretty\nsubstantial warm-up time for the team of\nlike getting used to some of that was a\nlot of the team just not having\nexperience working in any like workflow\ntool before\num so like for me where I'd written some\nstuff in kubeflow and I think for both\nme and Anna who at this point were like\nmaybe over overly familiar with the the\nclassic abstractions of these kinds of\ntools it was pretty quick but I think\nfor a lot of the team it was\num it took quite a while to get\nacclimated to the tool I think pretty\nmuch everybody is is is quite on board\nnow and really really likes it as a tool\nbut there was I would say at least a two\nor three month startup time where we're\nlike moving over code always felt\npainful and you're like ah why is flight\nforcing forcing me to rewrite this you\nknow\num\net cetera et cetera so\nwe've seen the cold start problem but I\nthink usually what we see is the cold\nstart is a hump we have to we have to do\nas a community something to make sure\nthat the course that goes away but\npeople once they go get over the house\nthey you thought to go back to other\nthings but you know\nyeah so that's why help us\nyeah the thing is you'll forget about it\nyou'll not remember it yeah\nyeah like so many things\nawesome yeah that was awesome that's my\nquestion great any other question\nuh\nyeah one one last one so if you uh\nI think you probably are in the biotech\ncommunity and we have seen a lot of\nuh you know adoption of light in the\nbiotech Community as well as other\ncommunities but\nwhat would you tell like a new so many\nbiotech companies come and talk to us\nand ask like what what would you tell\nthem what's like the one line and you'll\nsay like hey this is why you should use\nlike not use right whatever you know\nhonestly I wonder how much of it is like\nthere is just a little bit of gravity\naround it's the name that's being passed\naround in the biotech Community I don't\nthink that's how we got it actually but\num but\nsome of that I guess you're gonna have\nto decouple like if it's a biotech\ncompany and it's a little bit less\nfamiliar with this programming Paradigm\nyou're gonna have to decouple explaining\nwhy you should use a workflow tool and\nexplaining why that workflow tool should\nbe flight uh uh and I think both of\nthose are pretty easy cases to make and\nthen you put them together and it's a\npretty hard case to make because you're\nlike constantly having to go but like\nthis other tool has this thing it's like\noh yeah that wasn't one of the fight\nspecific things that was just like a why\nyou should use a workflow tool in\ngeneral thing\num\nso like in terms of flight in general I\nwould say the easiest thing is like it\ndoesn't\nchange how it feels to write python as\nmuch as most of the other libraries and\na lot of I think biotech people at this\npoint are pretty comfortable you know\nworking in Python and\num\nand you don't need as much familiarity\nwith the other tools like the the more\ndevopsy side of the tools as for example\nsomething like Argo the other thing I\nwould say is it's not as opinionated\nabout the use case as something like\nkubeflow where it's like coupon is a\nmuch more like ml-centric tools\num around it\num\num I may have to switch to Anna's\ncomputer because my my computer is about\nto die but\num\nuh yeah I guess that's that's my other\ndoes anybody else have\nthoughts on biotech in particular why\nfor flight\nI think those are the two main things\nit's a little it's it's uh it's a little\nbit more friendly to people who aren't\nlike really hardcore devopsy you know\nlike I was able to deploy kubernetes you\nknow to my toaster\num type people\num yeah I think something that's not ml\nspecific is that in biotech you're using\nalso a lot of these classical tools that\nhave very different infrastructure\nrequirements so flight is doing a really\ngood good job here providing this\nflexibility when we need it so as we\nsaid at the beginning like one one task\nthat requires one terabyte of ram one\nplus that requires like as many CPUs as\nyou can find one plus that needs to run\non a100 and so on so we have like in one\npipeline multiple tasks that have\nextremely different requirements from\nthe infrastructure perspective and in\nfact it's pretty much transparent when\nwe write this\nyeah on the ml side we have like\nembarrassingly parallel GPU enabled you\nknow fancy algorithms and on the biotech\nside we have like requires one terabyte\nof RAM and is single threaded c\num you know uh and and being able to\nsupport both of those things is is\npretty nifty\nso that's pretty good because I think\nit's combining you know your domain\nspecific stuff with them and that's\nwhere the power light it's not just\ndoing ml or just doing it\nand eventually data\nyeah thank you that that that's a lot of\nmoney so\nawesome that was great yeah well we're\nwe are at time and um yeah again thank\nyou all thanks to the entire cradle team\nfor sharing for taking the time I know\nit's not easy so thank you again I I\nfeel that we need some follow-up\nconversations I feel like that some of\nyour workarounds are actually will be\ngreat additions to the project and also\nsome of the feature requests uh deserve\na better discussion probably in the\ncontributors Meetup where we meet with\nall the maintainers and the steam\ncommittee will be a good sink place to\ndiscuss this but anything\num yeah again thank you all for joining\nthank you to cradle for for sharing your\nstory here and uh yeah we'll see you in\nthe next one for sure\nall right thanks thanks thank you so\nmuch guys\nthank you"
    },
    {
        "title": "Community Member Spotlight | Flyte Community Meeting - August 22, 2023",
        "transcript": "today uh unlike the previous\npresentation formats we're gonna have an\nopen conversation about flight with is\nthe arche Siddiqui consider this a laid\nback space to discuss about flight be it\nflights use cases pain points and so on\nuh so we want this to be an open-ended\ndiscussion so please ask your questions\nand feel free to chime in uh let me stop\nsharing my screen\nall right\num\nlet's get started uh is there would you\ncould you introduce yourself\nuh hi uh I'm originally from Bangladesh\ncurrency I'm in Germany I am pursuing my\nmaster's degree in data science uh the\nofficial title is amazing data and\nknowledge engineer uh I'm working in a\nmaster thesis so um I was um actually\ntrying to parallelize my workload in\nbare metal kubernetes cluster that's uh\nthat's how I came into a flight and I\nwanted to explore uh what are the things\nthat's possible but I was having some\ntrouble then I joined your slack Channel\nthen here am I\nokay so could you briefly talk about the\nuse cases within your company that led\nyou to use flight\nuh well it's not uh uh you know it's not\na company so I\num working my thesis on uh under a\nprofessor so uh the use case is\nbasically um I have I I have a bunch of\nuh data a bunch of model uh to train in\nhyper parameter tune so uh so it will be\nmore like 72 sets of uh hyper parameter\ntuning and model building so it's a huge\namount of tasks that I cannot do on a\nsingle machine let alone my own uh\ncomputer so I have a bare metal uh\ncluster of kubernetes then\num I was initially trying with Ray but\nuh it's a little bit cumbersome because\nof the versioning and uh it's uh it\ndidn't work out properly let's just put\nit that way so I'm not trying\nAlternatives basically\nokay so how are you finding flight\nuh so far it's good because\num but the documentation is a little bit\num\nnot on that spot exactly for example I\ncould not find proper documentation for\nhow to set up in uh barometer and that's\nwhy I had to uh come into slack but if I\nif I have to talk about flight as an\nexperience it was good I mean everything\nworks if uh if I set it up then I can do\nparallelizing uh without any worry\nactually it's good\nso have you been able to deploy flights\nsuccessfully on your bare metal cluster\nokay yes but normally\num official cluster but on my mini Cube\ncluster that is also running on bare\nmetal\nokay got it got it so does anyone have\nany more questions\nyeah sorry um you can find the button to\nraise my hand so\num yeah well again thank you stiak for\njoining it's it's great that you're here\nso I'd like to dive a little deep on in\nyour use case that probably for now it's\nunder the research\nArena but\num I I would like to know about you\nmentioned you want to you need to\nparalyze uh some tasks right and um\nprobably if I ask someone out there on\nthe street\nStreet full of data scientists uh what's\nyour top of Mind tool for paralyzing\ntasks I don't know if flight comes uh in\nthe list at least in the first\nuh at the first\n[Music]\num\nuh yeah this is the first option or not\nnot sure so my question is have you\ntried any other tool\nalongside with flight or before flight\nor or live flight for for this kind of\nuse case or parallel workloads\nyes that's a great question actually\nbecause I as I mentioned earlier I did\ndrive Ray and uh it took quite a bit of\neffort to set up a initially uh as um\nlike when I uh when I was successful in\nsetting a play uh it turned out that uh\nuh submitting tasks in Ray it's a little\nbit difficult for me uh I tried a lot of\ncombination uh eventually I was\nsuccessful but it was a bit cumbersome I\nneeded a a more flexible way to uh\nparallelize so for example if I create a\ndynamic workflow I can run multiple uh\ntraining jobs at the same time that's\nsufficient for me I don't have to uh\nparalyze the algorithm per se if I can\nrun uh one uh uh one tuning in one uh\ncontainer I'm good with that I don't\nhave any problem\nbut if I have to run them sequentially\nthen it becomes a problem but\nsolved this problem uh initially but the\ncumbersome nature was very difficult to\nhandle because eventually what will\nhappen is after I am done with my thesis\nthis whole setup I need to hand it over\nto other people so they also need to\nunderstand how these things work\neventually otherwise they can't operate\nso in that regard flight scenes uh bit\nmore user friendly when it comes to\nprogramming\nhi yes this is Martin I'm with Union as\nwell so I have a question for you I\nthink it's awesome to to first of all to\nlearn\num you know your journey so but and then\nthe experience with documentation I\nthink you would you've seen a lot of\npeople nodding here uh yes it's it's a\nit's a thing we have to improve\num what surprised you in the positive\nside you know when you when you set up\nflight what was the thing that he that\nhe didn't expect but surprised you\npositively\num thank you Martin for the question so\nuh when I uh had the first uh suggestion\nfrom uh one person I don't remember the\nname uh from from your slack channel uh\nthat suggestion that advice uh like you\nknow\nexactly at that position so I didn't\nhave to Google anymore it just worked so\nuh after that when I uh deployed my\nfirst flight job there was no other\nissue so it ran smoothly which was\npretty relaxing from my side because uh\nmy experience with Ray it was extremely\nfrustrating because it took around one\nor two months for me to eventually\nfigure out how to do it so it took a\nhell lot of time\nwow that's a lot of time thanks for\nanswering yeah I think it was Victor\nVictor shurikov who helped you\nis a fantastic\nyeah the the other thing that I was uh\ntrying also to understand is\nyeah in terms of your experience so far\nbesides the documentation or well maybe\ndive in a little deeper on Doc's piece\num bare metal right you you mentioned\nyou're you're trying to run fly you run\nrun and fly number middle and I I can't\nimagine a number of reasons why is this\nthe case I don't know budget\nuh universities tend to have a certain\nbudget\nrestrictions in terms of consuming Cloud\nresources Etc performance uh but but\nwhat is in this case the main type of\nconstraints that drive you to use\num flight on a local kubernetes\nenvironment or on-prem kubernetes\nenvironment\nyou actually made the uh mention the\npoint it's about budget so uh we don't\nhave that much money so that I can\nafford uh AWS cluster or something like\nthat on the other hand um the lab I'm\nworking on uh so they have\n12 or 15 I don't ex remember the exact\ncount 15 computers and uh only once in a\nwhile in order to computer gets some uh\njob so when I asked my professor whether\nI can set up a cluster she allotted me\nsix computers so I said okay that's good\nI mean I don't need anything else\nactually\noh\nyeah one other thing that I found sorry\nI was just stalking you on LinkedIn and\nuh no problem what is that most of your\nexperience is in the software\nengineering space and now you're you're\ncompleting your thesis on on the ml side\nand using a tool like flight\nso in in your opinion what is the kind\nof main differences between software\nengineering practice and ml as a\npractice\nuh what what have you observed so far\nI think it's pretty early for me to\ncomment on this uh because so far\nwhenever I code I I always keep those\nsoftware engineering principles in mind\nso uh it's a never a notebook for me all\nthe time no I try to maintain a clean\ncode structure because eventually uh if\nI have to put it in GitHub and someone\nstalks me then they need to understand\nokay this guy knows how to code it's not\nlike he's just you know doing some\nrandom thing\nso are you planning to run any other\nworkflows besides the hyper parameter\ntuning workflow that you are currently\nworking on no\nonly happy penalty to me and model uh\ntraining basically\ncorrect\nand how are you using sorry sorry go\nahead go ahead\nso are you using any Integrations\nbecause we have\nqueue flow to run distributed training\njobs or even rate\nyeah I am interested in Ray um\nsome\nuh uh what you call this documentation\nsome worked out examples on Ray maybe\nI'll use it because uh I have not got\nthe time to experiment with that on the\nother hand I have experience with three\nso uh that will be easy for me to set up\nbut uh on the matter of cube flow um but\nI'll not experiment with deep learning\nmodels these will be traditional psychic\nlearn models so I don't think I need\nCube flow in this case but yes I need ml\nflow so I will use human flow\ngot it got right so it's like if if you\nwould give the devrel team here's some\nsome feedback and advice about uh how to\nexpand\num the reach to find people like you or\nfor you to find flight easier let's put\nthis way what what do you think should\nshould be done I mean now you're in\nGermany\num do you think meetups speaking at\nmeetups is a good thing uh what comes to\nmind that you think would have helped\nyou to find flight sooner\num Community meetups can help so for\nexample I'm also connected with data\nscience for social good Berlin chapter\nso we meet once in a while and I see a\nlot of uh like-minded people in one\nplace then we uh chat together and we\ndiscuss about a lot of scenarios\nI think that can help\ngreat great and I feel and what was your\njourney again I missed this earlier you\nsaid uh you you read about flight was\nthis just by by proactively searching\nfor solution or by just you know hearing\nit uh you know in a in a contextual\nconversation uh no I was actually\nexploring airflow then uh um flight and\nuh prefect all those orchestrating uh\nplatforms and uh airflow seemed very uh\nluxurious on my local computer and uh\nprefect was okay but it was not\nresponding properly then I came to\nflight\nand the flight worked pretty quickly\nfantastic so now you're working in\nresearch obviously\num but there's also an application to\nthis if you would let's assume you would\nyou know start your own company and so\non and so on I mean and you have an\nengineering project that you want to\nrealize\num do you think or what do you think is\nthe best way to to to to to number one\ndiscover technology like flight for\npeople in that situation I mean your\nresearch you have time you do research\nyou figure out what you need others need\na solution quickly right and uh when\ncompanies and organizations that might\nnot have enough time uh to figure out\nwhat works what do you think is a good\nway to have those uh teams that are\nsoftware engineering teams so to David's\nPoint earlier that don't have an ml\nbackground but they have to look across\nyou know some other Solutions how can we\nhow can we make sure that they can find\nflight what do you think I think you can\nemploy some uh bloggers those who write\nin uh medium and towards data these kind\nof places because I I see whenever I\nGoogle about anything uh towards data\nscience uh gets a very high ranking on\nmy search page so uh if if you have if\nyou can afford to have some bloggers to\nwrite about these stuff their\nexperiences how these things works also\na comparative uh articles about how uh\nwhat was the experience between flight\nto airflow prefect all these things\nmaybe it can help\nyeah\nI think it's great feedback\nyeah that's great\nthank you\nis there any other question\nup there if I have one more question\nbecause I think I think you're such a AC\nis like so interesting so looking\nforward I mean what do you I mean\ncareer-wise and obviously you right now\num I think you said Masters you're doing\nyour Masters right\num so what do you what do you what do\nyou think your career path will be down\nthe road I mean what what direction will\nyou take this with all the knowledge\nthat you acquire right now\nuh I plan to pursue a machine learning\nengineering uh so that I can work uh the\non the entire pipeline starting from\ndata ingestion to model monitoring I\ndon't intend to go down deep into into\nthe data science part like you know\nmodel building and things like that but\nyeah\nthat's pretty cool and I think do you\nthink last question now related to that\nuh because you because when you use a\ncollaborative platform like flight where\nyou actually you know could have you\nknow obviously data data engineering\npipelines and so on and and the modeling\npart where people you know you use\ntensorflow whatever\num do you think this is a good pattern\nto to do cross-collaboration a thing\nbecause I asked the question because we\noften hear you know people sit in silos\nand don't know what's happening on the\ndata side and my question is do you\nthink a platform like flight makes it\neasier to break down those silos or do\nyou think conceptually yes but\npractically you have not seen that\nbecause maybe because it's just working\nalone right now\nyeah because uh I'm or I'm still working\nin a kind of Silo mode but uh what I uh\nrealized that because uh\nbecause flight does not require us to uh\nbreak the pattern of coding structure\nlike airflow so it becomes much more\neasy to follow for everyone so if anyone\nwho knows python if anyone is interested\nthey can just read the code and get an\nunderstanding of what's going on so uh\nif I have to say okay fine I I have a\nteam of data scientists that doesn't who\ndoesn't know\num much about flight but they can write\nfunctions so that's okay we can just\ntake this for those functions and wrap\nthem in Flight\ntasks or workflows then things will be\ngood I guess okay fantastic so sorry I\nhave one more because that's so\ninteresting to hear you see I mean\nobviously\num\nwe look at to expand the flight\nCommunity to to people like you it's\njust amazing to hear you know how you\ndiscovered and how you work with it and\nyou know your experience in bare metal\nclusters six machines that's awesome I\nthink it's super cool\num we also have a conversation sometimes\nabout you know what people are looking\nfor when they when they search for\nflight do you think flight what what\nwould you describe flight is is it I\nmean just to give you a couple of\noptions and orchestrator is it AI\ninfrastructures data infrastructure tool\nwhere would you how would you classify a\ncategory slide\num right now I consider flight as an\norchestrator so that I can um like you\nknow give my task and or flight will do\na whole lot of orchestration for me so\nthat I don't have to go inside my\nkubernetes cluster with my command line\nso flight will uh solve the task for me\nat this moment\nawesome thanks a lot and I think uh the\nother folks here Jan and Fabio who are\non ceremony with you right now there I\ndon't know if you have a different view\nbut do you see I'm calling you out Fabio\nand Jan do you see it as an orchestrator\nas well or do you think uh this should\nbe considered more an AI or data\ninfrastructure tool a platform if you\nhave any opinions there\nI included myself I'm not trying again\nif you want to\noh but I go first and you can talk\num\nit depends on how you use it I guess if\nyou\njust\nschedule a bunch of patent function\ntasks I guess it's an orchestrator but\ndue to the Integrations with plugins\nlike tasks to keep flow training\noperators spark Etc so it's really\nhelpful to spin up infrastructure and\nnot just construct a workflow graph\num so if you are doing these little bit\nAdvanced use cases I think it's more\nthan an orchestrator\nyeah\num I also would see it as an\norchestrator in the first place and\nsomething else I wanted to mention as\nwell I have a colleague also in the ml\nOps department at kenil who's also\nworking at some bare metal setup so um\nI'm happy to connect on this one um I\nknow he made progress already maybe it's\nworth worth catching up\nthat's awesome you should connect with\nDavid let's play here because I think\nDavid is spending some time on that\ntopic too right David\nyes I didn't\nokay thanks\nhmm\nthat's great right\num anyone else\nany other comment question\nArthur um this is a question for you is\nthe\nistia right yeah are there any\nparticular features you desire for\nflight to have\nuh I think it's a little bit early for\nme to comment on this because I just\nstarted my journey with flight maybe\nafter a while I can comment if I need\nsomething more\nokay got it\nall right then uh if there are no more\nquestions then yeah thanks as they are\nfor being a part of a community meeting\nand sharing insights on such short notes\nnotice we truly appreciate it\nand yeah that's all for today the next\nCommunity sink is on September 5th if\nyou would like to speak just let us know\non slack thank you so much for your time\nand participation see you all at the\nnext community\nI think so yeah"
    },
    {
        "title": "Flyte Contributors Meetup - August 17, 2023",
        "transcript": "started\num all right welcome to the flights\ncontributors Meetup August 17. and this\nis usual reminders the meeting is being\nrecorded it's been also transcribed\nusing fathom Ai and also\nit falls under Linux Foundation code of\nconduct\nokay let's move to the agenda and if\njust in case if you don't have it handy\nit's here\nand you can either ask any discussion\ntopic or\nadd yourself to attend this list or both\ngood uh today I don't see anyone joining\nfor the first time\nuh all right so let's move to the next\none new rfcs\nagain I don't see any new RFC\nI just see the David okay can you share\nyour screen yeah I think we should move\nsome rfcs to either\nimplemented or\nyeah\nlet's start from the left hand side in\nreview this simplified retry Behavior\nare we ready to move this to the final\ncoming period\nand merch in two weeks or less\nor do you think it needs\nmore time\num we've we've had\ninternal discussions about you know\nretries and like\num related to this unification or\npossibly an alternative\nI think we\nwe should work internally to either\nupdate this this RFC or\npropose an alternative I think I I'm\nmore inclined to ask\num the team internally to you know go\nwith the form of like updating this\nclarifying you know how I'm gonna do it\nbut like it's not clear yet so\num\nalso in the RFC itself there were a few\nquestions that I don't think we ever got\nto the bottom of\nright Fabio especially like that that\nguy what's his name Harry\nDennis my colleague or is it Dennis is\nthen it's not Harry yes\num sorry I'm just all right there are\nyeah you scroll to the bottom David\nthere were\nyep like in the NFC itself there are a\nfew questions that I don't think we\nreally\nyeah let me see here\nI mean the actual RFC here the own\nresult questions\nnow the rock they were comments in the\nRFC there were a bunch of results ones\nmaybe we can\nunresolved them to take a look\nyeah\nbut most of most of them\num\nso I wrote it I asked Dennis to comment\non it and\num I basically adapted most of them\num so in the beginning there were a\nbunch of open ones that these were\nreally comments of Standards to me about\nwhat he would change of CRC because we\ninitially worked together and then I\nmade the formal RFC and take a commented\non it so there were a bunch of questions\nbut I would say these are really\nresolved the ones that I resolved okay\num that's not to say that there isn't\nanything that needs to be discussed here\nabsolutely feel free to also make change\ntrick like changes to the Pharisee\num and let's discuss there\num I think it's important to get it\nright and to get it super quick\num\ncool I well yeah I'll I'll get um\nthe people at Union to comment on this\nlike I know that we've had\na few questions that were not really\nanswered by the RFC yet so it would be\ngreat if we could you know\nget to a point where we're not\ndissatisfied with the retry mechanism in\nFlight which I think is the intent of\nthis RFC yeah\num Ben also had a like a prototype ER\nand we intend to next week or the one\nafter just deployed it on our staging\ncluster and just drive it nice\num\nso from I would say it sounds like let's\nnot move it to the next period let's\nkeep it open we try it we tried dance\nversion on our staging cluster you have\nmore discussions and I totally see their\nquestions\nbeing worth asking that we didn't think\nof so feel free to when questions arise\nand discussions just put them here and\nthen we think about what we do with it\nthis is awesome thank you\ngreat yeah\nyeah the only ask will be if there is\nany discussion be it internal or\nexternal that\nyou'll be captured here so anyone has\naccess\num maybe one last word\num also feel free to Ping us and we're\nhappy to jump on another call to discuss\nsome things\num if you want yeah\nI I'll ping you okay\nsometime next week\num\nthank you uh oh\nall right so the accepted proposals you\nmean right Lord I mean some of them yeah\nlet's move some of those two\nwhat about performance burnage marking\nwhat is this\nit doesn't RC that Dan worked on for\nsome time\num\nthat there's\na bunch of ideas there some are\nimplemented some are not\num we\nI don't think it would be a huge focus\nin the short term but like it's a thing\nthat it's still on our Raiders very much\nso\nokay\nso it's still not fully implemented I\nmean the RFC is done like it's really\nlike\nbreaking it into smaller items and like\ngetting them implemented but I I think\nthis is it like we're not\nlike I don't\nI hope I'm not misrepresenting what\nlike dance plans here like I know that\nhe's not in the call but um\nthere is a lot in this RFC I don't think\nwe're iterating it\nmuch it's time to get some of those\nideas implemented\nso\nthat's excited and\nall right the external plugin service\nI think is that one fully understood\nis this yeah this should be\nusing the new name right the flight\nagents right this is my opinions\nyeah\nyeah let's just rename it then you know\nmove it too yeah I can rename it\nthank you\nthank you Bernard\nokay\num array notes yeah so array nodes\num shipped featuring one nine\num there's more work describing the RFC\nthat's now it's planned so let's\nit's being implemented\nlike yeah probably it's in it's in the\nprocess yes yeah so it's somewhere\nbetween you know completely finished and\nimplemented might not be we should\nprobably rename implemented uh to like\nimplementation phase or something yeah\nyeah I think that's that's better\nlet me see if I can do this\nand\nand I will\nuh description\nuh to this column\num okay\num these yeah this one it's just an RFC\nat this point\nthis is not implemented yet no okay\nokay\num\nsystem tax\nyeah so that's also an experimental\nfeature that went out with when I so\nthere is more three\ndown there but you know\nyeah\nright and finally eager mode I think\nthat one is planned for 1.10 that\ncorrect yeah it's like in the\nimplementation phase like okay it's also\nyeah I think we should\nthe first thing is we need to merge this\ncool and let's do it yeah\ncool all right\num\nfor the ones that probably needs\nimplementation I was thinking if\nif we we can think we don't need to\ndecide right now but if we want to\nreason about probably forming a working\ngroup is because the idea would with\nworking group is to\num and gather some folks in the\ncommunity who could help with the\nimplementation phase we want to discuss\nrfcs we want to accept rfcs and we\ncertainly want to implement accepted\nrfcs\nso yeah so like for for the flight\nagents like though we\nwe can stay with a straight face that\nthis is an implementation phase and it's\nthere's work being done in the community\nlike there's people out there working on\nagents also already you know okay\nso yeah\nright I don't think we have a\nan official working group but this is\nclearly in the implementation phase like\nwe already have a bunch of examples and\npeople out there also already\ncontributing okay so let's\nconsider an Implement yeah I know I know\nwhat you mean\nokay so yeah probably I will reach out\nto Dan and Katrina the authors of this\nproposal and see what they think about\nmaybe four minute working group and\nand yeah how to move this to\nimplementation\nokay\ncool any other comment around rfc's\ngreat thank you yeah that helps\nall right next one uh working group\nupdates so far in the conflict overrides\nby\nI don't know of any recent update\nyou're aware of\nthere have been discussions again\nbetween Byron and Dan but I haven't\nfollowed up on them yet\nso I think\nfrom what I saw on a high level Byron\nrealized that there's one use kit that\nthey have that wouldn't be really\ncovered by it with reference launch\nplans\nand there have been some back and forth\nabout how this could be integrated into\nthe idea\num but I can't say more about it I need\nto like read this read the thread okay\nif I've read the the Google Doc that\ncame out it seems like more people from\nLinkedIn are working on this from what I\nunderstand\num and there has been a whole different\nproposal from from there and in terms of\nux different and to the to the arc\num those seems to be pushback both from\nthe union side and\npotentially for the last meeting also\nfrom the open source site in terms of\nwhat the ux looks like\num kind of passing in configuration is\nas parameters into workflows I think\nthat's something people are somewhat\nopposed to but I don't think there has\nbeen a final\nI'm offendable yeah I'm personally not\ntoo much of a fan of it with hook syntax\nbut so I think this needs some more\ndiscussion I'm not sure how involved the\ntwo of us are still uh love you maybe we\nshould get back in the loop there\nyes\nyeah\num you captured well like there were\nthere was\num\na meeting with LinkedIn where they ex\nthey basically talked about you know the\nthe failure in the current idea\nexpressed in the RFC\num you mentioned this Google doc which\nis where like the latest thought about\nthe alternative that Byron in the team\nis proposing I think\num\nI think we reached a point where the\nalternative\nsolves their use case\num it's not really\nit doesn't look like um\nsorry what I'm trying to say is I I\nthink we we should\num work with Byron to get the\nlike this newest idea like ported to the\nRFC you know so that we we can\num\nlet the community at large know how\nconfigurable rights are going to work\num\nyeah this is all very recent like I\nthink they were like there was a lot of\num\na lot of conversation to like get to a\npoint\nwhere the the implementation would solve\ntheir use case and it would not break\nlike some of the assumptions that that\nflight you know has\num I don't know if he wants to jump in\non this and maybe add more color to this\nthing\nif you're not in the call it's it's all\nright I think we should um um\nI don't have too much more color to add\nso what happened was they had one\nesoteric use case that made the hook\nidea to\num eux Y is less appealing so he kind of\nreverted back to the original plan of\nadding something to the inputs but\nsomehow magically would not affect the\ninterface uh we didn't like that so we\ncathen came up with the idea of\nmoving the overrides into like a\nworkflow level context object declared\nin The Decorator of the workflow\nthat would then get resolved basically\nby propeller at execution time and\npassed down I believe the structure of\nthe override object is\na\nsubset of the fields of the task\ntemplate right yeah yeah and then that's\nwhere we landed smart merge so last I\nheard I just pinged Byron and asks if he\nwanted to come on\num I don't know if that idea has been\nvetted by them to see if it's workable\nbut I think yeah once once they're happy\nwith it if if they are happy with it\nlet's get them back on\num to explain like an implementation\nplan\nyou'll need to read the document here\nI'll do that\num\nforeign\nthank you\nright okay next up ideas in the\nincubator I saw a new one from\nuh Katrina\nI don't see her here but it's basically\nthe proposal to not only say they create\nexecution requests parameters\nbut also really resolve the actual\nresult execution time parameters in the\ndatabase feel free to comment\nand remember that anything that you post\nhere is is kind of an open question is\nthis idea\na good fit for an RFC\nor not and um\nyeah feel free to leave your comments\nthere that's\nI appreciate it\nanything you want to\ndiscuss about this one\nright here no\nthat feeling is that regardless of how\nValic or invalid that proposals I don't\nthink you need an RFC right if it's a\nsmall thing to save in the database it's\nnot changing any behavior that user\nfacing\nsorry\nokay\nyeah feel free to leave your feedback\nthere yeah in general kind of the rfcs\nfor for features that will change the\nuser experience\nand you know significant way not sure if\nthis one fits but\nI think the background on this one was\nwe noticed some tasks or some workflows\nwere being reported is still running\nafter like days and days that like they\nthey had kind of uh\nwe weren't resolving the fact that they\nhad completed and so I think this is\nlike the flight changed\nthat's required to\navoid situations like that I can dig in\nmore and figure it out but I think that\nwas the context so in terms of like the\nuser experience that's where this could\npotentially touch the user but it seems\nlike it's like a\nmonotonic Improvement in experience that\nyour workflow that has actually stopped\nis now showing you that it stopped\nso maybe not agreeing with that maybe\nthis is not quite a huge RFC\nyeah\nright okay well thank you and thank you\nJohn for the context\nthat's something\nokay next one the open like section\nfirst one mono repo\num I added this to the agenda but I\nheard that you plan to yeah no it's it's\nall right let's talk about it so\num\nI've been working on this change to\nbasically make the the dev experience of\nyou know the flight related\num repos user by pulling them into a a\nminorable essentially\num\nI have it working in a fork as you can\nexpect it's like a\nseveral step process like we\num\nreally the idea is to\num\nhave have\na contribution guide that\nit is simplified in case you want to\nmake these these cross component changes\nreally like\nthe way we\nwe make these changes today is really\nlike non-non-optimal like there's no\nclear way to even like how do you make a\nchange to IDL and test it like in in\nadmin and propeller in a single you know\nway so\num\nby moving to a honorable this will be\ntrivial really like as part of the pr\num cycle you get you know like a like a\nsingle binary thing that you can deploy\num\nyeah uh I I don't know like nothing\nchanges in terms of like how people\nusually contribute to the to the poly\nrepo case you know like if you have a\nchange in in propeller Nothing's Gonna\nChange besides you popping into the\nflight propeller directory under now the\nflight repo and doing what you're doing\ntoday\nsimilar you know to all the the goaling\nrepos like this will\nall work the same way\num these will happen in phases the phase\none we're going to import only the\ngolden Rebels so propeller\num admin data catalog all the um\nthe\nauxiliary repos that we we have like\nplugins STD lab\num\nthen we're going to import you know\nflight IDL console and only in the phase\nthree\neven flight kit and flight snacks where\nwe we store the examples they're all\nthey're all going to live there this\nwill simplify documentation also\nit and it's not an in\na crazy amount of code like as I said I\nhave this thing working on my Fork like\nwe have about\n6 000 commits overall across all repo so\nlike check out times will not be\naffected hugely and\nthe this will open up this this um\nthe ability for us to to have a more\nmuch more streamlined like Dev\nexperience\num yeah\nif you guys are interested I can publish\nthe the notes that I've been using to\nguide this project\nit doesn't quite fit the the RFC model\nyou know it's more of a\nalmost the implementation guide of you\nknow what\nwhat are my plans because like I had a\nfew requirements there for example I\ndidn't what I didn't want to lose the\ncommit history of each project\nso like figuring out how to do that\nproperly was in itself a small project\nbut I figure out the right way\num\nonce this first wave of imports happen\nwe're gonna have like a freeze period of\nI don't know two three weeks\nwhere\num PR's that are currently open will be\nmoved to to the flight report and we're\ngonna prevent new PRS from from being\nopen in the\nin in each respective Ripple and then we\nmarked out the repossess read only\npointing them to you know the\nthe flight people\nso tldr for people who are contributing\nright now for you know in in each of the\nthe back end components like\nthere will be a slight change in that\ninstead of going checking out each\ncomponent separately you're just gonna\ngo to\num the flight repo but all the tooling\nRemains the Same we're still going to\nhave like the same make files the\nall the tests like all nothing really\nchanges in Dev experience\nit's more like the the goal really is to\nhave\num in having a PR cycle like this this\nstreamlined\num devx where you know you get like a a\nsingle binary running running your tests\nand\num as I said there are some improvements\nthat we're planning to you know leverage\ntheir move to the moon are able to make\nthat devex overall better but\num I just wanted to let you all know\nthat like we're we're moving to a\nmonorail nothing changes expected yeah\nthis is going to happen in the coming\nweeks\nany comments\nis anyone unhappy with their their\nrespective mono repos or they have\nsomething to share or\nI don't know maybe\nI think I think sure go ahead\nno question\none is the go mod file gonna move\nyeah\num so you what what's your address to to\nexplain like um\nwe are gonna have a multi module repo to\nstart with\nso\num\nin the first import like the first phase\nof this project we're still gonna have\nseparate go mod files one per component\nthere are a few advantages to having a\nsingle go mod file\nbut we're not doing this in this first\nimport\ndoes that answer your question so like\nit'll it'll be like either after we\nimport it'll be definitely after we\nimport all the gold Golding repos then\nwe can start thinking about how to\num this worked you know really unified\nego mod files\nI try to make this in my repo for you\nknow just the main ones like data\ncatalog flight plugins\npropeller and admin\nin Flight CTL and like I found a few\nincompatibilities so I said no no let's\nlet's import the code first then\num\nseparately once we have you know the the\nrelease process figured out like the we\nget people used to like how\nyou do things which again Nothing's\nGonna Change you're still gonna make you\nknow a single PR\nin the respective component directory\nusing all the all the\num\nthe the tooling that you're already used\nto like pop to directory to\num go test to run your your tests and\nwhatnot\nsorry tler is not happening when we\nimport the code it will happen later\nit's it's\nDoes this answer your question he\nbut now that you had a question yeah\nmostly you want to do it I think I think\nthis is great from a contributor's\nperspective I definitely felt the the\nstruggle of touching six things at the\nsame time\num so so I think it lowers the entry\nbarrier for people to actually\ncontribute for example contributing docs\nin the same PR that you do the the rest\nis quite nice\num yeah\nthrough AF migrated from Monterey there\nand back a few times throughout my\ncareer\num from experience neither way is easy\num and usually they're hidden\nhidden traps everywhere\num yeah but yeah but I appreciate it I\nthink for this project it makes sense\nyeah that's what I was always going with\nlike for this project like given the the\namount of contributions like how we\nthink about releases\nand even like the interdependencies of\nlike the components that exist today I\nthink it's\nit's the right call and that's why we're\ndoing it\nokay\nyep go ahead\nmaybe an add-on to this will there be\num one of the struggles for example that\nI faced contributing and it's one of the\nthings that's currently blocking a PR\nwith the task plugin will it be\nend-to-end test because sometimes I feel\nlike that like I don't want to release\nmy my PR before fully testing it and\nthere isn't a nice place at the moment\nwhere we could where these full\nend-to-end tests could live so I could\nsee this being easier with\num with the more Reaper yeah\nyou're hitting the right note like we've\nbeen\num waiting for the move to the moon or\nable to get you know an easy way to have\nend-to-end test that\nI mean they don't really exist today but\njust like very cumbersome so moving to\nthe mono Ripple will allow for this\nnaturally to happen and it's in our\nplans also\nit is that's great thank you\nawesome thanks everyone\nall right another question or comment\nwe'll note the next item\nwe the gcp identity aware proxy from\nfile young I'm excited about this one\nand you can you open the issue please\nthe first plan to make an RC but\nbasically I had to build it to make sure\nit works and now it's built and I think\nwe can do PR review and discuss whether\nthis is something that's not upstreamed\num so let me quickly give context of why\nwith why we needed and I think other\npeople needed to so gcp identity where\nproxy is a\nit's a service that allows you to have\nlike a gatekeeper at the low balancer\nlevel that doesn't let any requests Pass\nunless they are authenticated with\nGoogle identities\nand it's pretty easy to configure for\napplications deployed on gke you create\na secret in the namespace where there's\nthe Ingress\nand then you have a small manifest\ncalled the back-end config that just\nsays Hey I want to have IAP for this\nback end and then you have an annotation\non the service that says use this\nbackend config and then you have zero\ntrust\nuh model for that deployed service where\nno request can hit it without being\nauthentically authenticated with Google\nand because it's so easy to to enforce\nthis your trust model my like many\norganizations basically require it for\ninternal tooling\nso in my current organization my\nprevious one and also others that have\nseen on Slack\npeople cannot deploy tools without\nIdentity or proxy because it's so easy\nto\nbasically not worry about things\num\nwith that I'm not saying that I think\nthat I believe the authentication\nmechanism flight is not read from what I\ncan judge it looks very solid\nbut the difference is that when you\ndeploy flight with like its own\nauthentication and unauthentic\nunauthenticated request hit slide and\nthen it's redirected to login but it\nmeans that the unauthenticated requests\nthey do reach the back end\nand with gcp identity by proxy then they\nnever do they're already blocked at the\nload balancer level\nand that makes the security team sleep\nwell\nand that's why they just say does it use\nidentity by proxy no it doesn't well you\ncan't use it\num\nand workarounds typically are that\npeople that I have seen or I've I've\nimplemented or other that have other\npeople here talk about in Slack are that\nthey deploy identity by proxy at the low\nbalancer level deploy flight behind it\nwithout authentication and then either\nport forward flight admin or use a VPN\nto talk to flight admin or make the\ncluster VPC native and then talk to\nflight admin from VMS in the in the VPC\nbut they don't talk to flight admin from\ntheir notebooks basically because they\ncan't get past then the third proxy\nand\num\nyeah that's not that's not that works\nbut it's not great\num and what comes with that is that\nwe're also not aware of who the workflow\nbelongs to you can't click on only my\nexecutions right we want to be flight to\nbe able to know who the user is and we\nalso want to be able to use it just from\na notebook\nso we looked at what it takes to to make\nit so to integrate flight with identity\nwhere proxy\num and deploy it also with the GCE\nIngress which wasn't super simple you\ncan't use the nginx in grass for that it\nhas to be the Google one\num and the Google one doesn't understand\nJPC helps like health checks and\nReadiness probes requires encryption\nbetween the load balancer and tobacco\nand because of this HTTP 2 which on\nGoogle requires TLS\nbut we do have it working now in a\nsandbox\num\nand I opened already I think three PRS\nthere will be like one or two more\nto to basically make this work\nand the idea is that there will be an uh\ncreated one plugin which provides an\nexternal command which generates ID\ntokens for identity web proxy\nso that you can authenticate\nand then the way I propose to do it is\nthat the users can configure and their\nplatform config that they want to send\nproxy authorization headers\nthat will be dead or to basically\ngenerate the ID token for this proxy\nauthorization header we run this plugin\nthis external command\nand then\nthe request they they make it through\nthrough the edit the identity where\nproxy and then flight kit does its own\nnormal authentication flow with flight\nadmin and there yeah the pi that is\nmissing is how to actually deploy it\nneeds a few changes to the home chart\nbut not changes it needs basically\ncustom annotations and some other\nobjects that need to be created in the\ncluster\num\nyeah any any questions about it\nhow about this uh this encryption\nbetween the the proxy and admin like how\ndo you how do you solve this\num actually flight admin can't do that\nso you can create so that the load\nbalancer will not check the certificate\nAuthority it can be self-center\ncertificate\nand\num you can create one we create a good\nterraform to save it as a secret in the\ncluster and the helm chart already has a\nway to mount additional secrets into\nflight admin flight admin can already\nload till TLS certificate and open an\nhttps server\nso it can already do that I think\nthere's still a bug and flight scheduler\nthat it can't talk to flight admin them\nbut they'll fix that\nand luckily luckily it already can\nthere's one bug that took me like a day\nto figure out in\num because when Flight admin uses TLS it\nstarts a single server on part 80 not\none on part 80 and one part 81. and\nHunter doesn't account for that and\nthere's also PR to fix that one\num but the answer is flight admin can\nalready do it luckily\nwell that's pretty awesome do you have\nto worry about\nrotating this search like how is this\ngoing to work with admin\nI know this is already supported but so\nwhat we will do I guess is\nI'd say like this this communication\nhappens between the load balancer and\nthe duck end and typically this is not\nencrypted at all right the reason why we\nencrypt it is because Google needs us to\nencrypt it but I'm not worried that much\nabout the security aspect there I\nencrypt it because I have to otherwise\nit doesn't work so we'll use a very long\nrunning yeah that's what I'm just\nthinking yeah the case like\nwhen they expire they for whatever\nreason they they have to be rotated\nyou know how are you gonna do it\nso in a way that doesn't cause\nunavailability like yeah we don't have\nto discuss this now this is yeah but but\nthen okay we will do it with the\nterraform provider that or with the\nterraform resource that generates\nself-cent TLS certificates and built in\nis that when like when you if you\nregularly literally from apply it then\nit will realize that when when the end\nof life or the certificate is\napproaching that it needs to be rotated\nthen we'll do it for you you still need\nto restart admin I guess\num\nyeah you don't have to use a self self\nsigned one you can right you can\nat least in the way that I would\ndocument how to deploy that's how I\ndescribe it but I think if there's a\nbetter way you can change that it\ndoesn't have to be a self-sent one I\nwouldn't know how to get the managed one\nto be honest\num\nbetween the load balance from the back\nend\num\nbut if it can be done then most\ncertainly that we can also do that I\nguess\ncool\nthat's nice do you know if there's a an\nequivalent for AWS\num I don't know to be honest but I was\nchatting with Matthew I forgot the last\nname he's from from freenom I think\nbecause they're also super interested in\ncutting this to work\nI'm not sure if you saw the messages on\nslack but they will basically try to\nreplicate it and just try it because\nthey really need us to and he was\nmentioning that he\num\nthat that there was a service on AWS to\nthe similar like one like that maybe\nthere's one more thing that we could\ndiscuss can you scroll down a little bit\num there are two ways that one could\nintegrate\num identity where proxy there's another\nmessage below this one there's another\nlong message\num scroll more down please\nthere's another one yeah so um that's\nwhy what I was discussing with Matthew\nand when he mentioned that there is a\nsimilar service on AWS\num there are two ways that one could\nidentity by proxim and here I'm making\nthe case where I chose the one that I\nchose so you can integrate it on the\nclient Side by sending proxy\nauthorization headers which basically\njust say hey I need to go through this\nproxy but then I'll do the normal\nauthentication flow with the with the\nactual app\nor you can send authorization headers\nand then gcp identity where proxy will\nvalidate the authorization header strip\nit and replace it with this x goog IAP\njwts version header here it's not the\nsame header that is currently used per\nflight with\num like to integrate with Google\nidentities\nand I tried to integrate it in Flight\nadmin and there is a very ugly chicken\negg problem that to verify the audience\nyou need to start a service but when you\nstart the service you get the circuit\nback-end service ID but you need that\none to to validate the audience so you\nhave to strict an ad problem where you\nneed to start it figure out the backend\nservice ID and jacket by an environment\nvariable restart it before you can fully\nvalidate the token I found the rather\nhorrible\nand I was also thinking that if we have\na special case for the special managed\nGoogle service for tokens well we might\nhave the same one for Azure for AWS in\ncase they have similar services and then\nwe'll have a bunch of different token\nhandling logic and admin that all come\nwith their dependency to validate these\ntokens\nwhereas the other one is basically a\ngeneral solution that if you have a\nproxy in front of flight admin and run\nan external command to generate a token\nauthenticate with the proxy and then you\ndo the normal Authority normal\nauthentication the authorization with\nslide itself\num so it because of these reasons I\nchose to\nimplement under the client side now\nbecause I think it's easier to do I mean\nboth it's not easy to deploy but I felt\nthat's not the more General solution\nthat doesn't have a special handling for\none special managed service on flight\nadmin\num doesn't mean that it has to always\nstay this way I felt it was the easier\nfirst step if somebody really needs the\ndeeper integration with it in Flight\nadmin that I guess that can also be done\nin the future\num the price that one pays for it is\nthat one logs in twice right you log in\nonce with the identity by proxy it's\nvery similar to the Pixi of the um to\nthe user experience that you get when\nyou do the Pixi flow with flight\nit's not pixie it's a standard os2 flow\nbut the browser window quickly opens to\nwith the redirect server it also reuse\nthe logic that flight has for that\nand then there's the second browser\nwindow opening for\nflight itself\nnot this very rarely open at the same\ntime because they they don't have the\nsame life life span these tokens\nso sometimes you will see a browser\nwindow opening and it is to do the call\nof two flow with Google identity by\nproxy and then another time you a\nbrowser window will open to\nto the Pixi flow with flight\num\nthat is the price that we pay for that\num\nfor us that surprised very much work\npaying we would say because currently we\ncan't authenticate the flight from our\nnotebooks we need to be in the bpc and\nthere we're Anonymous to fight so\num\nwill basically would absolutely take\nthis\num\nthere's a small downside here but I\nguess this is an important this text\nhere is important because I basically\noutlines why I chose to do it this way\num\nand there was a lot about tokens so feel\nfree to ask questions\nyeah I think Bernard had a question\nno I think that results by scrolling\ndown I saw that yeah there's a proxy\ncommand similar to SSH or uh Cube cuddle\nor any other tool right where you can\ntell it what command to use to to\nauthenticate from what I saw\nbut it was on these games over\nthey say again I don't understand I'm\nsorry um so in the in the config for\nflight there is a you don't have to\npaste the header right but you paste the\ncommand that should be run to yes\ngenerate the thing exactly basically I\ndo the currently one kind already\nauthenticate with an external command\nand slide right\nand I extended that idea to say okay you\ncan do that but you can also run a\nsecond command if you want if you have a\nproxy in front of it to generate a token\nfor that and I open the pr for a plugin\nthat provides a CLI called flight EA\nflight IAP generate user token or\ngenerate service account token and you\ncan just basically use this plugin and\nput it into your conflict file it will\nrun this command and get a token for you\nokay great thank you\nthat's nice like why do we need like an\nextra CLI like don't we already have\nthat\noh because you need like a separate\nendpoint sorry to forget about it you\nmean because you already have the Pixi\nflow and\num in Flight kit or in like a way to run\nlike a custom\num like\noff Command right\nbut in this case like you need you need\nyou need to I need two yes I need\nanother one yep\nand what I like about doing it this way\nis that\num\nwhen some other somebody else has\nanother type of proxy in front of flight\nbe it another managed service or some\non-premise deployment where they have a\nproxy in front of it they can use the\nsame exact method exact mechanism they\njust need to provide their own command\nto generate a token and it will work for\nthem too\nso I think that's also cool that if\nthere is such a service on AWS I don't\nknow if there is but I guess the\nmechanism will work just as well you\njust need another command\nyeah that's pretty cool\nI'm clearly not competent enough to\nreview this and think about the\nimplications so who can\nyeah there's still some tests that I\nneed to fix but yeah I'll need somebody\nfrom you inside to look over this I'm\nnot modifying flights authenticate like\nI'm not modifying the wave flight at\nlike the flight's lies authenticated\nwith the back end I'm just adding a\nsecond header basically so I'm not\nmodifying any of the flows\num a slightly refactor the this\nauthorization client that is in Flight\nkit and depicts the authenticator\nbecause I want to reuse the code for the\nor to flow\nbut there's no change in behavior in\nthat um the behavior is absolutely\nuntouched\napart from did you also make it using\nflight CTL Fabio or not yet but there's\na\nto-do item here and um\non the issue that makes this needs to be\ndone\nnice\num\nyeah in terms of like who can help you\nget this to the Finish Line like I know\nDavid you did the Deep diving too off in\nflight\nbut I know\num also hatum is going to be back next\nweek\nand he really is\nthe off expert\nso we'll probably Loop him in in this\nfeature specifically okay I think we can\nwait until the guys from Freedom tried\nit because they already while I was in\nthe holiday they were super eager to get\ntheir hands on it I didn't have my work\ncomputer so I really couldn't push it\nbut they really I was writing with\nMatthew I don't know\num but they already allocated time I\nthink next week or so to also got a\nSandbox deployment with this and\nprobably they will just discover some\nrough adjectives that still need\npolishing\nand maybe then one status done\nit's time to review yeah that that was\nprobably my my question what what do you\nneed what help do you need because this\nlooks like an important effort thank you\nso much and I can think of at least one\nadditional or an additional user who\nwants to see this\nand who could help testing\nI had a call this week with with folks\nfrom ADT and they are they are also gcp\nusers\nand of course they would like to see\nthis happening\nvery open to help so probably I don't\nknow probably getting everyone together\nin at least it's like Channel probably a\nworking group is what we need here but\nuh if you need someone to test out\nsomething contribute in some ways uh I\ncan definitely\neveryone that would be awesome\n[Music]\nthat's pretty awesome power thank you\nI learned a lot about hidden stuff in\nGoogle documentation\nthey say in a very small side sentence\nthat you need PLS between the low\nbalance and the back end and they have a\ndemo app that they deploy where that's\nkind of\nwithout telling they create a secret\nwhen startup so you never see you never\nknow that there is a TLS certificate\nthat's being generated and you really\nneed it and the error message just did\nreally don't tell you that that's what\nyou need\num\nfun it's great thanks so much\nright any other question comment\nall right\nwell I seems like that's it in terms of\nagenda so if another comment question\nEtc\nthat's it for today\num yeah thank you Chief we can also test\nit or not awesome\nright uh thanks everyone for joining and\nI hope to see you next one\nbye"
    },
    {
        "title": "Continuous testing and monitoring for production ML pipelines | Flyte Community Meeting -  8/8/2023",
        "transcript": "no\nall right so with that we'll move to the\nnext item in the agendas and I'm really\nhappy and thankful for uh for this day\nuh where we get the chance to introduce\nuh Elena and Emily from the evidently AI\nuh company and project and\num yeah I'm happy to welcome you\num so I don't know if you like to\nbrilliant with yourselves and share with\nus probably something you enjoy doing\noutside work for your presentation\nlet me make the start so my name is\nElena I'm seeing co-founder of evidently\nand well outside of work I do enjoy\nworking on other stuff I guess that's\nhow it happens when you are an open\nsource co-founder right so there is some\nthings that I enjoy doing for example\nwriting blog posts or reading stuff so\nthat occupies quite a lot of even my\nfree time and hello to everyone from\nLondon today\nyeah hi there I'm Emily I'm co-founder\nI'm still at the Olympia and uh well\nthat's pretty good question but you like\nto do outside the board because they're\nreally very very excited about evidently\nabout building different teams metrics\nand tasks we're going to talk about it\nlater so maybe that's it in my free time\nI like to walk a lot so explore in\nLondon that's new city for me and uh I\nlike to play basketball\nawesome\nthank you for sharing yeah probably okay\nthank you and relate to the amount of\nfree time that a Founder has\nall right so welcome yeah yeah let me\nstop sharing my screen\nit's all yours\nthank you for the introduction just to\nexplain what we're gonna do now I will\nshare my screen and walk you through the\nslides but it will take you two three\nminutes I know that no one likes lives\non this calls and then we will see the\nactual code so you will enjoy it\nprobably a bit more\nso here it is just a really quick\nreminder hopefully you can see my screen\nnow of what evidently is and well what\nare we doing\nso evidently is an open source python\nLibrary it is built for data scientists\nand machine learning Engineers to be\nfair it's used quite a lot by data\nEngineers as well and probably titles\ndon't matter that much but the general\npremise is that we focus on machine\nlearning models in production and we\nhelp evaluate test and monitor whatever\nis happening with these models when\nthey're actually being in use\nso we are happy to say that we are\nprobably the most popular library in\nthis segment so we have over 3 million\ndownloads 3 000 GitHub stars and so\noverall we have been building the tool\nnow while approach in three years\nfocusing on uh building first a very\nvaluable open source project and like\nyou all know it actually takes quite a\nlot of time to cover all sorts of use\ncases and Integrations that might be\nrelevant\nwhen we talk about machine learning\nmodel monitoring it often question that\ncomes up is basically how the different\nfrom well traditional monitoring or\nservice Health monitoring application\nperformance monitoring and so on and the\nidea is that of course when you have a\nmachine Learning System in production it\nis still probably either a service or a\npipeline so you need to actually monitor\nthat's how it operates it's the it works\nas expected but there are additional\nlayers of complexity which are related\nto the data that is flowing through\nthese machine learning services and also\nto the health of the machine learning\nmodel itself so for example the model\nmight be predicting something like maybe\nforecasting sales but then you actually\nneed to know if the forecast is good or\nmaybe there is a drift and model\naccuracy or maybe there is a change in\nthe incoming data or maybe the model is\nbecoming biased or is underperforming in\na particular segment so there's actually\na lot of other stuff which is not\nrelated directly to the software\nperformance and this is the components\nthat we focus on it evidently\nit has three different things so three\ndifferent well components of the tool\nthat can be used together or\nindividually because we've seen that the\nsolution of this task how to monitor and\nobserve and debug your machine Learning\nSystems in production can be solved\ndifferently so one component is called\nreports and this is basically a well\nbeautiful visual report that you can\ngenerate directly in Jupiter notebook or\nexport as an HTML file and it allows you\nto understand a particular aspect of\nyour data of your machine learning model\nit can be used even for monitoring for\nexample if you only do batch predictions\nonce per month you probably don't really\nneed to set up anything more complex so\nyou can just generate this report look\nat it and see what you want to do with\nit\nthen we have a component that is called\ntest switch which is probably most\nrelevant for this talk and this is best\nfor well use and automated checks and\nautomated pipelines so for example every\ntime you receive a batch of data or you\ngenerate a set of predictions or maybe\nyou receive the label data that comes\nlater you might run a test Suite to\nunderstand that there is no change to\nthe data the data quality issues relate\nmodel quality and do it in automated\nfashion so that you don't have to\nactually look at the report if\neverything is fine\nand lastly we have a component that is\ncalled monitoring so you can launch it\non top of everything else so for example\nif you're running a lot of checks or you\nactually have like a live system and you\nwant to closely monitor what's going on\nyou can also deploy and evidently\nservice and have a user interface where\nyou will see how this metrics change in\ntime\nuh so all these three components are\ninterconnected so the monitoring UI is\nactually sits on top of reports and on\ntop of test Suites but you can also just\nuse the test Suites or reports if that's\nonly one sense we believe that's a very\neasy way to start because well you can\nstart very quickly and now Emily will\nexactly show you how there are a lot of\nchecks I will not go into details\nbecause I think it's best to see exactly\nwith the example but to just explain\nevidently covers data quality data drift\nmodel quality and there are about a\nhundreds of different things that you\ncan evaluate under these dimensions for\nText data for tabular data or for\nembeddings\nso\nhere's the animated presentation now\nlet's go to the code\nall right so I'm going to share my\nscreen now\nshould be fine now cool\nnow we can see that today is the day\nwhen I actually downloaded\num\ntonight\nyeah sorry for that so now that's fine\nand I thank you so hopefully it should\nbe fine starting from now and I'm going\nto show you quickly how you could use\nevidently for testing and validating\nyour machine learning model pipelines\nand data related pipelines so basically\nI want to start from the um\nuh point that evidently is a python\npackage so in order to use the event you\nneed to install it and it's available in\nPIP so I can just write it install\nevidence or contents evidently if you\nprefer to use conda and uh you'll get it\nuh the only one thing is it's better to\nuse a virtual environment to make sure\nthat you do not damage any of the\ndependencies you have installed or other\npackages so basically that's it I\nalready installed it and then yeah uh\nand I'm imported and together with\nevidently I imported a couple of things\nrelated to different metrics and best\nevidently includes so that's Matrix for\ndata drift and many many tests which we\nare going to see a little bit later\nin order to have some data for\nadministration purpose I decided to use\na bicycle demand prediction data set\nwhich is very popular and unknown data\nset where we have a lot of data related\nto new decision to temperature humidity\nwind speeds and the task is to predict\nhow many bicycles will be taken to\naround\nbehind this case is to try to optimize\nthe usage of the bicycles and maybe if\nyou have some information about how many\nbicycles must take insurance in\ndifferent places we could optimize the\namount of bicycles we place in the\ndifferent locations so that's basically\nthe idea and in our case that's\nregression problem so we predict how\nmany bicycles will be taken to rent\ndepending on the weather and the season\nso this is the preview of the data we\nare going to use and they have a date\ntime as an index\nwell I also portrayed regression model\nso uh I have some helping variables like\nthe name of my target function the name\nof prediction and I have some different\ntype of features some numerical like\ntemperature humidity wind speed our\nenzyme categorical features like Seasons\nholiday and working day\nbasically for demonstration purpose I\nsplit the data into the parts so we have\nreference data which are data from the\nbeginning of January to the\num 28th of January and we also have\ncurrent data set it's the next four\nweeks and the idea is that we process\ndata in a batch mode so our reference\nbatch which we use to train the model\nwill be from January and the current\nbatch is like an expatch where we never\ntrain the model but we are going to\napply model to this new part of data and\nsee what is the quality of our data and\nmodel performance so that's basically\nthe idea now it's simulate the batch\nusage of the model and if you use some\nworkflow manager to orchestrate your\ndata related by pipelines I hope you use\nflight a lot so that's basically uh the\nexample of the task you could Implement\nis help of task of record manager\nso I trained the model which is random\nForest regressor then I generated my\npredictions for reference data and for\ncurrent date so basically and now my\ndata frame looks like this so I have\nadditional column which is called\nprediction and now we can see how\nidentity can help you with the testing\nand validating your data related\nPipelines\nbefore starting to generate the seeds\nand reports I want to create a column\nlevel which is the health and object to\nmake sure that evidently virus is data\ncorrectly basically evidently can parse\nthe data automatically in this case you\ncan just skip this step is generating\ncolumn 11 but in case you have some\nlet's say non-trivial cases where for\nexample initially you had categorical\nfeatures and then you transformed it to\nnumerical for example to be able to use\nthose data in machine learning models so\nin this case evidently we'll think that\nthose data are numerical just because\nit's\ntakes into account\ncolumn tabs from your from your Biden's\ndata frame but if you if those features\nare meant to be categorical it's better\nto say it to students straightforwardly\nso that evidently we'll then choose uh\nthe best algorithms for example best in\ndatabase.test detection methods to\nprocess your data correctly so that's\nhow you can do that syntax is very\nstraightforward it looks very similar to\na standard python dictionary where I\njust assign the values of your\num\nparts of this Frame to\num eventually so the target prediction\nlist of numerical and categorical\nfeatures as well and there are some more\ndata well now it's time to build first\ntest series and for doing this you just\nneeds to import test Suite objects from\nevidently here it is and create our own\ntest Suite I will start from preset and\npreset is the well combined number of\ntests which helps you to toggle specific\nstep from your machine learning pipeline\nfor example here I'm going to use data\nquality test preset I also imported it\nfrom the very beginning and basically to\ncreate your test Shield you just need to\nimport this object and then specify\ntests here is the list in our case I\njust use my preset so that's the number\nof tests which helps to figure out what\nis data quality and then we need to run\nour tested from doing this we need to\npass the reference data current data and\nthe cloud mapping and then basically we\ncan explore the results\nso uh evidently can help you to set the\nconditions for your test automatically\nwithout pressing uh specific conditions\nto each individual test and in this case\nyou need to pass reference data this is\npretty handy if you have a lot of models\nand you don't really have a lot of\nresources to decide on conditions for\neach individual test and that's a very\nnice starting point so let us try to do\nthat\nso basically I run the cell and uh for\nnow I'm going to just show the results\ninside of the pattern notebook because\nthat's pretty handy for ad hoc analysis\nand understanding how to works for doing\nthat I use method show which shows me\nthe output and mod is inline so I'm\ngoing to see the results right there we\nhave a summary with our tests so data\nquality test preset in this case\ncontains 26 different tests and you can\nsee the 22 tests were successful and\nfour were failed basically here you can\ngrow group our tests for example\naggregate by the status\nand explore only interesting tests which\nare fails right so we can open up and\nsee basically\nthose are tests related to the most\ncommon value of some features right\neverywhere we have\nsome additional information and we can\ncheck what has happened for the last\nmost popular most common values in\ncurrent data set but was the most common\nvalues in the reference data set and see\nwhat's going on there\nuh together is preset you can use\nindividual tests so basically for doing\nthat instead of presets we need to\nspecify which tests we are going to\ninclude in our test Suite so here in my\ntest list I have individual tests and\nthis example I decided to use test for\ncolumn drift for the column temperature\ntests for mean value of the column\nthat's numerical column so I can do that\nand test for quantile\nhere I need to specify for which column\nI want to perform this test I used\ntemperature and when it comes to\nquantile I need to specify which Contour\nI want to calculate for example it can\nbe a median right so again I have my\nreference date so I do not have to pass\nany test conditions in this case I just\nrun myself and see that all my three\ntests\nare failed right and I can open up the\ndetails for example for drift per column\nand they can see that well there is a\nspecific shift in my distribution so my\nreference data is shifted to the left uh\nand my current data is just to write\ncompared to the reference data so that's\nwhy drift is detected\nand well if for example I see that my\ntests are too sensitive or I want to\npass different conditions I can\nperfectly do it\nin this case I just add some transitions\nto my very same functions so again I use\nmy test column value mean and it's just\ngone quantile well you're right but now\ntogether with the column name I specify\na specific conditions here for example I\nuse two conditions that's left and right\nare less than or greater greater than so\nhere I prefer to have my mean value for\ncolumn temperature\nless than 0.3 and greater than 0.5 right\nand very same logic for quantile but\nonly one condition here and uh you can\nsee that now I do not need to have any\nreference data anymore because I passed\nall conditions and well reference data\nis not needed to any calculations so\nthat's how I can operate if you do not\nhave any reference\nand well\nluckily now I have all that success\nright\num and uh again any details that you\nmight want to see for example like here\nhere is my uh current\nvalue right uh that's the value for\nquantile uh equal to 0.5 and that's my\ncondition so I can see that my contact\nis like a writer than my condition so\nbasically that's why it passed the\nthreshold because I expected it to be\ngreater than 0.2 so that's how it looks\nlike\noh well that's very convenient to\nexplore the results of such tests inside\nof the pattern notebook if you need to\nrun some analysis but if you orchestrate\nyour task uh like you can do in Flight\nyou might prefer to have some different\nformats and I'm going to show you how I\ncan transfer your outputs to some other\nformat which is more suitable to\nautomated runs maybe to log in your data\nto write some logs to database or maybe\ncreate some graph which includes\nconditional tasks and with help of\ndifferent output different formats of\noutput for example if you output your\ndata not as the visual objects but as\nthe python objects uh I mean well it's\nalso find an objects of course but if\nfor example you want to derive some\nspecific value and then decide whether\nto run the dependent task based on it\nfor example based on the test status you\nmight want to have it in a like text\nformat or python dictionary format so\nbasically together with strong methods\nwe have quite a lot for example Json in\nthis case you can use this output for\nexample to store your data and analyze\nit later\nElsa we have pretty nice uh dictionary\nformats I don't know why I don't have it\nhere but that's pretty easy to just\nshow it so that's\njust as\nas if methods\nand that's how it looks like so\nbasically in this case you can specify\neach value you want by keys right for\nexample tests mean value of status and\nuh you can derive this value and act\nbased on these results as far as I\nunderstand uh you can if you use flight\nyou can you implement your task as the\npython function then use task decorator\nand for example if you want to Output\nthose information you can just output\npython dictionary and this should be\nfine\nuh well if you want to share your\nresults and use it somewhere outside of\nyour scripts or your workflow you can\nalso generate some assets for example we\nhave methods like save HTML or save Json\nso let me run it in this case you need\nto specify the path where we want to\nsave your results and I can open up I\nhope you can still see my screen because\nit should be like this yeah cool so you\ncan see that I just saved this report so\nI can open up it separately and it will\nbe available as the signal on HTML file\nso all the formats and all data are very\nsame right\nso that's my damn pretty Canadians\nand well sometimes you might want to\nperform uh some post-processing with\nyour data for example grades and data\nthen save it and later maybe if you have\nsome failed test or maybe you perform in\nsome historical database analysis you\nmight want to load it back for example\nsome data from the previous year or\nprevious months but it's not very handy\nto for example store a lot of HTML files\nbecause they are pretty heavy so in this\ncase you might save the snapshot from\nevidently and you will be able to load\nit later and restore the whole object to\ndo any post-processing with this object\nfor example here you can see that I just\nsaved the object snapshot in Json so it\nshould be here yeah just created and now\nagain I can load this data\nso let me load it and restore like\nregenerate test suite and basically well\nafter I load it so this is something\nweird so I'm just loaded from here and\nagain basically and show it and it will\nbe fully restored which is quite\nconvenient to store and perform some\nanalysis later\nwell finally I want to quickly show you\nreports which is quite similar to test\nsheets but reports include some more\ninformation and more visual plots which\nhelps you to figure out what has\nhappened with your data for example if\nyou see some tests uh failed you can\ngenerate report and explore more visuals\nin order to dig deeper and debug the\ndata quality and model quality\nnow we have a regression model so I can\nstart from regression presets to see\nwhat's the quality of my model\nso\nyeah here it is and I can see that there\nare a lot of metrics like mean error\nmean absolute error Min absolute\npercentage error so I have values from\ncurrent and reference data set and they\nhave many many plots which helps me to\nfigure out what's going on there like\npredicted values and actual areas in\ntime so all plots are interactive so I\ncan zoom in I can zoom out for example\nswitch off something or yeah probably\nyou razor switch on actual and see how\nunpredicted that is but still it's all\ninteractive if you want to share with\nsomeone just the um\nspecific visualization you can load it\nright and download it as the picture and\nthen use it whenever you need and well\nthey have a lot they have the errors in\ntime metrics we have well error\ndistribution so we can see whether your\nmodel tends to underestimate or\noverestimate your target function which\nmight be quite important for a specific\ncases right that's their normality and\nmany many other visualizations that you\nmight be interested to look like and I\ncan't help myself by sharing using my\nmy favorite reset I don't know why I\nhave heard it but still that's data Jeep\npreset where we have information about\ngifs of your features Target function\nand prediction so here is the summary\nhowever detect data set drifts so\nbasically we check the share understood\ncolumns and if it's higher than 50 then\nthey say the district detected so in our\ncase it had 11 columns five columns were\ndrifted and we have pretty nice table\nwhere we can open up each individual\nfeature and explore what's going on\nthere for example here is our Target\nfunction count number of bikes you can\nopen up and see the value at the same\ntime so basically that's the current way\nyes the green line is the mean value for\nthe reference state so mean number of\nbicycles which were taken to rent was\nslightly higher than 50 56 okay and\nthat's uh how this value changes over\ntime and we also have distribution which\ncan help us to figure out whether there\nare some shifts that can explore Tales\nof our distribution and see that it's\nslightly moving to the right so the\ndemand for bicycle bicycles increases\nwhich is I believe a good sign for\nbusiness\nwell same information available for\nother features so you can open up and\nsee what's going on there are some\nshifts for wine speed as well which\nmight be useful to derive from the data\nand basically that's how I can generate\nreports from evidence reports are also\navailable in different formats which are\nJson and python dictionary so that you\ncan get it in different formats and well\nI hope this will be very convenient to\nintegrate evidently in your pipelines\nand see what's going on with each step\nof your pipeline starting from loading\ndata and data preprocessing feature\nengineering till model training and\nusing your model for new batches of data\nwell basically that's all I wanted to\nshare with you\nyeah if you have questions some habitual\ncommands\nyeah I have a good question this is\nMartin so this is very impressive uh and\nI see here KSP evalu komogorov snorov is\nwhen you use as soon here right\num so an attack side what what do you\nuse uh for the probability estimation of\ntext side for a puzzle I mean what's the\nmetric there that you use to detect\ndrift\num\nwell\ndrift and is the default\nmethods which is which is chosen by\nevidence automatically depends on the\nsize of your data and types of the data\nright but together with kamagoras\nSmirnoff and uh that test for example we\nhave quite a lot of other methods for\nexample we can calculate the distance\nbetween distribution and between\nprobabilistic distributions for example\nwith help of Western standrief detection\nmethods or we also use Google webner\nDivergence so you can choose and uh well\nwhen it comes for example to Text data\nthey also have some specific metrics\nwhich is not statistical tests but also\nhelps to figure out what is the\ndifference and if there is a difference\nbetween the R2 texts for example in this\ncase we use the main classification\nmethods where we build binary\nspecification model which tries to\ndistinguish between your reference data\nand the current data and if it can let's\nsay lab the distinguished currents did\nfrom the reference we detect the drift\nand we also shows you the strongest\nfeatures which are in our case if you\nbest evidently row texts these specific\nwords and if it's embeddings in this\ncase we use uh we cannot really give you\nsome information on like what words or\nwhat sentences are responsible for gist\nbut still just to give the information\nabout the share of compliance and the\nquality of the separation of reference\nin current data\nokay great thanks\nawesome thank you\nyeah\num first of all great talk thank you for\nthat\num yeah I've been tracking your work for\nsome time now I'm curious\nif you've found\nthat there's some data set shift that\ndoesn't actually\ndecrease model performance\nbecause in theory our models should be\ngeneralizable meaning that they can like\nkind of extrapolate outside of the data\nset distribution ideally\num\nand I imagine if you're catching all\ndata set shift or drift\num\nmight lead to a lot of alerts that may\nnot actually have a model like a\nnegative impact on the model predictions\nis that something you've seen or or no\nyeah that's a great question thank you\nfor bringing it up indeed it it's a\ndesign this especially for example you\ntrain your model on top of allow\nthousands of weak features in this case\nuh you might see in this situation when\nyou have a lot of others but it doesn't\nreally drastically change your model or\ncause the decrease in quality and uh\nyeah we saw that and and this is the\nreason why we do have quite a lot of\ndifferent uh algorithms to detect drift\nwhich has different sensitivity and we\nalso allow us to pass different\nthresholds for example if it comes to\nmeasure or drift as the distance between\nthe probabilistic distributions you can\nfor example by default we have\num 10 size of uh that 10 transition to\nthe deck drift like if the distance\nbetween the distributions normalized by\nthe standard uh a year error is higher\nthan 10 percent than it is but you can\nfor example specify to a 30 or 50 and\ninto drastically decrease the number of\nfalse alerts and and yeah by the way I\nstrongly believe that this is something\nthat someone should do so customized\nlike all the conditions in order to\navoid false others and also if there are\nreally really really really a lot of\nfeatures and I don't want to have a lot\nof other Arts I would rather\num use metrics metric like share of\ndirected features and maybe if you have\ntwo or three very important features\nindividual drifts for those features so\nthat you can like decrease the amount of\nalerts\nawesome thank you\nI had one question uh just to follow up\nto what Neil said I often see\nfolks talk a lot about data drift\num but to actually verify whether that\nalert is even false positive you need\nground truth\nand in some cases ground roots are very\nvery delayed\nso now\nwhat should be like and I think this is\nfor the community to help them\nunderstand like you know how do they\nhandle areas like you know in some cases\neven we work with companies that do\nfraud detection where ground truth does\nlike the counter factual doesn't even\nexist right like so you don't know uh we\nwork with companies let's say a\nRideshare company and the final ride\ntime is three months uh or you take\num an AV company which are like probably\nyou know using some data which the\nground truth is on the next run on the\nsame place right so there are like these\ndifferent types of things and and hence\nto kneel this point it can become very\nnoisy uh and you really are not\ncapturing enough information in that\nthing what that ends up doing is people\nget stop looking at the data which is\nnot the right thing right so there's a\nhappy medium and what do you recommend\nhow do you how should you go about\nthinking about this problem\n[Music]\nwell that's complicated I would say so\num\nI guess small step back I would say that\nif you have some delays this case in\nyour target function you need to come up\nwith some solution in order to set up\nsomething like early monitoring so\nbefore you like get your go to your\num labels on data you can see what's\ngoing on here and uh I believe that in\norder to um\nmake sure we monitor for everything we\ncan measure not only data Drift But as\nwell as their detection and for outlier\ndetection I fully agree that we need to\nhave some labeled data and maybe if you\nhave some pretty strongly detected all\nFlyers you might send them to manual\nreview for example and see what's going\non maybe to um\nspeed up the moment when we get the\nlabels right but when it comes to drift\ndetection basically what you do will not\npay attention to the individual objects\nright but we are either compare the\ndistribution distributions and in this\ncase we don't uh really need this label\nof course if you are not talking about\nthe targets and drift right where we\nexactly need those labels but if we have\nsome strong features uh and as soon as\nwe get our models output we already can\nmeasure the drift between for example\nthe model's output from the previous\nbatch where where we were happy with the\nmodel's quality with the current batch\nand if you see that the output has\nshifted and for example there are like\nnot of no obvious reasons for doing that\nI believe it's pretty strong signal that\nsomething is going on and uh well I\nwould personally never like create\nanother for the drift for each\nindividual feature I would rather\nmeasure for the share of directed\nfeatures but again if you have a like\nkey features which influence is the\noutput of your model stronger then it\nwould be nice to have some individual\ndrifts so maybe combination of the\noutlier detection just for important\nfeatures and share of different features\nmight be the good setup and uh yes per\nall the rest features I would rather\num measure something like data quality\nto make sure that there are no like a\nlot of empty values or corrupted Pages\nor whatever and I think this\ntheoretically shouldn't be enough\nthank you yeah I agree\none of the important things is as you\nactually pointed out the correlation is\nunknown unless you study the causality\nright now you have to study it across\ndifferent things that people that have\njust create like oh yeah maybe this data\nchange caused the problem but that may\nnot be the case it might be something\nelse\nthank you because maybe to add a quick\ncomments in the probably you should not\nlook and drift at some near real term\nalert right so it's more like a regular\nanalytical process which tells to a data\nscientist like something is happening\nyou might want to look with your own\neyes and decide what to do\nwhich is actually a very very\ninteresting point because there are a\nlot of people who are trying to do\nreal-time data drift and so on and we\nlike this is a great point for so thank\nyou\nawesome thank you all any other question\nno\nright with that thanks so much again\nElena and Emily for joining for sharing\nyour knowledge here that was great\nand uh with that we are at the end of\nthe agenda so\nthanks everyone for joining and hope to\nsee you in the next one\nthank you foreign"
    },
    {
        "title": "Flyte Contributors Meetup - August 3, 2023",
        "transcript": "all right okay welcome everyone to the\nflag contributors Meetup today is August\nthe 3rd\nI'm glad to be back sorry for my boy\nselling an ages catchy very annoying flu\nso I'll I'm trying to do my best\nokay\num\nyeah this meeting is being recorded and\nit also falls under Linux foundation's\ncode of conduct\nas you all know\num Sharon again here the link to the\nagenda and notes you can either add\nyourself to attend this list or not or\nyou can add any discussion topic to the\nopen mic\nsection please\nall right let's kick off anyone joining\nfor the first time\nyeah I see a couple of new names\nMelody Maybe\noh yeah yeah from my team\nawesome welcome\nalso Wendy\nis also from YouTube\nand you yeah okay\nthat's amazing yeah I have to see it\nLinkedIn team\ngoing strong\nall right welcome welcome everyone\nokay\nuh\n[Music]\nokay uh next item in the agenda after\nwelcoming new members will be to\nintroduce new rfcs uh well let me share\nmy screen\nwe have an lrc that is kind of new in\nterms of how now it's a formal RFC but\nit was\num it was discussed already as a\nincubator entry and already discussed in\nits\nfull RFC forms value is not here but\nwondering if there's any comment\nabout this proposal\nor we can move to the next item\nyeah I'm taking over for Fabio here\ntoday\num happy to answer any questions\nbasically I think he already introduced\nit like at a previous\num meet up here\num and we also like discussed with Dan\nways how we can simplify the retry\nBehavior especially in that sense of\num the for example pie torch plug-in\nbehaving differently than when you just\nhave like a regular kubernetes pot in\nthe sense of that\num like following retries never moved to\na non-preemptable instance and generally\nwe also had the feeling that\num our users had like a\nhuge issue with like understanding the\ntwo different uh retry mechanisms in\nflight and this is like a proposal how\nwe could like unify this one without\nlike having it as a breaking change so\num we would want to introduce a flag\nwhich then unifies the two\nokay\ngreat thank you Dennis\nany comment\nnope\nright yeah I think the discussion will\ncontinue and on the proposal itself it\nlooks great and thanks for\nuh creating the RFC itself\nawesome uh next up not sure what was\ndiscussed regarding these two proposals\nin final coming period the eager mode\nand console UI upgrade so wondering if\nthere's any objection\nto move this to accept it\nfor the the uh UI upgrade is already\nimplemented actually\nokay great so yeah absolutely\naccepted so yeah that it happens uh kind\nof with with other proposals that the\nthe it's already implemented so we just\nwant to make sure that that the RNC\nprocess is observed and the status is is\nclear for anyone who doesn't join these\nminutes\nso I will ask anyone to provide their\napprovals and and we will merge this\nsorry about that no problem no problem\nat all uh same thing for eager mode I\ndon't think this one is implemented but\nuh we consider this accepted\nI think it's on its way David\nyeah like we're actually shipping this\nin an experimental fashion in the next\nrelease so yes let's move it to accepted\nis is\nvery advanced\nyeah I I need to say that considering my\nexperience with our projects this is a\nhappy problem so yeah implementation\ngoes even faster than discussion\nsometimes so it's fun that the process\nitself so it's fine\nright\nI added the RFC for the artifact store a\nwhile back I don't know if it made it\nonto the board\nbut\num okay\nyeah we can add it later it's fine I\nalready I uh went over it\num\na couple meetings ago and I'll I'm\nhoping to do it again maybe next meeting\nand give a formal demo so okay\nmost of the implementation is\num POC able\nso\nright that's that's great you mean this\none right artifact service no\nyes yes this one okay cool got it we'll\nadd it to two notes\ngreat thank you\nall right\n[Music]\num\nI don't see any other new rfcs\nuh so we'll move to the next item\nuh working group updates so I I put this\nin the agenda especially for the config\noverride working group I\nand the potential bug on the hook idea\nso\nyou know if Byron yeah I can share my\nscreen yeah but I need a permission okay\nokay let's see\nyeah it's okay\nokay yeah I also just came back from a\nvacation two weeks ago\nyeah so\nyeah so we continue to work on the\nconflict over our part and\nwhen I want to start implementing me and\nKevin think of some potential problems\nso I pause it in your Channel but I can\nbriefly explain it\nso can anyone see the screen here\nyeah yeah we can see okay\nso\nuh take this as an example so in the\nworkflow you call a software flow\nand inside the software flow you call\nanother sub workflow but it's the same\nstock workflow okay so we name it as\nflower flow one sourflow two and inside\nthe Sasa overflow you register a hook on\nthe task\nso\nnow if you want to override the uh\nyou you have no way to override\ndifferent config\nto the task in substant work for one or\nsuch a worth of two because they are all\ntreated as model 2 resources hook\non the top level workflow so I I still I\nalso saw\ndance comment that we can use the\nfunction then or the No Name to\ndistinguish them\nbut the problem is that for example for\nthese two Sub sub workflow how can I\nknow that there are both named Sasa\nworkflow\nI mean like we said in the thing there's\nan override for like with node name\nright now so essentially it does the\nexact same thing as this with hook so\nthis width Hook is just adding an\nadditional unique identifier\num yes we're saying why don't we just\nreuse the node name that already exists\nif you want to specify it yourself you\ncan specify it yourself\num if you don't want to you can just use\nwhat's already there\nokay hey Dan so I think the\nthat's true the the reason I feel like\nfor the hook idea I think was\num if you don't call it a hook if you\ncall something like an override class\nlike you can use you can kick off\nmultiple tasks and not have to specify\nlike multiple times right so if you call\na task like five times or five different\ntasks you can give them all just one\noverride and then one override hook or\nclass and then you don't have to call it\nfive times in the parent one but I feel\nlike you should be able to\num either use the node idea or add a\nadd some sort of semantic to the middle\nsub workflow whereby you allow the\nauthor of this middle sub workflow to\nlike cancel out the hook potentially\nright like override the the hook\num\nI think are there either either way is\num yeah so workable yeah so the hooked\nidea aside me and Kevin has thinking\nabout the other idea so this idea is\nvery simple\nso we will we want to introduce the\noverride method on the test no but a\nspecial thing about this override method\nis that this can take the input so\nbasically this can take the promise\nlike this so if they want to overwrite\nThe Container image so basically it's\nlike with overwrite but it can take\npromise but I think it's yeah so Byron I\nI saw this this is reverting back this\nis effectively the same\num idea initially right like yeah yeah\nthe whole idea was to allow config as an\ninput and we were like no that seems\nweird because now config is changing\nyour interface this is still doing the\nsame thing right yeah yeah but my idea\nis that is it possible that so it won't\nbe appended to the input so it will be\nanother file on the test now so\ncurrently the test now has input output\nand I believe during the registration\ntime they are the placeholder for South\nS3 URL is it possible that we can add\nthe config over a files\nvery definite definitely possible but I\ndon't think it's the right pattern right\nbecause propeller is already downloading\na file called inputs.pb right yeah and\nuploading it for the task to use so that\nfile is the input output mechanism if\nyou add another file for config like\ninput config.pb like it's effectively\nthe same idea right\nso I I don't know if that's a problem\nnecessarily but like it seems like a\nweird pattern and I feel like this use\ncase like you can still do it with the\nhook idea like you don't need yeah\nbut I think this is much more intuitive\nfor us so for example uh take this as an\nexample\nso for example we want to have the\nuh for offline inference\nlaunch plan reference option or\nsomething\nso this is a\nblah blah blah okay\nand so the Launchpad can expose a image\nand then they can just override the\nimage\nin a test by\nby using that\nlike so\ninstead of if you use the hook idea you\nhave to\nlike override you also need to get the\nnet of the task and\nuser won't know what the name is when I\nonly have the launch plan we have to\nprepare a separated dock let's specify\nwhat's the test stroke ID that can be\noverwrited\nand also for this idea we don't have to\nchange anything on the UI side\nbecause\nthe overwriting will just become input\nbut on the task level it's not input\nit's a\nyeah so I so it will be your new felt\nthat\nthis is already Google on using Dynamic\ntasks though\nyeah but I remember Dynamic task is not\ncannot cannot work with reference\nlauncher\nor something\nthere is no good reason for that but yes\num\nwait is that true yeah is there what's\nthe problem you mentioned that if we add\nanother config fail that can takes a\nthree path\nlike it's this it seems weird right why\nare we adding why are we making\npropeller create and upload another file\nyeah\nI think the benefit of this method is\nthat it won't mess up the\nthe UI part you should see because\nthe the input and output are still a set\nand the country can be another tab like\nthe override can be another tab so like\ndon't actually mess up the original\ninput\nand all the\nimplementation happens on the backend\nside so\nthey won't know I mean yeah all the\nimplementation happens\non the backend side and I think it makes\nsense to me too treat the\nlike image\nas input to enable user to override\nbecause\nyeah we have seen a lot of use case\nabout this for example\nsome users are building a ref a\nworkflow and they want to override the\nRay config test config inside\nbut when others use that reference sound\npress they have no way to override\ncan we write out a couple of examples\nthen because I feel like this is not\num complete enough\nyeah I have several example here like a\ncomplete because I don't feel like\nanything here is not doable maybe other\npeople on the channel can disagree with\nme but\num not doable with the hook idea plus\nthe node plus dynamic\num plus Dynamic tasks with potentially\nsome like minor modifications to the the\nhook idea\nyeah but I think the major problem is a\nhook idea seems that\nvery intuitive to use because users have\nto get the\nhook\nin advance the whole idea in advance\nso it means that we have to like prepare\na duck for each component and list down\nwhich state task is overwriteable\num yeah\nfrom a gut feeling I would also I've not\nthought this through completely but it\nseems Seems a lot to me as well\num that you have a configuration part as\nan input we do that right now\num and then run into issues of a little\nbit on the\ntask level\num\noh sorry I'm not breaking up yeah yeah\nuh I'll switch maintenance sorry I'll\nno need to wait\nanyone else have thoughts on this\nthey know well what did you say\nuh I think burner is changing his mind\nbut does anyone else\nKevin do you want to say something\nyeah I just discussed with you earlier\ntoday like yeah some real use cases at\nworkflow for example in linking I think\nthat those two solutions have different\ntrailer so we can see the real workflow\nin the linking first and then we can\ndecide which solution is better for you\nyeah so I think this is a very typical\nworkflow for us\nso user view the\nuh workflow I wrap it as a reusable\nworkflow\nto other users so inside the workflow it\nhas several tasks\nwe're on the user side\nthey want to use it as the reference\nlaunch plan and they want to assign\ndifferent value to\nthe task\nbut why can't Dynamic workflows work\nwith us because reference launch pins\nmight not work\nyeah but that's a separate problem right\nlike maybe we can make that work I don't\nbut if we because the majority of our\ncases\nis likely so\nwe have to run everything as time they\nmake workflow\nbut if you have a if you if you're going\nto spin up a task\num\nif you're going to spin up a task to\nbecause the whole idea of the second\nfile was that\nthe there is going to be a container\nthat spins up to read the file right\nthat happens before the Pod is created\nso that effectively is a dynamic task\nalready\nalways say that again\nlike if if the information is already\nunless you're saying that propeller just\ndownloads this file doesn't upload it\nand then it is it uses that to configure\nthe the pot yeah yeah\nthat's a plan\nso what what's the major problem of the\nlike this override method\nor what what why is it aware\num why is what weird\noh this method\nI mean I think overwriting a container\nimage is weird but that's fine that but\nthis is already\ndoable today right or it's doable within\nthis design at least oh I mean English\nthis time\nwell just the first block is already\ndoable in this design so today you can\noverride\nthe resources correct yeah but only with\nthe stack take value\noh correct yes\num so I want to override it\nat execution time\nand that's\nI mean I've been a vocal uh opponent\nopponent of adding this stuff to the\ntask interface just because like for\ncaching for everything like that\num it kind of breaks some of the\nfundamental ideas of how workflows and\ntasks are built in my opinion\num that's why I I\nI don't I prefer the UI the ux not not\nlike that\num kind of where I stand\nyeah\nalso this can get unwieldy really fast\nright like here you give an example of\nlike specifying one parameter one extra\nparameter like what if you have like two\nthree four things that you want to to\noverride\nlike this\nbut I think actually the user experience\nis better than the hook because for the\nhook on you I you show everything\nuser can override Like You Shook image\ncache all the stuff but in this method\nuser can decide what they want to expose\nto override they don't\nI mean I have to expose everything yeah\nI'll push back on that a little bit\nByron I mean yeah if you have a workflow\nthat has five different tasks and you\nwant to make everything configurable for\neach one of those tasks then you have to\nhave the workflow definition have a\ncontainer image five different container\nimages five different CPUs five\ndifferent memories all of this stuff has\ninput\num\nyou know I I don't think that that's a\ngood good ux\num in my opinion\nyeah but I think for\nbut it's the same for the hook idea\nright they still have to pass five\nthings into it\nI mean the hook idea just like uh\nthe workflow header will be the exact\nsame if users want to override it they\ncan add and override for the hook name\nbut if they don't provide anything then\nit just uses what's there by default\nright\nso they can still run that workflow with\nwhatever the resources are set to right\nnow it's just if they want to override\nsomething they have to add it\nyeah but for this they can also have a\ndefault for the image\nso they don't have to pass it every time\nby using a launch plan with fixed inputs\nis what you're saying\nyeah or just\nhave a default here\nblah blah blah\nhey Byron I think we should think about\nthis a bit more but I want to I think we\nit would benefit us to see a real world\nexample like redact stuff but like\nsomething more than just\num the the shell that's here like it a\ncomplete use case\num that's number one and number two I\nthink it not wanting to put something\nnot wanting to do UI work is\num a sentiment that I share but I don't\nthink it should lead to back-end design\nchanges so if like a better ux can be\nprovided\nby making UI changes that's\nI I would not shy away from that\nnot that we have resources to do that\nnecessarily but like\nokay I don't know after that right\nso yeah yeah but I think\nthis is pretty much cover art\nuse cases\nand the dynamic one word won't work\nokay let's I don't want to take up too\nmuch time unless at least one okay we\ncan talk oh boy\nyeah and the other\nproblem on the top\nuh yeah hi hi everyone I'm Bill I'm from\nKorean yeah very closely and I'm fairly\nnew to flight community so yeah I have a\nfew questions and yeah feel free to tell\nme if I have wrong assumption I'm going\nto share my screen and\nyeah this is a\nthe problem I'm having right now so yeah\nas we already know all the tasks output\nof in-flight is a kind of a promise\nright and we we yeah we yeah we have\nmade some\noperation we want to evaluate the\npromise actually but uh not not that\nkind of also to use it we just want to\ntry uh pass it into the next task input\nlet me go through this example a little\nbit\nwe have a name the top which will be the\noutput type of one the first task and\nthe first task will be generally this\nkind of name taboo yeah it has a screen\nand it had another dictionary and the\nnext one task will be consume the the\noutput from the first task\nyeah so you will be trying to consume\nthe string and it will try and consume\nthe dictionary\nyeah and we found like uh when we pass\nthis one the the screen or maybe we need\nto\nyeah\nwe found like in the first one so this\nshould be\nyeah uh when we're trying to get a\ntrainer model is it works yeah and but\nwhen we try to get the dictionary to get\nget a\nthe value from the key is felt the error\nis uh the problem is subject object is\nnot a secret uh subscribe capable and\nbecause uh we are trying to evaluate it\nmy question yeah I I know this is a\npretty large design but my question is\nthe things that we're not trying to\nevaluate and print it out can we make\nsome work around to make it possible to\nthe next task\nforeign this is something that we've\ntalked about in the past there's an open\nticket for this\nthe ask actually is more General it's\nnot just a dictionary it's any Json\nserializable yeah you want anything Json\nserializable well no sorry just\ndictionaries not lists uh subscriptable\non the propeller side\num I we've talked about that internally\nI don't we haven't scoped out that work\nso no one has done the research to\nfigure out how much work that is but\nit's certainly an idea that's been\ntalked about multiple times and if we\nwere to do it I'd probably request that\nwe do it for the general Json case\num\ngood to know\nand\nyeah sure maybe yeah I can show the\nissue I can take a look\non it another thing I want to yeah it's\npretty similar things so I'm using a\nname type so that's why we can get the\nactually the first one works right we\ncan pass the trainer model into the next\ntask\nit works pretty well because uh the node\nactually you can can find it out but if\nit's a class with a property we will see\nanother error which is uh something like\nthis uh we are trying to get a trainer\nmodel but the note the\nsubnauts01 or two I think is uh it's\nhidden it doesn't have the actual\nproperty name so this is a more like a\ncomplex complex data type with the\nproperty\nand in this case we cannot even ask it\nbecause not even about talking about the\ndictionary even a string cannot be\npassed yeah I think this is uh because\nthere's no actual name Tuple in flight\nthere's actually no tuples at all right\nnow\num so the what you see as name Tuple is\njust syntactic sugar and it just means\num two outputs so that one task outputs\ntwo outputs so that's and there's more\nadditional syntactic sugar that allows\nyou to\ndereference and zero that trained model\num but once you make it a anything other\nthan that name Tuple then all it is\nbecomes non-syntactic sugar and then you\nfall back to the first case where you\nwant you're trying to basically\nsubscript something that is not\nsubscriptable\nI got it and for the name Topo I got I\nsaid let me repeat a little bit so for\nthe name Topo uh is due to some\nuh sugar syntax so we can get the first\nlevel of the property right we can never\nget the maybe the nasty the correct\nprobably also yeah okay it's due to some\nokay great likely enhancement but for\nthe other complex data type we cannot do\nthat\ncorrect uh actually we we uh we're what\nwe wanted the second rule and the second\ncar uh case to be supported more because\nuh internally we're trying to pass in\nthose kind of data types between tasks\nuh in our pipeline because for for\nexample for a model training task we're\nnot trying we're not outputting as a\nsingle string we are outputting a lot of\nuh a lot of stringer but it's a it's a\ncaptured in a data type\nlike the model the model pass and the\nsound like AUC rate or maybe some\nsomething about the model slab summary\npath\nyeah in that case uh do we have any work\naround about it I see some discussion\nabout ego mode but I feel like that\nshould be a very a big effort\ndo you have a concrete example\nor for this one uh I will be making a\nconcrete example for for now I have the\nlet me talk maybe I will try to just\nchange this double to your class and we\nwill find like we cannot try trying to\nfind pass anything from yeah that's that\nthat's the first case right that's the\nsame reason why you can't index into\ntrained models yeah yeah I'm yeah this\nis pretty like the same case I just want\nto justify our use case\nneed this and just you want uh do you\nhave any\nlike plan to support this kind of thing\nor maybe it's an eager mode going to\naddress this problem\nso could you tell us a little bit about\nwhat you're storing alongside the the\nmodel like are you going over a pass are\nyou storing um yeah it would just be\ncurious to see what what are the other\nelements in the class although animacy\nwill be all stream butter you will be\ndictionary or something like that yeah\nwe we don't we don't want to pass the\nentire model that would be pretty big we\nstore in the hdfs or something well in\nour internal artifactory but uh yeah but\nit would be something like this but\nanother name Topo\nthey will be Pro it will be a property\nin the yeah something like that then we\neven lose the Google sugar syntax from\nthe Len topper\nyeah am I clear yeah it's just um I'm\nmore asking like if you're storing a\nstring what is what are the what are the\ncharacters of the string like what what\ntypes of information are you storing oh\nyou should be passed\nto model\nand internally our next task will be\nunderstand it and they try to retrieve\nthe model let's say the next task is\nanalysis another analysis on the model\nright we need to fetch this model and\nthe next one we just load this model and\nmaybe they need to load something else\nyeah it's yeah this this is interesting\nbecause there it relates at least this\nexample that you provided relates to\nsome of the artifact work\num\nsomebody artifact what do you mean by\nartifact work\nyear Kevin you might want to chime in if\nthat if I've made that connection\nproperly right like if you're passing\nthem\nuh\nI think that is\nrelated but somewhat orthogonal\nthis is the the pattern here and the\nrequest to make Json so scriptable in\ngeneral\num is because people often will have one\ntask that returns a group of information\na bucket of things uh they typically put\nthat bucket of things into a data class\nor a pedantic model\num and but then they would like to kick\noff like two different tasks Downstream\nof that that use different components of\nit so one task produce like Source data\nand then the downstream tasks consume\ndifferent portions of it currently the\nonly way to do that is to have multiple\noutputs\nthe way we do multiple outputs that are\nnamed in Flight kit is named Tuple\nso if you can make\ndata classes or anything Json\nsubscriptable in general then you can\njust bind to a sub uh path of a prior\noutput instead of the whole thing\nso that's the general request the\nartifact thing that uh John's talking\nabout is the fact that you can reference\nthe\num individual outputs of a task\nindependently\num without necessarily having to have a\ndirect\ninput output relationship between them\nlike you don't necessarily have to pass\nthe output of that task into the\ndownstream thing you can make that a\nruntime bindable thing but that's that's\nI think that's relatively far off\nokay\nyeah\nthat's not the first day a case you talk\nabout maybe it's more close to our ask\nto make it uh subscribable\nyeah I'll yeah and I'll present like a\nbetter use case for the artifact thing\nnext next meeting\nokay\nso in this case uh what is our action\nitem right now so I I guess I needed you\nto show me the issue just to talk about\nhow to make the Json subscribable maybe\ndo I need to create an issue in flight\npeople yeah the issue's already there\nwe'll send it to you I don't know right\nnow okay and uh yeah I will be making\nmore concrete example for the the next\nuse case I have but I think you already\nknow that yeah this is really the same\nthing actually because that's we're now\npassing name on top or actually in our\ncode and this is only example because we\nalso found the name type of dictionary\ncannot be retrieved I mean cannot be\nsubscribed\nDan anything to add\nuh or about mine yeah ask\nthank you yeah I think it's all covered\nwell\nthanks everyone\nany other comment\nor we will move to the next item new\nideas in the incubator I don't see any\nnew ideas from from the past weeks\nso we'll skip that one and uh yeah\nprobably\nuh much lighter the community pools this\nsection is intended again to try to\nsurvey some observations from\ninteractions with the community uh from\nfrom the past couple of weeks\nand um yeah I I had a couple of sessions\nthis this with with users\nand and kind of all of all of the issues\nrevolved around the same\nuh having more informational or more\nactional your error messages will help\nwill save time\nuh and and O'Neill's Statin effort\ncapturing some of them and also labeling\nwith improved error message and uh the\nidea of this section is not to bring any\naction item at least by default\njust observations uh from from the field\nand um yeah any effort or is improving\nerror messages\nwill be appreciated by many\nany comments on this\nnope\nall right uh next one Open Mic seems\nlike it's only me so\num I've been following the metrics for\nflight it's public you can just create a\nLinux foundation account and you can get\nthe metrics\nuh since the project was the native and\nuh I've been following some of the\nmetrics that that measure at least\nindirectly the responsiveness of the\ncontributor slash maintainer\nteam and\num so far it's it's been positive but\nI'm noticing a trend that is something\nthat probably will need to\naddress with some tactics eventually uh\nyeah the the time to First review the\nfirst thing that I need to say is there\nthere are no good or bad numbers right\nit's it's very hard to to put a baseline\nhere what's important here is the trend\nso um we've been decreasing the time to\nFirst review which is great and also the\ntime to merge has been consistently\ndecreasing even when having\nuh kind of a team of the same size a\nsimilar size but having more\ncontributors helps definitely helps\nbut there's some negative trend on the\nissue side uh one of the\num\nmetrics that is not directly available\nin in these dashboards but you can just\num\nbuild it with the current numbers is the\nrate from close to creative issues\nbasically indicates if it's more than\none it indicates that the team is able\nto close more issues than the number of\nissues that are created and less than\none will is the opposite\nso\num we've been yeah we we decreased from\nfrom last year uh probably because we\nhad a major bump in the number of new\nissues we just a quarter we have more\nthan 200 new issues\nwell in the past quarter it is April May\nengine\nwe had an important reduction on the\nnumber of issues but but not an\nimportant Improvement on performance\nuh so basically this means that\neven with with more issues with less\nissues we are still not able to close uh\nat least the same number of issues that\nare being created\nuh so these numbers could reflect that\nuh especially because of the new wave of\ncontributions we are much focused on on\nworking on PRS emerging PRS there are\nmany issues that stay open for for\ndifferent reasons\nuh but for me this is kind of an\nindication that everything we can do in\nthe near future to better on board and\ntrain a new wave of flight collaborators\nis you may remember the governance model\nnow includes the the collaborator role\nsits between the the general Community\nmember and the contributor and the\ncollaborator who constantly help with\nwith answering\nuh questions on issues and helping\nprocess issues in some way so um\nagain probably not not a direct action\nitem right now but but an observation a\npattern that I'm seeing and the it's\npart of the growth pains and we will\nneed to to create more resources to to\nempower or collaborators and\ncontributors uh to help us stay\nresponsive and accountable for the\ncommunity\nany question\ncomment\nnope\nall right uh seems like that's it in\nterms of agenda anything else any other\ncomment\nnope\nall right it was great to see you again\nand hope to see you in the next one\nthank you\nthank you\nthank you bye"
    },
    {
        "title": "Flyte Contributors Meetup - July 20, 2023",
        "transcript": "okay I guess it's time\num\nI don't think we have any\nnew members today\nso you can\njust\nskip that\nlet's take a look at the rfcs uh wait\nsorry I'm not sure if you can see oh\ncrap this didn't even work I'll have to\nshare again give me a sec\nsure\na window yes this is\nall right I should be seeing a board\num I don't think we have any new rfcs\nbut we have a few that were accepted\nthere are you know\nbeing implemented already\nincluding eager mode which\nat this point I think we should\nmove to accept it if everyone everyone\nagrees I know that News is here too talk\na little bit about what what's happening\nthere\num\nI don't think we got a lot of movement\nthis\nconsole UI upgrade though right like\nthere are\noh yeah I remember\nwhat we're gonna do about this let's not\ntalk about it now\nbut yeah no new rfcs a bunch of them are\naccepted including the execution\ntags which are totally coming in the\nnext flight release\nboth like the back end and flight kit\num PR's are out I know Kevin's not here\nbut I'm helping you know Shepherd those\nalong\num we just got this RFC for a more sane\noverridable defaults Catherine is also\num working on on this making this change\nit's pretty amazing I also learned that\nyou can actually\nroll on the climate\num just like people out there who worked\non operators to help facilitate that\nwithout you know you having to manually\nroll out the deployment like Decay it's\ndeployment like there's a thing called\nit's called reloader that does that for\nboth like config Maps in Secrets which\nis\na great feature I don't understand why\nkubernetes hasn't you know implemented\nthat yet\nthey've had an issue open for I don't\nknow seven years\nI'm not even joking\nanyways super excited about this this is\nan area of you know flight that I\njust find it\nso complicated so great\num\na Reynold is also getting\num\nto an experimental stage so it will be\nout for one nine at an experimental\ncapacity this would be awesome\ngreat no New York sees a bunch of them\naccepted and I don't know news if you\nwant to talk about eager mode what's\ngonna happen there\nfeel free\nyeah so it's just then\nmany months in the making\num\nthe latest thing that I implemented was\nto actually\num kill the subtasks that any ego\nworkflows has\nso yeah any good workflow has spun off\nif I if I terminate that eager task\neager workflow it'll\nrecursively\nterminate the rest of the subtasks so\nthat that was kind of like the last\nusability thing that I really wanted\nbecause I think shipping that even an\nexperimental without that feature would\nhave been pretty bad\num\nyeah\nso I'm I'm working to clean up that PR\nthe last thing I really I kind of wanted\nto sneak in there was\nto somehow\nAuto register\nlike if I Pi fight run an eager workflow\ntoday\nbecause it's not an actual workflow it\nwon't\nlike register the subtasks called inside\nof it\num so like you kind of have to do a pi\nflight register step\neverything in the script beforehand\nbecause all eager workflows do is like\ntake get the latest tap version of the\ntask and then run that so if it if the\ntasks haven't been registered it'll just\nfail\num so this could be like a documentation\ntype thing that we solve later\nwhich I think is okay actually\num but really I think this should happen\nby default you buy flight run for all\nbest like this is the implementation\ndetail today I think like we yeah start\nwith specific flight entity and register\nonly only the\nthe entities are related to that you\nknow workflow or launch plan so\nwe have an internal registration script\nthat does very similar things to Pi flat\ndrum but we have our run command\nregisters and then search execution\nso\num because we have the same problem\nwhy not because you have to compile\nwhy can't yeah I mean you can solve it\nfor this you can solve it for dynamic\ntasks\nso that you can run pathlight run a\ndynamic task and somehow pick up all\ndifferent\nlanguage the way it works today for\nworkflows is that we run a compilation\nstep so we know which tasks belong to\nthe workflow\nif you can't compile it which I don't\nthink we can at least that's the whole\npoint of it\num you you can't do that\nit's a little bit true yeah yeah he's\nright it's a little bit more complex\nthan them making it out to be\nI thought I'd register Imports the\nworkflows and then registers all flight\nentities that are imported\nyeah so for register that is the case\num so that that that's what I was saying\nabout like documenting\num eager workflows\nby saying hey you need to register\neverything first\nbefore\nrunning it and oh yeah I guess that this\nalso applies to Dynamic workflows so\num\nI guess my comment was more like towards\nthose regular pipeline run where today\nwe only register the entities there are\nin the path you know\nAST parsing\nnot going there yet okay sorry cool it's\nall right\ncool let's keep going uh\nno neuropsies\num\nwhat's happening to the config overrides\noh Bennett and I and Byron throw com\nslack this week and Linkedin they have\nan internal deadline until they want to\nhave this done\nand that's why we discussed with him\nthat they will take the lead on\nimplementing that because not a feature\nthat either banhad or iron need that\nworks so we can't work on it at work\ntime\num and that's not fast enough for them\nso they have a deadline that they want\nto have it done this quarter and he will\nask internally whether he can have\nengineering capacity for that and if yes\nthen they will take our overprs\num because they need it quicker we can\nbuild it\nthat's the update for this week\num because I'm thinking like this week\nnext week like\nI'm thinking um I'm more than I do you\nthink it's worth like\njust to help them get this thing out\nfaster like have I don't know\nmore frequent syncs with them like how\nhow are you\nhelping them now who's like hand that\nover like you'll be\navailable for reviewing PRS or like yeah\ndefinitely and we also offered like\nfirst of all you have to check whether a\nWindows Manager I guess whether he kind\nof whether he kind of capacity and I\nguess we'll have a meeting with them\nwhere we explain to them what we did\num what we think needs to be done\num\nand definitely we can review the PRS\num\nbut let's see what what he said when he\ntalks to basically his team\nyes we didn't answer there first\nall right cool thanks\nit's um\nbecause we have one of those big\nfeatures yeah awesome\nall right let's\nlet's keep going\num I think we only have a one new idea\nin the incubator but like a few updates\non the other\nso\nnews how about we\nstart out with you it's Jupiter one\nthing what is this\nyeah\nso\noh I guess\num I guess can you go to the um\nthe link on your screen share\nI can just talk through it\nright now we see the uh flight rfc's\nproject board\noh really I'm sharing my whole window\nwhy isn't okay oh maybe it's just me\nno oh we see they are super\nI don't have to share again\nand zoom is killing me today\nright my entire window why no wrong one\nis it this\nyeah\ngreat do you see it now\nyep\nyeah so the just to be very clear this\nwas just hacked together in like a day\nor two just as a proof of concept\num\nbut it has been\nbased on like feedback you know that\nwe've had in OSS and also closed source\nbut um\nI guess like the user story here is\nas a data scientist I want to like run\nflight code flight kit code directly\nwithin my notebook\nso that I can bypass the whole step of\nlike writing the python script Regis\nregistering that and just like making\nthat a\nsmoother experience\num so this is actually this could be\ncompletely the wrong approach\num\nbut if you look at this notebook if you\nscroll down\nthe interesting part is\num well first we load that run remote\nnotebook remote thing\nup there\nand then\nat the bottom is a\nlike Jupiter Magic\nthat's been imported\nthat is basically Pi flight run and all\nthe magic that's happening here is it's\ndumping it's like treating The Notebook\nas a script it's like\nscraping out the python code throwing it\ninto a python script\nrunning that thing and then just\nforwarding a bunch of these arguments to\nPi flight run\num\nso that's\nthat's pretty much this experience I've\nI've chatted with like some other folks\nlike jeeve and like I think\nit just like really depends on how\npeople who use notebooks a lot would\nwant to do something like this\num\nso I'm chatting with Greg geidesh next\nweek just to get a sense of like what\nthat might look like\nbut as a proof of concept like Jupiter\nnotebooks is like super flexible like\nyou can with Magics you can just like\nsplice the you know you can like with\nMagics and tags you can just like select\nspecific cells you can run bash scripts\nand SQL scripts with like percent SQL so\nit's like a lot you can do here\nbut it's you know a matter of how we\nwant to exactly make the experience work\nyeah these\nlike I I don't know how complex people\nmake\num notebooks you know\nas far as I can tell like the Magics are\nbounded so like if you do like a Bash\na percent sh and then look you have a\nbunch of bash code in the Cell It's like\nbounded\nit's like run I think in a sub process\nthat code\nso\nI think it's like reasonable to expect\nif I say percent flight\nlike if we want to invert this model\ninstead of like having this be treated\nas entire as an entire flight script if\nwe want like local code local code local\ncode and then flight code and then bring\nthat back into your local\nruntime\nI think we could like\nfigure out something there as well\noh that look like news like you'd have a\nmagic that would say\npercent flight and then\nwhat would happen\nlike you can treat you can have a magic\nthat is both so in this case with just\none percent it becomes like a magic\nfunction\num\nwhere the arguments after it\nlike are just passed into the the\nmagic function\num sorry that's like really confusing\nbut there's a at decorated function\nsomewhere that takes in that line and\nyou can do whatever with that line you\ncan parse it with that shlex and then\nforward these\nto like some other sub-process call or\nsomething else\nbut if it has two percents\nyou get both the cell content below the\npercent sign\nand you can pass it arguments\nso you could have a thing that's like\npercent flight\nkit\nyou pass it pipeline run arguments and\nthen it just like treats the cell below\nit as\nlike a self-contained script\nI see\ninteresting okay\nyeah\nokay\nyeah I have one question are we able to\nbuild the image or image back in The\nNotebook\nso so like\nif everything just reduces to a python\nscript\nthen yes we can yeah\nassuming that the local Jupiter like\nwherever the Jupiter server is running\nhas Docker installed\num so I did test this would work on a\nGoogle collab but you would have to use\na pre-built image\nnice\nyeah\ncool so what is happening there do you\nwant people to\ngive more use cases or like what are\nyour expectations there\nyeah I think Gathering\num\nuser-facing code like what the ideal\ninteraction model or set of interaction\nmodels would look like would be great\num so we can like share this in\nannouncements or something but\nlike if if anyone here has Works in\nnotebooks a lot or has seen\nlike what people have tried\num in terms of like using flight inside\na notebook then I think\nthat should inform the design because at\nthis point like it will work\nyou know with some constraints but uh I\nthink it's like nailing down the user\nexperience\nyeah I I like it how it\nsimply reduces to you know by flight run\nlocal run like there's no extra concept\nthat people have to grow like\nif you know what by flight brand does it\njust applies that model too\nto one single\num Jupiter notebook\nbut I I don't know enough of that like\nthat's that's enough\ncool\num\nokay the the thing we want to talk about\nany of those more recent ideas in the\nincubator\nlike I know that we had at least\nOne update to\nthese two I don't know\nwhich feedback yeah so\nforeign\nwhich I like\nit's two comments up\nthree three comments that's the same\nsame page to scroll a little bit up\num\nyeah so having like a sidecar and um\nvery common it's like people do this but\nany other concern besides what's what's\nhere then like you thought about this\nmore or\num I don't know if you saw this comment\nthat that\nis\nhere like\nessentially relying on on your PhD plans\nto do\nstore those logs those special Vlogs\nright\nuh this is\nquite a lot of feedback it's about\ncreating a Communication channel from\nflight kid running in a task pot to\npropeller so that in a task but we can\nsay hey we have this new login please\nshow us in the UI or hey we updated the\ndeck please update it\nelse Dan I just no I don't think so it's\nabout creating a Communications channel\nthat doesn't require like a message\nqueue\nthere's a million things that we need\nlike as things are progressing and\nhaving some kind of feedback mechanism\nlike it to flight propeller would be\nadvantageous if we had this if we had\nthis would be if we had this would we\nuse it for eager mode\num\nI mean the idea behind like you're\ntalking eager mode like bringing back\nlike compiled workflows Etc\nno no like instead of kicking off a\nbrand new task as an execution through\nadmin which is I believe what the eager\nmode engine does\nit would invoke all the tasks and sub\nworkflows directly through\nyeah I wouldn't use it for that um I\nmean this is more so for like a a not uh\nlike reporting metadata back from flight\nkit to flight propeller I think right\nnow like uh\nyou know for flight decks we write a\nfile out to S3 so we're talking about\nhow can we report log Links at runtime\nfor like One DB things like that\num appending those to a file and S3 I\nthink is a very poor idea\num we're talking about reporting span\nmetrics from flight kit and right now\nthose are just you know the proposal was\nto just continue to append those to a\nfile in the blob store and then have you\nknow existence checks and flight\npropeller and things like that\num as we want more and more information\nto come from flight kit and be reported\nby flight propeller this is a really\nhacky way to do it to use this blob\nstore for like inter-process\ncommunication never what it was designed\nto do not not a very good use case for\nit so the proposal here was like if we\nhad my original idea was just to emit a\nlog propeller can check the logs of a\npod\num that uh very rightfully so you know\nsome kinds of production level use cases\ncan have millions of lines of blogs\ndepending on what's enabled so checking\nthe logs isn't necessarily a great idea\nbut iterating on it thought maybe we\nhave a sidecar where the flight kit can\nconnect you know through some kind of\nlocalhost socket or a shared volume or\nsomething like that to a process running\nin a sidecar that emits you know regex\npartsable logs propeller can check those\nlogs and then that can be a very like\nlightweight feedback mechanism you know\ncould say hey add this log like hey add\nthis\nspan information add this you know etc\netc\num but kind of event is like an\nextremely lightweight feedback but then\nit would be in real time as well so you\ncould say hey I just wrote this flight\ndeck add this and make it available you\nknow it would be a very fast feedback\nmechanism whereas right now it's it's\nnot\num\nsorry go ahead one thing for context I\nthink we're the idea is to find\nsomething that does not use a message\nqueue right I think we're aware that\nthat would be the proper way to do it\nbut we want don't want to complicate the\ndeployment\nthe question is is there a smart way to\ndo it without using pub stop basically\nor yeah or like any kind of external you\nknow it pops up certainly The Right Use\ncase but you could use you know redis\ncues anything yeah exactly yeah\nsomething like that so yeah more\ncomplicating infra I don't think is is\nanyone's gonna want that\num\ncan we can we talk about that though\nlike what is like besides you know\nbringing in more complexity to the\nsystem like\nwhy can't we not have strict guarantees\nfor these kinds of events\nand introduce some complexity like as\nyou mentioned then using like red is for\nthis\nlike\nI can\nit's like obviously put you know\num\npressure like mechanisms to show that\nyou know propeller doesn't\nfeel over if for whatever reason the\nflight keep on like\nlogs\nbut\nwhy can't we assume more complexity to\nbring in this feature in a more\nI think kitten's argument was that a lot\nof people on slack already asked today\nhow to deploy it and that it's not easy\nfor especially people coming from the ml\nworld\nand that more infrastructure is not\ngoing to help\num and I agree with that\num on the other hand I want to ask the\nquestion there's there are discussions\nabout slack notifications and that also\nwill use SNS and sqs and then Google\npops up so there there are there are\nalready features that will rely on a\nmessage queue system\ncan we piggyback here\num I think we should I think it's also\nlike a nice idea here but the question\nis are there Services you that already\ncan't we use it here too should be a\nconscious decision at least\nyeah I think a lot of that has to do\nwith like what I mean more complicated\ninfra\nuh\nwhat is this going to be available on\nyou know we're already running into use\ncases where like single binary mode you\nknow running in the sandbox and things\nlike that like we don't want to add Pub\nsub into that\num serves or for like you know\nproduction level deployments uh yes\nfantastic I think large scale production\nlevel deployments but\num further complicating the simple use\ncase this you know using these pod logs\nwould make it still available from in\nany any deployment\num\nyeah I'm okay kind of with whatever\nmaybe there could even be different ways\nto do it kind of the way that doesn't\nuse complicated infrastructure which\nwould be this year and then for people\nrunning like a staging in the production\ncluster and really like putting a lot of\neffort into this for those people who\nwill not be complicated to use SNS there\num and I certainly thought about that\nlike the way it's implemented in\npropeller you can have you know uh very\neasily say\nlike an enume in the configuration like\nwe do with the logs and everything else\nright now that says you know what\nmechanism are you using\num propeller creates the pods it's very\neasy for us to say like you know pass a\nargument to flight kit to say you know\nright to the side car and and propeller\nwill create this pod with the side car\nor we can say right to you know right to\nKafka or something like that as well\num so yeah I mean we could we can\ncertainly make this extensible and and\nfollow that\num I don't think we're locked into\nwhatever we decide to do from the\nbeginning but I think that like I said\nthis is blocking a number of different\nfeatures I think like the flight kit\nmetrics the logs so coming with the\nsolution is important\nso I personally I need to talk to Dennis\nabout this I can't judge whether using\nkubernetes events is a good idea I\nreally don't know but I like it you have\nthe side car as a fast feedback\nmechanism because I think it's super\nsimple to do that even in the demo\ndeployment and it's fast and if it's the\nside cars locks that we parse I don't\nsee performance issues there we could\nlimit how many lockdowns people can send\nto the sidecar right um\num like kubernetes logs my only uh my\nkubernetes events my only concern with\nthat is like the flight kit container\nrunning internally\num I am not sure I mean I don't know\nenough about all that mechanism right\nthere\nhow difficult is it you know what kind\nof dependencies are introduced if we\nwant to you know emit kubernetes events\nfrom flight kit and this and that\num and are there complexities there that\ncan pick and screw everything up\num I'm not sure I have to look into it\nI don't know either\nwithout any progress on this\nright\nforeign\nwe have our Ops weekly tomorrow and\nDennis is really good with kubernetes\nI'll just talk him tomorrow and ask what\nit would entail understand the question\nthat you are interested in what it would\nmean for flight kit which dependencies\nwould have to be put in and how\ncomplicated that would be I'll ask him\ntomorrow and I'll report back and then\nyou can just see what we do with it that\nsounds great\ncool thank you\num how about this I don't remember if\nyou had any more you know\num yeah also like update is then Dennis\nand I will talk next week Tuesday for\nlike\nwe'll talk next week Tuesday about this\nlook at it beforehand also to prepare\nand that thing could make sense to talk\nabout it the meeting after\nnice\nI think it's a just very a lot of very\nintricate details and it's very\ndifficult to talk about when you don't\nlook at the code\nall right thank you\nuh I think this is it let's just double\ncheck that there were\nnow Fabio I haven't looked at your\ncomments yet I will do that today in\nresponse to the artifacts totally fine\nbut I really like it um the entire time\nI found myself writing a comment and\nthen reading one was like okay it's\nanswered there um I would have done the\nsame thing so\num\nyou know there's not a lot of\nlike not a lot of goods critic there\nhopefully by the next meeting I can have\nwhich will rely on Reddit\noh man okay uh this is\nwe talked about this during the last\nmeetings\nshe she just dumped what we talked about\nlike her use case here right I don't\nthink there are\nwhat you like\nfeature requests essentially\nyeah it seems like the external stuff\nchanging and happening affecting flight\nstuff is very compelling it it I guess\nyeah I struggle to think of within\nflight use case where you can't just\ncompose it all into a single workflow\num\nI guess the best thing I can think of is\nlike\na batch inference pipeline that uses an\nartifact from a model training Pipeline\nand you want you don't want to like\nstrongly couple those things that you\njust want\nthe but yeah I guess that's not really\nreactive so I yeah I don't know like\nthe yeah anyway I'll stop rambling\nand remove some of the work that we're\ndoing for the airflow agent can like\nbe extended to start supporting this but\nlike oh\nyeah\nI don't know\nand I'm not sure anyone else is thinking\nabout this or at least very seriously\nnot yet at least cool\num\nthe one is going to in this game now how\nabout this locking mechanism like no we\nI don't think there was an RFC yet for\nthis yeah that was the latest all right\nthanks is enough for the RFC incubator\num\nI\nif\ncommunity pools\num\ndidn't really get\na benefit for this one in here so if\nanyone has\nquestions or anything that they want to\ndiscuss related to you know PR that's\nongoing like piantic might be a good\nyeah Arthur is here I think we should\ntalk about authentic\num\nwho gives a summary of what's been\nhappening do you want to do that right\nthere\nsure I can go so\nwe have a PR ready for an integration\nthat lets you pass up identic based\nmodels between flight tasks\nand the question came up about the new\npedantic version 2.0 and how this might\nwork in at first glance we thought that\nit would easily extend to that however\nit turns out that some internal methods\nare deprecated\nuh in pedantic 2.0 and they've been\nreplaced with a kind of a new\nserialization logic that they want to\nuse going forward\nso the question now is mostly around how\ndo we want to support both pedantic one\nand two\nuh and uh yeah what what kind of logic\nare are we willing to internalize into\nthe pedantic plugin in flight to enable\nserialization and deserialization\nsmoothly versus how much will we want to\nrely on on how pedantic does it with uh\nencoding to Json\nwhere some of the the flight supported\nobjects are yeah of course not very\nconveniently encodable to Json such as a\nbig tensor so the current solution is is\nrelying on both protobuf and and Json so\nthe flight supported objects are\nserialized into protobuf\nand separated out from the basement also\nthat the rest of the base model can be\nserialized using pedantics engine\nso this ends up with two two separate\nobject stores one is the the pedantic\nJson with the base model and then a\nseparate call it dictionary where you\nhave the flight supported objects flight\nfile flight directory tensors pandas\ndata frames you name it serialize the\nProto buff\nand these then get uh combined back when\ndeserializing it\nand so yeah\nwhy that doesn't work for pedantic V2\nthe the logic Works uh it's mostly about\nthe Aesthetics of the code as far as I\nsee it like the way that it's uh that\npedantic serializes in in B1 is that you\ncan add your custom serializer onto the\nobjects\nuh and then pedantic will call on on a\nDunder method in the object and check if\nthere's a serializer if there is then\nyou can use that one when once you're\nrealizing\nnow in pedantic 2.0 the logic is\nslightly different they expect you to\nto add for instance a decorator\nin in the base model itself\nwhere there's a function that defines\nhow to serialize the base model\nor alternatively you can add something\nsimilar on the on the object but it\nlooks quite different so I think we'll\nyeah it's\nthe logic as far as serialization and\ndeserialization goes on an abstract\nlevel is the same but the the code that\nputs that into action and like\nintegrates that with pedantic it looks\nquite different depending on if you're\nusing B1 or V2\ndo we have to when you when you say that\nthe validators\nso on V1 it's basically Json serial\naddress right and that are passed\nthrough to\num to Json\num when you say that the serializers\nthey have that they have to be methods\nof the base model and then you put a\ndecorator on it does it mean that we\nhave to kind of monkey patch them in for\nthat to work or\nyeah I I I'm not I'm not certain enough\nat this point I'm not so familiar with\npedantic 2.0 I would need to to look\ndeeper into how they do it under the\nhood\num but at the first step I did I\ncouldn't get it working by just\nreplacing the the old get validators\nmethod that is placed on objects with\nthe new one I think it's I forgot what\nthe the name of that method is but it's\nslightly it's slightly different it's\nvery it's not really sorry\ncan we just released this one for V1 and\n10 until less than less than two\nand then address B2 separately like\nthere's if if the pedantic core itself\nis like pretty different I think it's\nfair that any plugin based on it is also\nfairly different\nyeah I would tend to agree\num\nI think that the objects are like the\nclasses and the methods that we have in\nthe pr\nwill quite easily extend to V2\nI think all we need to write is the\nadapter that's\nadds them into the base model in B2 in\nthe right way and the right way is\nsomething I'm not exactly sure right now\nbecause I'm not so familiar with V2\nso uh here's I guess so here's a\nexample PR\num of like the diff\nthere I have to do this recently in\nPandera\num Pierre's not merged yet but it's just\nlike\ndifferent conventions\num\nyeah\nso if you go to like the model.pi file\nyeah get by identic core scheme yeah\nthis looks familiar yeah yeah so this is\nalready quite helpful yeah\nlike this is how fast API does this as\nwell supports both so I just kind of\ncargo culted that pattern of just if\nV1 or V2 do this and then\nEtc so\num\nbut I I agree probably do the same here\nit's just that I I'm not familiar enough\nto do it right now I would need a bit of\ntime to to familiarize myself to do it\nright but I think this this would this\nlooks like the pattern that that I was\nexpecting\nyeah\nI do agree with you of just like pinning\nthis and then unpinning it once we\nget support for v2\nokay\num\nlet me think was there was there\nanything holding us back from actually\nreleasing this I think I think this is\npretty much\nwe solved the problem of figuring out\nwhich types are supported by the type\nended we have that right\nyeah yeah so that's that's all thanks\nKevin for providing that certificate\nyeah I think it should be fine I mean\nI'm I'm a rather new user of flights I\nreally like it but I think my capacity\nof testing whether this works in every\nEdge case it's probably not as good as\nif someone who has built a engine like\nif you you know where the serialization\nbecause I can use pedantic quite a lot I\ncan build an image with that version and\njust see whether our tests possible I\nthink I mean it doesn't cover everything\nobviously but we have everywhere else\nthat would be great just just as a last\nresort I've tested it and it looks like\nit works prediction I wouldn't know\nthank you and\nI can have a look at this V2\nI said I don't think it's going to be\nmuch much additional work it's just that\nmy on me to familiarize myself with how\nit's done\nall right thank you so Fabio you'll run\nthrough a quick test we'll run we'll\ntest it off also of course we can also\nget a beta out you know if like this\nmakes testing easier\nyeah okay that sounds good we can all\ncut up\nlet's cut Alpha release off of the\nbranch\num we can can you fry it on slack when\nyou did that then install it\nperfect\namazing\num\nif anyone have anything else you want to\ndiscuss at the time sure\num no\nChrome\num\nthank you so much\nand I'll see you all in\ntwo weeks\nhave everybody dressed up today see you\nthanks everyone\ngood day because"
    },
    {
        "title": "Demystifying Flyte Agents | Flyte Community Meeting - July 25, 2023",
        "transcript": "yeah so\nso what what's fly agent so agent is a\nlong long-learning Slayer service they\ncan be used to as a group attacks\nespecially for the like API tasks or\nsensor so is it designed to like\ncommunicate or talk to the external\nservice like we can submit a job to the\nbigquery console or like we can check if\nthe file is six in the H3 bucket like\nand then we decide a simple API\ninterface which allow people to easily\nto extend to add a new agent in flight\nso here you can see like uh every flight\nagent service is essentially a Joby\nserver and then it runs many different\nkind of Asian each agent handle\ndifferent kind of tasks and then you can\nhave two agent service one for\nproduction one for development the\nreason why is we do that is that people\ncan when people want to add a new agent\nthey can add it to the development of\nfly agent service so when when when when\nthe agents are shut down for some reason\nit will not affect the production\nenvironment\nand then we add a new job business\nclient in the fly propeller and\npropeller will send the request to the\nagent service and based on the text type\nso\nuh some people may have may have some\nquestions about what's different between\nbig and plugging and the agent so in the\npropeller we have to kind of plugging\nthe first plugin is called KS plugins uh\nit used to create a new new part to run\nthe task for example when we run the\npython text it will create a part and\nthis part will run your python python\ncode and then when you want to run like\ntensorflow or Python's job class plugin\nwill create a custom resource kubernetes\ncustom resource and here is like for\nexample like tensorflow job or a python\njob and then you will create\nand run the distributioning and on the\nother hand the way back plugin instead\nof grid instead of creating a new part\nwebmap plugin will send HTTP request to\nthe external system for example\nsender query to the bigquery and then if\nyou want to run like Spar on the\ndatabase send the spot tags to the witch\nplatform or like snowflake Etc so uh we\nso basically the agent can\nPatron is going to replace the way back\nplugin so we want to decouple the agent\nfrom the propeller so we use the flight\nagent to handle this web API tasks or\nlike file sensor to check if your file\nis inside in the bucket\nso why why do you want to decouple uh\nthe plugging or agent from Propel so the\nreason why we do that is that we find\nthat a lot of people struggling with\nadding the new bacon plugin you fly\nbecause big and plugin is written in\nGolden and then most of our contributors\nare not familiar with Golic and they\nwant to write the big unplugging in\nPython\nand then\nuh we we Define the uh\nuh agent interface in the fly ideal so\nbasically you can generate the server\ncode in different language and then for\nnow we add some Asian uh interfly key so\nit's enables you to easily to test or\ntest your agent in the local execution\nand then another thing is that Asia is a\ncompletely different Surfer from the\npropeller so you can test this agent\nindependently and then the most\nimportant thing is that when you run\nmore tasks there's more uh overhead in\nthe propeller so in in each kubernetes\ncluster you can only have one propeller\nso we we want to run we want we want we\nwant to find some way to handle more\nrequests so we decoupled the agent from\npropeller and then every agent you stay\nless so you can easily just get out or\nskate down your agent based on your\nworkloads\nso here we take uh bigquery tags as a\npiece example here without agent you\nhave to write a backend plugin in fly\nplug-in ripples it's written in Golden\nand then you have to update the\ndependency in propeller and then\nrecompile it you have to figure out how\nto uh how to update the dependency in\ndifferent Ripple and then you have take\na long time to build an image for\npropeller and then you have to update\nthe\nconfig map to enable the plugin and then\nyou restart a propeller deployment\nand then you'll have to register a new\nworkflow the reason why is because uh\nyou cannot you cannot use flykey to run\nthe golden code to run the backend\nplugin so to test you you have to run\nthe workflow remotely and with agent you\ncan simply a new plugin in any flight\nfly key plugin and then you can add this\nmixing I will talk about this in demo\nlater this you add this missing to your\nattacks which allow you to run your test\nin the local execution that's it so it's\nvery easy to test and develop\nokay let's do some demo\nhere\ncan you see my terminal\nyeah yep okay thanks\nuh here we have a bigquery agent a\nbigquery agent is inherent from the\nagent base agent base is a qrp it's a\nclass generated from the photograph is a\nJoby server class\nand then what you what you need to do is\nthat you have to implement three Master\nuh create\nget uh delete\nand then propeller will extend the\nredirect the request to the agent and\nthe Asian tube agent everyone will know\nwe don't should know how to create a job\nor delete a job or get a status here for\nexample like uh propeller will send the\ntext template to the agent\num in the fly in the flight in the\nflykeep we will civilize every\nthanks to the text template so the text\ntemplate containing some metadata of the\ntext like\num\nI earn price uh text input output\ninterface or like jump config and here\nis like in this text template we have a\nbigquery country here so in the create\nrequest in the create function we get\nthe\num project and the location of the table\nand then we use bigquery client to send\nour query to the bigquery console and\nthen we will get the job ID here\nand then we return this resource meta\ndata to the propeller propeller will\nsave this metadata we won't we will not\nsave the state in the station so\navailable safety is metadata uh in the\nmemory and then it will also process in\nthe S3 pocket and then next time\npropeller will periodically\na code agent to check the status here we\nget a job ID from uh from the propeller\nand we use bigquery client\nto get the status here so so if the if\nthe job succeed we update a status and\nthen we create a charter data set output\nand then return to the propeller if we\nnot see we just return running here\njust start running the downstream tags\nhere delete we just here we just cancel\nthe job in the bigquery and then here\nand finally we register the bigquery\nagent and\nyour\nin the text\nin the task you just need to add the\nmixing uh I think agent has a good\nmixing if uh if we added this amazing\nthe flaky will try to mimic the a\ntrapeze software behavior and try to\ncode agent code in the local execution\nhere\nhere uh this uh Missing we have one\nfunction called SQ when so when you run\nthe\nfly keyboard flow flywaffle locally it\nwill call this function here we the this\nis amazing we'll try to get the agent\nbased on the text type\nand then you will code create create a\njob like submit a query to the big\nbigquery console and then here we have a\nwhile loop and then periodically check\nthe status and then the jaw will finish\nit until it Returns the scenes\nand then after drops I see we return the\noutput uh and then use this output for\nthis output will use for the downswing\ntags and yeah that's it and then\nhere is another uh and then you can here\nthis is a bigquery example without Asian\nyou cannot run this overflow locally\nbecause\nthe bigquery plugin is written in the\npropeller\nand then for now you can have a text try\nto run the query\nin the pickle console and then print the\noutput in this by using structural data\nset here\nit will show try to run run in the agent\nand then try to convert the output to\nserver data set\nso for now there's not there's there's\nno no data in the table so just print\nnothing\nand\nhere is another example for the arise\nanother example for the sensor here\nyou have a task this is a file sensor\nuh this file sensor only take one\nargument is passed you will check if\nyour file is six if it is six you will\nreturn so see yeah uh\ntake only take one document and then you\ncan check the page\nin the\nin the Asian we we try to uh you you're\ngetting a pass from the text template\nand then return it to the propeller and\nthen in the get request you'll use Evers\nback to check if the file is it if the\nsix will just Returns the C and then\notherwise we return running here\nand then\nyou finally register the agent\nhere is my uh is my example here\nI have a sensor if I find the file I\nwill run the T1 text here\num\nno it doesn't find a file so it keeps\nrunning the sensor\nand then if I add the file\nyeah and then it will stop running to T1\nso\nand I'm going to talk about how to how\ndo we deploy the Asian so to deploy\nAsian you have to build a image for your\nagent deployment your\nuh here's a Docker file uh the only\nthing you need to install is the uh your\nPlugin or your sensor this plugin have a\nflag key sensor and then\nand then you\nuh the entry point of the docker file is\npipeline server is essentially will run\na grp server in the container so when\nyou run the This Server\nwill automatically load the sensor or\nthe agent in the memory and then\nregister to the research registry\nand then you\nupdate the image of your deployment here\nand then deploy it to your community\ncluster here\nhere you can see I have two Asian\ndeployment one for development purpose\nand one is for production environment\nand then\nyeah so by by default other\ntask requires will be sent to this\nendpoint like for the send to the\nproduction environment but you can also\noverride uh the endpoint for example if\nI edit I just add a new file sensor I\ndon't want to run in this deployment I I\ncan override the endpoint and if I run\nthe file sensor text I run in this\ndeployment yeah foreign\nit was around the downstream text\nyeah so to summary like uh the Asian\nlawyer to run like the bigquery tags\nlocally you can so you can very easily\ndo taxes there's any big and plugging in\nlocal environment and then you don't\nneed to review or restart the propeller\nand then the most important thing is\nthat every agent deployment is Stella so\nyou can easily scale scale Down based on\nyour workloads and then you can also\nhave a scenario deployment\none for production of one for like\ntesting\ntesting yeah so the future word is that\nwe would like to add more agent we want\nto create a Asian universe and then we\nwant to make it a more agent and sensors\nso people can use this this feature\nyeah that's a\nthank you any questions\nI have a comment before but uh this is\njust amazing work I don't think you\nemphasized enough\nthe code\nyou can now run all services anything\nthat flight can connect to everything\nlocally\nas if the local development is now\nsupercharged right like so all of the\nkubernetes pods\nuh kubernetes Sage uh spark jobs\ndistributor training elastic training\nall of that used to run locally\nbut you couldn't connect to snowflake\nyou couldn't connect to sagemaker you\ncouldn't connect to all of those things\nfrom local prior to this work with this\nyou can now connect to anything so now\nopen up a Jupiter notebook start writing\nyour code everything will run save it\nhit my flight run remote and boom\neverything will run remote as it was\nrunning locally without any code changes\nand I think that's just a massive boost\nin productivity\num so yeah and it's super easy to add\ncontributions so I'm hopeful that\neverybody will add more and more agents\nit's going to be hard to keep up but\nyeah the the two that I really you know\nwould like to see is uh Atlanta labs\nintegration so they have a programmatic\nAPI now where you can request an\ninstance\nand I guess the trick will be to like\nfigure out how to run code in there from\na flight task\num\nand they're all there are all sorts of\nsimilar platforms like Fast AI like\nbasically for cheap cheaper gpus\n[Music]\num\nI guess the second comment here is the\nsensor pattern basically opens up\nlike a new\nway of orchestrating that has an\nexisting hasn't existed in Flight before\nand yeah\nI don't think we really realize this\nwhere you could have like an infinitely\nif we had infinitely running tasks\nwe can basically treat them as like\nthese orchestrators of various other\ntasks depending on like external\nassets being available or not\num so yeah it's going to be\nvery interesting to see you know\nwhat kinds of use cases this this opens\nup\nKevin this is amazing it would be\nawesome to see a kind of a roadmap I\njust heard Neil's suggestions and I feel\nlike it would be great to put this at\nleast in a timeline about you know\nwhat's on the horizon what what you have\nseen and what you're considering and\nwhat comes next it would be great to see\nthat somewhere\nso people can easily to know what to\ncontribute to the agent Series yeah\nyeah I think Stripes Spotify and some\nothers have are already contributing and\nwe would love to make sure that you know\nthis people are not duplicating their\nwork like it just should be it would be\ngreat to potentially have a board of\njust agents as a separate board uh yeah\nyeah\num one thing I also wanted to emphasize\nwas that uh you we all should understand\nthe flight always supported kind of like\nagents underneath\nuh but and the reason why they exist is\nlong-running tasks is essential for\ncalling services or doing sensor\nactivity you do not want to use\ncontainers you just don't want to start\nup a container and waste it doing\nnothing this is multiplexing lots and\nlots of requests onto the same container\nso the cost of operating an agent is\nactually dramatically low it's very very\nlow and because it horizontally Auto\nscales to multiple nodes\nit starts off at almost nothing it's\nlike a few pennies a day to can scale up\nto whatever you want so uh it's\nessential to understand the cost\ndramatic way changes with the agent work\nso if you are using writing uh plugins\nthat use containers to call services\nprefer agents long running better State\nflow and cheap"
    },
    {
        "title": "Flyte Contributors Meetup - July 6, 2023",
        "transcript": "ride welcome everyone to the flight\ncontributors Meetup\ntoday is July 6th I'm Davis pay home\nhappy to be your host today\na couple of reminders this meeting has\nbeen recorded and\num the recording will be posted to the\nflight YouTube channel and I'm also\nusing fathom AI service to capture notes\num the area the meeting notes are\ncapturing this document let me re-share\nthis with you\nand also\nah yeah or also we're receiving copies\nin the community repo\num you can add yourself\nto the attendance list I heard your name\nand your affiliation\nand also your questions or discussion\ntopics\nall right\num new members joining today I guess\nyeah we have a couple of\nuh new folks joining uh let's start with\nDennis who you like to briefly into\nintroduce yourself\nyeah hi my name is trinisa I'm an ml Ops\nengineer at recognized so the same as\nFabio is and\num like only made a small contribution\nto flight so far\num but like joining today because we\nhave an open discussion around the\npreemption handling or interruptible\nhandling in flight\nokay yeah I love using flight it's a\ngreat tool\nreally thanks a lot for all the work you\nput in there\nall right now thanks to you for your\ncontributions that's great\nokay let me see I have someone trying to\njoin\nobviously for the first time\nah right I guess that did so far in\nterms of new members\nokay uh let's go directly to the RSC\nboard I'll share my screen\nhere\num all right\num so I guess no new rfcs this week\nuh we have some proposals in review\nso wondering if for any of this\ndo you have anything to discuss for the\neager mode\nflight console UI or overrideable\ndefaults proposals is there anything\nyou'd like to discuss here\nI don't see anything in the agenda\nbut from myself at least the last one\ncan be moved to the next to the next\ncolumn\nthis one\nhonestly I feel like all of them come\nbecause some of them already have\napprovals yeah for those of you who are\nnot aware the final coming period here\nis where we move proposals that already\nhave undergo sufficient\num discussion from the community and uh\nthe community feels in a position to\nmake a decision to either accept or\nreject\nand yeah for most of them\nalready conversation happened so any\nobjection to move this to the final\ncoming period all three proposals\nno\nagain even during the final coming\nperiod where comments are welcome right\nthat's the idea of the FCP\nokay and also uh just one neat comment\nhere the external plugin service\nproposal that is not yet merged it\ndoesn't have your the approvals but I\nthink it's already in the implementation\nface is that correct\nyes\nokay\nso yeah just to give enforcing the RSC\nprocess so we appreciate reviews to give\nyour approval so we can merge this and\nuh and reflect the current status\nokay\ngreat next up working group updates is\nthere any update uh this week for the\nspecially for the uh configurable rights\nworking group\nwe're trying to understand where an\nadmin that needs to be done I looked a\nbit further but I mean nothing worth\nreporting here okay\nyeah it's similar from from mind are we\nalso put timer doing a quarter wrap up\nat the moment so there's a lot of work\ngoing on and should be a bit easier the\nnext next couple of weeks\num I've looked into the code as well\nthat um both then and\nI think there were two comments that\nwere posted\num but I have not played around with it\nyet\nall right great thank you um thanks so\nmuch sorry Grand tab\nokay\num and yeah welcome Maria finally you\nwere able to join great to have you here\nhi yes sorry I thought it's also\nall right great\num yeah we'll we'll discuss your\nquestion in a little bit\nokay\num next up\nnew ideas in the incubator we have a\ncouple of them\num so I guess if then I think I saw Dan\nif you wanted to briefly introduce this\nproposal this idea\nsure yeah we can be very succinct about\nthis\num\nyeah the feedback mechanism from flight\nkit to propeller is kind of blocking a\nnumber of different efforts right now\num so this is just a kind of back of the\nenvelope idea of leveraging pod logs um\nto do it\num comments are very warranted validated\nthe uh you know trade-off space between\num you know containers having lots of\nlogs things like that\num kind of just using this as a as a you\nknow segue to to fuel this conversation\nI think we need to come up with a decent\nsolution for this it'd be great to kind\nof have a discussion\nright thank you\nany comment\nnope\nI commented about performance\num\nI'm not sure if I can fully judge it but\num the number of logs that can be\nproduced the mail training is the same\nright\nand the millions of lines of logs that\npeople put it into debug mode to figure\nout what's happening\nand\nI wonder what happens if somebody does\nthat so we recently had a spike and\nlogging because somebody just said\ndebugging for nccl and that produced\nhundreds of millions of log lines in one\nday\nand I fear that that would bring down\npropeller for everyone\nyeah yeah I think there's a there's a\nnumber of mitigations we can have there\none like propeller horizontally shards\nbeautifully we haven't had a use case\nfor it yet this would be a decent use\ncase\num I think in any like sane production\nenvironment\nthis tens of millions of lines of logs\nright this isn't going to be a a regular\nthing we see in production at least\nhopefully it isn't very well could be\num you know I had a thought process of\nlike you know do we have some kind of\nwarning message that we have a maximum\nnumber of log lines where propeller uses\nthis for that if it gets over 10 000 you\nknow some configurable limit we can say\nlike hey\num in the reasons on the tasks say we\nquit looking at them I think there's a\nnumber of mitigations I think\nyou know Pub sub like you suggested\nFabio is I think that that's a perfect\nyou know perfect end goal um I think\neverybody agrees that that would be a\nfantastic feature but uh the\nintroduction and complexity on the\ninfraside I think has has been um well\nthere's one real reason why why we\nhaven't gotten that road so far so I\njust want to make sure we exhaust\noptions\num before diving into something that we\nmight not want to do I agree that the\ninterested others complicated enough I\nwould argue though that many people use\nflight not in the production setting but\nfor experimentation\nand at least as as far as I from from\nwhat I have seen producing a large\nnumber of log binds is not very uncommon\nsure but yeah I agree that maybe we\ncould limit a number of lines for which\nwhen we stop considering this mechanism\nright I imagine like you know four\nreasons for One DB links we're using\nthis for flight decks and for things\nlike that like the first 2000 lines in\nthe last 2 000 lines would be where\nthese things would happen you're not\nyou're not frequently going to have\nstuff happening so it gets hackier and\nhacker as we start to introduce things\nlike this but\num you know for the for the 99 or you\nknow the 95 case where we're going to\nemit 100 lines of logs for\num you know a task\nthis seemed like a decent decent\nproposal um but like I said I I do\ncompletely understand the use case where\nwhere this could get out of control\nquickly and maybe it doesn't fit those\nas well\nis there a reason why it's not possible\nto implement a direct communication\nChannel between flight kit and the\npropeller\nthere's always been uh uh hindrance to\ndo that just because like\nmultiple cluster deployments get dicey\num you know communication between all\nthat gets a little bit dicey so going\nfrom flight kit directly to propellers\nis a difficult thing to do\num\njust because of deployment models\nbasically\nwouldn't it be enough if every part like\nwhen a part every part that runs in the\ncluster will have also propeller in its\ncluster right\ndoesn't have to be run inside of a\ncluster right like it just has to have a\nkubernetes\nclient connected to a cluster\nokay I think I need to look at how\nexactly it was like options work with\nmulti-clustrooms understanding what's\nthat you always need to you don't need\nan admin but you need a propeller that\nwas what I understood\nwhat I'm probably wrong about this\nokay the issue would probably like\nequally explode but maybe like we need\nsomething like a communication proxy\nthen in each cluster these selected\nthere's at least the minimal flight\ncomponent that has to be there\noh\nokay\nyeah I think this is the same same you\nknow vein as introducing you know some\nkind of Pub sub system right where it's\njust it just you know increases the\ncomplexity of deployment but uh I agree\na potential solution I mean I I think\nI'm I'm open to all conversation on this\nI think like I said just just getting\nthis right uh the first time it's easier\nthan it feels a lot of these things like\nconfigurable log links or we'd love to\ndo that at runtime and runtime metrics\nthings like that\nall right\nwell thank you all yeah current\nencouraging everyone to add your\ncomments here\nand\num yeah I think we have a whole new wave\nof new rfcs coming\nfrom from the incubator also an\nadditional\nentry it's not new but last time we\nagreed that basically we would talk\nabout it this time\num\nbut the question is whether you feel\nprepared to talk about it because I\nthink it's about nitty-gritty details\num I saw that you read it done\nis there anything any questions that you\nhave about it or\nI love it let's make retries make sense\num no I I support all of it I think you\nknow putting this behind a flag\num\nI think it's the right way to go about\nit so you know like I said there'll be a\nlot of technical aspects on\nimplementation level I think it would be\ngood to have a couple more eyes on this\njust just to get it but\num I I agree I fully support uh fully\nsupport this work and like I said\nthrowing up I mean flag to make it\nbackwards compatible or yeah yeah I\nthink changing the default behavior is\nmy only concern because some used to be\nrelying on what you know what the\ncurrent behavior is in this so if it\nchanges transparently there's going to\nbe issues there\num\ndo you have an opinion on whether you\nwould prefer to prefer to just have a\nsingle retry Behavior or\ngiving the information of the underlying\nthoughts to the plugins that until now\nonly deal with the custom resource\ndefinitions these are the two opposing\nideas that we propose in this um in this\nI've seen\num\nI don't feel strongly\num\nI think I think because the biggest\ndifficulty here is with like the pie\ntorch uh you know there's this level of\nindirection between like the custom\nresource for the for the cube flaw\noperators right is that that's I\nunderstand that correctly I'm trying to\nrecall what I when I read it last\num and just that like that interruptible\nbehavior is not necessarily propagated\nthrough the custom resource and and\nbrought through that\num\nwhat do you think\num in our opinion like something or\nyeah it's just something I wanted to add\nthere is that um what we saw a lot that\nat least me myself\num\nat least like a little bit puts me\ntowards like the first up or was it the\nfirst one like the one where we would\num unify the system retry budget in the\nuser retry budget\num is that we saw a lot of our users\ndon't really understand the flight retry\nmechanism completely they see the user\nretry budget as like something you can\nconfigure in your own code and kind of\nexpect that this covers all retries but\nonly when we talk about like what the\nsystem actually does in the background\nthey then learn about the system retry\nbudget and that\nmaybe due to like a bug or like at least\nnon-expected Behavior it works\ndifferently whether you have like a\ndistributed task or whether it's just a\nsingle python function task and it's\njust very hard to understand which I\nthink we also saw like here in the\ndiscussions\num how this really works in detail and I\nthink it would greatly simplify it\nespecially I would be also curious you\nprobably know that better like where\npeople actually use the user retry\nbudget uh specifically like throwing\nthis user retrievable error and want\nflight to retry that because like from\nour thoughts this could mostly also\nhappen all in the user code the three\ntry Behavior\nyeah and I know there's another a couple\nother people I don't want to play e on\nthe spot here but you might have a\nlittle bit more context than I do on\nthis regard you know retries was a thing\nthat started out initially\nbasically just just for like\ninterruptible pods right you know the\nwhole notion of retries in flight was\nlike hey we have this task that runs it\ngets destroyed in the background we'll\nautomatically retry that and then like\nthe integration of recoverable failures\nand then there's this you know partition\nas well that not just the recoverable\nand non-recoverable but also the system\nand the user and that you know introduce\nall this complexity\num like I said I I like the idea of you\nknow just making the partition a\nrecoverable failure and a\nnon-recoverable failure and then that\ncan be decided by like flight kid about\nyou know whether this should be retried\nor not retried and just do away with the\nthe system retry mechanism\num I just have you know one budget\nhere's you get five retries or whatever\nspecified and then uh you know if a\nfailure is recoverable we allocated\ntowards that\num as far as how that applies to like\nthe cube flow\nPi torch stuff is a little bit more\ncomplex\num\nactually if we go that route it doesn't\nmatter anymore that the kiplo operator\ndoesn't propagate this information right\nit's a failure it comes against either\nretro budget\nsure and then it would just be if it's\nrecoverable or non-recoverable um you\nknow would you decide that all of them\nwould be recoverable or how would you\nhandle that case\nyeah we would have to see all as\nrecoverable yes\nwhich is probably also not completely\nright here\num what I was going to I've been doing\nyou know talking about a lot of users\nand one of the things that came up\naround retries was that for most people\nthat's not a very human friendly thing\nlike much more is like I need this task\nto start by this date this time like so\nI was just wondering about\nyou know I want to derail that but\nthat's kind of the feedback I got was\nlike retries doesn't mean anything to me\nas a person like I just I'm more in\nterms of time like when does this have\nto get done and so I'm wondering about\nuh you know and continue something\ndifferent thinking about that a little\nbit as a as a like be able to exposed to\nusers that kind of a thing\nI mean I I think that's valid but I\nthink it's a different feature and flag\nright\num\nwell the only reason I'm bringing this\nup is because like I specifically got a\ncouple people saying to me like I I\ndon't understand or like they have to\nset retries like I just like\nand it makes it harder for me to like in\nother words there could be tasks that\nare not that time sensitive like hey if\nthis gets done in the next day I'm fine\nand but there's no way for him a person\nto specify that they'd have to they have\nto think in terms of retries\noh okay I just want to make sure I\nunderstand that Tim and I don't want to\nbe real this conversation we'll come\nright back to it but like that would be\nlike here's this task in the definition\nsay\nit needs to run in one day and there\nwill be unlimited retries just as long\nas it runs in one day is that kind of\nthe idea behind it yeah that not that\nthat I got and like the the linkage was\nfor people to be able to like that's a\nmuch more like\ninterruptible task you know up to some\npoint at which flight goes okay I now\nneed to get this done so I can't put it\non a you know I can't put on a spot\ninstance I have to put it on something\ndedicated\noh interesting I cannot understand what\nyou mean yeah that would be I mean we\ncan certainly write up like add it to\nthe RFC incubator\num there's constructs internally in\nFlight that like we have what's called a\nretry mechanism defined on at the task\nlevel and right now the retry mechanism\nis pretty simple of just like\nhere's the number of retries that it has\nbut we can extend that certainly to be\nlike you know here's a more complex\nretry mechanism that's not just a number\num it's it's more so defined by like\nhere's a here's a time you know\nsomething like that\num you'd have to look at implementation\nwise but like I said I think fiber like\nit's a little bit of a different than\nwhat this problem is exactly right\nin in my opinion the biggest argument\nfor a single retry budget is that\nlet's take for example out of memory\nerrors I would really have to look into\nthe go to go code to figure out against\nwhich budget they are counted them I\ndon't think there's a way to figure this\nout other other than that\nand that's just an example right there\nare other errors where I really couldn't\ntell without going through the code\nwhere what the contact count against\num\num and I think that people when they see\nthe argument in the toaster record that\ngreater I think most people will assume\nthat everything comes against that that\nare surprised when it doesn't\nwell\nyeah maybe maybe I have to do a deeper\ndive there too because I think system\nrecharge like I said they were it's\nsupposed to be like you know\ninterruptible failures on pods and\nthings like that but I think it also\nencompasses\nlike during node executions where like\nyou're doing a cash lookup or something\nlike that that would be a system retry\nor instead of just like hey if if\npropeller fails to do a cash lookup that\ndoesn't then uh that's also counts\nagainst that as well but I would have to\ndouble check that to be honest with you\nand if if that is true then maybe\nI would rephrase what we wrote here and\nsay everything every every reason that\neverything that makes the Pod fail comes\nagainst a single Retreat budget right\nright maybe then what the scope would be\nlike of course there can be things going\nwrong in propeller that are not the user\nretry budget but everything that happens\nto the Pod basically I would\nI could imagine we count against one\nbudget\nand you know how we sometimes see the\nerror message like 51 out of 50 retries\nuh exhausted and then the Pod fails do\nwe have any sense of how many times how\noften\num like things retry like 20 like is\nthere a histogram of that do you have\nany idea like\nsomething retry 20 times or 30 times and\nthen succeeded uh because I don't know\nCube API have a split key or something\nalmost flaky but but like the the is the\nthe dispersion of like system retries is\nlike substantially different than it may\nbe\num like maybe it's just like a different\nclass of errors that is worth keeping a\nseparate counter for\nyeah I don't have any numbers on that\nbut I can't imagine it's a normal\ndistribution right if you have 20\nretries it's going to succeed on the\nfirst one two three or it's going to go\nthrough all 20 and fail you know\num I would imagine\num yeah I think my own my only\nuser facing comment on this is the same\nas last time which is if like I would\nlike to be able to mark a task as\ninterruptible without having to specify\nretrace at all\nAmazon's that works either if you even\nif it's handled by flight kit is fine\nbut like from the user's perspective\num you should be able to Market task as\ninterruptible without a reachment\nyeah I think that's dissimilar to the\nkind of feedback I've been getting\nhow many desired Behavior then though\nthat it's like how many times do you\nwant it to be re-dried then\na reasonable number of times\nyeah I mean so like this is also an\nargument we're bringing up here that\nmaybe this should be like the default it\nshould be a cluster setting\num or at least something the admins can\ndecide\nyes I think that I'm getting is that\nretries or not it's not a very intuitive\nthing and when you think about it it's\nnot an argument against right like\nusually like something I'll tell you\nlike I'm retrying but like I don't ever\nhave to worry about that or it'll tell\nme Network failure timed out but\nbut no one ever asked me how many times\nI want to retry\nI have a question regarding what you\nsaid\num but I only want to ask it if nobody\nwants to uh respond to something that\nattempts up to the attempts at all so\nwe're just saying there's another uh\nthere's another uh use case or there is\nthe maybe a real life use cases when the\nif there isn't a spot as it's available\nlike we've had users tell us like\nI want to introduce like some delay\nafter every retry like some type of a\nback off period between retries but the\nreal root of the issue is like\num they just want to ultimately what Tim\nsaid like they're willing to wait so\nit's totally again probably totally\ndifferent subject my guess is even if we\ndo do some future feature where we say\nyou don't specify retries you say you\nspecify a time or like a a window you'd\nstill that would build on this what this\nis talking about which is like a system\ndefault number of retries that were\nor like I guess a unified budget of\nretries right so I think you ultimately\nprobably need this in the system still\num he what would you say to a scenario\nwhere there is a default for the number\nof retrys configured on the platform\nside\nand we make the in the in the the\nliteral\nof although let's say if we make the\ntask argument\nargument in the task decorator if that\nsorry I need to take a step back so if\nwe could distinguish in the back and did\nthe user didn't set a retry budget in\nthe task decorator so we use the\nplatform one or the user set one of the\ntask direct decoratory let's use that\none it would have to be a one-off\nprobably in the product above then\nbut that would solve that problem right\nso the user sets interruptable equals to\nTrue doesn't specify any retry budget so\nwe go for the platform to find one\nor that they set one explicitly in the\ntask decorator then we'll use that one\nif interruptable is not set to true we\nstill just use the platform rechase no\nI'm saying there there's a single budget\nthat has a default on the platform side\nand it will be used if the user doesn't\nset it in the task decorator and if the\nuser explicitly sets it in the task\ndecorator then we use that number\nsure\num I was actually just looking pulled it\nup right now because I was like it\nshould be how we do it by default but if\nit's not\nI can take a look here\nI\njust want to make sure that existing\ncode doesn't break\nno I mean\num we definitely discussed this in the\nin the um in the print in the RFC here\nthat there needs to be a flag and the\nhum values that says like normal\nbehavior or new Behavior\nthis isn't the breaking change\nand the new behavior wouldn't have to be\nthe like probably wouldn't be the\ndefault right\num I think we need to think about this\nsomewhere like absolutely my my concern\nI guess is the\nthe\ndeprecating system retries I feel like\nthat's kind of embedded everywhere in\nlots of places like like propeller\num and\ngetting like ripping that out completely\nI\ndon't know what the impact of that is so\nI want some way of like assessing\nI think we need some way of\nunderstanding what that impact is and\nhow hard that how hard that will be to\ndo\nI I did find here too like propeller has\na default Max attempts field but it's\nnot exposed through configuration so\nit's just hard coded to one right now it\nwould be a really easy change to\num expose that through configuration and\nthen every task you could save give it a\ndefault three or whatever\nforeign\n[Music]\nabout this\nproposal\nis there anything that we can do to\num to try to figure this information out\num I think for us it would be very hard\nto to\nget an overview of how difficult it will\nbe to to\num to remove that budget completely I\nthink something that we could look into\nis whether what you would have done that\num there's this cache of other statuses\nof the part\nwhether we can just put the other parts\nin there as well which would also allow\nto unify the behavior right maybe maybe\nis it okay for you if I ping you on on\nslack then so that maybe you can give me\nsome guidance of where in the code that\nis and then we do an experiment and see\nwhether that's that would be an optional\nabsolutely yeah let's explore that okay\nawesome that's great\nuh yeah we're running short on time for\nthis section So yeah thank you all for\nyour input\nuh next up in what we call for now\ncommunity pools where we try to discover\npatterns or Trends in the community uh\nrecurring issues feature requests Etc\nthey're worthy to discuss here\nuh today we are bringing here what we\ncall the sensor mechanism\nidea it all started on this draft\nuh and this trade also broke Maria here\nwith us we're welcome we're great to\nhave you\nthe idea of having a flight native\nmechanism for executing a scheduled\nworkflows based on delayed external\nUpstream dependencies\nthere is a tangentially related entry in\nthe incubator\nuh but wondering I don't know if\nMaria do you want it to\nelaborate more in the request\nuh yes I I can do that and maybe I can\nlike\ngive you a couple of use cases that we\nwe have\nso this is not only about sensing\nmechanism I think this is about sensor\nand concurrency\num and they\nthey related but not necessarily uh the\nsame thing\num can I share my screen I can maybe\nshow you\n[Music]\num\nyeah please\n[Music]\n[Music]\noh it asked me to quit\num okay is it okay if I'll be back in\nyeah yeah sorry about that yeah in the\nmeantime I Fabio is a news prepare a\nproposal something like this yeah I\ncan't find it I'm searching I'm\nsearching in there I think better but it\nis fine he created a bunch of entries\num\nexecution graph here there is one for\nyeah this one here\nright if understood correctly what this\nis about\nreactive execution graphs that was like\nTriggers on on certain events\nyeah it seems similar\num\nokay yeah in the meantime I guess we can\nah yeah Marie is back\nthey're muted\nyes uh I'll try to be quick\num okay\nuh can you see my screen yes\num okay so this is one example\num very simple pipeline\num\ngiven some uh date\ninput we post it we read data from S3\nand then we write data to S3\num reading and writing tasks as Fork\ntasks\num so here everything is okay and\neverything works fine but the problem\noccurs when we for instance try to\nbackfill\num so we will have multiple workflows\nrunning with different states\nbut they will all try to write to the\nsame S3 bucket\nand\nsince they're all spark jobs they will\nbe\nruining it for each other they will be\ndeleting each other's temporary files\nand you'll end up with corrupted data\nso this is one way this can happen and\nanother way would be if you just\naccidentally trigger the work for\nseveral times\nso here what we want to do what we would\nwant to do is to\nnot allow the writing tasks have\nparallel runs\num some Global lock\num\nwhile the first writing task is running\nthe other writing tasks are cute once\nit's released but feature exists right\nisn't it called scar serialized\num does it work\nwhen tasks\nhave different\num so this will be will have different\ninputs okay\nuh so yeah I guess yeah what if it's\ntriggered accidentally the same\nthen then cache serialized will work but\nif it's that feeling\nthen I guess\ndon't worry then it won't work\num that's one and I can quickly show\nanother one\num\nhere\num the problem is when we have a\nfrequent schedule so this is the\nacademic example of a pipeline\num\nmultiple\num external dependencies and they these\nare also a three files or multiple files\nthey arrive on different times anything\nbetween midnight or 8 A.M latencies are\nnot uncommon so we have frequent\nschedule right now we have a chrome job\nthat turns every 20 minutes\num but we also have long running tasks\num so\nlet's say the first file arrived\non the next schedule kick in the\nworkflow started it is executing a task\none and\nnothing else is running because there is\nno\num external dependency for task 2 is not\nfulfilled yet\num\nbut\nsince we have this schedule kicking in\nwe don't want\nuh\nconcurrent workflow runs\nso what we want is just to wait till the\nnext data set arrives and start the\nsecond task\nand as far as we understood right now\nthis is not possible because if we have\na launch plan with a chrome schedule\num\nto uh\naround the workflow every I know several\ntimes an hour and we have this case that\nwe will be just creating a new workflow\nevery time\nyou understand correctly that you don't\nwant to create a new execution if there\nis one that is still running yes exactly\n[Music]\num\nyeah I can stop sharing though that that\nwas it\nso that was on concurrency side and\nsensor side\num\nas I showed we have many of our\npipelines have\nuh external upstream dependencies and\num\nright now\num the majority of our pipelines uh\nuse Luigi framework and there is this\nfunctionality to\nuse external tasks\nyou can invoke the external dependencies\nexternal tasks and then there is a\nmechanism to check if the success files\nexist meaning the external task is\ncompleted and then you can depending on\nif it's completed or not you can start\nexecuting your pipeline\num\nand this is something we also couldn't\nfigure out yet how to do in flight\nwe\nsort of walk around we\nare\nwe did this just to add a task that\nwe'll be checking if\num the file exists or not\num and depending on that we have a\nconditional to either run the workflow\nor\num have like a dummy task to load the\nmessage and exit successfully\nbut\nI'm in the case where we have many\nexternal dependencies\num\nthat can become more complex\nso I don't feel like our solution will\nscale for the cases where we have many\nmany experiments\nthat was on the Sense part\nis there an RFC or incubator discussion\nbusiness\nI haven't seen anything on the sensor\npart but on the concurrency I think the\nreads\noh\nwell there was an issue for parallel\nruns or qurans and I think there is also\na large\nI just I just linked one of the um\nthe lacking mechanism to prevent to\nprevent two or more workflow tasks\nrunning in parallel that was submitted\nto RFC incubator last month\num\nthis is a feature that we've wanted to\nyou know cash serialized stuff we wanted\nto generalize that for quite some time\nand I think you know\nupload it hopefully we can get some\nmovement down this because I think it's\nsomething a lot of people are using the\ngeneral discussion here is is\nyou know serializing scheduled\nexecutions you know I think the Second\nUse case that you had shown is a much\nsimpler problem than generalized\nserialization between like task\nexecutions\num and so you know just just the amount\nof effort that it would take I think we\ndon't have a good scope on it right now\nso so one problem's easier but certainly\nyou know serializing everything\num supporting it across task executions\nand things like that is a lot more\npowerful\num\nhonestly I I can't uh comment\nintelligently on it\nyeah I think for for the first use case\nthat I showed\num yeah we uh I don't think we thought\nabout them\nuh\nwell actually for the first use case\nwhere we need to backfill I think we are\nthe workaround right now is just to have\na python script that will manage that\n[Music]\num\nand that is I guess for for now okay\num\nbut and for the Second Use case yeah we\nhaven't thought about cash a certain\ncertifications\num yeah\nyeah thank you Maria for preparing the\ndiagrams and everything right any other\ncomment I think a written version and\nour singer better would help it's\ndifficult\num\ndifficult to come up with a solution for\nme\num for both uh Spencer and\nI mean for the lock one we already have\none right\nmaybe comment what what you would miss\nand if that one was implemented what\nwould what you will be missing that\nwould be hard for\num\nyeah\nall right\nthank you\ngreat uh\nlet's move to a couple of items\nin the open mic questions section\nuh there's a nomination for a new\ncontributor\num share my screen real quick\nand um\nUh Kevin I think is the first person if\nyou use the the um\ntemplate in the community repo to\nnominate a new contributor\noutlining the contributions and repos\nEtc\nit already has a couple of boats but it\nrequires\na couple of additional votes to be\napproved\nso please\nthank you Fabio\nall right uh next one is related\nbasic media PR with some updates the\ncontributor letter of the governance\nmodel\nand I will read them and thank you Fabio\nfor the comments yeah\nobservations\nuh that's great the the overall idea is\nto introduce a new role that it it sits\nin between of the general Community\nmember and the contributor with write\npermissions it was a a higher barrier\nfor Access so the collaborator role\nfits rights in the middle\nand I I I really need I really think\nthat that we need to\nnot only nurture a community of\ncollaborators and contributors but but\nalso\ngive them resources to\nto help with with all the tasks that are\ndefined there also in the previous\nversion of the governance model some of\nthe responsibilities were not really\nwell defined so we're trying to address\nthis here and also touch on the current\nstatus of the maintainer rule right so\nplease give it a read and add your\ncomments\nand uh yeah finally type support for it\nfor python based models\nI guess yes\nI guess so I I asked take them also like\nthe only concern that\num that we both had is around the the\nscalar literal map\num I don't think it's a clear-cut\na clear-cut shoe-in in the sense that\nthere it will\ntake some validation probably to get\nthat in if that makes sense I'm sorry I\ndidn't even read your comments\num or your response\num but\nperhaps I should have\nI just if you're referring to mine that\nwas very briefly before the for the\nmeeting\nthis year ther of what I've wrote is\nif we don't have a at least I don't have\na strong opinion how the literal should\nlook like as long as they fit the\ninformation in there so if we're I'm\nvery open to another\nshape of the little to put it this way\nis it possible to put everything into\none stripes\nyou need to ask a dumb question would\nthat corresponds to basically putting\neverything in a single dictionary in\nPython and then\nputting it into one struct is that\ncorrect yes\nif the if the single dictionary can have\ntwo keys and basically we just\ncurrently we have two structs we could\nput it in one if the first dictionary in\nthis track would have just two keys and\nthen two other dictionaries\nwhy is it necessary for like what what\npurpose does the separation serve I\nthink I guess I never really got that\nyeah so\num maybe Arthur do you want to explain\nthat one or\nsure so is that the separation between\nthe flight objects and the base model\nJson itself yeah yeah so so pedantic uh\nserializes to Json uh it doesn't really\nserialize to other formats and then\nwhy it has support for some things that\nidentic can't serialize for instance\ntensors or pandas data frames and so on\nso the problem started with uh the\nflight files and flight directories\nwhich uh pretty naturally would fit in\nas just strings uh into the pedantic\nserialization but then because they\nabstract the access to that directory or\nfile we had to add in uh the serializer\nand the serializer from flight to be\nable to to grab those things correctly\nand then then the idea came what about\njust adding everything from flight so\nsupporting every flight file\nuh and it was sort of like flight object\nincluding the pandas data frame Etc but\nthen then we have to serialize\nseparately the base model uh and and the\nflight objects because the flight\nobjects get serialized to protobot and\nthe pilantic base model gets serialized\nto Json\nso then the solution was to to separate\nout the flight objects just just pick\nthem out of the uh the base model\nso you realize them separately and then\nduring deserialization\nwe deserialized them with the flight\nTransformers and and put them back into\nthe base model wherever they belong\nso that that's why they're separated\nbecause the the base model everything\nsupported by python gets serialized to\nJson and then there's just placeholders\nfor for those objects that get\nserialized with the flight Transformer\nokay I think I need to look at this\nagain but um even if it serializes to\nprotobuf a couple of things you can\nstill like somehow binary represent\nmaybe it's not space sufficient but uh\nyou can binary represent the part of\ninside a pedantic Json right and\num I think most of the objects if not\nall the offloaded objects currently\ntoday\nlike the files directories pandas data\nframes they also serialize to Json\num they're like it will first see your\nlast two protobots but there is a Json\nrepresentation of the of the object\nright there that's that's what that\nwould be a possibility to to just check\nin the the Json representation of the\nthose objects into the base model itself\num\nyeah I I don't think that could make\nmuch uh difference in the current like\nthat that could could be configured\nto do that rather than than have them\noutside\nI personally thought it would be more\nelegant to have a placeholder rather\nthan a a really really large string\ninside the base model representation\nbecause then schema is more evident from\nlooking at it\non the other hand yeah\nwe have the objects in two places now\nthat we're taking the flight types out\num yeah okay I think it's fine as long\nas it's if it's like one object that\ncontains these two objects together as\nlong as it is a\nscalar struct I think I I would be a lot\nmore comfortable\num the the split part is\num you're kind of like it also serves as\nlike a\nuh\ndecision that decision but like it it\ntells the Transformer like when to like\neverything that's stored in that\nseparate object is and should be invoked\num like it should invoke the flight kit\ntype engine right so it's kind of using\nthat also as like a\num as a determining mechanism\nright that that is right yeah\num I I think yeah it's\nit saves the assumption that things can\nbe represented in the base model now of\ncourse we could always like put as worst\ncase binary uh into the serialized based\nmodel but in some sense there's there's\nthe separation in serialization and the\nserialization logic that's uh pedantic\nsupports everything that can be written\nto Json and then in case flights support\nsomething that can only be written to\nprotobe of like binary yeah\nthen\nthat that would be in that separate\nstore rather than than b and I've put\ninto as the base model as a string\nyeah got it okay cool let me re uh read\nthe response to this\num and\nI think I had one more concern but\nyeah let me read the response to this\nand I will we can uh continue to say\nthink\npretty great\num\nyeah a question from Bernard is this PR\naffected by identity version two\nuh as it currently stands it it should\nnot uh change the logic uh there's there\nis support uh in in that there's a\ndifferent keyword for getting the\nvalidators from the base model in V2 and\nwe check for that in the top of which\npedantic version is\num that said though I think that that\npart could could uh deserve some\nhardening to me it's still a bit up in\nthe air what pedantic B2 exactly is\ngoing to look like\nis it in Alpha release no it came out I\nthink uh this week or so we saw we had\nsome missing version paints and it like\nbroke a couple of things so if you want\nto try I think it's okay yeah that's\ngreat okay that's that's a relief that's\nperfect timing so then uh let me make\nsure that it runs as expected then when\nI actually add it in\nall right it's great\num\nhey it seems like that's it in terms of\nthe agenda anything else you'd like to\ncover here in five minutes or less\nnope\nall right so with that thank you all for\nyour time it was great to see you\nhope to see you in the next one\nthanks bye thank you bye thanks a lot\nbye\nforeign"
    },
    {
        "title": "Flyte Community Meeting - July 11, 2023",
        "transcript": "we want to flights community meeting\ntoday July 11th I'm David Espejo and I\nwill be your host\ntoday so a couple of reminders first the\nmeetings being recorded\nand it will be posted to flights YouTube\nchannel\nand also these meeting falls under Linux\nfoundation's code of conduct\num so in summary be nice to it to each\nother\nuh yeah a nice agenda today again\num I'm posting here the link to the\nagenda notes feel free to tell yourself\nto attend this list\num we'll be great to see you there so\nanyone joining for the first time\nbesides Jay I will introduce you in a\nwhile\nuh no seems like\nit's not the case today oh okay so let's\nmove to the next item I will share my\nscreen here\nuh let me see not that one all right\nyeah\nso news from the ecosystem real quick uh\nsome news that that\ncall my attention for different reasons\nthe pie torch approach to ml\ninfrastructure\nthe team had grown house proposing a\nreally interesting way to approach some\nof the challenges in mlops\nand there's also an interesting\ndiscussion Hacker News around this\nso definitely recommend it you can also\ncome in here if you want\nright\num next one inflection AI\nannouncing I think it's not the first\ntime that I see this but someone\nannouncing the intention to build the\nlargest or one of the largest Computing\nclusters in the world\nuh with a bunch of gpus\nand uh some performance benchmarks when\nthat really interesting\num and and also needed a further\nclarification on Hacker News on the\nperformance benchmarks but yeah\ninteresting to see how much\nmoney and talent and time investment\ngoing through uh trying to make LMS more\nperformant inflection AI is the company\nbehind Pi dot AI\nso um yeah interesting\nuh the fast API dot 100 release\naccording to the author Sebastian\nRamirez who's by the way a fellow\nColombian\nuh it introduces supports for pedantic\nversion two it if you're using it if\nyou're using fast API and play that big\nversion two probably you won't get all\nthe massive performance improvements\nfrom from pied antique version two\nthat's according to the adapter not yet\nbut you will still get some really nice\nperformance boost\nwith the new version\nall right and uh finally open AI\nmove into GA with gpt4 uh well GA for\nthe paid users\nwho will be able to use the gpt4 API\nokay any other news any coming around\nthis\nnope\nall right next up in the community your\nproject updates will the flight call\nback for launching was merged last week\nuh so we're really happy to see this\nhere uh there was a blog post a couple\nweeks ago introducing the whole idea of\nand the kind of the benefits that the\nCallback has for for both flight and hug\nand face users\nand uh here it's now merge Upstream so\nyeah definitely exciting\nand finally if you or probably someone\nyou know happen to be attending the\nSci-Fi conference in Austin there will\nbe a Hands-On worship happening in two\nhours\nleft by Niels ventilen who is part of\nflight's steering committee and also\nthere will be open source prints during\nthe weekend it's an opportunity to find\nways to contribute to open source\nprojects and one of them is flight\nall right so with that I will be the\nupdates and unless you have any other\nupdate to share here we can move to the\nnext section which is roadmap updates\nawesome\num my name is Eduardo I'm the age\nmanager for open source at the union and\nI'll be giving um some updates to the\nroadmap of flight\nso um\nflight 1.8 is being released today\nit's mostly a stabilization release like\nwe had a lot of investments in\num two features that we recently\nrecently announced namely image spec and\nagents\nyou notice that uh image spec had a lot\nof rough edges and we are constantly\nimproving that the idea is to make real\nimage spec the the\nthe defective way that you think about\nlike building your dags so\nkeep an eye out lots of exciting\nfeatures there\num\nalso you notice that there is there\nthere was like a gazillion bug fixes in\nthis release\num but I'll call out uh a few we have\nbetter support for optionals which is\nyou know um this idea that in languages\nthey they have different names in Python\nthey're called optionals some people\ncall them know them by\num the maybe monad so it's great all you\nyou see improvements across\num a lot of touch points in in-flight\nincluding you know flight kit all the\nway to flight console in the back end so\npretty awesome there\num\nwe've also heard about you know the fact\nthat using secrets in gcp was was hard\nso we also fixed that now you can have\nlike secrets that can go like cross\nproject in um in gcp and one tiny nugget\nthat uh it's\nthe community always asked about and we\nhaven't really found a great way of\ndoing it and we we invested in this past\nrelease which is my pie compliance so\nNotting this this d818 release like\nwe're just like\num enabling it for the flight gate code\nbase but going forward and here I'm\nalready speaking about the the near\nfuture of flight and flight kit\num\nwe plan to open this up so that people\nwho install flykit and rely on like my\npie and pyrite and these other static\nanalyzers will be able to\num also you know\nuse them use it in their um flykid code\nwhich is you know feature that a lot of\npeople wanted for the longest time and\nit's finally happening\num one call out is that we have been\nmaintaining\nuh a really a branch for people who\ncouldn't update the protobuf 3.\num it was the 1.2 release\nso we are up to the\n1.213 release and I'm probably about\nthree just just hit end of life last\nmonth and with that it also concludes\nour support So alongside\num flight 1A there will be a final\nrelease that will back part you know the\nchanges to to the one two Branch so\nwe're gonna have\none 1.2 release\nI think it's 1.214 and um yeah sorry\nhave to update the pro buff for now I\nguess\nbut I'm looking forward you know\num looking towards like you know what\nFlight 19 and Beyond will look like\num there are a bunch of\nfeatures that are coming down the\npipeline\num a few that I'm I'm super excited\nabout are eager mode\num\nit's this idea that you you can have\nlike available Dags\nthat can be built like programmatically\num we also heard the community that the\nflight kit as uh is a massive package so\nwe are\nwe're breaking it up in smaller you know\num packages it will still be fight kid\nwould just use\num that idea of uh optional packages\nthat you know the the python ecosystem\num kind of like\nadopted so\nexpect that you know flight kit will\nflight get the number of five flight kit\ndependencies that come from well by\ndefault will drop dramatically\num\nanother feature that is also coming down\nthe pipelines like it's less of a\nfeature but more like a hardening of\nAgents which is one of one of the bats\nof the um the flight team this idea that\nyou'll be able to have\num back-end plugins written in Python\nwith a much much better uh Dev\nexperience so we are finding the rough\nedges there and helping you know the\ncommunity like come up with a with a\ngreat way of not only writing agents but\ndeploying them you know and integrating\nthem into their um flight workflows in\ngeneral\num we also have some support for well we\nhave more support for for image spec so\nthere are some features that we are also\ncoming down the pipeline there\nand finally one final feature that I I\nwant to call out is\num\nalso came from you know the feedback\nfrom the community\num we we heard that\ndealing with data classes send data\nclass like structures like you know\npithetic models adders Etc is is hard\nso there's a lot of movement\ncontributions coming from the community\nso Greg Eli you know he like thank you\nso much so we're also improving that\nit will be much simpler and more\npowerful to use data classes in by\nDynamic models in the future\num\nfinal call out not a specific to any\nfeature or anything but I I just want to\nthank the community\num a lot of really beefy features are\ncoming and are being worked on with this\nyou know\nglobally distributed team that is\nkicking ass so thank you so much who\nwould not be here if it weren't for you\nand with that I think we're we're done\nwith the release updates David\nright that's great yeah a lot to cover\num thank you any question comment around\n1.8 release\nnope\nright thank you\nso moving on to the next item\num this is a section that is intended to\nbe short the lighting talk is\num designed to for everyone here to\nshare anything you have learned around\nflight a specific feature how to use it\nEtc\nin this case it's me\nthat wasn't Michael Scott knowing it's\nme I've been spending some time with the\nold implementation in Flight in the past\ncouple of weeks and\num yeah the main artifact of this is the\nthis PR to update the configuration docs\nand also the the authentication the hard\nway guy that is also live but I thought\nit was interesting to kind of briefly\nintroduce the topic here uh for anyone\nout there especially for those of us\nwatch the recording so I will go real\nquick here\num so I will cover basically why we need\nhealth in general and how it's done in\nflight so\num in the top graph you see kind of the\num the traditional client server\narchitecture where the user will need to\nprovide credentials let's say username\nand password if the if you wanted to use\na client to access a resource protective\nand handled by by a resource server here\nI'm using uh on purpose jargon from from\nthe oauth protocol\nuh this is completely worth coming from\nthe oauth RFC so the resource ordinary\nis you the user decline is whatever\nprogram that you're using to trying to\naccess this protected resource\nuh that could be handled by let's say an\nAPI server or in this case a resource\nserver uh the problem with this form of\nof access is that uh well if the client\nis compromised a malicious actor who\npretty much access your credentials and\nalso it's it's there is not a\nstandardized way to revoke access for a\nclient for a specific client once it has\naccess to a protects it protected\nresource there's no really standard way\nto rebook access among other\ncomplications so that was the original\nproblem that the oath protocol aim to\nresolve having kind of a mechanism\nfor the resource owner to\nlog in or provide the credentials to a\ndifferent component not decline itself\nbut a different component that is\ntrusted both by the user and by the\nresource server\nand the this is the out server or the\nauthentication server and the auth\nserver will basically if you as a user\nare successfully logged in and it means\nthat you are authorizing the client to\nperform an operation the old server will\nemit a token or a permission\nthat the client will use the token\ndoesn't have it doesn't carry anymore\nthe original credentials from the user\nso the client will use the token to\nrequest access and the resource server\nwill approve or not and finally it will\ngive the client access to the protected\nresource\nso it's really helpful\num some folks say that that I that the\nRFC is hard to read I I don't think so\nit's it's really interesting to read\nit's a recommended read but basically\nthe end the steps are here so there are\ntwo main steps first you\nauthenticate or the user performs\nauthentication which is basically\nvalidating the the user is who they say\nthey are\nand next the authorization which is\nbasically validating the the operations\nthat this user is allowed to you on on\nthe protected resource if any\nthat's great it has worked well for some\ntime but there are some places where all\nthese model could break especially for\nexample this is kind of the flow this\nall comes from the RC from the oath RFC\nand uh these are kind of the flows or\nthe interactions between the client the\nresource owner the user uh the client\nand the old server and the client\nfinally with the resource server uh the\nproblem is that when the auth server\nresponds the user with the access token\ntypically this path is not TLS secure is\nnot secure\nso a malicious actor or a man in the\nmiddle could\num kind of intercept the token and use\nit to access resources on behalf of the\nclient\nso this is a problem and there are\ndifferent ways to manage this but I\nwould say the whole category of of the\nresponse that the old server provides to\nthe client is the um there are four or\nfive types of responses the first is the\nclassical authorization code\nthat will make use of the old server\nthere's another form of permission or\nauthorization Grant which is the\nimplicit\nauthorization not really recommended\nbecause it gives the client an access\ntoken directly it's not great\num\nthe other one is client credentials it\nis when decline has a pre let's say a\npredefined pre-established trust\nrelationship without server and it's\nable to store\num some credentials to\ngo to the resource server and request\naccess\nso it's typically used for for example\nfor CI infrastructure\nwhere the system is not really a user\nbut it's trusted and it's able to\nrestore credentials and you assume it\nwon't be easily compromised\nand uh kind of the the most popular way\nto address this problem is the using the\nPixi flow is it it's a kind of a\nseparate protocol on top of old\nand this is a verifier I could verifier\nto mitigate this problem of the\ninterception attack so the the path is\nstill insecure but the client or the\nmalicious actor will be will need to\npresent and verifier code that probably\nwon't be able to get easily and it will\nprevent or mitigate the risk of this\nkind of attack\nso and and finally the device flow\num it's also a separate RFC uh its\noriginal the original design was for\ndevices that don't have a browser\nand uh but but enables users to review\nand authorization request on a second\ndevice so the good thing is that all\nthis is available in Flight is the first\nthing that that was surprising for me uh\npleasantly surprising for me to find how\nkind of easy is to access uh who are\nwith compliant implementation in Flight\nof both the Pixi flows device flow here\nin client credentials uh so that was\ngreat\num so here is kind of the visual\nrepresentation of how it's done in\nFlight you have here in the left hand\nside the potential clients\nfirst uh will be the resource owner or\nor the user uh we'll make use of both\nCLI tools or the or the UI to access\nflight admin and the protected resources\nwill be probably the workflow executions\nand whatnot so flight comes with a\nnative oidc client oidc is used for for\nthe identity part of the of the problem\nand\num you you will every time you enable\noath you will make use of oidc\nright the kind of the question is what a\nlevel of\nfeature set you will need for the\nauthorization part for the out server\nimplementation flight admin also comes\nwith an internal authorization server\nbut it's kind of limited it handles some\nScopes predefined Scopes and it will\nstore the secrets for the authentication\nof both the CLI tools uh the access to\nthe flight console and flight propeller\nwhich is not client facing not user\nfacing but it for for the out\nimplementation it comes as a client\nright remember the flight preparer is\nthe controller that will handle the\nexecution of workflows in the in the\nplatform and infrastructure\nso if you by default it's enabled and\nyou can use it but if you need something\nmore\num\nwith more feature let's say and or your\norganization needs more control around\nthe Scopes policies and all of that you\nwill need to use an external\nauthorization server provider but it's\nprovided by your identity provider\nbasically the same\nmechanism you will use or the same\ncomponent service you will use for\nidentity you will use it for building a\ncustom old server\nand the whole idea of the pr that you\nfind in the notes is basically to update\nthe instructions on how to implement all\nthis stuff be it using octar key clock\nGoogle and partially Azure AV this is\nwork in progress and trying to improve\non that part\nuh but uh I'm yeah I'm really thankful\nfor for all the interactions I had with\ncommunity members including Michael\nwho's here and many others who were\nstruggling or are struggling with oath\nand with them we learn a lot of lessons\nand most of that is in that PR\nuh because the implementation itself of\nthe out implementation in Flight I think\nit's great uh but I think what we needed\nwas better instructions on how to make a\nbetter use of it\nso um yeah with that I will maybe uh\nreal quick demo so kind of let me see\nuh yeah I don't know if that's better\nbut uh kind of the the expectation for\nexample for the pixie flow is something\nlike this if you try to execute a\nworkflow from the command line it will\nit should point you to your ID pin in in\nthis case I'm using OCTA and the data\nplane is an AWS\nuh so let me see if I remember\nAndy yeah flight still and it should\nemit an authorization token it will send\nit back to the service and here is\nhere's the operation\nwhen it obtained the token token is not\nthere and from that point on the\nexecution goes as usual so\num it's the same for the user interface\num every operation in this case now that\nI already log in and if the token is not\nexpiring I will I will be able to use\nthe same for folder executions\nyou can also tweak this the the talking\nexploration policies and all of that but\nthat kind of this is how the pixie flow\nwill look like in in real life\nof some sort so well that's it any\nquestion so far\nno\nyeah I know that was quick\nbut hopefully the APR will be better for\nyou to understand\nall right uh if no other question well\nit's my pleasure to move to the next\nitem in the agenda\nand welcome here jhia I hope I'm\npronouncing your last name well\nco-founder on at eventual Inc and core\ndepth maintainer welcome Jay it's great\nto have you here\nyeah thanks for having me and yes you\npronounced it uh well it's Charlie\nchassis\noh that's great\ncool yeah great to have you so would you\nmind briefly tradition yourselves and\nprobably sharing something you enjoy\ndoing outside work uh sure yeah so hi\neveryone my name is Jay uh I currently\nam a co-founder at a company called\neventual\num we are building an open source open\nsource tool called Daft which I'll be\npresenting about today\num before eventual I worked at freenom\nuh and also lift level five so funny\nenough two very big consumers of light\nand contributors to flight I guess\num was part of some of the first uh\ndiscussions that we know about which\nworkflow engineer we're going to use and\nthen flight you know kind of came out on\nAWS reinvent and so that was part of\nthat whole transition into kubernetes\nand flight for freedom and building out\na lot of the machine learning data\ninfrastructure there\num and then later on when I went to lift\nlevel five I ended up also building a\nlot of the uh distributed ml data\ninfrastructure left for you know all\nthese autonomous driving uh cars and\nimagery and lidar and radar and a lot of\nmulti-modal\num data mediums\num yeah so I'll be working in this space\nfor quite a bit and uh I guess I found\nmy way eventually starting my own\ncompany to build ml tooling for these\ndata modalities right so things that\ndon't really fit in a table you know\ngenomic files\nor you know images or tensors and things\nthat we deal with on a day-to-day basis\nbut for some reason this doesn't fit in\na SQL table\num a lot of the time I guess what I do\nfor work outside of uh what I do outside\nof work for fun\num I like to climb rock climb you know\nme and every other I guess software\nengineer in the Bay Area but yeah I type\na lot so if anyone ever goes to dark\npatch Builders hit me up\num in San Francisco yep\nthat's awesome really interesting thank\nyou\ncool should I launch into my slides then\nyeah sure okay awesome so yeah I\nprepared some slides for today's chat\num and let me know if this slide show\nshows up for everyone which okay yeah so\nyeah uh so I'm gonna present to\neverybody today about uh Daft which is\nthe data frame Library we've been\nworking on uh I call it the python\ndistributed data frame for complex data\nand it's quite a mouthful but really the\nidea the main idea is that why can't I\nhave all my data in a table right why\ncan't I have my images why can't I have\nmy tensors and why is it only limited to\nuh strings and dates and numbers right\nwhy is it like like Excel like we're\nworking on python we should be able to\nhave you know python objects there\nshould be native types for everything\num\nand so I'm going to skip over this\nbecause I just give a brief overview on\nmy background and jump straight into the\ntalk\num so today's talk we'll be talking\nabout multimodal data engineering right\nmultimodal here meaning uh different\nmodalities different You Know Rich types\nuh such as images\num and and you know potentially even\nhaving different views on the same data\nright for example you may have images of\na scene but then also maybe you have\nlidar or some audio and so this\nmultimodal view of your data\num and then we're going to be talking\nabout how to do this data engineering or\nmulti-model data using both your ML and\npython kind of code right uh stuff that\nyou've been working on you know\nalgorithms custom algorithms that you've\nbeen developing and SQL relational\noperations because a lot of the time\nwhen you do data engineering you're not\njust processing the data with your\npython code you're doing a lot of\nanalysis right you're using the the\ntable abstraction uh to do joins to do\ngroup buys aggregations to understand\nyour data you know when you're working\nwith images you want to understand are\nmy images all the same size\num how many people are in these images\nand goodbye say a location and count the\nnumber of people and these all things\nwhat we have to do uh when we're doing\ndata engineering and combining these two\nthings together really make spelt and\nmodel data engineering work\num so again I have here an illustration\nof what this is a Daft data frame uh\nwhat what the world will look like if\nyou're in Daft right you can have any\nkind of data really in a Daft data frame\nand we have native types right now for\nimages embeddings sensors are coming out\nsoon and so the idea is that we have a\ndata frame that's capable of working\nwith all these complex types\num so I'm gonna make it a little bit\nmore concrete by you know doing an X key\nseries style uh I guess uh\nsituation where so so that people can\nunderstand where we're definitely be\nuseful right so let me know if this\nsounds familiar but uh you know imagine\nyou have you know some Cloud object\nstorage and then a database right very\ncommon and then you know oh I have some\njpeg images in my Object Store in S3 and\nthen you also have some URLs and some\nmetadata and a database uh filling out\nsomewhere that points to your Cloud\nobject so very difficult setup a very\ncommon setup that we see\nand then your boss says hey Jay you know\nwe need a clean data set right so I'm\nlike okay I roll up my scenes I'm like\nokay I'm gonna do this data set curation\nright I'm going to get a promotion I'm\ngoing to do DS activation\nuh and then you know I'm gonna do this\nlike this so I'm going to start by\nvisualizing the images right data\nscience 101 always visualize the data\nfirst okay so I go to my relational\ndatabase I write as people query I'm\nlike okay now I export let's see as a\nview file I load the CSV files my python\nnotebook because I need to run some\npython code now and ah finally okay an\nhour later okay I'm looking at my\npictures I'm like okay a lot of these\npictures are pretty dark right maybe I\nneed to add a new like brightness field\nto get a clean data set I need to filter\nout all these dark pictures\nso uh now one right it's like a million\nimages how do I how do I run this so\nokay no problem\nI'm going to write a simple python\nfunction probably like 10 lines of code\npretty simple I'll just like you know\ncount the brightness of each pixel and\naverage it right but now I need to run\nit on all the files I'm like okay how do\nI how do I do this I need to use the\ncloud right quote unquote the cloud so I\ngo to my team infra channel on like that\nchannel help me please no I need to do\nthis in the cloud I need to run out all\nmy images\none week later I'm like oh my God\nfinally I process all these one million\nimages I've collated one minute images\nall the metadata about the brightness\nand you know all the other metadata into\na CSV file who knew I had to learn you\nknow kubernetes is darker and sliding\ncaster and Spark and all these different\ntools so just get this ones and more\ntask done right\nbut it's okay\nat least that's done I never have to do\nit again right famous last words but\nyeah I'm done with it I don't want to do\nit ever again so I upload the CSV into\nbigquery I join it back into my table I\nmake a new view right I do some\naggregations and group by to make sure\nthat the numbers look good I'm getting\nthe brightness values that I want I'm\nlike okay let me dust off my old python\nnotebook and visualize again like oh my\nGod why are there so many Sky Images\nright now that the brightness is gone\nI'm getting all these images of the sky\nI no wonder my model is doing badly you\nknow I'm trying to detect people there's\nno people on this guy so okay I start\nall over again and I start crying\ninternally right so this this is kind of\nlike the the process I think that a lot\nof developers go through when they're\nworking with uh these two like\nmultimodal data types where data doesn't\nreally fit in the database but then you\nhave you know these two systems that you\nuse to process this data right so the\nproblem here is the modern data stack is\npretty ill-equipped to handle this\nmultimodal or like complex data\nworkloads and um you know we often\nmaintain two separate systems right one\nfor your tabular data that's usually is\nthings like bigquery snowflake postgres\nyour databases right that handles\nstrings URLs uh and and dates and\nnumbers and then you have another data\nsystem for your complex data or\nmultimodal data whatever you may call it\nthings like images uh jsons CSV files\nyou know XML files PDF files and\nmaintaining these two separate systems\nand and uh working with them\nsignificantly slows your iteration\nCycles right because as a developer now\nyou're maintaining consistency between\nthe two systems you're running queries\nover two systems and and that's a very\nnon-iterative process and it's not just\nimages right data is very messy and very\ncomplex at the ingestion point so\nwhatever company you work at you know at\nthe adjuster point and kind of\nboundaries of your system where you\ninterface with the outside world you're\ngetting all this data like csvs\nnumpyries product Buffs Json XML PDFs\nlogs and it's\nand often doesn't fit in your SQL table\nand you need a way to like get it into\nyour SQL table first do some iteration\nand potentially close that Loop and go\nback again uh to do to do that Loop\nand so really Daft here is meant to be\nthe solution\num where you know now my Stickman is\nhappy because that just works really\nwell with something like a object\nstorage right you don't need a separate\ndatabase you just need a cloud Object\nStore like S3 and then you need\nsomething like CSV or parquet as your\ntable format and then all of a sudden\nyou have a functional kind of way of uh\ndoing both your uh relational queries uh\non top of these uh tabular\num files and also anything else you want\nto do and that's because you know again\ndepth is really good at relational\nqueries and we ingest CSV parquet from\nS3 really really efficiently\num they're also really good at complex\ndata processing I'll demo a little bit\nabout that later with images\num we have these native kernels we've\nbuilt in Rust for common operations such\nas resizing images or converting images\nto 10 series and we also let you run\nyour own custom python code really\neasily right we have a concept of a user\ndefined function a UDF so you just write\napply function and uh the final the\npython function will probably take in\nsomething like a numpy array or pole\nimage and then you can yeah you can just\nrun that function on top of um\na Daft table really easily the third\nthing is that we're distributed right so\nyou can scale up to the terabytes the\npetabytes uh on a distributed cluster\nsomething like Ray and this is important\nbecause oftentimes when you're working\nwith these multimodal complex types the\ndata can get quite large because you're\nno longer dealing with every row doesn't\njust contain a string or a number uh N64\nanymore right a real could contain an\nimage a row could contain a PDF uh or a\naudio file and those things do get\npretty large and then you want to start\nleveraging the distributed capabilities\nof something like a ray cluster\nand last of all that depth is really\nfast we've built a lot of our kernels\nand rust Optimizer of the i o and so\nwe're about three times faster than VMR\nspark right now we have on our blog\npublished on benchmarks so go take a\nlook there\num but yeah it's constantly improving a\nvery costly uh identifying a lot of\nthese bottlenecks and depth to make it\nrun even faster the idea here is that it\nshould be really nice to use\ninteractively in a notebook but also\nwhen you start running in a production\nsetting uh in on large-scale Data in a\nkind of a batch setting it should be\nreally really fast and has high\nthroughput\nso why why at a flight uh Meetup right I\nI so I've used fight for quite a bit I\nthink that you know beyond the similar\nteeth and color schemes where we both\nlike purple uh Great Taste of flight\num beyond that I think uh flight really\nexcels at automation with\nreproducibility\num right these are all things uh that\nyou want to have in a workflow engine uh\nbut below the virtual engine you kind of\nwant also a data processing utility a\ndata engine a data engine right that\nworks for engines and orchestrates and I\nthink they have to say perfect fit for\nthat it allows you to scale up from just\na single single single node into a\ndistributed cluster with Ray and I'll\ndemonstrate some of that later on\num but with flight and with uh with\nflight and depth you have this kind of\nall all in solution for a lot of your\ndata processing not much data processing\nneeds\num so yeah let me launch into my demo uh\nso I have a just as a simple\nillustration you know I am that stick\nfigure again I'm now going to do some\ninteractive uh mudging of my data right\nso\num first off I'm going to read this part\nK file that's available on hugging face\nit's one of the lion data sets if\nanyone's been following along with a\ngenerative Ai and all that hype\num it's a data set with a bunch of\nimages and also a description of those\nimages\num I'm reading that with theft it's\npretty simple read parquet right daf dot\nb part K the six in HTTPS you can read\nS3 you can read your local files\num and I'm selecting from the data frame\nthree columns the URL uh text column and\nthen this custom column we have called\nan aesthetic score how pretty are these\nimages essentially\num so also adapt is what we call busy\nand so when you execute these uh\nfunctions you have to tell Dev to\nexecute right or otherwise it just kind\nof collects them in a plan or kind of\nlike a queue almost and we'll only\nexecute it when you tell it to run\nbecause we run a lot of these\nintelligent optimizations under the hood\nfor example when you re-parkay we don't\nread the entire file right because\nyou've told us that you only need three\ncolumns and so when this is executing\nit's only pulling those three columns\ndown and then I'm only showing five rows\num and so it's very efficient with the\nio and optimizations there so yeah this\nis kind of what the data looks like uh\nyou know in our data frame there were a\nlot more columns but I've created The\nColumns down to these three um URL these\nare just images and\nthese images\nyeah it's just you know images sitting\naround in random places on the internet\nvery dirty data some text describing\nthat image also a very dirty text and\nthen score\num cool so you know same thing I want to\nfilter my data set maybe for you know\num\nuh where these strings contain Darkness\nright maybe I want the dark photos just\nlike in my example earlier so I can just\ndo a filter right on my data frame where\nthe text column contains Darkness so\nthat has this very powerful expression\nof the API that lets you express your\ncomputations in Python but under the\nhood when this is running this is\nrunning all in Rust in a very very\noptimized code in depth itself\num and so once you once you filter you\nget all these uh\num you know rows where they contain\nDarkness there's no like without\nDarkness right uh the top three corners\nis filled with running the shadow clouds\nand something about Darkness way too\nlong I can't read this\num but yeah so now uh cool we have this\nbut how do we start working with the\ncomplex data right so far we've only\nreally worked with like Text data and\nnumbers\num but that's you know maybe pull some\nimages down so again we provide super uh\neasy utilities all right now we have\nthis dot URL dot download utility which\nwhen we run it on this URL column brings\nit from a right now it's a string column\ninto a bytes column right and then we\nhave another kind of expression let's\nsay dot image dot decode expression that\ntakes a byte column into an image column\nand so in one line of code we've kind of\ngone from URLs to images right and\nimages again are native types and Daft\nand so now we have a a Daft image column\nit tells us mixed because these images\nare from random sources on the internet\nand you know they have different sizes\nthey have different number of channels\nand all that as well so it's a mixed\nimage column but yeah these are all the\nimages\num and I'm gonna run a very simple\npre-processing step here which is very\ncommon in most machine learning steps\nwhich is just just make these images the\nsame size because right now they're all\nwith different sizes and so a lot of\nthese again common uh operations that\nyou will want to run on your data that\nprovides your utility so here I'm saying\num take the image column from the filter\ndata frame run dot image.resize right\nresize this column into 32 by 32\na show five rows again and yeah so then\nthis uh by showing five rules you only\nrun it on the first five draws so we\nonly pull down the first five rows of\ndata and the images and uh and and on\nbehalf now these like uh super pixelated\nlooking because they're all 32 by 32\nresized images then you know I can do\nsomething like write it back out to\nparquet and\nand I collected this will tell me what\nfiles I've just written right so a very\nsimple kind of a workflow but that also\nyou know will support uh doing things on\naggregations right maybe you want to\ncount the number of occurrences of the\nword darkness in your data frame\num and or you want to maybe do an\naverage of the aesthetic score of all\nthese images those are all also very\npossible very typical uh data frame\noperations with that\num the last thing I did not demo here\nwhich is important to call out also is\nthe ability to run python functions and\nso if you had a custom model you were\ntrying to run uh you can just run a Daft\nuser defined function or UDF for short\non these columns and run a pytorch model\nrun you know any opencv or um you know\nconvert it to a tensor and run numpy\nwhatever you want really\nso that that's kind of how you would\nplay with that in the uh kind of\ninteractive environment right\num so maybe now you're ready to you know\ngo into production that's it and one\nreally good way of doing that in spread\nflight because flight gives you this\nability to do reproducible uh pipelines\nso I reproducible uh batch workflows\num and so I've I've gone ahead and\nwritten a simple flight to workflow with\none step just to demonstrated this is my\nflight Workshop uh it has one step which\nis called produce resized image data set\nwhich is very literally produced as a\nresult image data set\num and it takes in a limit right which\nis the number of rows you want to run\num and this this step all it does is it\nuh runs that steam code we had in our\nnotebook earlier except that I've\nremoved all these dot shows because\nthese dot shows are just you know for\nvisualization in the notebook and by\nremoving the dot shows I've I've\nessentially\num made this into a lazy pipeline that\nthat I can then execute all the way at\nthe end right so this is the copy code\nfrom The Notebook without the dot shows\nand all the way at the end I just write\nto parquet and then I returned the\nwritten uh data frame\nso let's try and run this and see what\nthat looks like\num\nlet's see so here I'm gonna run so I\nhave running locally a Sandbox\nenvironment for adapt\num\nthat I can then go ahead and run\nso when I run it it's you know uh if\nyou've ever used that before nothing too\nspecial it runs a workflow and this\nworkflow runs a single task and this is\nrunning down under the hood right some\ninteresting things to call out here is\nnumber one I changed the runner for Daft\ninto a ray Runner\num and this is running right now in a uh\nRay cluster right because I've taught\nflight to start a ray cluster I'm using\nthese the the flight array plugin to\nstart the array cluster on the side and\nthen when Daft is running it just\nautomatically picks up hey like you know\nuh flight is already running away and so\nit connects with that Ray cluster right\nso if we go back into our actual\nkubernetes\num\nenvironment we will see that we'll see\nthese clusters running and this these\nare the Clusters that have been started\nby flight and Daft is actually running\non these clusters and so uh right now\nI'm the cluster is pretty small because\nthis is running on my laptop I have one\nreplica of uh Ray worker running but if\nyou're running in the cloud on on your\nactual kubernetes cluster you could\nscale this up to like 100 replicas right\nyou could you could you know allocate a\ngpus and have Daft memberships gpus when\nprocessing\num really uh the it's it's it's a very\nvery very flexible and you can do\nwhatever you want\num and have kind of flight and way\nhandle the infrastructure portion but\nthen use Daft as kind of this interface\nuh to your to your uh Ray cluster this\ndata frame interface which gives you all\nthe power of having the relational\nqueries and also the data processing\nqueries of uh data frame\nand so let's check our task yep the task\nhas succeeded it outputs you know we\ndump data to this location and uh the\nthis this stuff all ran on grade right\nso let's see if it displays it correctly\nyeah so when you go to the graph view\nyou'll see that this was a flight array\ntask which means that this path is not\nrunning on a single part is actually\nbeing driven by that part but running on\na clustering off pods uh array cluster\num and so this was a super simple\nexample but you could you know\npotentially use Daft for more\ncomplicated uh complex workflows for\nexample here I have one uh dummy\nworkflow which runs you know maybe you\ncan use that to query some data and then\nyou can analyze that data in a separate\ntask using data as well by doing group\nbias aggregations but then you can pipe\nthat data into training and then use\nthat for evaluation\num and so with with flight land down to\nkind of have this power uh for\nprocessing your data and and kind of\norchestrating it all in one end of space\nso that was it for the demo\num I guess uh to round up uh my\npresentation today I'm going to talk a\nlittle bit about like what's on the\nroadmap so tensors are coming soon uh we\nhave native tensor type columns which is\nreally exciting for us and nice pie\ntorch Integrations too\num so that's I think Landing in about a\nweek or two uh we're optimizing a lot of\nthe S3 i o and rust uh URL download that\nyou saw earlier has already been I think\nabout five times faster than most other\nuh Solutions we've tested but it's about\nto get three to five times faster again\nbecause we've built a lot of our own uh\nS3 i o in Rust to make that really\nreally efficient and we're working with\nAmazon to make that happen\num you are building a pytouch data\nloader we do have a local version of\nthis working but then we're also working\non a distributed story so if you're\ndoing distributed training and you need\npre-processing for that come talk to us\nand we'd love to work together and then\nLast of Us just working on robustness\nand we're working a lot a lot of\nbenchmarks and working in a lot of chaos\nengineering right run the thing kill all\nthe all the nodes and make sure that it\nstill work and so these are all things\nthat are uh uh on the roadmap for Daft\nand if you already have a use case that\nwe don't you know cover uh come chat\nwith us we'd love to collaborate and\nwork together\num and uh yeah that's that's it that's\nall I have for the presentations open to\nany questions that people might have\nthat was great G that was awesome I mean\nthank you for the damn one\nuh all right any question\nor Jay\nyeah Jeff\nhey yeah thanks uh Jay just a couple of\nquestions uh so you you showed a a dag\num you know with a more slightly more\ncomplex\nuh I'll slightly more complex dag that\nuses both uh flight and Rey\num can you comment on\nlike would that like could you achieve\nthat layer that level of like\norchestration with Ray alone or like why\nmight you use sorry not right\num well Ray and I guess Daft on top of\nRay but why might you use flight\num\nin that kind of a situation yeah that's\na pretty good question right a lot of\nthe dag workflow engines\num have this\nuh\nyeah whenever I use a darker engine I\nask myself this question like where do I\ndraw the boundary between tasks right I\ncould technically make every line of\ncode a new task in my program but\nobviously you don't want to do that\num and so I think the way you want to do\nit is you want to organize your workflow\nbased on steps that you want to be\nAtomic and reproducible right so in this\ncase for example my query data node\nright I want that to be like its own\nnode because I might want to record the\ndata and rerun it or have that step\ncached and this can run as a single Daft\num uh data frame and then I can you know\nafter the workflow finishes I can store\nit and then pass maybe a pointer or a\nsmall data frame to The Next Step\num this step here the analyze data data\nsets that could run together with the\nquery data set step but by having it as\nin separate step it makes it very\nreproducible because then you can unit\ntest it separately you can you know uh\nrun it on a different data set and have\nthat be kind of this item potent uh uh\nstateless piece of code that you can\nhave on the side right so I think\num the question of where do you draw the\nlines is really a question of how do you\norganize your uh code and and how do you\nmake your code reusable\num and I guess the last thing to call\nout is that it is true that like Daft\ncan't do everything a data frame is not\nthe answer to every problem right\ntraining for example you don't really\ntrain using a data frame uh that's not\nthe appropriate abstraction you kind of\nneed something more like a data loader\nright for distributed training you want\nto distribute a data loader and so what\nyou would do is you would factor that\nout as a separate task have that run on\nthe appropriate abstraction but have\nthat interface with all the other steps\nthrough the appropriate apis and that's\nwhy I brought up one training here as\nkind of a separate step\num but I mean long story short it really\ndepends on the application and what\nyou're trying to do\nand and how you want to organize your\ncode to make you reusable on your\nprevious\nawesome thank you\num I am happy to yield to anyone else\nthat has any questions but I do have one\nmore but that's that question\nuh yeah I had one I don't know the same\nas yours but how what will be the scope\nof let's say a potential flight in that\nintegration how I wouldn't look like\nmaybe I feel like we're already pretty\nwell integrated because we ran on Ray so\nI didn't really have to do any changes I\nliterally just like my code was just uh\nuh start uh start the the flight task as\na retest right and then I just adapt use\nrate and then you know it picked up the\nthe reconnection that that flight was\nalready producing it worked pretty well\nuh one thing that would be nice is I\nguess more tight integration about how\ndo I pass\num data frames between tasks that could\nbe really cool\num uh right now the way that I would do\nit is I would write the data frame back\nonto S3 and then read it back in from a\ndifferent step but just actually find\nfor a lot of\num these workflows because like I said\nyour steps are kind of very Atomic and\nkind of uh can run independently from\neach other and oftentimes you do want to\ncheckpoint in between by writing data\nback up to storage\num but it could be really cool to be\nable to pass uh these data frames back\nand forth between steps because what\nwe've done is that we've made these data\nframes very lazy right so when you when\nyou run say this code it runs\nimmediately because we are not running\nwe're not executing the code right so if\nyou've printed this it'll say like hey\nlike your data is not your data frame is\nnot materialized and that's because\nunder the HUD it's actually just this\nobject which is a plan this is a plan of\nhow we are going to run and so if you\ncan serialize this and pass this around\nbetween the flight between flight steps\nthen it can it will get really really\npowerful\nthat's great thank you\nthat's cool I'd like to understand sort\nof your information your thoughts on how\nwe would achieve this serialization and\nand passing of you know these I guess\nunmaterialized data frames between tasks\nor whatnot in a lazy way is that what\nyou're thinking or yeah exactly you\ndon't have to execute it if you don't\nneed to right so you can have these\ntasks\num at operations on as they need to and\nthen the hand can have one final task\nand they'll just be like yeah execute\num or you can have intermediate tasks\nthat will perform analytics oftentimes\nit's uh the the way that a lot of these\nworkflows work is that you have kind of\na main path for data processing and then\nyou have kind of these branches out that\nwill perform analytics as you go along\nright like how many now since I call\nthem have you know uh these sorts of\nanalytics on the side and so your main\nbatch can be fully lazy but then your\nside branches can execute adjust the\npieces that they need on just the\nfilters that they need if that makes\nsense\nthank you\nall right any other question\nJay\nall right well again thank you Jay for\nsharing thank you for joining it was\ngreat I think we will we'll need to\nfollow up for the potential\ncollaborations here yeah let's do it\nthat will be awesome and yeah thank you\nall right any other\nquestion any other comment\nyour last chance\nno all right well that's it for today\nthanks everyone for joining and hope to\nsee you in the next one\ngoodbye"
    },
    {
        "title": "Flyte Contributors Meetup - June 22, 2023",
        "transcript": "all right welcome everyone to the flight\ncomputers meet up uh today is June 22\nand uh well on the display home having\nto do your host\nuh three reminders this time and a shout\nout before I forget and first one is uh\nthis meeting has been recorded\num and recording will be posted the\nflight YouTube channel\nuh this time I'm testing out the Phantom\nAI service for automatic transcripts\nuh trying to have better notes\num so let's see how it works and also\nwell this meeting falls under Linux\nfoundations code of conduct\nand uh yeah big shout out to Bernard for\nall his commitment ownership and work to\nhave the\num\nopen source licensing\nworking at least for non-union AI folks\nwhich is great so yeah thanks again\nfor another for this\num\nall right\nwith that let me share my screen\nreal quick\non the agenda\nif we tell yourself to attend this list\nincluding your affiliation all right\nanyone joining for the first time no I\ndon't think so right well\nuh next up\nall right any new rfcs\ncompletely new rfcs here in the world\nuh not this time\nbut I was wondering if\nwhat you want to discussed you would\nlike to touch it in the open mic section\nor you would like to talk about\ntiny URLs now\nuh let's do Open Mic whenever we do then\ngreat thank you\nall right next up and the working groups\nupdates well right now we we only have\none working group The config overrides\nuh working group\nwith Fabio over North and Byron\nuh for now and uh yeah thank you for the\nnotes there wondering if there is\nanything in particular you would like to\nshare anything blocking you\num\nor what's the current status\nmaybe I can share something we've heard\na bit um between three of us\nso far I think we have most of the\nthings in place there was an open\nquestion that Dan has answered I think\nsometime last week\num around like how to exactly pass what\nassociated with some\nsome things that were unclear to us I\nthink we should have everything now\num\nmostly probably the the issues to time\nto implement the things so yeah this\nweek I couldn't\nI think at least we know what to do\nfirst thing we can given status update\nwhen we had a bit of time to implement\nsomething\nokay yeah I'm wondering if I don't know\nif it's too early or not but if you just\nto keep in mind that will be great to\neventually have a feedback on what do\nyou think about the the whole RFC\nprocess uh what do you think about the\nidea of uh using working groups as an\nAvenue for implementation and ownership\nagain probably not now but we're open\nfor that kind of feedback anytime\nright\ngreat next up new ideas in the incubator\nyeah we have a new one this week uh by\nFabio and yeah Venice\nis my colleague\nyeah\nthis is an annoying one sorry about that\num\nso so that was my colleague we're both\non the Ops Team and run the platform for\nML engineers and something that our\nEngineers discovered or complained about\nis that uh the the retry behavior on\npreemption is different for different\nplugins\nso there are\num\nthere are two types of budgets for\nretrials one is set by the user in the\ntask decorator the other is set on the\nplatform certainly hum values\none is called just retryable failure or\nwhen when in plugins and propeller or\nre-tribal fader is returned this counts\nagainst the user defined budget in the\ntask decorator\nand then there's also system retrieval\nfailure that is counted against the\nplatform\nconfigured retry budget and\nwe use a lot of preemptable or spot\ninstances because we can reduce cost a\nlot with that\nand\num we discovered that when a normal\npython function task gets preempted in\nplugins we try to call demystify\nfailures that we look at the status\nupdate from the pod\nand then when there are certain\nstatus codes in there like node\nterminated node shutdown we interpret\nthem as preemptions and then we count\nthat against the the system retry budget\nand when the system retry budget is\nexhausted but one we switch to a normal\ninstance a non-preemptable one which is\na really cool feature so that at least\nthe last one runs true now\num we do a lot of distributed training\nwith the kubeflow login and there we\ndon't have access to these status\nupdates from the Pod we only get the\nstatus updates from the the cube float\njob and they don't contain any\nmentioning of any any status codes or\nthe shutdown events that could indicate\nthat something is being preempted\nso any failure even if it was a\npreemption which is mostly what happens\nto our Kepler jobs there kind of they're\ncounted against the other retry budget\nwhich is usually defined\nso it's a bit unintuitive that\num\nwhen a python function task gets\npreempted it is something where the user\ndoesn't have any control over how often\nthey can that can be retried that needs\nto be set on the platform side\nwhereas they do have that when they run\ndistributed training but in the latter\ncase it's not switched over to a regular\ninstance for the last three try\nand we look pretty deep into the code to\nsee what we could do about that and we\nhave several proposals of what one could\ndo about that\num\nyeah but we would I think we would like\nto to if possible harmonize that\nbehavior that all plugins behave the\nsame way on preemptions\nand I'm interested in what you think\nabout that but at least our Engineers\nwould really like to be able to control\nhow many times something can be retried\nupon preemption because we have some\ntrainings that run maybe one hour two\nhour this should be pre-ended like once\nor twice maximum and we have other\ntrainings that run two three days\nand we don't care whether they get 30\ntimes because we have good checkpointing\num\nbut um\nwe can only for certain plugins\nconfigure that on our side but for the\nplugins where we can do that for the one\nwe don't switch to the non-premitable\ninstance and ideally what we would want\nwe outlined that here in The Proposal\num would be that basically any any\npreemption gets counted against the user\nretry budget and we switch over to a\nnon-preemtable instance on the last we\ntry there are other options that we this\nthat we see maybe there are other ones\nthat we outline here and we like discuss\nsome pros and cons\num but I'm really curious what you think\nabout that\nthat's necessary today that's um it's a\nlong track over so somebody has to say\nsomething about that I would be\ninterested but we can also\npostponents to next time everybody else\ntime to read it\nor did it was that too fast is their\nquestion about this\nforeign\nification process has a bug in the case\nof plugins that we might be able to fix\nthe problem is that uh\num I don't think it's a bug so the\ninterface of the KH plugins is basically\nbuild identity resource build resource\nget State get sort of called get task\nphase or something like that\nand the get taskbase function of the\nplugin it gets it gets the kubernetes\nobjects in this case that's the key flow\njob right it gets that thing and the\nstatus section of that object has\ncertain events and we look at the events\nand then\nthe events tell us the job failed then\nwe'll return an error okay the problem\nis that it keeps our job doesn't have\nthat doesn't contain any information\nwhether it was a preemption so we don't\nhave access to information at that point\nto decide whether it was preempted it's\njust not there the information\nyeah and that's\nwhere I was going with this like\nmaybe there is a way to you know\nadd that information to the events\ncoming from group flow but I don't know\nlike\nbut even if you do like the the next few\npeople might not happen\nyeah\nthat's the thing so we tried to not make\nit people of like you've lost the\nexample that we discussed that but we\nwe try to find something that would work\nfor everything\num\nbecause there might be like many\noperators supported in the future right\nwe can't implement the preemption\ndetection behavior that we use in slide\non any and all of them I don't think\nthat's right that's suitable\nso this is just defaulting to count\nagainst the preemption return\nso we have several several ideas\num and probably that there are other\nones but our preferred solution would be\nto say we detect the preemption that is\nthe user retro error because we have had\nrequests from some users that say Hey I\nwant my thoughts to be predictable like\n20 times because it runs for two days\nand I I can check quite quickly it\ndoesn't matter\nand for others it has to be a low number\nbecause it doesn't run very long the\ntraining they don't have checkpointing\nright so we would love for that to be\nuser configurable\nand that would solve the problem because\nthen\nthe only plugin that can do that in my\nopinion is the pop plugin\nright and if the pop plugin would count\nthat against the user retry budget then\nwe wouldn't have the problem anymore the\nonly thing we would need then is the\noption to say okay even on the user\nretry budget\nthe last drive is not preemptable so\nthat we have at least better chances to\nmake it to make it succeed on a last try\num\nyeah the Alternatives would be\nthat one needs to\ngive the get task phase function of the\nplugins access to the under to the\nstatus objects of the underlying parts\nwe also have an idea how to do that\nbasically it would boil down to\neach plugin\nlike in the interface also providing a\nway to query for the underlying pods and\nthen we could inject the status of the\nobjects of these underlying pots into\nthe get task based um function tool\nbut I think on the implementation side\nit would be more involved and the\ndownside that we see is that we still\ncouldn't say hey this task here can be\nprinted 100 times we don't care because\nit runs for two weeks whereas this other\none if it's more than five times let's\njust not do that that's uh we don't have\ncheckpointing it's not worth it\num\nwhat happens in this case if you have\nuser that have that wants to write a\ntask test interruptible with no retries\nthat's interruptible\nwith no retrials they just want the\nsystem retry\nhow interruptible would mean they\num\nthey want to use spot instances but they\ndon't want it to be retrievable\nthey don't want to retry outside like if\nthe task fails then it should fail\nif the past gets preempted then it\nshould retry\ning not not on their own exceptions but\nonly on yeah yeah like only indicates\nthat the node goes away okay then retry\neverything else don't retry\nso then you're asking the user to add\nretries right\nthat is yeah we would be asking the\nusers to add retards yes and that\nproposal\num\nso I see two scenarios where a user task\nwould be reached right that's not a\nsystem stated one is out of memory that\nis counted against\num\nthe user retry budget and the other one\nis if the user raises a recoverable\nfailure exception I don't know exact\nname right in that case it will be\nretried too\num we we don't use that\num that feature so for us retries are\nbasically out of out of memory and\npreemptions that when we retry because a\nnormal exception in user code doesn't\nget retried right it's not not a good\ndriver\nexceptional\num but yeah what's your what's your\nstatus about it yeah\nmaybe we give a sign give everyone your\ntime to read and think yeah yeah there's\nno time pressure on this one\njust some two questions one's this\npre-essent thing is that because like\nsomehow it's this this fight decides\nthat we need to use that resource for\nsomething else at this moment\nso the note goes away it's like you pay\na lot less money for the note but they\ntake it away at any moment from you\nand then you need to figure out why why\nthe task is gone and it's gone because\nAWS needed the resources for somebody\nwho paid more\nokay and then another question I have is\nyou have a statement in there it says we\nuse interruptible tasks for most of our\nworkflows as we implemented robust\ncheckpointing and saw great cost savings\nwhy\num so spottedness has cost roughly like\na quarter to a third to note to regular\ninstances but they might take them away\nat any moment right okay because they\nmight want to take it away at any moment\nyou have to be able to to restart from a\nfairly recent stage so that's why we\nhave checkpointing yeah and if you have\ngood checkpointing it doesn't matter\nwhether your task gets killed you can\njust retry from the last checkpoint and\nif you do that well you can save a lot\nof money\nthank you\nokay\nthat would be it from my side about this\num it's a longer text I'm interested in\nyour opinion but understand that now\nit's difficult to say something about it\nprobably\nall right I think if I have any other\nquestion or comment\nuh just a quick comment on this retry I\nfeel like\num\nwould be a good opportunity to sort of\nthink about this again uh just like the\nwhole experience because I need to think\non this more as well but there have been\ntimes that I've been confused by the\nbehavior\num so may or may not be related to this\nspecific proposal but uh just voicing um\nI think it is yeah\nwe also read about that here that\num it's very difficult to understand as\na user why your task was reached right\nnow which budget accounted against\num\nand we also have actually just how to\nmake that more transparent and easier to\nsee\nno we can't David yep actually this is\nthe first time I'm learning that there's\na system level retry that's separate\nfrom the recoverable flight retry I\nthought it was all the same\nall right\nthat's great\nyeah please keep your comments and going\ndirectly in the discussion and yeah\nremember that the goal of the incubator\nis to I basically answer the question\nis this idea a good feed for an RFC\nit seemed like this one it is\nand uh yeah\nplease add your comments to the\ndiscussion and thank you for you for\ntaking the time\nall right\n[Music]\num\nyeah hitam was reporting some problems\nwith the screen sharing but I think\nseems to be working just fine\num all right uh next up in the open mic\nsection we didn't have any other new\nideas in the incubator from the last\nmeeting\nso we will go to the next section uh\nyeah talk about tiny urls\nwait\num yeah I just put that there that that\nmaybe it's not accurate this is more\nsomething that probably should be in the\nincubator if not in the\nuh or it could have been an RFC was not\nand\nthe next part of it should also be in\nRFC\num it is currently sitting in design can\nI share my screen\nabsolutely\ncool\nyes\num so a while back I added this thing to\num this thing to flight admin\num like the the\nuh I guess key point is that if you\nspecify the\nproject domain execution ID node retry\nand the output name uh you can uniquely\ndetermine an output of any thing in the\nflight any metadata in the flight system\nso from that I came up with a\nURL pattern that starts with flight\ncolon slash\nuh this has already been implemented in\nadmin so if you send a\nURL that looks like this to the get data\nendpoint you now get this will now be\nfulfilled by the\num the data proxy service get data\nendpoint so you can you can send this\nover and the idea was uh this is a quick\nway that you can take outputs from\num well assuming we're going to add some\nfront-end changes to allow to expose\nthis so that you can make this kind of a\nlink click copyable so click on the\noutput that you want it copies this urls\num and this can be dumped into\num like a Jupiter notebook uh more\neasily than trying to Traverse like\nuh instantiate the the remote object and\nthen like fetching the execution syncing\nthe nodes and then somehow traversing\nthe roof all the nodes and getting the\nfinally getting to the thing that you\nwant so this will just immediately dump\nthe\num\ndump the literal that you want\num related to that is that this starts\nthen too and this was done kind of like\njust experimentally like we can if the\ncommunity does not appreciate this or\nlike I said I'll just can be removed and\nit's fine\num the\nthe starts to look like a more\ngeneralized artifact Service uh which is\nkind of what I wanted to talk about and\nthis is currently an internal uh design\ndoc basically but I will at some point\nafter this meeting uh need this and RFC\nuh so that we can talk about it more\nbroadly but the I\nintroduce a few things here\n[Music]\num\nand I want to quickly go over like what\nan artifact service would be good for\nand what it would do so as a user you\nshould be able to do the following\nthings so one you should be able to use\none locally by which I mean like the\nJupiter the Jupiter use case example so\nI have an artifact I have an output\nit was produced somehow by the system I\nwant to be able to inspect it locally\nyou should be able to use one as the\ninput to a new workflow execution so\nyou found something that was produced\neither by a workflow or a task somewhere\nin the past now you want to use it as an\ninput to one of the inputs for a new\nworkflow run and\nyou may be able to provide additional\nmetadata if that person's not an ideal\nyes\nyou want to be able to control how the\nsystem creates this when so by by\ndefault anything any successful run of\nany task will produce all of its outputs\nas individual artifacts any successful\nworkflow execution will produce\nevery single output as a separate\nartifact and artifact here again is just\nbasically in literal like one one of the\noutputs of the assist of the system\num\nso you should be able to control how\nthat is done\nyou should be able to create one locally\nso this is supporting the upload case\nlike I have some some data that I've\nplayed around with\num uh in a shoe grinder book I now want\nto run a workflow against it currently\nthe behavior for the the u x for that is\na little awkward uh so this should make\nthat streamline and it should mirror the\nthe upcoming uh UI change as well\num and then you should be able to make\ngeneralized queries so\nI want to dig into a couple of them\num more succinctly so I think some of\nthese patterns have changed in the PRS\nbut\num when when I say control creation\ncontrol its creation I mean the name\ncurrently is going to be by default\nagain this is all for debate but the\nname is just like some\nrelatively unique thing that you that\nusers don't have control over if you\nwant to specify the name you have to\nwe're going to flip this and make this\nannotated but annotate your output type\nwith the artifacts instance and then\ngive it the name that you want and then\nopt optionally you can also give it a\ntag and this part again is not as much\nyet but we can we can think about adding\nit\num so in this way you can like I think\nwe call this Alias actually so tags can\nbe uh you can have multiple things\ncalled Tags like with the same tag and\nAlias you can only have one for for a\ngiven artifact uh primary key I guess\num\nand then create one locally this is like\nyou should be able to again like take\ndata that you have locally or like a\nvery since this is a generalizing on\nliterals like if you have a very\nimportant number a very important string\nor something\num it doesn't have to be large like you\nwant to keep the track of it like you\ncan create an artifact out of it and\nthen send it to the artifact store and\nthen reference it in the creation of uh\nin the execution of workflows\nuh this part we're gonna pass on uh and\nthen query query is uh users should be\nable to\nlike\nfind artifacts but more importantly in\nthe\num\nsorry I apologize I was going to set all\nthis up before but\num users should be able to\ndeclare as the default input a query to\nthe artifact service so that at\nexecution time whatever is that has the\nlatest Alias and this name in that trash\nagain domain\num that will do a search for it and find\nthe like materialize that artifact and\nthen use that as the development\nand then this is the updated example\num okay it's not updated but uh using\nAlias uh setting the output of a task as\nthe latest so whenever this runs\num whenever this runs this will like\nupdate that tag and then the next time\nthis runs it will use that and then this\nhas implications also like to I don't\nknow like a generalized Eventing\nframework where\num like when this changes and event\nshould be published and then things can\nkick off based on that changing\num so\nthat is\nmost of the design or the the use cases\nthere is an idlpr out for this that\ncreates a new service\num called the artifact service and some\nit's\num it's\nuh all the all the messages for it it\nwill look something akin to the data\ncatalog because as people here now the\ndata catalog service currently is\nfunctioning mostly just as a\nmemorization\num I think there's one team that uses it\nmore broadly at Lyft but for the most\npart it's the it's a bit of a misnomer\nit's a data cache it's not a catalog of\nany kind so this kind of\num eventually possibly May deprecate the\nwhat it was what data catalog was meant\nto do uh not what it is currently doing\num so encourage everyone to look through\nthe data model and the\num and the service and see if anyone has\nany feedback we're going to do I think\nI'll probably get Ethan and I will sit\ndown at some point and do like a\ncomparison between this and the existing\ndata catalog structures to see if we\nmissed anything\num so there's also a flight kit PR again\npartially done the implementation of\nthis is not currently done so we're kind\nof like blocked on it but I'm looking\nfor like we're going to write a stub on\nthis\num like non-performant fast iterating\nlike sub service that we can quickly\niterate on these ideas and the idea like\nand assuming that that's their\nuh most of the changes in the current uh\nthinking in Flight kit and admin are\navailable\nhere\ndata events so yeah so it's going to be\nan event-based system where on\nsuccessful completion of\num of\nuh task executions and workflow\nexecution events the outputs are\nextracted and sent off to as events to\nthe artifact service there is some\nadditional information that will be need\nto be filled in and that may happen\neither\nas a reverse call or as part of the\nadmin Eventing\num\nso that is that I wanted to\nquickly also talk about\nany questions on that\ndoes that sound interesting useful\nreasonable if anyone or is or\nare we like\nno definitely does we use something very\nsimilar like that from one to do\ncurrently where you can also register\nartifacts\nand then we got this plot where we have\nthis lineage graph where we can exactly\nsee okay the model version from\nyesterday is this data set version model\nfrom the day before use the same data\nset but then a few weeks ago the same\nkind of model but in an older version\nuse another version of the data set\nand I think also in your RCA briefly saw\nlike 10 years implications that manage\nI think we could build the same thing\nwith this artifact service here\nI think the idea was I wanted this the\nservice to be General enough that it\ncould be implemented like we would\nImplement one and then like if waste and\nbiases wanted to they could Implement\none\nyeah we wouldn't know I'm not saying\nintegrated with weights and biases but\njust do what we do with words and biases\nwith flight instead yeah yeah that's\nright I'm not talking about an\nintegration\nI have a question about the versions\nconcept is that like if I have made an\nartifact and then I like write again to\nthe store we're just gonna that's you're\ngonna automatically make another version\num so in in the current thinking it's\nthe the things that you overwrite are\nthe aliases\num so it duplicate and another a new one\nwould be written and then just the Alias\nwould move like that point here\nbasically from the older ones in a new\none\n[Music]\nand then okay one other question is is\nwhat about like metadata around the\nartifact like is that automatically kept\nlike you know someone's looking at finds\nan artifact that's interesting in the\nstore can they figure out like how'd\nthat get created where did that come\nfrom\num so that's a good question\num there is a bit of an abuse currently\nin this uh PR and the hack if you want\nto call it that is this part here\num so this says an artifact came from a\ntask execution a workflow execution or\nwas an upload and then in the upload it\nwould probably just start a principle\nif you think about what this is it\ndoesn't really belong in this model this\nis more leaning towards the lineage side\nso a generalized lineage uh system which\ndoes not Implement would also be able to\nwhen like when you use the output of one\nworkflow as the input to another or\noutput of a task as it were input to\nanother workflow that relationship\nshould be kept track of\num and that information is available in\nthe\num like in in if you like scrape and re\nlike parse the admin database it's\nderivable\num it is not currently like this PR in\nthis effort does not cover the emitting\nof that because I think there's so\nlittle bit more to be\nthought through on the lineage side\num I think the the lineage side I think\nwe're mostly going to be well if my my\ncurrent thinking is that it's mainly\njust tracking that like when a user\nuploads something and one uh users\nactively take the action of like using\none output as an Ubuntu another and\nthat's the only\num if you don't name anything if you\nhave a given name then there's more\ntracking but if you don't have a if you\ndon't have a named artifact then it's\nthe that's the only lineage that is\nuseful so you end up getting like lots\nof very very short lineages\num\nbut that's okay\nI have one question directly to that but\nif we do return named artifacts from\ntasks\nthen we can create like a larger lineage\ngraph right yes yes okay\nthat's nice\nso again this is just to start I'll I'm\ngoing to re-write everything in open\nsource\num or rewrite the the RFC and open\nsource\num and I'll circulate that but there's\nlike this kind of ties into the tiny URL\nthing because\num admin can emit in the creation of\nthis request like set that URL to this\nthing and then it would be\num like instantly workable effectively\ndoes that make sense\nand then\nis there anything else on the agenda\nDavid\nyeah there are a couple of these new\nitems but if you had something else to\ncover I think we can do it\nI want to ask completely pivot and then\ndo one ask users everyone here uh one\nquick thing which is\num\nthere's some\num\ninterest in getting a more General data\nclass Transformer in Flight kit\nUm this can\nI wrote a small idea that can kind of\nlike generalize what it means to be a\ncontainer type\num so then this you can just Implement\nlike a pedantic one and data plus one\nand an address one\num and then it would it would just work\nbasically mapping arbitrary containers\nto flight IDL or flight containers flood\ncontainers there's only two currently uh\nunivariate lists and univariate Maps\num this is kind of limiting\num it's currently not going to work\nbecause in this PR it will start it by\nEli Greg picked it up and I'm trying to\nmake it another PR into it\num it's uh the data class is translated\ninto a literal map and literal maps are\ncurrently entirely univariate so the\ncompilation step fails so this is not\nworkable\ntoday as it stands the reason that the\nexisting data class Json Transformer\nworks is because it serializes\neverything to a scaler\nand the mapping there is something that\nthe compiler has no problem\nunderstanding so\num I wanted to get people's thoughts on\nI don't know how the if people wanted\nthought this might be useful if they\nfound the\num\nif multivariate container types are\nsomething that we should even think\nabout investing time\nEthan will have an ability and he walked\nin that right at just the right moments\nuh I'll say hey guys\num\nyou're welcome\noh thank you oh yeah it was a little\nlate\num\nbut that should not be the reason why we\ndo something or not\num I think my my thing is I think we\nwill we lose the metadata like the once\nwe do this\nthis today which doesn't work very well\nI've realized but the Json schema that\nwe extract from the data class drives\nthe\nfront end validator and the farm Creator\nif we move it to a pure map\nor dictionary\nwe have no schema\nso you could essentially pass whatever\nyou want\nand in the end that will cause\nDownstream problems at the point when\nyou receive it this because there's no\nstructure now that may be acceptable I\njust don't know if that's the right\nthing that we should do over now\nlike entering jsons in the UI is\nhorrible\num it may be a bit of feedback from from\nus we do have lots of complex\nconfiguration\num and for for Workforce and input and\noutput serverization as well as\ninputting that into the UI is one of\nthe biggest use of feedback things that\nwe get usually either some stuff that\ndoes just doesn't serialize and people\nwork around it in the weirdest ways we\nhave constructs that are somewhat crazy\num and sometimes also yeah entering a\nJson uis because it's\nhas its own tricky bits\nso we've also worked around that a\nlittle um that's interesting so we use\nthis Library called react form\nI think react Json form or something uh\nuh and the problem that it has is it has\nmultiple schemas it just doesn't work it\nonly works with one schema and usually\nwhat ends up happening is if you use\nlet's say a data class that has a flight\nfile and something else in it it creates\nmultiple schemas out of it and then the\nUI just say that I'm not gonna render it\nuh and it does this\nit's not obvious to the user when the\nJson will be a big text box let me just\nenter Json or when it becomes a form\nright and I think it's just a\nnot a great ux\nin flight cl9 or pipeline run I think\nthis will become better where you can\nactually put a Json\nas an input and the validation happens\nside\nbecause the code exists so you can\nactually try and assert\nthe creation of the data class and that\nwill fail putting the UI that doesn't\nexist and if you ever want to do\npipeline run from a remote entity that\nwill not work so\nI do think this this is not a great\nexperience\nwriting jsons in the UI but I do see why\nhe wants to do this because it's much\nmore\nuh much more bug-free in the actual\nengine and translation Etc or in the DSL\nthis gives a common thing that people do\nin that is not like the antonym parallel\nright like people very often do\nfree form\nstructs data classes whatever in Python\nyou know for training ml models they\ntypically have a conflict that has a\nlearning rate that has a batch size\nmaybe it has your model in there then\nsome strings for for loss function\nI think\nyeah\num one point that I want to bring up\nArthur I think it's also on the call and\nI I mean Arthur hasn't been doing the\ncoding work I've reviewed it have been\nworking on one implementation of a\npedantic based model Transformer\nand I I had a thought of it would be\ngreat if we we could just convert it to\na dictionary and then there was one\nliteral type in Flight that could just\ntake the dictionary\nand then we transferred it to the other\nside and take it out of the dictionary\nagain and parse the dictionary into the\nbase model\nand\nhave such a type that works for\nbasically a dictionary a base model\ndata Cloud so it was a part of me I wish\nthat existed\num because that would make it much\neasier\num\nand I've I mean one thing that I want to\nbring up is that um\nI think what one thing that happens very\nearly when people adopt flight is that\nthey they send their fast addict to\nsomething and the dict has a float on an\ninteger and then they're very surprised\nto they later find out that their code\nbroke because all of a sudden their\nbachelor doesn't float and then you need\nto sit down with them and have to talk\nabout materialization deserialization\nmeans which is something that works so\nwell that people don't realize that's\nhappening right\num and it happened several times already\nthat that was the first time that\nsomebody like that I've explained to\npeople okay this is what's happening in\nthe background to make this all happen\num and it's very important that Jason\nand protobuf don't don't have like they\nonly have numbers and not on the floats\nand integers\num yeah I I can't comment whether this\nwould work well but I have wished\nseveral times for just one\nimplementation to put it multivariate\ndictionary into a slightly literal and\nbe able to transport it to another task\nbecause it's just up just something that\nhappens often machine learning that you\nneed to do it yeah um\nit's a great argument yeah great\nargument because I think multivariation\nare supported by flight idea it's not\nit's just literally hidden like kid\nwhat no one is a very tight it's\nsupported in the liberal it's not\nsupported in the type uh\nmultiple things to them\nhow do you know I know it so for data\nclasses when they convert to jsons it\nbinds by comparing the metadata but it\nonly binds one data one binding per data\nclass thank you\none binding per data class yes it's a\nscalar it's point one per data class\nyeah Oh you mean how do you combine\ntogether right so and and I I guess this\nis something that I thought I would I\ndon't know I thought was interesting\num\nuseful maybe it's not useful but\nif I can't see it\num inside that I mean pedantic exists\nfor a reason in Python right because\nthat's a big python problem\nand um\nwhat we we have an internal we have an\ninternal type transformer for um for\npedantic based models now we're working\non the public one\num and we you can from a base model\nextractor schema there is a DOT schema\nmethod and we put that schema actually\ninto the flightplit drill to have it at\nthis serialization time and compare it's\na bit ugly\nbut it makes all these issues with the\nintegers and floats go away because we\ncan just extract from the schema what it\nneeds to be we can ex we can compare the\nschema of the serialized value to the\nschema of the expected python type and\nthen\ncatch when somebody tries to see it\ndeserialize a child class that has more\nattributes from the literal parent form\na literal Parent PLUS value\nis this a um Fabio is this a\num Json schema like what is this output\nyeah it's a Json scheme I know I see no\nI guess one\nlike my from my understanding of this\ndiscussion so far the reason we haven't\ndone this approach of just I think in a\ninto a dictionary and capturing the\nmetadata is is it the UI experience\nbecause because like my thought there is\nthere are tons of tools out there that\njust make the Json editor in the UI like\nmuch better\nyou know instead of like producing a\nform having a Json type text field that\nlike behaves nicely right you you press\nenter and then it'll you know it'll like\ndo the indentation you know it'll like\ncheck certain things like is that\nsomething that you can do I think that's\non the UI side that's true on the and\none of the issues I think is and where\nwe've seen a lot of problems is we want\num people put in recursive structures so\nstructures like a file inside a data\nclass\nand then that in the or data frame and\nthen that needs special handling because\nthat needs to run through the type\nengine again\nit's not recursive it's like nested\nnested sure yeah\num but like he's saying that you know\neven if I think we should improve the\nJson editor I really like it instead of\ndoing forms the problem is there's no\nenforcement on any of those like you can\nput whatever you want right instead of a\ndata frame you could say x equals to\ninto one\nit doesn't work right\nbut but if if we get like I'm assuming\nhere that we have like the Json schema\nis somewhere accessible in the literal\nright or am I right today\nyeah can't that be extracted at the UI\nside so that you can do validation on\nthe Json blob or string that's being\npassed in that's what we do today but\ninstead of just validation it generates\nthe entire form using this camera so\nwhat you're saying is take the Json\nvalidate through the Json schema\nvalidator\nyeah like basically offload the\nthe this problem just the UI problem to\num\nkind of like you know I'm imagining a ux\nwhere it's like the thing is wrong and\nthen it'll kind of like highlight a\nlittle red thing where the you know kind\nof like the vs code when when yeah I'm\nediting my config\nand uh there's an invalid option\num but yeah the nested thing uh yeah I\nguess we can chat more about that\nI have one more common\nyeah I do have still time or\num I like the idea that there's that I\njust got a text field in the UI I can\nwrite a yaml or a Json and I get um it's\nthe mark it's marked in red if it's not\ncorrect and and you give me you raise\nthe question what happens if we put a\npanel's data frame into the data class\nor some nesting level right\num Arthur and I in the in the in the\nproposals for the pedantic type\nTransformer we use a little bit of a\ntrick we\nwe don't see realize for example pandas\ndata or the flower the slide times\nbasically we don't make them Json\nserializable we detect them during\nserialization and take them out of the\nbase model and put a placeholder in\nthere so what if if you put a pandas\ndata frame into a data class\nin the UI you don't have the problem of\nrendering a partner's data frame but we\ncreate an artifact for that with what he\nproposed and then in the UI you see okay\nyou have your Json representation of\nyour\num of your data class and you don't have\nthe data frame in there because how\nwould we render that but we make it an\nartifact which is offloaded somewhere\nelse\nand put another artifact in there of the\nsame type if you want\num\nyeah that's good\nwhat we are getting to is actually the\nJson schema is kind of required oh\nthat's right like or to do any of this\nyou need a Json schema and so we keep\nwhat it is today it seems like we kind\nof like make it work somehow better\ninstead of adding a new multi because\nthere's no multivariate type you can\ncreate\nthe multivariate type is going to look\nlike a Json scheme\nwhich is this is very hard to build\nlet's use Json schema now the the\nproblem the other problem is what Bobby\nalso mentioned is the stupid probe of\nJson doesn't support integers or\neverything the number right which is a\nslot if we could maybe we should use\nbinary serialization name but that's not\ngood to visualize it in the UI\nI don't know what this is\nyeah you guys are smarter than me it's\nit's\nno there is right which is structure\nwhat will the type be in product\nit's a man the key is a string and then\nthat value is later yeah so also that's\nwhat you could do instead of using\nstructures uh transport use\ndictionaries as the transport and I\nthink it still has a metadata field does\nit not that's not what he's saying but\nyes yeah like you don't have to create a\nnew type you can just use dictionaries\nin the back as a transport with a\nmetadata field and we change the\nbehavior in UI then if you if you see a\nmetadata field then you give a Json\neditor\nwhich you can edit with the schema\nyou can do that\nyeah okay so that's how he signed yeah\nOkay Kevin is saying make it a map so\ninstead of map value type being a single\nthing it's a\ndictionary\nwhich is the recursive then one other\nproblem of this recursive thing of\nliterally this performance there's no\neasy way to deserialize this like from\nJson to data class you'll have to\nmanually write all of this\ndecentralization\nspend so much time in improving the\nperformance of this stuff right like\nit's just like they literally built by\nidentical performance\nwhy are we taking that away okay is\nthere any\num any like\npossibility or is this a very bad idea\nit probably is of being able to bind\nmultiple things to a scaler\nwhat do you mean yeah\nor even with multiple things multiple\ntwo outputs from two different tasks\ngoing to one scaler\n[Music]\nI would make data class Json jsons\nsomething like a just find a better way\nto represent Json the system that's\nliterally what it turns out to be in my\nopinion\nand\nit could be message back and then we\njust say that the format of a binary is\nmessage pack and the UI has a message\nthat it shows nicely in the UI right and\nthat's okay right that works great\nmessage back is nicely there are\nlibraries I think available everywhere\nand they would have support for\nengineers yeah\nright that's great we will have to think\ndiscussion this topic I think\nSo yeah thank you for sharing with the\ncommunity I will ask you to please share\nuh the links to the for the relevant PRS\nin the agenda yep so anyone can take a\nlook\nuh all right we're short on time next up\ntype support for pedantic or touring\nfabulary working on this which is great\nbut they will need a review from someone\nfrom Union\nyeah I'll say once and we're penetrating\non it um like it's two rounds of\niteration we'll talk tomorrow to discuss\nsome final questions I'll leave one\nreview and then I think somebody from\nfrom your side needs to look over it\nit's for a plugin that introduces a type\ntransformer for\num for base models that we just talked\nabout\num but what you're thinking about have\nimplications for that right if if there\nwas a type that we could just put the\nmultivariate\num dip into it then that would maybe be\neasier\num\nbut this is just a headshot that um\nthere's stuff like we're going on there\nwe'll need some other ice in the future\num does this um implementation support\nthe whole data frame thing like flight\nfiles and flight directory thing all\nflight types and it also does so with\nonly\nuh like a due to the flight type engine\nit doesn't you don't need to you don't\nneed to implement any Json serialization\nof the flight types and for pedantic and\n[Music]\nbeautiful\naround me so what's happening then all\nthis is are all of these people working\ntogether to get one afternoon well\nbecause Fabio has one Arthur has one\nthere's one world Eli fabulous and mine\nare the same uh\nwe're working on the same thing yeah oh\nfantastic\neight\nyeah but we haven't synced with Eli that\nis true that she's also working on it\nyeah I don't know if if there's not\nmultivariate I don't know if there's a\nway to generalize data class and\npedantic and Adder seriously have to be\nlike I I would think there there could\nbe a link between these uh as long as\nthey serialize to Json\num the framework we're using is pretty\nmuch agnostic to pedantic that the only\nthe only assumption we're making is that\nwe're able to catch these flights\nsupported objects when serializing to\nJson and pick them out\nand then when we deserialize we just\nreplace those placeholders back with the\nobjects themselves and and that will\nthen deserialize into the object again\nso\nI can't recall it after it supports that\nbut I know you can you can Implement you\ncan configure it to do that somehow\nat least the Json serialization part if\nyou have reasonable objects in there\ngot it\nyeah I think we should solve the Json\nTransformer but there's one thing I\nwould like to also keep in mind when we\nsaw the Json transport is allow us to\nhave the capability of our Json path\nindexing\ninto it because that has been asked by a\nlot of people where you output a data\nclass and then only a parts of it is\njust sent to a task\num\npropeller can do it if it's Json path\nbased stuff\nuh because it already loads the Json\nmemory right I mean that that particular\nwhat do you mean that can decorate name\ndouble right\noh why\noh use always use yeah okay I always use\nthat yeah okay I don't know the answer\nno that's good we want to get rid of\nthat okay yeah\nokay propeller can do it I don't think\nthat's a lot of work you just have to\nhave a capture in like the bindings of\nthe Json path of the Upstream object and\nthat's it right I think that's fine yeah\nand then propeller will work I know that\npart\num okay one quick thing I wanted to\nmention uh Neil's again mentioned we\nwere talking about this earlier the\neffort for fractional and uh different\nGPU types\num we're looking for input on the flight\nkit user experience side so anyone who\nhas a opinion\num we will be reaching out cool\nactually we have a talk next week like\nnext Tuesday at part of that is about\nselecting GPU types\noh awesome cool community in the\ncommunity I I created a quick stub\num RFC incubator\noh nice uh nothing much in there just\nstating the problem and like giving an\nexample of what white Kit API might look\nlike\num but yeah we're pretty short on time\nso\nalways\nyeah\nI would have one small question on\nwhether Dan is out or not if someone\nstands out yes he's on vacation\nokay for this week and you'll be back\nnext week\nsorry what uh we'll hit you back next\nweek just because so we have uh\nokay sweet perfect thank you\nall right\num yeah just added to the agenda the\nlink to the RFC incubator in entry\nthumbnails and with that we are at the\nend of the agenda anything else that you\nwill be able to cover in less than two\nminutes\nI don't think so right\nright with that thank you all for your\ntime it was great thank you for joining\nand I hope to see you in the next one\nthanks everyone thank you thank you bye"
    },
    {
        "title": "Flyte community meeting - June 27, 2023",
        "transcript": "all right um welcome everybody to the\ncommunity meeting my name is samhita I'm\nwith the Union AI team and I'm your host\ntoday\nuh so do we have any new members who are\njoining the flight uh community meeting\nfor the first time if you're comfortable\non muting and introducing yourself\nplease feel free to do so\num\nall right um You can also add your name\nto the attendees list uh let me\nshare the link in the chat\nyeah\ncool\nand this meeting is being recorded and\nthe recording will be posted on the\nflight YouTube channel\nuh so the first item on the agenda today\nis news from the ecosystem I'll quickly\ngo through the topics that we think are\nworth knowing about if you have any\ntopic that you want to talk about please\nfeel free to add it to the agenda so\nfirst comes uh vllm\nyeah it's claiming itself to be an easy\nfast and cheap llm serving with paged\nattention it has been developed at UC\nBerkeley and deployed at chatbot Arena\nand uh double for the past two months uh\nyou can try it out with a\nsingle command you can just pip install\nvllm load the appropriate model and\ngenerate predictions and the performance\nof uh vllm is better than the hugging\nphase Transformers and hugging phase\ntext generation interface so we llm\nachieves up to 24x higher throughput\ncompared to headshift and up to 3.5 x\nhigher throughput than DJI and the\nSecret Sauce behind the vllm is paged\nattention uh so paste attention is an\nattention algorithm inspired by the\nclassic idea of virtual memory and\npaging in operating systems uh give this\nblog post to read if this sounds like an\ninteresting idea or if this is something\nyou want to implement\nand next uh\ngive me a second\nside\nuh Andromeda cluster so not fried\nFriedman and Daniel gross have set up a\ncluster for startups and it's called the\nAndromeda cluster all of these are like\nthe technical specifications of the\ncluster they provide it provides up to\n10 EXA flops of computing power this is\nlike next level support for startups\nand then uh gorilla so uh gorilla is an\nAPI store for llms it enables llms to\nuse tools by invoking apis given a\nnatural language query gorilla comes up\nwith the semantically and syntactically\ncorrect API to invoke so it allows you\nto use llms to invoke about 1600 plus\nAPI calls accurately while reducing\nhallucination it is trained on three\nmassive machine Learning Hub data sets\nnamely torch Hub tensorflow Hub and\nhugging phase and it's a fine-tuned\nllama based model and the Creator's\nclaim that it surpasses the performance\nof gpt4 on writing API calls\nand next uh GPT engineer it is made to\nbe easy to adapt extend and make your\nagent learn how you want to want your\ncode to look so it generates an entire\ncode base based on a prompt this\nrepository is all prompt engineering and\nit proves how powerful prompt\nengineering can be\num and yeah uh David do you want to talk\nabout the other two items\nuh yeah could you click the link for the\nfirst one please sure yeah I thought\nthis one was interesting because the um\nit was a survey on a number of the\ncompanies in the Sequoia Network\num which is an interesting Network and\nuh yeah kind of 90 or more of them are\nare using LMS and at a certain extent to\ndifferent applications but what's\ninteresting for me first is that the\nonly consensus is that there is no\nconsensus on what will the end stack for\naliens will look like\nuh yeah the majority of these folks are\nusing foundational models open API open\nAI GPT or entropica\num on topic but um there's also a\ngrowing interest on on two things first\num orchestration and and development\nFrameworks which is interesting and also\nopen source uh well it's not easy to\nbuild a custom lens from scratch using\nopen source uh a growing number of these\ncompanies is is trying to do it\nand uh also one of the main drawbacks or\nthe main issues that blocks them from\nmoving to\nfull-fledged production is security\nand privacy which is as we have shared\nis very early days\nso yeah that was that was interesting\num I don't know if there's any coming\naround this\nyou know right and the next one was uh\noh yeah well news from the ecosystem\nagain interesting move from databricks\nacquiring Mosaic email\num they are well known for some of their\nfoundational models and uh kind of for\nme is the idea part of the of the space\nrace or the competition right now in LMS\nis mostly on where to run this stuff\nit's not where if we need it or if we\nneed to train them or whatever where to\nrun stuff uh that will make for the most\nefficient\num kind of infrastructure option so it's\nagain a\ninfrastructure problems so yeah that was\ninteresting to see\nall right thanks David yeah\nuh and next uh flight 1.7 release so\nthis was recently released it uh\nfeatures flight agents revamped kubeflow\nplugins uh and numerous bug fixes and\nenhancements so flight agents actors uh\nlong-running Services stateless Services\nit reduces overhead and provides\nscalability benefits uh since the agent\nservice can easily scale up or down\nbased on workload demands with flight\n1.7 you should be able to write back-end\nplugins uh in Python and also test them\nlocally without the need to run the\nentire flight cluster which I believe\nenhances the developer experience like a\nwhole lot and I hope they'll have a demo\nin the near future uh so stay tuned for\nthat and thanks to all the contributors\nuh who made this release possible yeah\nthank you and next up is a demonstration\nof how to fine-tune llms with the\ndeclarative ml orchestration Nails over\nto you\nawesome thanks Amita\num yeah hi everyone\nmy name is Niels I do machine learning\nstuff at Union AI\nand uh today I'll go through\nhopefully uh about 20 15 20 minutes\npresentation and demo of fine-tuning\nllms using flight\num I hope to spend\nmaybe five minutes motivating why I do\nit in the first place another five\nminutes\ngoing through a few techniques\num and then the last 10 minutes\nkind of showing you flight code flight\nUI show you a few things going on there\nso\num also as a caveat there's a much\nlonger presentation\nthat we did at the Toronto machine\nlearning Society workshops\num\nthis is a much condensed version of this\nI've given the time constraint so I'm\ngoing to Breeze through a lot of these\nslides\nwe'll share this also and it will share\nthe the larger presentation as well\num\nfor people who want to dive in deeper\nso quick motivation\nup over to the slideshow\nin a very toy schematic of the llm give\nit an input prompt produces an output\nprompt engineering is the regime of you\nchanging the prompts and you know\nworking with the same\nmodel\nin order to get some desired output\nit's kind of an in-between of prompt\ntuning uh there's a lot of stuff that\nthis field is moving very quickly so\nterms are changing and evolving and you\nknow things like that but prompt tuning\nis the idea of having like a fixed Suite\nof K prompts or particular set of tasks\nand you\nmodify you update just the input\nembedding layer of the llm\num thereby changing the weights\nthereby changing the representations to\noptimize against some kind of output\non the other end of the spectrum is fine\ntuning where you have some data set\nwhich is not pictured here but you\nupdate the weights of the model directly\nor to like bias it towards some\ndistribution maybe you have some legal\ndocuments or some internal documents\nthat you want to\num update your models so that the\nprobability distribution changes towards\nthose that token distribution\num I'm not going to go through the pros\nand cons of each of these uh they are\nessentially the inverse you know there\nare a lot of inverses here\num for example with prompt engineering\nyou tend to have you know you're\ninteracting with it as an API generally\nso you don't have to worry about machine\nlearning per se and then on fine tune\nthe fine tuning side uh while you do\nhave access to the weights and updating\nthem it is a significant you know\ntechnical Hardware software challenge to\nlike get these things to work properly\nand then here's kind of like a\ntongue-in-cheek sort of flowchart\ndiagram essentially generally speaking\nif you care about building an\napplication on top of these like prompt\nengineering is generally the way to go\nand you should try really hard to like\nget the thing to work\num\nbut if if you can't then uh yeah you're\nyou go down into this area of needing\nlike machine learning Knowledge and\nSkills to to fine-tune these these\nmodels and you know a lot of this is\ndata set cleaning and data set curation\nsome prompt engineering resources this\nis not the topic of this particular\npresentation but\num\nfolks can dive into these\nthe the high at the high level it's like\nto just write clear instructions\nso\nnow I'm going to go into like\na bunch of techniques and actually I\nprobably am going to skip a bunch of\nthese slides because a lot of them are\nkind of intro to Transformers and NLP\nkind of stuff so uh the one I'll I'll\nstick to which is the the topic of the\nthe flight code that I'll show you is uh\ntwo types of fine tuning the first one\nis continued pre-training which is just\npre-training but maybe instead of all\nthe internet or Wikipedia and all\nscience scientific papers you're like\ntraining it to predict the next token of\nsome you know proprietary data or\nsomething like that\num supervised fine-tuning is kind of\nlike people distinguish this from\npre-training by having a little more\nstructure to the data so instead of just\na pile of tokens you have like a prompt\nresponse type data set and typically\nunlike pre-training uh you don't need\nthe data quality to be super high\nin sft supervised fine-tuning you do\nwant that\nyou know to be of high quality\num\nI'm going to skip through the\nTransformers intro I'm gonna jump\nstraight to kind of like thinking about\nhow much memory a model needs uh what\nhow much memory your Hardware you want\nyou want to have when you're training a\nmodel so in the blue you have like the\nparameters in this case of the neural\nnet and the activations you'll need for\nthose parameters\ngenerally speaking let's say you're in a\nfloating Point 32\nto do updates on those parameters you\nneed to hold the gradients\nthose parameters\nand assuming that you're using some\nstateful Optimizer like atom you're\ngoing to also need momentum variants so\nthese are two additional things you'd\nneed\num so essentially if you have\nuh you know 100 parameters you'll need\nabout 200 2000 bytes of memory to scale\nthat up\num to think about how much you would\nneed for a larger model\num one thing you can do to increase the\nthroughput of your model of your model\nduring training so you can use floating\nPoint 16 this is typically what you'd\nsee as mixed Precision training\num you typically want to\ndo uh represent your Optimizer space in\nflow 32 because of uh numerical\nstability reasons but you you know you\ncan represent parameters activations\ngradients with the floating point 16.\nthere are a lot of different training\nsetups so this is just one of them\num there are a lot of tricks that people\nhave done historically to train models\nfaster\nright so there's data parallelism you\nknow various optimizer\nbreakthroughs and schedulers for\nlearning right\num for faster conversions\num\nthe first technique I'll go through\nwhich maps onto this deep speed package\nby Microsoft that um I'll show you in\nflightload in a sec\num is zero redundancy optimization which\nis kind of a it's a data parallelism\ntechnique that kind of takes inspiration\nfrom from model parallelism as well but\nat a high level\nyou shard\nthese various states rights the\nparameters activations gradients\noptimization State Optimizer States and\nyou you have like n gpus and you Shard\nthem\nacross those ngpus and there's a very\nlike complicated pipeline for\num\nlayer by layer you kind of do the four\nforward pass and the backward pass\nthrough the network and there's a whole\nlike communication protocol of like okay\nsharing the weights temporarily across\nthose ngpus for that first layer doing\nthe forward pass\nsaving that state somewhere and then\ndeleting all of the the temporary\nweights and then like going through\nlayer by layer\num that way so in sudo code\nessentially you do these various\noperations so for each layer in your\nnetwork you do this all gather you like\ndistribute all the weights across all\nthe gpus do the forward pass with that\none layer discard the weights and then\nkeep going and essentially do the same\nthing on the backward passes so this\nat the stage three so-called stage three\nzero parallelism you essentially get\nmemory reduction linear with the number\nof gpus you're working with\num\nI'll go through these real quick 8-bit\nquantization essentially you're\nrepresenting floating points\nin lower Precision perhaps an integer\nrepresentations\num there are a lot of\nproblems with this in terms of straining\nstability and optimization of the\noptimizer stability\nat a high level there are like a lot of\ntricks you can do to like get 8-bit\nquantization for both inference and\ntraining to work really well\num this is all work done by Tim detmer's\nand Co a lot of people\num\nso yeah there's a lot of resources to\nget dig through here\nuh the the punch line though is\nwith these bag of tricks you can reduce\nthe memory requirements by a whole lot\nand keep the same\nlike performance for your models\nuh the last technique is low ranked\nadaptation or Laura and the intuition\nhere is that when you do updates of your\nmodel right you have some model weights\nW you have some model updates Delta w\nand to do that update you do you know W\nPrime is W plus the updates right fairly\nstraightforward\nand to do the forward pass with the\nupdated weights is yeah you know W Prime\nX\num to get the the new predictions\num the intuition of Laura is you can\nkeep those things uh you can keep those\nthings separate\nand mathematically you get the same\nthing so you do WX plus Delta w x it's\nthe same\nmathematically\nso\nwhat if you can decompose the Delta W\nweights into a smaller set of weights\nand so this is the whole point of Laura\nwhere\num\nas you can see it's a diagram you freeze\nthe weights here so you don't have to\nkeep any gradients or Optimizer States\nfor the Frozen weights and the weights\nthat you do update\nand the orders of magnitude smaller so\nyou have a matrix a and another Matrix B\nand if you do the Matrix multiply of\nthose if they're the right\ndimensionality you can recover your\nDelta W uh weight dimensionality\num then you which is basically this and\nyou multiply multiply that by X and then\nyou effectively get the same thing\nthe thing is that the capacity of this\nthese two weights is much lower\num so the theory is that for\npre-training you want to update all the\nweights but for fine tuning you're like\ntraining on a much smaller\nkind of like set of Concepts\num\nthat your weights have to learn so\nthat's the the hypothesis there which is\nwhy you know justifying why you can use\nmuch smaller weights and and get\num\npretty good performance on certain tests\nuh okay there are a bunch of notebook\nsidebars here that kind of implement\nLaura and 8-bit quantization uh Toy\nexamples of it so I'll point you to\nthese slides later\nand okay\nlet's jump into fine fine-tuning LM\nSupply so all of this code is open\nsource\num I'll point you to the repos later\nbut uh these they're essentially two\nworkflows\num that are featured in this this repo\none is for continued pre-training\num so essentially grabs Wikipedia data\nlike a subset of it and then with deep\nspeed which is the zero redundancy\noptimization I showed you earlier\nyou can train a three billion parameter\nper model on a single T4\nand the pipeline will essentially fine\ntune it\ndo 8-bit quantization and then publish\nboth versions to hugging face hub\num\nor model evaluation this could be done\nby flight but uh you know that that's\nongoing work for now we I have like\nthese two Google collab notebooks for uh\ninteractive inference uh just you know\nusing the notebook to generate text and\nevaluating the model\nuh this supervised fine-tuning workflow\nis something that um samita has been\nworking on for uh slack data so that you\nknow a little teaser on on that work but\num for now we're just using the the\napoca data set to find you know even\nlarger model with Laura\num so this is the technique where you\nuse lower capacity weights and swap you\nfreeze the main model and use lower\ncapacity weights on that\nso without further ado\nI will show you the code so here's the\nllm fine-tuning repo this is available\non GitHub\nover here Union AI OSS\nand Alum fine tuning\nand\nthere's a lot of helper functions here\nbut the the main Crux of the code is\nover here the flight code that is\num so you have this workflow that\nessentially does the uh what I pointed\nout in the diagram\nand the main thing of Interest here is\nthe training task\nwe're doing a couple of things here with\nthe whole you know declarative ml\ninfrastructure peace\num besides requesting resources for the\ngpus we're also using the pytorch\nelastic distributed training framework\num I'm debugging some issues here for or\nspecifying more nodes but for now I'm\nusing just one node\nthat that would be one instance\num in AWS\nthe additional thing that needs to\nhappen here is I need to provide a\ncustom pod template\nthat mounts a shared memory volume and\nthis is used this is important for deep\nspeed\num\nthat uses offloading CPU offloading for\nweights whenever it's not actually using\nthe GPU so this is something that we\nneeded to add to this task definition in\norder to get the Deep speed plug-in in\nthe trans hugging phase Transformers\nlibrary to get to work\num a few other things secret requests\nfor the weights and biases API key\nyou'll see that in a second\num and then there's a bunch of code\ntraining code down here so you can look\nat that at your own time\nbut uh\njust to give you a sense of how this\nworks\nI'm using pi flight run pointing to a\nunion AI hosted flight cluster\num using a pre-built image here and I'm\npassing a bunch of configuration files\nand these you can see\nover here\nusing this red pajama inside base model\nthree billion parameter base model\nusing the simple Wikipedia subset data\nset subset\nand then for my deep speed configuration\nthere's a bunch of it up here\nthere are a lot of options\nuh\nfor the optimizer the you know Precision\nyou want to use\num and then a few\nothers here for what kind of zero\noptimization you want to use\nso hopping over to the flight UI\nI've cheated here because uh all of\nthese steps are cached\nI do have a\nthing that's running right now so\nthis is hooked up to weights and biases\nso this is uh just giving you a sense of\nhow you can hook these things up\ntogether\num so you get a access to your\ntraining metrics\na few things to point out here\nthe\nyou know besides the regular kind of\nflight stuff with the graph and timeline\nview\num this is actually a cached workflow so\nif we go back to the original Source\nexecution\nyou can get a sense of you know there's\na bug here so I fixed it and then rerun\nit so that's a nice flight stuff\nin the timeline view just a demo like\nthe extra metadata you get here so you\nnot only get a sense of like okay it\nmakes sense it's a training job it's the\nlongest but it took about six hours in\nuser code to run run the training job\num and yeah that's that's it for the\ndemo there's there's a lot of more stuff\nto show\num perhaps I'll make a video that's a\nlittle longer that goes through this\nentire pipeline\nbut um this is the\nllm inference notebook we're basically\nloading the the model that we publish on\nhugging face hub\nyou generate some texts about Alan\nTuring\num and then this is a notebook for\nevaluating the language model against a\nevaluation harmonist that's uh\nmaintained by a Luther AI so this\nevaluation is ongoing but this is\nsomething we'll we can add into a flight\ntask uh produce a flight deck and all\nthat nice stuff\num\nthe last thing I'll show is just to\nprove to you this is on the actual\nhugging face\num this save to hugging face hub\ntask creates the string for where this\nis published so\nyou get access to all the model\nartifacts so this would be the model\nartifact the tokenizer and stuff like\nthat\num\nand you can you know use it the\ninference endpoints here\nposted by hugging face\nand yeah with that I'll I'll pass it\nback\nthank you\nthanks names so Caitlin has a question\nfor you is it possible for you to show\nthe utilization\nuh yeah I strictly had my open source\nhat on but sure let me do that\nso just to be clear this is a union\nCloud feature next to your\nexecution attempt the little button that\nshows up for viewing the utilization\num and this gives you a sense of GPU\nutilization on you know pretty much that\ncapacity here this also shows you\num CPU memory quotas and utilization GPU\nmemory utilization\nyou could have loaded a much larger\nmodel right than this no so actually\nyeah we should take that offline but\neffectively yeah no yeah\ncool\nawesome thanks everyone\nthanks thanks ah that was amazing uh so\nanybody has any questions or feedback\num all right if there are no questions\num joining us next is Fabio gretz senior\nsoftware engineer at recognize and a\ndedicated contributor and advocate of\nflight Fabio will share insights on how\nrecognize has been leveraged in Flight\nFabio the floor is all yours\nand let me check if I can share my\nscreen can you hear me\nyeah\nyeah as many screens ultimately to find\nthe right one\nwhere is it\nyeah\nit's starting to find my the right\nbrowser window\nah okay because I have another tab open\nthat was shown\n[Music]\nright okay perfect\num\n[Music]\nand you'll see my presentation in the\nfull screen mode\nyeah\ncool\nokay\num yeah I'll briefly introduce myself\nand then I'll also briefly introduce the\ncompany that I worked for called\nrecognize I'll\nthen talk about why we use slide where\nwe love using flight and then highlight\nthree key features that uh that we use\nvery very often and just to just make\nour lives a lot better\nso um I'm Fabio I'm somebody to\nintroduced me senior software engineer\nat recognize it's a member of the\ntechnical steering committee of flight\nmy background is in theoretical\nastrophysics and before joining\nrecognize I built up the matlabs team at\na company called neurotic momentum we're\nalso introduced flight and based in\nBerlin Germany\nso record time\num a dragon knight we develop\nin like neural network inference systems\nfor the automotive driving industry or\nautonomous Mobility applications\nI to the best of my knowledge we have\nreally unmatched performance amongst all\nof those Automotive inference processing\nsystems\nfor example if I if I quote an ml perv\nBenchmark for single sorted single shot\ndetection like large object detection on\n1200 times 2000 200 pixel inputs we can\nrun them on our systems at roughly 300\nframes per second and that at a very low\npower consumption of somewhere below 25\nwatts so\num\nyeah that is really like it's really\nfast really low energy also very low\nlatency\nso to make that happen we have a\npatented processing system for neural\nnetworks that uses a logarithmet number\nsystem that allows us to greatly reduce\nthe power consumption for the\ncomputations of neural networks\nwe are based in San Jose and Silicon\nValley in California and in Munich and\nGermany if you're interested about us\ncheck out our website they're linked it\nhere\n[Music]\num\nand I'll talk a bit a little a little\nbit about what our mail Engineers do in\ntheir daily work\num also have a nice video here that\nshows it on the right so we develop an\nin-house perception stack covering\nperception applications that are needed\nfor\nautonomous Mobility applications like\nclassification segmentation of driving\nscenes through the object detection\nlandline detection in 3D Visual odometry\nand others\nand apart from developing this this\nperception stack our ml Engineers also\nwork on compressing and mathematically\nconverting the models so that they can\nrun on our Hardware\nand on the right here you see two videos\nshowing the same the same traffic scene\nthe top is the floatner network and the\nbottom one is one that has been\nconverted mathematically to run on our\nchip\num\nI have another example here in the\nprevious one was semantic segmentation\nthis one is 3D object detection and on\nthe right you can see it in bird's eye\nview again the top is a float model\nregular FL 32 model the bottom one is\none it has been compressed and converted\nfor our chip\nprobably it's very\num very interrupted through the video\num\nyeah when sharing streamers with zoom\nit's probably a bit interrupted\nso\num\nto\nto bring a float model onto our Hardware\na few steps need to happen then we\noutline here so in the beginning you\nhave typically an X graph of a float\nmodel\nand the calibration data set and then we\ndo Post training quantization with\nknowledge distillation using this on\nindex graph and this calibration data\nset and outcomes a a model that has been\nconverted for mathematically converted\nfor our for our hardware and we have a\ncompiler then\nthat this converted model is the input\nfor and the compiler produces also\ncalled Drex file recogn executable and\nthat one can then be uploaded to the\nhardware so that's the process that our\nemail Engineers typically do to bring\nfloat models to hardware\nand that's also for where flight happens\nlike a lot for internal development\nthere are many steps here here it starts\nwith the with the conversion of the\nfloat model but even the float model\nneeds to be produced in the first place\nright so we have workflows that automate\neverything from curating and\npre-processing data sets training float\nmodels and then converting compiling and\nprofiling them for our Hardware\num and since we're honing all aspects of\nthat of that process we need a platform\nfor our ml Engineers that allows them to\nto self-serve their infrastructure first\nof all\num because we want to quickly try new\napproaches that might need very\ndifferent different infrastructure and\nhardware and\nwe want our ml Engineers to do that in a\nsubserve manner they might want to try a\nnew architecture need a different GPU\ntype different number of nodes we want\nthem to be able to do that on their own\nand we want them to be able to to\nautomate everything\nso that our experiments are repeatable\nwe can change like a small part and see\nwhat the outcome overall will be when\nthe network runs on Hardware\num that's what we use flight for\nbasically automate everything that's the\nmotto\num and that's why we love it\nso I'll\num\nI thought a bit about what I want to\ntalk about which features and I I will\nfocus on three the first one is\ndistributed training and elastic\ntraining the second one is pot templates\nwhich in my opinion is one of the best\nfeatures of the last half here probably\nand then the last point that I'll\nhighlight a bit Is How We Do continuous\ndelivery firm l and how we use flight\nscrum schedule feature which is a small\nfeature but I think it does great things\nokay\num\ntalk a little bit about distributed\ntraining so\nthe the data sets for autonomous driving\napplications are typically very large\num and in order to speed up the training\nwe want to Crunch through the data not\non a single node and a single GPU but\nactually on a larger number\nand\nlight integrates very nicely with\ncubeflow's training operator which\nprovides custom resource definition with\nthe definitions for kubernetes for\ndistributed training with the different\ndeep learning Frameworks we use Pi torch\nso we use Q Plus pytarts job and\nto our Engineers it looks like this year\nso the top decorator they just say task\nconfig equals to pytorch they say how\nmany workers they want how many gpus\neach worker needs to have and that's\nbasically it the rest is handled by by\nthe platform that we provide\nbut to the engineers it's all abstracted\nthe way and they don't need to know the\ndetails of how the different workers are\nstarted how they communicate with each\nother that's really cool because it\nmakes it very easy for L Engineers to\nuse distributed training without having\nto to deal with the details of the\nunderlying obstooling\none of the business requirements that we\nhave is that we can also perform\ndistributed in quotation marks training\non when executing workflows locally on a\nsingle node a workstation that you might\nhave under your desk\nand\nsupport that we we Upstream the plugin\nto slide that uses torch drum\nwhich you can use locally to do this\nshould be the training in a local worker\ngroup but you can also do it in a\ncluster obviously\num I have\nI captured a small video that shows what\nthis does so\num if you replace the the task config\npipe works here that you have in your\ntask decorator with the newer plug-in\ncalled the code elastic\nwe replace tighter good elastic here and\nyou you provide the arguments that you\nwould typically provide to torch from\nthen you can execute the workflow\nlocally\nthat's something that you can't do with\nthe pi torch with the existing pytorch\nplugin and then what happens is that\num\nsomething called part distributed\nelastic launch which is used to Launch\nokay I think I'll go step back and\nexplain what the cool thing is that's\nhappening here so\num when you when you use torch on or\nelastic training you can have so-called\nlocal workout groups which means that\nyou run multiple processes on a single\nnode and then together with the work\ngroups on the other nodes they form the\nentire world\num that you can't do with the the older\nversion of course just to be the\ntraining\num so for example let's say we want to\ntrain a model on 16 gpus\num you would with vanilla torch\ndistributor training start 16 containers\neach container gets one GPU and then\nthey communicate with each other with\nportrait you could say for example that\nyou want to run only four four nodes for\nworkers but each no each worker gets\nfour gpus\nand then within the one one node one one\nworker or one one pod in this case we\nhave four gpus and run four sub\nprocesses\num so the the processes in that that\npart form also called local worker group\nand the cool thing about that is that\nthat works also locally\num so in the cluster so this is an area\nhere we want four workers and four\nprocesses per worker so when you run\nthat with five flight in the cluster\nyou've got four pops and each part runs\nfour processes so together it's 16.\num\nand when you replace the number of nodes\nwith one here and you can execute it\nlocally so nothing changes you can okay\nnow it's the wrong part of the video but\nyou can do\nPython workflow.pi and you get\ndistributed training quotation marks\nlocally so you can use the same code you\njust change one number and you can go\nfrom running\ntorch distributed training locally to\nrunning torch distributed in the cluster\nwith more more than one worker so\nperhaps that's really cool because it\nallowed us to use the same workflow in a\ncluster where we have access to large\nnumber of gpus\nbut also on the workstation that we have\nunder our desk where it's two or four\nmaybe\nthat flexibility is really cool\nyeah\nnext topic I will talk about potent list\nso what time percent\nhave been the feature in flight for a\nwhile they were configured on the\nplatform side that could be configured\non the on the entire platform level on\nthe project level project and to main\nlevel I think I'm not sure if maybe even\non the workflow level but they were\nconfig configured on the platform side\nand since end of last year beginning of\nthis year\npart templates are exposed to the user\nas part of the task decorator and that\nreally gives a lot of power to users\nthat I really like and I'll show you two\nexamples of what we do with that\nso\num\nI think one concept that is very\nimportant in Flight that also really\nlike is that it hides your weight your\ncomplexity of the underlying Ops tooling\nfrom from the from the users that's\ngreat because not while kubernetes is\namazing I don't think any not everyone\nwants to use it right it's very verbose\nand many ml Engineers they don't they\ndon't know how to work with it and slide\nallows users to to provision their\ninfrastructure in a self-serve manner\nwith python\num that makes it much easier to use\nand here I have a task decorator where\nuser might want to request one GPU and\nthen slide turns this into a kubernetes\nmanifest for a pod where you need to\nmove\nI need to move to the videos and it\nturns this description of a task here\nrequesting one gpus into a pod manifest\nwhere in the resources section\none resource of type nvid.com GPU\nrequested\nand for us that wasn't enough because\nour\num our different\ndifferent perception applications are\nreally optimized for different gpus and\nthis didn't allow us to switch between\nlet's say a T4 or V100 or an a100 for\nthe different types of GPU so that was\none of the key features that we needed\nwhile\nwhile introducing flight at recognize\nand\nwhen pod templates got exposed to users\nby the task decorator where we were able\nto do that\nand I want to show you how here\nso\nwe brought a small wrapper around flight\nticket task called recognize task but\nit's really only a light wrapper it\nprovides addition an additional argument\ncalled GPU type there are different\nthere's an enum that you can pass to it\ndifferent ones that are available in our\ncluster and then in the python code but\nbehind the scenes so that the user\ndoesn't have to deal with it we turn the\nGPU type in this case V100 into a node\nselector that is that is understood by\nby gke Google kubernetes engine so the\nuser says Hey I want a gpu.b100 python\nenum and with platform engineers then\nturn this into the Pod Network that has\na node selector here so with this\nfeature\num sorry the exposing the pop templates\nto the users we wrote a wrapper around\nflight kit start that allows our users\nto switch between different gpus types\nwhich was\nbasically the only blocker where we\ncouldn't draw it out to all teams at the\nsame time like when we first introduced\nit\num we do two things that two other\nthings that will recognize tasks that I\nwant to uh briefly talk about\num we're just preventing that\num we run tasks on nodes that are more\nexpensive than they would have to be so\nwhat could happen for example is that we\nsomebody requests a node with four gpus\nthe task finishes uh task is still\ndangling but then somebody else comes\nalong and leaves one GPU then the\nscheduler might say hey there's a 4 GPU\nnode perfect let's just put the one GPU\ntask on that one and preventing it from\nshutting down and then we have three GPS\nidling in the worst cases nobody else\nneeds you to use and to prevent that we\ngave additional labels and changed to to\nour node pools\nwith the GPU type again\ngke doesn't do that by default they only\ngive a label we also created the\nToleration for either type and the count\nand then in the background in this\nrecognized task here we set the the\nrequired node selectors and tolerations\nfor these these times that we created so\nin this case that means that when\nsomebody starts a four a node with four\nmaybe 100 gpus somebody that requests\nwho can't use them so we prevent that\nresources or dangling and not being used\num\none other thing that we do with pod\ntemplates that is really cool is that we\ncontrol\num rather in detail how the different\nworkers and elastic training are being\nscheduled\num for example so there are two things\nthat I want to show you here so when a\nuser does recognize task task config\nequals to elastic so that's the sight\nwords distributed training we want two\nthings to happen the first thing we want\nto happen is that we want all the\nworkers to start only if all workers can\nstart what do I mean by that\num in this stage of likes language\nmodels and generative AI gpus in the\ncloud rscars resource especially when\nyou want to have the ones that have more\nmemory like a100s for example\nand let's imagine for example it's an\narea where\ntwo of our Engineers they start a\ndistributed training with a100s and they\nboth need 20 20 of them so that's a\ntotal of 40. and if we're unlucky gke\ndoesn't get past these 20 because\nthey're themselves are out of a 100 gpus\nbecause people are training their large\nlanguage models\nso what can what could happen is that\nboth started training both 120 and 100\ngpus and worst case both of them got 10.\num that means that neither of them can\nstart but both of them started partially\nand then we get timeout errors and parts\ndistributed nothing happens we still\npaid for GPU hours and um yeah that went\ngreat right like Engineers are waiting\nor they started in the evening and\nexpect to have results the next morning\nbut then they didn't because those tasks\nuh didn't get enough resources while one\nof them might have right another one\nwouldn't have but at least one of them\ncould have maybe so we wanted to\nwe wanted to solve this problem in a way\nthat a distributed training task only\nstarts when all the pops can start\nand what we did is that we installed so\nthe so-called Coast scheduling plugin\nwhich comes from a kubernetes interest\ngroup into the cluster and we configured\nthe Kubler training operator to to use\nthat scheduler the background that\ncreates a so-called pod Group\nwhich ensures that the parts only start\nwhen all of them basically get a node\nand then in this recognized task here it\nsets the scheduler name to the scheduler\nplugin scheduler making sure that for\ndistributed training tasks this\nalternative gang scheduler is being used\nanother thing that we want to do when\nscheduling distributed training workers\nis making sure that they're placed\nphysically close to each other on node\nso latency is higher and also you can\npay for Network egress when you when the\nworkers of distributed training get\nscheduled in different zones and we need\nRegional clusters because of the quick\nbecause it makes it a lot more likely\nthat we can get gpus right\num but in a single single zone but we\ndon't want the workers of distributed\ntraining to to end up in different zones\num and for example then what record that\ntask does it's it creates a label a\nscheduling group and it just puts some\nrandom string there we have to register\nwe have to create this at registration\ntime unfortunately be nicer to do this\nat runtime and then we also put an\naffinity into the Pod template that says\nhey\nall parts that have that match the label\nselector scheduling group and this\nrandom ID\nwe want to be in the same Zone basically\num sorry they're both here but that's\nwhat it ultimately does all the workers\nof that of that distributed training\nthey will end up in the same Zone\nand you can also configure things like\ncompact placement that even within the\nsame Zone they're placed near to each\nother and server directed are very close\nto each other if you really want to get\nthe last bit of performance out of it\nand it's really nice that we can do this\nnow at the task level because\num before the Pod template was exposed\nonly on the project level we could have\ncreated this a different project for\nthese things right but what happens then\nto a task that isn't distributed\ntraining that we need for saving a model\nit wouldn't work if this thing then also\nhas this pot Affinity here but\num so\nyeah what are basically the tldr of this\nit's I think it's great that flight\nHeights is a way to complexity of\nkubernetes because many people don't\nwant to deal with it but by exposing the\npot template to the user you give power\nuser stability to do what they want from\nmanifests and I think that gives us the\nbest of both works so that's really cool\nokay\num the last topic or the last cool\nfeature that I want to highlight is\nstrong schedules\num\nmight not sound like a big feature but I\nthink it's really cool what you can do\nwith it and our use cases that we want\nto detect regressions in ml performance\nvery quickly so we have\noh I didn't count but\nquite a like quite a large number but we\nhave many ml Engineers that work on\nshare code base and developers their\nperception applications\nand we want to prevent a scenario where\none engineer makes a change and doesn't\nrealize that it has some unwanted effect\non somebody else's perception\napplication and then this could then\ngets discovered three months later and\nthen it's very hard to figure out which\ncommit exactly introduced the regression\nright because yeah will be very\ndifficult to find that out then so\num\nwhat we do is that\num actually I think I'll jump ahead to\nthis slide here so we've run we register\nflight workflows and cicd and we do that\nwe do that anyways everything that is\nrunning on a feature or any any workflow\nthat does\npipeline running on a feature Branch\nregisters to slide workflows in the the\nflight development domain everything\nthat runs on on Master builds he\nactually builds registration to the\nslide staging domain and when we create\nGitHub releases we register in the\nproduction domain\nand\nI have this this picture here from a\nreally cool article article from\num from Google Cloud basically the tldr\nis\nit's it's great if you like treat your\nml workflows like\num like something that you release so\nhave your workflow code check it into a\nsource code repository and then you see\nI actually register the pipeline and\nthen run the pipeline so don't do it\nlike don't have\ndon't treat it like a manually executed\nexperiment that needs a human to to be\nrun treated as something that's\ncompletely automated you only do get\ncommit and then the rest is automated\nthat's the tldr of of this picture here\nI think there's not enough time to go in\nmore detail but imagine it like this\nengineer checks in their their workflow\nchanges make a PR merge it and then on\nthe master build we register this in the\nsite staging domain with the crown\nschedule and then this gets executed on\nthe schedule\nso\num might look like this in Python code\nthe the engineer would Define their\nworkflow here at the top add workflow\nand overboard workflow name here maybe\nnot a good idea and Define some Trump\nschedule which is either for us it's a\nweekly or a nightly if there's a lot of\ndevelopment on that application and that\nmeans that every night or every week we\nrun the latest version of a preferred or\nwe train the latest version of a certain\nperception application we log metrics\nand losses and statistics in into 1tb\nand then we also compare to the to the\nlast run into the best run and then\nautomatically alert and slack if there's\na regression and that allows us to\ndetect rather quickly when we introduce\nthe change that either leads to a five\npercent F1 score drops for example or\nleads to memory consumption doubling or\ntraining time to Triple or whatever\nwe'll notice these things right away and\nthen is typically not a large number of\ncommits that we need to check to find\nout which one it was and\nby registering represents the ICD\nand then having this ground schedules\nthat we run regularly\num\nI think it helps us a lot to to improve\ncertain metrics that we're interested in\nover time and not risk regressions there\nand with these launch points here with\nthe crown schedule I think it's pretty\neasy to do that which is pretty nice\nmaybe one last word about this what I'd\nlike is that automatically always the\nlast version gets run so you don't need\nto worry about deactivating previous\nones it just register with python\nregister modules module python module\nthat creates these lines of code here\nand then the last version of Master is\nthe one that we execute\npretty convenient\nokay\num yeah I talked a bit about the company\nthat I work for quadrachic 9 we\ndeveloped Crazy Fast inference processes\ninference processing systems for\nautonomous Mobility applications and we\ndevelop perception tooling and\nconversion compilation tooling for bring\nnetworks onto our onto our Hardware I\ntalked a bit why we use slides for for\nthat and then highlighted three key\nfeatures that we like a lot which are\ndistributed training and elastic\ntraining pod templates which really\nallow power users to do whatever they\nwant with the kubernetes kubernetes\nparts\nand lastly trans schedules that allow us\nto detect regressions pretty early\nand then yeah I want to thank you for\num for listening to\nwhat we do if you have any any questions\nI'm happy to answer them\nthat was a fantastic presentation to\nhelp you escapes\nit's great to see and we love having you\nin the community so thank you for being\npart of the community and thank you\nhelping the product make it so much\nbetter every day\num so I would have a question that's\nslightly on a tangent uh so uh Martin\nfrom black shark\num\nhow did you convince your ml Engineers\nto start working with it\num\nI think I was hired to to solve a\nproblem what that was existing right on\nthe first conversation that I had with\nthe company that told me hey we have an\norchestrator but we were growing very\nfast and it's difficult to onboard\npeople we need something that scales\nbetter\nand\num then when I joined we looked at\ndifferent\ndifferent tools that existed\num and made a list of requirements what\nwe need in order to make it work\nand I had done such a comparison with\nthe company that I worked at before and\nback then we decided to use flight and I\nknew that like I saw the list of\nrequirements I knew that I could build a\nprototype that would take all those\nboxes and then I built a prototype for\nthat and um we then compared it to\nanother tool that one of the other ml\nEngineers proposed but in the\nanti-worthy bottles that we couldn't\ntake there and yeah I mean when the the\nplatform users saw how it would look\nlike I think it was very very easy to\nconvince them\num one feature that it didn't exist back\nthen was the ability to switch gpus GPU\ntypes and that users have control over\nthat and that was the blocker that we\nhad in the beginning\num why it took I think like four or five\nmonths to\nbring all the the applications that we\nnow run on it over\nbut with this patented featured then we\ncould do that and then\nyeah that allowed us to to switch\neverything over basically\nokay cool thanks\nmatter that was actually a very good\nquestion because I think most Community\nfolks have this sort of questions like\nhow do I\nyou know uh in our community we have two\ntypes of people practitioners uh which\nare the ml engineers and the platform\npeople\nand many times we end up having that the\nplatform folks like a flight a lot and\nthey end up using it because of xyv\nproperties and it fits and solved many\nother problems right that they have like\nin your case from the data as well as\nthe ml influence time\nare also part of the community and so\nthis exchange was extremely useful and\nDavid hopefully we can capture it and\nshare these as a set of like you know do\na quick prototype and share and\nand adding that uh different GPU\nselection I think we have to do it just\nthere is a bedroom I know probably we\nshould just accelerate through it\naccelerate the accelerator selector\none comment I had was uh you said the\nlabels cannot be passed at runtime they\ncan\num\nthere can be parsed at runtime\nbut we need to create so in this\naffinity\nwe say hey all the parts I want to be\nthe same Zone with other parts that have\nthe same label key and value\nand we need a value that would be\ndifferent for\nfor a different like execution ID right\nand so we we need something we need a\nkey value pair that will be different\nfor different executions I did that but\nwe but that we know at registration time\nbecause we need to write it into the Pod\ntemplate so we need to know the value\nThat Is Random during registration time\nand that's why we have to generate one\nfor us that's not a problem right now\nlike it would become a problem if we run\nthe same workflow version a lot on\ndifferent inputs right because then they\nwould all share the same Affinity so\nthat that point we can't do that anymore\nbut\nthat doesn't happen currently for us\nbecause we run these weeklies but that's\njust one instance that's the latest\nversion and then we have\nmany different versions and feature\nbranches\nbut there we register again right\nbecause we do quick quick iterations so\nfor us that's not a problem currently\num we thought about doing it with a\nmutating mutating admission web hook\num there we could have just patched this\ninto the Pods at runtime\num and I guess we'll do that if we run\ninto the problem where we have a larger\nnumber of workflows that attract each\nother even though we wouldn't want them\nto right and I guess we go to\num mutating admission workbooks\nbut until then I think this is a\npragmatic solution that does that does\nthe trick\nmarked in just commented that they have\nhad GPU selection for still September\n2021\nMartin where is the Upstream\num\nyeah so I uh for probably about that\nthough I would love to understand did\nyou even try other schedules like I know\npeople have talked about Utica now and\nthen there's other one uh volcano badge\nwhich I think has fallen out of interest\nfor a lot of regions there's another one\nin\nkubernetes six so yeah if you have if\nyou have tried any of those let us know\nwe've also tried we've not had very good\nexperience with some of them\nscheduling feature that we built a while\nback we compared volcano and\num scheduling plugins from kubernetes\nand\nboth of them would have worked for me\nand the easier one to get to work with\nscheduling plugins so I pick that one\num\nyeah\nso volcano I think has more features but\nI didn't need these features and it came\nwith a lot more overhead\num also like the scheduling plugins you\nhave to build the help you have to\npackage to hunt right yourself\num so that you already you would have to\ncheck out the repository so it's not\nsuper convenient to do but you can get\nit to work and\nany problems since\nany more questions\nall right um I personally love your\npresentation Fabio thanks for sharing\nyour journey with us today\nand yeah that's all for today uh the\nnext Community sent is on July 11th if\nyou'd like to speak please add your name\nto the table present at the top of the\nagenda document or you can also just let\nus know thank you so much for your time\nand participation see you all at the\nnext Community sync\nsee you thanks everyone\noh spring"
    },
    {
        "title": "Flyte Contributors Meetup - April 27, 2023",
        "transcript": "welcome everyone to the flight\ncontributors Meetup today is April 27th\nI'm really glad to see you all here\ntoday broadcasting live from the union\noffice in Seattle\num that's cool\nall right uh yeah a couple of reminders\nthis meeting has been recorded and it\nwill post it to the YouTube channel\nlive YouTube channel and uh by joining\nthis meeting you would like to abide by\nthe Linux Foundation code of conduct\nall right\num yeah I just share with you the link\nto the agenda notes feel free to add\nyourself to the attendance list with\nyour affiliation that will be helpful\num continue conversation and also have\nany question or topic you like to see\nhere discuss do we open mic\nsection\num\nokay cool\nanyone join in for the first time I\ndon't think so\nno\nokay no no new members today right okay\nlet's get started with reviewing\noutstanding rfcs or let's say open rsis\nuh we will start with with new proposals\ntypically this is a place to introduce\nnew proposals in this case this one\ncomes from mine\nuh the idea to\nintroduce let's say community groups\num so\nyeah you're invited to read it and and\ngive you reviews and opinion\nuh basically this is a proposal to\nintroduce communities inside the\ncommunity\nsome sorts or a way to\nenable collaboration with uh more with\nmore structure and well-defined focus\nhopefully\nand\nit will expand the current governance\nstructure of the project we have amazing\nsteering committee that will also have\nthis leadership role on community groups\nso we will have special interest groups\nand working groups\nand in the in the sixth category we will\nhave\num horizontal what what we have called\nhorizontal and the main specifics\nspecial interest groups\num in some ways they're similar but it\ndepends on on how ambitious or\nproject-wide uh goals they are trying to\nachieve horizontal six are are more\nrelated to project level\num strategies\nissues some example here there like\nsecurity testing architecture Etc\nuh typically they they the process to\nget them started and even to disassemble\nsix is there in The Proposal\nbut they but they typically have a wider\ngoals and um kind of more organized\nstructure the main specific six are\nwhere you will typically find\num narrower Focus\nkind of joining community members\ntogether to work on to improve specific\nsub-components of the project I don't\nknow flight kit flight console or to\nwork on Integrations like every week\num we see ideas popping up hey what what\nif we improve this integration what can\nbe done to extend this feature of\nfunctionality of the project so that\nwill pretty much fall into the domain of\na domain in the scope of the domain\nspecific seek\nthere are also kind of practitioner\nFields like I don't know a sick biotech\nor sick\nmapping or computer vision or something\nlike that it could also be a domain\nspecific\nand working groups are kind of\nlighter in terms of how they are\norganized because typically they they\njust live at the the life cycle is short\nand they have a clear goal and a\ndeliverable and after it's done the\nworking group is disbanded or\num finished\nand I bet they typically they typically\ncollaborate with one or multiple six so\nall of these could sound like a lot of\nbureaucracy or unnecessary structure\nright now right now\nuh probably working groups are are more\nsimilar to what we have seen\nnecessary right now but I wanted to\nextend it a bit beyond of what we need\nright now\nuh to provide the structure there in the\nrelationship between six and working\ngroups if if someone wants to start sick\nhopefully they will find the information\non how to do it right there if this\nproposal is accepted\nand um yeah that's it I mean it came as\na result of multiple conversations with\ncommunity members who have ideas who\nwant to do stuff\nand uh it's it's a matter of not only\nstarting a slack channels something like\nthat or or joining together to write a\ndoc\nit's it needs a clear goal it needs\ndeliverables it needs some sort of\nstructure and the proposal takes\ninspiration from other projects uh like\nkubernetes project and some others who\nare very much on the cell governance\nside of things a high level of autonomy\nand it's organized in a similar way it's\nway more complicated I try to you know\nremove everything that was not practical\nright now and leave only what I consider\nimportant parts\nso that's it that's the proposal\num yeah feel free to\ngive it a read give your comments ask\nany question and I have a quick question\nyeah how is it related to the concept of\ncore contributors and commuters\nyeah good question that's also there I\nmean the the dependencies with\ngovernance uh because\nfor example for for six and and working\ngroups anyone from the Community member\nand above can join\nand\num join nsq or joining a working group\ndoesn't make anyone a contributor\nuh right away I mean the the the\nrequirements or the criteria to move\nsomeone to be a contributor remain the\nsame that's that's a parallel thing that\nI also want to share and the call it's a\nPR to make some minor extensions to the\nthe contributor ladder we have right now\num but of course six and working groups\nwith more focused collaboration will\npotentially\ngive community members more ways to\ncontribute and more easily\nprogress on the contributor ladder I\nmean next to the computer role or even\nmaintain a role so it's it's a\nyeah and it's a relationship there that\nthat will even improve the contributor\nexperience I hope I don't know if that\nanswers your question so you're saying\nthat anyone can join any Sig but for the\ncontributor leader they need to\naccumulate the contributor okay\nyeah that's that's right even someone\nnot being a contributor can be a sick\nleave or sick chair or a working group\nleave I mean you don't need to be a\ncontributor to to live one of these\nCommunity groups\nbut at the same time it will give you\nprobably more backbone to provide more\ncontributions and get access to\nhigher positions let's say in the\nproject\nthank you good question anyone else\nright cool great\num in review okay let's see the system\ntags in metadata I saw a reason coming\nfrom Greg on this proposal\num not sure if there's any update or\nthere's anything else\nyou want to comment around this specific\nproposal I'm not sure if it was this one\nor other\num any update or comment around this\nproposal\num\nmy opinion\nmaybe we can tag people like request for\neveryone\num\nbecause like the field\na sense that when we discussed it I\nthink it was two two meetings ago\neverybody was like hey that's a great\nidea\nand then we just didn't discuss it\nanymore but I think\num in my opinion it makes a lot of sense\nis this\nEtc\nokay\nI had a question as kind of a flight\nneophyte uh if I might ask it which was\nlike I get the idea this is about\ngrouping stuff right but I'm just\nwondering about like\nthere's already sort of an inherent\ngrouping around like the workflows and\ntasks that I created in you know my code\nwhich right now the UI doesn't support\nmuch in terms of comparison across those\nthings either but so that's what I'm\nsort of trying to wonder like is this\nwhen I read his proposal it doesn't\nmention anything about that inherent\ngrouping that already exists it's like\nwe're going to add this new kind of\ngrouping mechanism but it's like well\nthere's already structure in like you\nknow like in the decorations in the code\nso I'm just kind of wondering\nI think it's um if I can if I can answer\nfrom my perspective yeah I think the\ngrouping that I'm interested in from\nthis feature is that I have multiple\nruns to train a model and then I\ndiscovered a set of parameters that kind\nof work great and I want to keep that\none and was like Hey that one is a good\none and then the same code I run another\nexperiment choose another learning rate\nand that one is just not good\nso I want to discard that one so it's\nnot really about structure in the code\nbut rather\nkeep like grouping good experiments to\nfor later for later for example or\ngrouping stuff to show to other people\nin some stand-up or something so it's\nnot about code structure I would say but\nrather experiment results for me\nand thank you and the experiment where\nyou're experimenting with really is the\ninput parameters\ncan also be code\num can also Implement a new loss\nfunction for example in the code\nsee hey the results are better let's add\nthat to like let's give that attack to\nthem so that they'll find it again\num it's very similar to what for example\nif you use some MMA metadata stores like\nyou might I don't know ml floor 1tb or\nsomething like that typically you can\nassign tags to experiments there\nand that's what we currently do we link\nfrom from flight directly to our\nexperiment tracking\nserver and then we assign the text there\nbut with this feature we could just do\nit in flight and have these um basically\nthese labels for experiments right there\nokay and then again I'm not arguing with\nyou I'm just learning um is uh but like\nwhen you talked about changing the code\nright then you get a new version right\nbut but that's too cumbersome to use as\na as a way to to locate something\nhmm\nso they played like this we\ngenerate we register new versions very\noften basically we make a change in the\ncode we register a new version don't\nreally use them for grouping so\nother people might do that differently\num I still like imagine I'm\ntrying to make something work that\ndoesn't work I'd start a bunch of\nexperiments in an afternoon I want to\nlike just group them give them the same\nlabel so that I know that they were from\nthis afternoon where it tried to fix\nsomething that didn't work and I think\nthat can be code changes even though\nlike I would for at least the way I use\nflight I would have probably 50 versions\non that afternoon\nand I will not remember which one it was\nbecause we do the versioning\nautomatically from bitco mid\nand feature Branch name so\nI don't think the version would solve\nthat problem for me that I'd find a\nthing and yeah and I think you're also\nsaying like really what you're looking\nat is the execution result so that's the\nthing that you want to group and and\nthat's in the that's not independent of\nthe version in the but it's it's later\nokay I'm getting it now thank you for\nexplaining that to me I mean that's how\nI I would use that feature maybe there\nare other usages of it\nforeign\nset of executors for the same workflows\none use case but you can think of using\nlabels across you know different\nworkflows also you know switch to like\nmodel like a different kind of\nexperiment\nso yeah I guess my follow-up question is\nlike once we have these tags like then\nwhat like how do you\nI mean I I don't know if I read that I\ndon't know if I read this closely enough\nbut like how are you gonna are they\ngonna manifest in the UI or like what\nhow are you gonna use them\nthat's a good question like I I think in\nthe\num some of the conversations that we had\nfor for this feature like we're just\nreally thinking about the\nthe programmatic access to the labels\nfor the discussion about like how I'm\ngonna expose this to the why I was sort\nof delayed like I I can still see like\nin the the example that that fabulous\ngaming was gaming like where we\nfor a given workflow like we put it so\nlike labels as like a top level filter\nyou know but we haven't fixed that point\nyet in the discussion team but there's a\nquestion that's a good point though I\nwas thinking about the UI right now I\nforgot that that's not what was\ndiscussed\nI mean I also think we can you know\nmaybe someone can take a look at is like\nyou know systems that provide this\ntagging how do they represent like how\ndo they let people access it and\num I mean\nI think that's some just look at\npatterns the other patterns that we can\nsee if there's UI patterns that are\nassociated with it would look at\nexperiment tracking Stores\num I would look at them Alpha 1tb and\nthese tools because do you mind putting\nthat in the chat what you're saying\nbecause I can't I'm not exactly I'm not\nfamiliar with what you're talking about\nso I don't know\num\nbring that up\nyeah okay I have like I can share my 1db\nit's my private one it's not the one\nfrom my company\num\nso wait where's the can I share screens\nhere\nyeah I was even just okay I was just\nasking if there's a product that you\nwere mentioning I I didn't maybe I\nmisunderstood and it will take 10\nseconds it'll be very brief okay\num\nhere so that's that's my my private\num 1tb tracking server and you have an\noverview of runs\nso it's very similar in the sense to the\nto the flight UI that these represent\nexecutions you can click on an execution\nand then here you can just add tags here\nso I added some tags here I\num stn stands for special Transformer\nNetwork\nbut I didn't use the specific loss for\nit or these are the labels I can choose\nI didn't apply any here let's say I do\nthis one here\nand then after like after an afternoon\nwe're a trend I don't know like 50\ndifferent models I can just filter by\nthe ones where I applied this part of\nthe network\num and I do it in one PB here I could do\nit in Flight\num but that's what I would use it\nbasically as a way to add a tag here in\nthe UI and then say okay what did I do\nin the last two hours let me take a look\nagain at all the runs where I used where\nenabled for example this loss function\nand then take a look at the curves or\nsome other results\ngot it thank you for showing this this\nis great\nwhat does the filtering mechanism look\nlike on that\num you know it's adding tag seems pretty\nsimple\num\num yeah of course give me one second\nthen try again\num\nso wait so let's\ngo to runs and then there's where's the\nfilter filter\nadd filter and then here's there's tag\nsomewhere\nhere there's tags\nand then I can say is not or in\nand then I basically get a subset of\nthem of the runs\nand when when I kick off a lot of\nexperiments that's very convenient\nbecause then I can just quickly jump\nback to uh\nwho wants to just to adjust the subset\nof the experiments for example I can now\nenable I mean that's a lot of horoscope\nfor what I would do because it doesn't\nshow these plots but then I can overlay\nthem in the plot here and see\nimmediately okay that so the last the\nloss curve looked like if I enable this\nlast function how does it look like when\nI do something else\nit's rather convenient for that\nthanks\nthat's great thank you for sharing our\nview\nall right\nso regarding this proposal sorry sorry\nit was like this is not specific to 1 DB\nI think all experiment tracking servers\nhave this\num it's basically the center of them\nsorry\ngreat thank you thank you\nokay so regarding this proposal\n[Music]\nis there any objection to move it to the\nfinal comment period I will give time\nfor\nfor their comments and for the missing\nsteering committee approvals remember\nthat for a proposal to be accepted it\nneeds at least in this case three uh\napproval votes from the TSC this one\nonly has one\nand um yeah any objection against moving\nit to the FCP\num I have one question here\num\nwhen we like what are the\nthe consequences of moving this to being\naccepted proposal nothing like\nwhat if we don't find the champion to\nyou know implement this does RFC so like\nhow how we\nyeah that was that was something that I\nalso planned this because this because\nfor example we see here and an accepted\nRFC and one that is close to be accepted\nthe the array node Etc\nand well right now the the RFC process\nsay that the the the Community member\nwho writes The Proposal doesn't have to\nimplement it uh\nso uh there is no clear mechanism to\nassign a champion if someone and then\nplay the next discussion is uh who wants\nto collaborate uh to help\num on implementing this\num because yeah that's sometimes in in\nit's usually an open source that\nsometimes is a missing part idea is\ngreat who's going to make it happen\num\nbut yeah at least at this stage you need\nto you know finalize comments reviews\nobjections Etc\nso\nyeah good good question I mean I I\ntotally get the separation between like\naccepting RFC and like\nI just want to see\num\nan idea just died because we went\nthrough the motions here we accepted it\nand like there's no more movement you\nknow yeah accepted is not implemented\nso um\nyeah that's that's probably we need to\nrefine the RFC process this is a\nconstant struggle in in our OSS projects\nuh but um yeah certainly it's it's an\nopen Avenue hopefully uh for for\ncollaboration for contributions to make\nthem\num make them happen\nright so yeah we will move this to the\nfinal command period\nand in the meantime we will be exploring\nhow to handle that missing portion of\nthe implementation\nany further comment about this\nall right next up in review the config\noverwrite RFC is there any update yes so\nyeah and kids and uh we are talking\nabout this but currently we put this on\nhold because uh we focus more on the\nAzure 80\nuh integration I'll talk about this\nafterwards so yeah we are still\ndiscussing about this and\nokay\nso I can we can maybe discuss this next\nslide in the next thing\nthere have been some discussions going\non also I just want to thank you there\nhave been some discussions going on on\nthis one um I replied recently because I\nwas talking yeah it's not stale this one\num\nmm-hmm\noh uh why are we not using\noh yeah because you can uh uh pass the\nuh past the promise\noh I think their idea is that let me see\nyou mean why not using with override\nwith constant or with the variable\nI didn't mean to say we need to discuss\nit now but we're also happy to discuss\nit in the okay on the outside as you\nprefer okay can you explain a little bit\nabout your passion\nso you ask the question\num can you I'm referring to I think two\ncomments up can you can you scroll there\nplease\n[Music]\num\none more\nso here you said my concern with name\noverride method is that users can only\noverride the configs on UI CLI but\ncannot overwrite them inside the code\num and my question was when you have\naccess to modifying the code why do you\nneed like an override hook in the first\nplace why can't you just use the\nexisting with overwrites if you have\naccess to changing the code\noh because our use cases that we will\nhave a lot of reference workflow managed\nby ml componenty but for the user side\nthey need to find some way to reach to\noverride the config in the reference\nworkflow\nso they cannot use with the overwrite\nwith reference workflow to override\nokay yeah I'll provide more context\nlet's discuss it here okay I'm happy I\nlike this feature so I'm happy to keep\nthe discussion okay sure\num\nforeign\ngreat\nthank you\nall right next up the external plugin\nservice\num I'm not sure if Kevin is here today\nno is there any updates or comment\naround this proposal\nbesides the fact that we we implemented\nlike\n90 of it already yeah\niterative process we're still learning\nhow to have animals like\num\nyeah we should move this one to accept\nit especially because like we're wearing\nthrough we went through a lot of\ninformation like okay that's gonna look\nlike you know or like\nthat makes sense to me the tldr is this\nis not at the ideation level by any\nmeans like you already have most of the\nall the all the changes done okay\num\nyeah okay you've accepted and um the\nfinal\num Mayo in the Coffee\nCabin is here but yeah like\nright any objection any comment around\nthis no that makes sense I mean we saw a\ndemo of it already right\num yeah it's already there\nso yeah I'm happy to move things around\ncool great\num and all right and then one that was\nin the FCP was the array notes uh uh let\nme guess is it already almost\nimplemented no\nI'm iterating on it so it's it's\n60 done\nokay okay\ngreat in terms of the RFC I think we are\nwe're happy yeah I don't see\nany any\nyeah there's some recent comments but\nreally no no objections yeah for the\nsake of the process will be great for\nthat for TSE extreme committee members\nto give you a review and approval and\num yeah for now we will move it to\naccept it because it's even the\nimplantation process\ncool\num all right for any of these rfcs about\nany of any of the accepted rncs do we\nneed help\nfrom the community to implement make\nthem happen long registry to organize\nyour\nuh I think the performance benchmarking\none of the only one that was in there\nthat doesn't need any community help\nokay\nokay\nyeah any specific area\nthat require help from the community\nor\nwe can we can chat about it more into\nyour computer Channel I mean that this\nis a good example of something like a I\ndon't know performancy for example a\nspecial interest group around\nperformance\nand that will be the the group of\nindividuals who own this and\nand help it make it happen also a place\nto Mentor new contributors Etc it's it's\ngreat but yeah any any help from the\ncommunity on this\num proposal would be great and thank you\nFabio you look at the duck\ngreat all right any comment around open\nRC's\nand uh\ncool\nnew ideas in the incubator\nuh oh David sorry can I talk about the\nAsia power first speak because I need to\nleave in 10 minutes\nyeah sure you need to share screen or\nsomething uh you can click the link\nadapt authentication flow\ncontribution\nyes\nSo currently we only have the\ndocumentation and the support for OCTA\nand Google and uh key clock so I no I\nhave figured out how to set up hrad with\nthe file authentication part and we need\nto make some change so I include both\nthe documentation and the important\nchange in this PR\nand yeah it just needs someone to review\nthat's great yeah thank you that's\nawesome yeah this is great\num\nthis week was a little too busy Byron\nbut I'm from South I'll talk to you\nabout this game okay okay okay\nI saw that um\nwe're adding\num email in the idea like let's discuss\nwe're gonna have to discuss it now I'll\npick you with no OSS flag sorry who is\nsaying who is talking about uh\nI'm in the same room as David that's why\nI'm not interview\nokay okay sure\noh okay okay sure I'll talk to you\nthanks and yeah thank you Brian that's\nawesome\nyeah I'll stop bye\nbye burner\ncool\num yeah there are ideas popping up like\nevery week in the incubator uh feel free\nto take a look to comment remember that\nthe the idea of the incubator is to\ngauge the interest of the community to\nmeasure if this is something that will\num\ndeveloper first secret specification I\nthink that's a new one we could discuss\nthat one yeah this is new from Nils and\num it has already some comments yeah\nit's a it's a kind of constant question\nin the community how to mount Secrets\nhow to use existing Secrets Etc\nso um the idea here from Neil seems to\nbe in that direction so\nas your comments remember that is this\nis also a place for you if you have an\nidea and not sure if it's an RFC or or\ndon't have the bandwidth right now to\nwrite a proposal you can put it here and\nhopefully eventually we can even\nmake it an an issue when you start the\nprocess from here right\nright\ncool all right\num okay next up\nin the open mic\nunless you have anything else there is\nalso a PR here\nuh kind of adding the process to\nnominate\nuh contributors\nand updating a bit the Privileges of\neach one of the roles like for example\nonly maintainers are able to create\nbranches\nfor computers it's Forks mainly\nand also specifying better the\nPrivileges of the steering committee\nand yeah\nsimple process hopefully a simple\nprocess to nominate but simple but\ntransparent process to nominate someone\nto be a contributor\nand how to accept someone as a new\ncontributor and also as a new maintainer\nthat would be great\nand uh kind of some notes on the life\ncycle maintainers I know maintainer\nburden is is something is real so there\nare some notes there to how to step down\nand a unit\nunless you're getting paid to be a\nmaintainer you cannot step down but uh\nthat's a different thing all right\nthat's great\num awesome and thanks for doing this\nDavid yeah\num someone asked that question about\nif the\nthat the forks are ready to run CI but\nnot like\num\nwe uh is it like whoever it was it Fabio\nor onyx\nhuntix right West about that\num I would love to understand more like\num what is the concern\nokay\nyeah like they should for the most part\nrun yes because we we rely on like\nGitHub actions we don't have like\nwe've been good citizens which means we\ndon't we don't really expose Secrets or\nanything like I'm I'm wondering like if\nthat's not\nthe direction that this question was was\nkind of asking and I was hoping that\nmoney sort of defend this but well so\nmaybe\nFabio\nsorry it wasn't my question I I tried to\ngive an answer I wasn't 100 sure it\nwasn't my question\nyeah I think we can cool we create there\nwe can talk to him in the in the channel\nhe's in the channel so\nto kind of elaborate better or here in\nthe in the UI itself\nI can only speak to my part so for all\nthe branches of all the works that I\nhave have had branches on um everything\nran as expected I think I've\nworked pretty much every group sorry I\nknow yeah\nI mean that's terrifying and good so\nyeah probably it's because the\nexperience I've I've had the problem\nwith with other repos that if you\ncontribute from a fork there are some\nyeah it doesn't run because some\nsecurity and stuff so I mean leaving\ncleaners do have to approve CI to run in\nour case but like it is as far as like\nthe interaction goes we see the VR say\nyeah the same series\nyeah let's let's keep chatting with him\nin DPR okay\ncool any other discussion topic question\nI have one technical question but I'm\nnot sure if this meeting is the right\nformat it's a detail about the pr that\nI'm working on it's almost done I have\none question but I can also do this\noffline I'm not sure if this is the this\ntype of question is what we will discuss\nhere\nwell I think it's it's a good forum for\nthis I mean for contributions and kind\nof technical questions around PR that\nyour\nuh too big like we can take this offline\nbut like it's open mic like let's see if\nwe can scope it to a thing that we can\ndiscuss here okay then let me share my\nscreen quickly\nyeah\nso I'm working on integrating torch\nelastic\num which basically means that in the pie\ntorch job manifest there needs to be\nlike one new section called elastic\nconfig\nand we need to create a new task type\nwhich are called pilot storage elastic\ntask and that starts basically multiple\nprocesses and you can so with the\nexisting Pi torch plugin every worker in\nthe distributed process group runs in\none part and which part with pytorch\nelastic\nyou can have multiple processes in one\npart that form one worker group\nand there's an agent managing these\nprocesses and that can also restart\nthese processes it's basically the newer\nversion of our pile of how python turns\nelastic distributed training and\num\nI am I am you can also show that quickly\nso that you understand\nwhat I mean so this is the the task that\nPi in the pi torch plugin\nand there is a new implementation of\nthis task\nand all this multi-processing stuff\nhappens in the execute method that comes\nbasically from PI torch\nand\num we need to the question that I have\nis concerned with separating exceptions\nthat happened in in the user scope from\nin System Scope so that we know against\nwhich retry budget we need to count\num\nand\nI copied\ncan we\ngoing to Resort it's not so that you can\nsee the code\nI copied from the python function task\nthis logic here in the dark string of\nthe Python function task that says that\nyou need to implement that you need to\nimplement as if you inherit so that\nDynamic is handled correctly\nand my question is do we need to handle\nDynamic here because I don't think we\nneed to but I'm not 100 sure\ndoes the question make sense or should I\ngive my context\nuh we can't hear you Eduardo\nthere you go there you go okay can you\nhear me now yes\ncool so um\nI had a discussion with Caitlin about\nthis and the theory is that you won't\nneed to worry about the distinction\nbetween Dynamic and regular tasks so\nwe're making a change to flag it so that\nthese kinds of\num new\nbite infections not have to worry about\ntheir Extinction\nokay that makes sense but it'll be\nseparate from your VR okay I miss your\nname\nyeah so um then it means we keep the pr\nopen a bit and we re remove basically\nthis execute method completely and\nremove the underscore from from this one\nhere\nand then it will be 100\num kind of like in another place\nyeah\nI mean it's you need to be like an\ninternational\nbut that we can take offline like the\nthe question that they were asking about\nlikely execute override like you won't\nneed to do this\nokay\nokay sounds good to me we can then get\noffline to organize how we will merge\nthis whether we wait for the other PR\num and whether we refactor that some\nlater point\nokay\ncool thanks\nthank you\nall right any other question comment\nI have one thing just on the uh like\netching of pedantic workflows\num I have a draft up I'm just like not\nsure where to put the entry point in a\nflight remote\nlike whether we want it to be you know a\nseparate fetch command for fetch\npedantic workflow or something there\num and that can be something that we\nalso discuss online it's more of like\nfrom a ux what would we want this to\nlook like in Flight remote\num\nI think it would need to be in a plug-in\nright because not everybody wants to use\npedantic\nyeah so it could go in a plug-in or it\ncould just be\nanother function on remote like you\ncould do fetch workflow or like fetch\npedantic workflow like it's just we need\nto decide if we want like fetch\nworkflows default to almost like fetch a\npedantic like object that does handles\nvalidation or whether that would be like\na flag that you specify that would kind\nof change the output for now I think we\nwould want to probably have it as like a\nseparate entry point for\nany sort of user experience roll out and\nkind of gauge how the community likes it\nor uses it if they do\nbut the thing is um like my concern is\nif we make it a new entry point of\nremote that would bring something that\nshould be a plugin into something that's\na core feature right\nlike not everybody wants to use\npedantics so I don't think it should be\nin remote\nyeah I think it I mean okay the problem\nis that I don't think there's a plug-in\nmechanism for remote right\num\nbut you could maybe inherit from remote\num\nthe the like it's like blogging\nmechanism would like stand reasonably\nwell to remote it's just that we don't\nhave any plugins that applies on through\nremote yet\ncould you just inherit from remote and\ncreate a pedantic remote\nand then add another method there\nyeah and we could have like the fetch\nworkflow and fetch task just to return\nrather than like the workflow object\nthat flight produces it could produce\npedantic workflow and pedantic task\nobjects they handle the validation I\nthink I like that\nwould that be okay with everyone I could\ndefinitely like modify the Mrs or that\nit would be a plug-in\num\nand I think that makes more sense and\nthen we could always like iterate on\nthat see if we like that flight remote\nux and people want to use it they can it\nwould be you know purely opt-in Behavior\nyeah it's my Volkswagen\nwhat what no no no one's fault what are\nyou talking about\nno it gets my vote there's ideas oh get\nyour vote I thought you said it's my\nfault like what are you talking about no\none to fall here yeah but no I I think I\nthink I like that too\num so yeah I'll just inherit and\noverride the methods that I need while\nmaintaining the same API for things that\nI don't need to override\nyeah to me that sounds good\noh\nyeah same here curious to read the pr\nbecause that sounds interesting\nyeah\num I really liked using it and Folks at\nFreedom have really like using it so\num yeah excited I'll try and make the\nplugin and like right now it's just a\ndraft Mr so I'll uh make the plug-in a\nreal Mr because I think that sounds like\na better path forward\nawesome cool\nthank you Greg\nall right anything else I would like to\nshare comment\nhere\nokay cool as usual thank you all for\nyour time that was great and hope to see\nyou in the next one\nthanks have a great day everyone bye\ngood evening have a nice day bye bye"
    },
    {
        "title": "Flyte Community Meeting - June 13, 2023",
        "transcript": "and let's get started with\nright welcome everyone to the flight\ncommunity meeting today is June 13th\num\nand I'm happy to be your host today\nuh so the first thing I should remind\nyou that this meeting has been recorded\nand the recording will be posted to the\nflight's YouTube channel and the next\nthing is this meeting as every other\nCommunication channel on the project\nfollows under Linux foundation's code of\nconduct\ngreat so I'm sharing here again the link\nto the\nagenda\nand I'll ask\nfor you to add yourself to attend this\nlist\nI'll add my name so you can\nsee how it looks\nuh there you go\nand uh yeah with that let me see if\nthere is anyone joining for the first\ntime\nI'm not sure brother is this your first\ntime brother Peters is this your first\ntime in this meeting\nand sorry to call you out if you're not\navailable that's totally fine but\nwelcome to the community meetings\ngreatly here\nI'm also training for the first time so\ncan also remember this\nPeter here\nyeah Peter that's great we are here\nMongoose Peter\nand\nthat was\nthere you go\nawesome great to have you all here\nall right let me share quickly in my\nscreen\nright here\nI hope all of you have access to the\nagenda\nuh it's right there in the invite\nSo today we're kicking off uh\nMall section at the beginning of the\nmeeting news from the echo system from\nthe ml data ecosystem uh it's meant to\nbe crowdsourced so if you know of\nanything relevant that happened during\nlast week or last couple of weeks feel\nfree to add it there and we can comment\nbriefly\nso with that I'll start with the OST\nFoundation uh they're working on a top\n10 vulnerabilities list for LMS it's in\nthe draft state right now it's open for\ncomments I believe it's July 1st they\nthey have\num\ndecided to publish a first version but\nfor now it's open for comments\nuh it's interesting you know many of the\nartifacts that the Old West Foundation\nreleases are top 10 vulnerabilities for\ndifferent software stacks and uh there's\nbeen a lot of comment around this one\nsome people think that yeah this is just\ngeneral software best practices just\ntranslated to a lens uh some of them\nthink that it has a lot to do with\nsandboxing proper sandboxing Etc\nbut again the thing is\nyou know to give a first step trying to\ndefine the big issue on on responsible\nor secure AI so it's open for comments\nthe slack channel for the foundation is\nopen for you to join you can join the\nmeetings come and learn\nEtc it's similar to a project that they\nalready have released the ml top 10\nsecurity\nrisks or vulnerabilities\num that again is is open for anyone to\nimprove and finally in this security\nworld the meatree because I'm not sure\nhow to pronounce this well they release\na also a draft as you can see this is a\nvery nice and filled and very early on\non the development of security best\npractices in AI everything is in draft\nright now\nthey release this draft of mitigations\nfor different ml\nburnabilities very much online to the\nOAS Foundation is doing so kind of\ntrying to give a Next Step okay if this\nis the the top 10 have vulnerabilities\nwhat we can do about it\nright\num okay probably I should have put this\nuh in the topic because it also really\nhate it uh with the security side of\nthings Google release first approach\nfirst attempt of defining and secure AI\nframework kind of a set of steps I think\nthey have a a nice\nuh document it's over here\nand how practitioners can get started on\nsecuring an AI pipeline it's not\ncomplete by any means I mean just 12\npages of recommendations like many other\nstandards they Define the what not the\nhelp\nuh but it's a it's a first attempt to\ntry to tackle a big big problem\nall right\num right next up databricks announced ml\nflow 2.4 release\nincluding ml flow evaluate this is the\nperformance of language models\nalso a new artifact view a way to\ncompare input outputs and intermediate\nresults across multiple models\nand finally the data set tracking\nfeatures a way to identify which data\nsets were used to develop and evaluate\neach of your models\nall right and finally interesting and at\nthe same time\num yeah really fun to try yeah I I think\nI believe they are from Mera from\nFacebook research uh they publish a\npaper and a set of samples it's all\nthere in the link uh a music generator\nprobably this is not new\nbut according to the authors what is new\nis that they use a single stage\nTransformer uh together with the vision\ntalking interleaven patterns and in\nsummary it could be more efficient so\nthe paper and samples called Everything\nIs there for you to try\nokay\nto hear any comment any other news that\nwe're missing\nall right\nyeah again feel free if if you in for\nthe next meeting if you are aware of\nanything that's happening out there and\nyou like to come in here or share feel\nfree to put in there\nawesome next up and Community updates\num\nunionized sponsor for the Toronto\nmachine learning Summit and uh some\ncommunity members are there\num\nyesterday Niels ventiland part of flight\nsteering committee he gave a workshop on\nfine tuning\non flight and there will be also a round\ntable\nwith function from the community it's\nnot charging a round table on llm Ops\nyeah we're using that term LM Ops and\nwhy it's relevant how how to approach\nthis big big concept and uh you can find\nit there I don't know what's happening\nwith the website but yeah in the round\ntables I believe it's one table number\n16 if you happen to be attending\nit will be there\nthere you go yeah you can see it right\nthere LM Ops\nbecause probably that's a thing\ngreat\num next up seek out as probably some of\nyou already know where the flight\ngovernance model now includes working\ngroups and special interest groups\nand uh to tackle project wife concerns\nuh the model defined the idea of special\ninterest groups\nuh there's an issue that our good friend\nhonix\nfrom Spotify he created\nyou know kind of trying to take the next\nstep on the out experience in Flight\nSupport more providers\nand probably this is a good kickoff\nissue to go over the idea of having a\nout special interest group because yeah\nthere are many options here many\npotential providers and will be great\nfor for the community to to have a\nsaying to live so the call to action\nhere is to go over the issue and comment\nif you also liked uh to see a sick oath\num\nyeah leading and see if he's is not a\nnot really a burden it's a matter of\nfacilitating communication but it's\nmoreover trying to define a set of\nprojects at least a soap project first\ngoal for this sick to gather a part of\nthe community together and think and\nproduce something around extending or\nimproving the out experience implied so\nuh right now it's open for you to\ncomment and um consider if this is\nsomething you like to see happening seek\nout\nright\nnext up in terms of road map slash\nrelease updates\nI think I saw Eduardo here I don't know\nif you wanted to comment something yeah\nfor sure let me do it real quick so um\nwe're getting\n1.7 out today or you know just making\nsure that it's in Tip-Top shape we're\nannouncing uh a renaming of a feature\nthat we we had we had it in the works\npreviously\num it's this idea that now you'll be\nable to write\nplugins for the engine in Python we're\ncalling this uh flight agents now and\nwe're planning to expand this idea\nmassively so the the first version is\ngoing out with flight or with flight 1.7\nbut we\nyou know we're just gonna extend this uh\nthis concept in 1.8 and Beyond\num so keep\nin touch it will be uh you see how\num\nthis this tight feedback loop where you\nyou can go from you know your local\nmachine all the way to like running a\nplug-in in in a flight deployment in a\nmuch simplified manner but that's not\nthe only thing that we're working on\num we also heard the community like\nthere's there's an RFC related to tags\nor execution tags so you'll be able to\num\ntag executions with specific labels so\nthat you can query for them in the\nfuture\num\nthere's an RFC in the works but you know\nbeing the engineers that we are we just\nwant to experiment to see like how\nthings are so there's already a PR in\nthe works also and we're planning to get\nthe RFC approved and also the the\nat least the view one of this feature\nsoon in 1.8\num\nwe also briefly touched on this in\nrecent presentations this feature that\nwe're calling eager mode also you know\nlike we we heard a lot of feedback from\nthe community where there's there's some\nconfusion around like how how to how to\nwork with the with the flight DSL in\nworkflow so we are proposing\num an extension to how uh you can build\nyour your dag using Python's async like\nkeywords so\nyeah uh and I think that one of the\nfeatures that you know it's it's it's\nbeen a constant source of pain and I\nI'm so excited that the community jumped\nin to fix this problem of like how do\nhow do we use data classes or how we\nexpose data class usage in Flight kit so\nEli Greg like there's a lot of people\ninvolved in this effort and I hope we\ncan we can get it in for um one eight so\nyeah lots of things happening like it's\nit's amazing that the community you know\nis\nlooking at problems and proposing\nSolutions and iterating and like I yeah\nit's awesome thank you thank you so much\nright great to know thank you Eduardo\nany question comment\nrun the new release nope\nokay\nwe'll move to the next item so we have\ntwo not only one we have to guests today\nwho will share their knowledge with the\ncommunity so I'm happy to welcome first\nprofula\num thank you for joining thank you for\nsharing with you know and um yeah I\ndon't know if that's part of your\npresentation but will you mind briefly\nintroducing yourself\nuh to the audience and probably sharing\nsomething you enjoy doing outside work\nhey guys uh this is profula I've been\nworking with the flight Community for\nalmost like two years now I started with\nthe open source contributions then moved\nto some of the core functionality and\nnow working on the uh the cloud offering\nand yeah and mostly I do love uh to go\non Hikes I I live in the Bellevue area\nand I love hiking around up here and\nit's an amazing place to hike in this\nyeah\nokay with that being said I'll just uh\nstart my talk regarding how uh him\nmonitor your flight books do is uh this\nwill be more focused on resource level\nmonitoring\nuh let me share my screen first\nokay see my screen\nyeah but okay cool\nso uh so the stock is going to be\nfocused on resource level monitoring but\nwe we can extend it to uh other levels\nof monitoring for our uh for\nspecifically flight workflows or in\ngeneral like any sort of workload that\nyou run in your Cube cluster like how do\nyou monitor those uh so I'll just go\nover like a general ways why you need uh\nresource monitoring or uh is basically\nlike if you're running uh your workflows\nand underneath you have like\ninfrastructure being provisioned or deep\nprovisioned you want to know like how\nmuch money are you spending on that like\nif you have like heavy workflows which\nare requiring gpus and you're not\nconsuming any of those and underneath\nlike you have uh some automation kicking\nin and spinning off new workloads uh it\nso that could be cost prohibitive\nsometimes so understanding the need of\nhow much uh resources you need for your\nworkloads is crucial and uh\nalong with that like uh sometimes it's\nvery necessary for you to debug some of\nthese failures that happen like it might\nbe like you do not have enough memory to\nrun your resources uh you're you're\nrunning into out of memory exceptions\nlike uh figuring out that like what is\ncausing it uh could be helpful and how\nwhat is a pattern of your uh your memory\nallocations would be good to see like\nwhenever you have any workload running\nunderstanding that patent to debug uh\ndebug such failures\num another thing gets back in in\nspecifically in the flight world like\nyou have\num\neach of the executions are version for\nthe launch plan that they use so if you\nhave been changing your uh workflow as a\nlot and you want to be able to compare\nlike uh the resources across versions\nlike it would be crucial to understand\nlike what exactly caused a change like\nsuddenly you have more uh memory or uh\nCPU being used so uh what change across\nversions of your workflows could be\ncrucial to understand so that's why you\nwould need some sort of resource\nmonitoring to across your flight\nworkloads\num\nI just go over like how you do it right\nnow in your uh in Flight world is\nbasically you have a decorator in your\ntask\nand you you specify how much resources\nyou're going to be using and this\ntranslates into the kubernetes world of\nhow it specifies a resource and limits\nfor various resources and uh\nand you can also specify gpus in here\nand uh and you have kubernetes\nconstraints of uh resource and limits\nbeing respected whenever you have your\nflight developers learning so you could\nalways imagine like if you're consuming\nsome of these uh if you're over\nconsuming resources uh then you would\nhave your flight workflows running uh\ninto om issues and\nso\nso how how do you exactly choose some of\nthese resources some of the existing\nparadigms like people just use whatever\nif they're using some post from an\ninternet or a Blog uh and an example uh\nuh starting algorithm is something that\nthey will use whatever is available from\nthe author and they use that and or or\nthey just pick a good enough number to\niterate on and usually what happens is\nthen you start uh iterating on uh your\nnumbers for your resources and uh until\nyou find the right number to run your\nworkloads you keep changing those\nuh but it would be good to have like a\nuseful data to understand like how how\nyour workloads are behaving to a right\nsize or all your requests and limits uh\nfor your various resources in your\nflight uh workflows\nso this is usually like how the memory\nallocations and uh like I picked one of\nthe examples like where uh the workflow\nis having a certain configured limit and\nyou have uh you have a request being\nspecified and then uh it constantly has\nan increasing memory and then it reaches\nthe limit and where kubernetes just\ncrashes uh it just terminates the part\nbecause consuming all resources so\nhaving such a monitoring helps to\nunderstand how uh how you're consuming\nresources and what exactly do you need\nto change in your workloads uh\nunderstanding the pattern of your\nallocations\num would would help a lot\nand as I mentioned earlier this this is\nalso helpful across if you are working\nacross different versions of your\nworkflows like you have uh a version one\nagainst version two of the workflow\nwhere you have uh more utilization and\nyou're getting uh some of the workflows\nare becoming cost prohibitive so you can\nkeep history of your resource\nutilizations and uh and compare those uh\nto see what what change\nlet's go over like how do you go about\nsome of uh doing some of these like you\ncould uh design your own Monitoring\nSolutions and there are different ways\nyou can do that there are uh Cloud\nproviders like grafana datadog Splunk\nwhich uh which have their own uh version\nof monitoring they usually have like\nagents that get installed in your Cube\ncluster and you have uh you have like a\nconfiguration which allows you to scrape\nyour workloads and then it has a remote\nright which actually publishes those\nmetrics into their Cloud Solutions which\nuh has a person which which are\npersisted\nit comes with its own visualization it's\na managed solution so you have lower\nmaintenance of doing any of uh\nmaintaining that solution uh usually the\ndisadvantage is the cost basis of these\nSolutions which is like the amount of\nmetrics being uh sent to the cloud\nprovider is usually how you get built on\nuh on other ways you just build your own\nsolution you just uh you have all most\nof these are underneath there would be a\nmonitoring solution like Prometheus uh\nand you could have have it managed on\nyour own in your kubernetes cluster uh\nprovide a persistent mechanism if you're\nrequiring a data renewability and and\nout of the box like there are open\nsource tools to actually visualize these\nMatrix the governor\nUI as well now whenever it comes to the\nflight world you want to be able to\nunderstand how to map your flight\nworkflows to the kubernetes pods\nunderneath like as you know like each is\na cumulative solution you have all your\nworkflows underneath would be uh spun up\nscubing it is part by propeller so uh\nyou would want to understand how this\nmapping exactly happens\nthis is one of the accounts like how\nsome things can go wrong if you're not\nuh\nproperly configure your monitoring\nsolution this was uh something that a\ndata log built for coinbase where they\nhad been emitting metrics and that uh\nmetric commission like you had like a\nhuge bill which uh was a big grower on\nthe community so having like the right\nsizing and control over uh what things\nare being sent to the crowd or even in\nyour local monitoring solution is very\ncrucial and and also utilizing enough\nresources for your monitoring solution\nas well\nuh here I will just go over like uh how\ndo you build about like a self-hosted\nsolution like you can set up Prometheus\nin your flight cluster there is an open\nsource Community version uh which has\nits own published Helm chart uh\navailable and you uh you configure it uh\nto scrape some of the the metrics uh\nlike you have kubernetes state metrics\nis another open source version which\ngives you information about like the\ndifference uh Cube objects and their\nstates across time and then you can use\nthose along with some of the\nother charts like GPU metrics and uh\nconfigure your\nPrometheus to escape these uh metrics uh\nfor a certain interval and uh you can\nhave uh have these exported and be\nvisualized in your uh in your cloud\nprovider or your own uh installation of\nthe visualization tool that you have\num here I've just gone over some of the\nlike things that you need for monitoring\nlike mostly resource utilizations like\ndecision exporter is mostly for GPU\nmetrics it comes from Nvidia and they\nhave an open source version for doing\nthat uh it's a very simple it comes up\nwith its own demon set which gets\ninstalled on the gpus on the GPU nodes\nand uh it scripts uh whatever workloads\nutilize those\num\nyeah and\nthis is some of the uh other things like\nuh you have CA advisor also which gives\nyou resource utilizations earlier this\nwas a community project which was\noutside kubernetes and people used to\nhave to install it to get like uh uh\nreal-time utilizations of like CPU\nor memory utilizations now it is all\nmerged into uh the kubernetes cubelet so\nyou get all this out of the box you just\nhave to configure your Prometheus to\nhave a target for CA advisor and you\nhave metrics being available from all\nthe pods that actually run uh on your\nkubernetes cluster and\nokay\num once you have like have all these\nmetrics you want to have some way of\npersisting if you want to uh\nif you have a need for uh surviving\ncrashes and be able to still use some of\nthe data which is scared by these uh\nmonitoring tools so there are ways like\nyou can send it to the hosted solution\nor you have your own persistence for it\nand like I mentioned like there are like\nwell-known grafana dashboards which you\ncan use it's uh widely available for how\nhow these are basically like prom ql uh\nqueries which you can just uh use in\nyour uh uh visualization tool and mostly\nmost of the uh tools have promcule like\nquery and you can use that to monitor\nmore stuff internally we have uh we we\nhave a published uh uh in our open\nsource uh Reaper we have like a flight\nuser dashboard which gives you a direct\nmapping from your flight workloads to\nthe kubernetes uh pause and this is one\nof the examples like it goes over uh uh\nthe resource consumption of the memory\nand how it uses like\nyour project domain which is a flight\nconcept and the workflow and uh computes\nlike it gives you a query which uh gives\nyou the memory utilization percentage uh\nfor your um for your flight workflow and\nit uses some of the existing mechanisms\nthat are there in flight to tag your\npods the right way to extract this\ninformation and uh Community can use in\nthis as a tool to\ncreate their own monitoring\nuh\ngoing over the like uh maintenance\nissues like you have to make sure that\nyour monitoring solution itself is\nmonitored so that doesn't om like\nusually like you have like card you're\nnot you could run into card like the\nissues where you have like yes scraping\na lot of metrics and which have like\nhigh cardinality uh they have issues\nwith the right-handed log being uh over\nconsuming your uh your monitoring\nsolution this is like uh usually a sub\nprocess in your monitoring solution to\nsurvive crashes uh you want to be able\nto manage your storage for your metrics\nthis is another thing that if you want\npersistence and also bright sizing all\nthese resources for your own monitoring\nsolution so that it behaves the right\nway when uh when you want your data\nyeah I mean just to recap we went over\nlike why monitoring is needed\nhow do you specify resources in flights\nand different ways you could design a\nmonitoring solution and the benefits and\ndrawbacks there are uh ways there are\nmultiple other ways you could be doing\nthis but this was one of the things that\nI found like would be useful for the\ncommunity\nyeah that's it from me any questions\nthat was great thank you is there any\nquestion right now\nno\nyeah probably I I have one uh in your\nexperience profula will be those key\nsignals those key metrics\nwe should take into account when\nbecause I I feel like yeah workflow and\ntask Etc I meet a lot of uh events but\nyeah besides probably the platform level\nmetrics of CPU memory and all that stuff\nwhat do you think are other\nkey signals or key metrics which will\npay attention to\nso there are so other than your resource\nutilization metrics like uh CPU memory\nconsumptions and how your request and\nlimit which are being said for in your\nkubernetes boards and how they keep\nchanging along with that just monitoring\nyour flight platform there are ways you\nthe flight platform itself emits metrics\nuh which are there like you can have\nmonitoring for those like how how many\nerrors that you're receiving uh for your\nworkflows or what is the rate of uh like\ncreation of pods in your kubernetes\ncluster and uh if there are any errors\nthat are happening so this would be\nspecifically two uh like how monitoring\nyour flight platform itself like your\nflight workloads as I mentioned like you\ncould uh there are various metrics being\npublished for CPU GPU and also like GPU\nhas like a lot of\nuh other metrics like you have the\ntemperature of your GPU how well you're\nutilizing the reads and rights uh uh for\nyour GPU so that that defines like how\nhow well you're optimizing your GPU\nresources so those could be like useful\nuh metrics to take out and understand uh\nhow your workloads are behaving and uh\nit's a very uh\nlike the community of like uh other\nmonitoring solution have made it very\nsimple for uh these metrics to be easily\nbe consumed and uh through like prompt\nql is a widely used way of querying\nthese metrics and\nso\nyeah\nthat's usually like what I find like and\nthese are like usually updated like if\nthere is a new metric that is being\npublished uh by and let's say the Nvidia\ndcgm exporter it would be available in\nthe in their version and you could\nutilize that to get more information\nabout your workloads\nall right\nthanks so much for fulan no other\nquestion\nanyone thank you\nyeah thank you a copy of profus like the\nkeys right there in the agenda dock\nor for the reference\nthanks so much okay next up in the\nagenda I'd like to welcome Jan fidler I\nhope I'm pronouncing right thank you for\njoining and we will share their Journey\nat enew.ai how they've been using flight\nso welcome Jan\nthanks for sharing your knowledge\nthrough the community\nthat's me uh thank you so much for the\nfor the introduction\num thank you for having me\non the screen\nyou should see stuff right\nyep\nsweet\num hi to all I'm I'm young I'm a machine\nlearning operations engineer at a\ncompany called kineo AI\num I'm working now for this company for\na couple of years and yeah my day-to-day\ngoal is really to provide the best tools\nit is to my data scientists colleagues\nand today I want to talk about our\nflight Journey so we're actually using\nflight in production for I would say a\nmonth now\nand if I think about it flight has been\nreally my life the last couple of months\nso first we kind of figured out how to\nrun flight on AWS then we dealt with the\nautomation of our deployment and\nintroduced by to the company and lately\nI'm also migrating to floor pipelines\nmyself to slide workflows which brings\nyou are which also got to be kind of a\nnice user experience\num and yeah this brings me to the agenda\nfor today I want to quickly introduce\nkaneo so who are we what are we doing in\ndetail\nthen like I said\ntell you about the story of migrating\nfrom YouTube Flow to flight so what was\nthe first idea what is the purpose how\ndid it go\num\nthen I want to point out our\ninfrastructure setup so as you can see\nwe have shown the path of the\nmulti-classer deployment and\num yeah I'm quite happy with the setup\nso far um also how how our Engineers are\nusing flight and I also think it's\nsomething special about our setup and um\nor maybe maybe not I'm not sure I'm\nconfused to hear from you\n[Music]\num\nso let's dive into it so this is us um\ncan you AI a bunch of\nreally cool people I think some of them\nare even in the calls\num we are a little startup in in Berlin\nin Germany\nand yeah we have three people on sales\nthree people an infrastructure team and\ntruly myself and the rest are data\nscientists and basically we are\nproviding customized ice solutions for\nour clients in Germany\nand as a result we work in many\ndifferent projects in many different\ndomains many different use cases but\nalso depending on the situation at the\nclient we also help with like\nimplementation of infrastructure to meet\nthe requirements of individual needs of\ndata machine learning use cases\nand really all in all our goal this is\nto build an AI a partnership with\nclients so really helping them on their\nAI transformation and also on their\nworkout jobs so like I said many\ndifferent projects many different\nclients and just to give an idea what we\nare doing in our daily business it's a\nlot of forecasting a lot of object\ndetection in the computer vision domain\num as well as production planning\noptimized through machine learning so\njust to give an idea what we what we do\num yeah so why did we why did we switch\nwhere did we migrate\num first of all\num we use the following slide to also\npitch slide internally so don't be\nconfused with whom I want to address\nwith these with these slides um we're\nusing Cube for now for almost two years\nI would say\nand yeah while we when we use this tool\nwe just identified what an extensive and\nyou can see now the main pain points on\nthis on the slide but what do I mean\nwith notebook code does not equal PayPal\ncode so let me first tell you how\num an AI partnership with our clients\nusually look like so we start usually\nwith a really small and fast project\nwhere we just get the data Tinker around\nthe Jupiter notebook and show business\nvalue and after this project phase we\nput the code into a pipeline and build\nan MVP\nand exactly at this project phase we\nhave the need for an orchestration tool\nand yeah for some reason just this\nconversion of notebook code to pipeline\ncode was just was us taking a lot of a\nlot of time\num yeah also doing release sample stupid\nstuff like if else statements and Loops\num it was not like learning a new\nlanguage but it took us it took us time\nanother big big point is\nthat we were basically not able to\ndevelop locally so we had to submit our\nruns during development against a huge\nkubernetes classes in the cloud\num and this sucks because of we are two\ntwo reasons so first one is the engineer\nalso needs some kind of infrastructure\nknowledge or support from them from an\nmlops colleague and we are also\ndependent on the cloud right which is a\nsector of costs as well also a factor of\nspeed for our development Cycles\num yeah just for something you know it\nslows down basically\nanother thing which is yeah\nit's used what we're doing in every\npipeline we are passing data right and\nfor keep flow we always needed to\nprovision the volume so some network\ndrive mounted to every pot and read and\nwrite data from this specific part and a\nreally long and complex pipeline you\nended up with the need of memorizing a\nlot of\nlocal paths on the volume of this yeah\nit's hard to keep track of at some point\nat last but not least in our old setup\nwe had this\ninfrastructure set up really where we\nspin up Cube flow instances for specific\nprojects and customers only when we have\nsomething scheduled in production so if\nit failed in production then the\ninstance also shuts down automatically\nwe cannot even see the logs and yeah for\nan engineer them to figure out what's\nwhat's going on wrong yeah you can\ncalculate an hour to spin up this Cube\nfor instance again and yeah and all the\nsummarize really feels like like this\njust uh yeah a little a little slow here\nand there for just for just putting\nJupiter look who called the Blue\npipeline\nand I we spent the last quarter\nbasically to where I look left and right\nagain in the\nin the open source world for\norchestration tools and\num\nyeah looked at a couple of requirements\nand compared them and yeah sixth floor\nslide and prefects made it in the end\nfor the top three and I'm not sure if\nthis graph makes so much sense but yeah\nyou can get it flight made it if I made\nit a new Challenger in fact the area\num\nand as you can see our main pain points\nwere pretty much are pretty much covered\nwith flight and maybe this is also this\nopportunity to say thank you to everyone\nto the community\num we are really really happy with this\ntool so far\num\num and now we have all the holy ghosts\nthat we all the good stuff we we wished\nfor so I think the most important\npossibility is to\ndevelop fully fully locally so really\njust installing flight kits connect to\nAWS with our own credentials and start\ndeveloping just without waiting for some\ncluster or some analogs and Ops\ncollector assists\num we can now say notebook it almost\nequals pipeline code it really feels\nmuch more pythonic in a way and\nEngineers can almost\ntheir copy functions one-to-one from the\nJupiter notebook set the task operator\ninside the task decorator adjusts sub\ninput and output and that's that's\nbasically it so this basically really\nreally nice\nalso pressing data is a pleasure since\nwe don't have to care about lock repairs\nanymore and also\nflight files such as the concept of\nfiles by direct views it makes our life\neasier on this one\nand last but not least um yeah we love a\nlot of the features we have now in setup\nwhere we have a control plane which is\nalways always online so also this last\npain pointers\ncovered\num\nbut of course there's there's more\nthere's more what we love\num and I will cover the costs in more\ndetail when you're following the slides\nbecause it's quite an important topic to\nus\num but even now with the setup of having\nour orchestration to always online we\nexpect the class to be half of it but\nyeah like I said I will speak about it\nin a minute\nauthentication this was a problem for\nfor keep slow used\num kubernetes authentication through\ndecks\num\nand just basically that we let Angeles\nended up with one password for each\nproject and yeah you can imagine how\nthey are one password looked like after\nsome time and now it's really really\nPleasant to just connect through this\noidc providers use the credentials that\nwe already have no password juggling um\nit's something small but yeah less\npasswords I mean\nand who doesn't like it\nlast but not least the the ecosystem I'm\na rich fan of map tasks\num\nI also heard rumors in the data science\nscene that nobody actually likes to\nwrite a spark job\num yeah we will probably use it in the\nin the future still for really big data\nbut it's still nice to know that there's\nsomething in between so I'll just put\nthe map tasks we also like great\nexpectations so that's negatively\nsupported even though we are missing\nkind of the data docs maybe as a flight\ndeck but I think I've seen this\nsomewhere already so probably I've seen\nfor the future\num now we're doing this on our own\num\num but to sum up in Cube flow we develop\nand maintain a lot of cube flow\ncomponents for ourselves and this need\nis now much much smaller because of like\nyeah later features and Integrations\nrequired which is which is awesome\num yeah like I said costs um 90 of our\nprojects are happening in the cloud so\nof course we are really interested in\nthe spend as little as possible for just\na tool\num and I think uh flight yeah I think we\ncould we were able to reduce the ec2\ninstance at least one or twice\num but actually this base operation cost\ndoes not matter too much for us it's\nmore more about the costs the cloud\ncosts\nduring the development so just being\nable to develop fully locally our AWS\ncosts drop drastically and to give you\nan example\nthis is in a report I mean historical\nreport of us developing a data pipeline\nfor two weeks\num and you can see almost it's just ec2\nit's just easy to it's just running\ncheap for cluster and\nand this course is going to be greatly\nreduced now so on this obviously also\nscales with developing and machine\nlearning forecasts\num\nI'm not saying that all costs go now\ndown to zero of course not\num but how now projects are going what\nwe have seen already now from the team\nis that we develop with the subset of\ndata make use of the great caching of\ntasks and then only really in the last\nday of a project we take the retake the\nworkflow online basically and which also\ngoes really really fast because we have\neverything in the terraform module that\nI will talk about it later\num and so we can basically reduce the\ntime being in need of this kubernetes\ncluster from maybe 10 days to one or two\nbasically and\nthis is great of course\num in addition to costs for cloud\nresources we also decrease costs for\npersonal resources and people just\nmigrate keep flow pipelines in a couple\nof days so this is what we what we have\nseen now in the team but also new join\nus are adopting to flight super super\nquickly\num it's really yeah because we're just\ndealing with python it just enables the\nengineers to be much much faster\nall right\num then let's talk about our\nmulti-cluster setup\num like I said we are having many\ndifferent projects with many different\ncustomers and this leaves us with a\ncouple of major requirements to our\nworkflow orchestration tool and our\nsetup in general so the first one would\nbe we would like to relate to Engineers\nwithout major infrastructure knowledge\ntools pinup tools as fast but as\npleasant as possible\num we want to keep the data the customer\ndata still any dedicated accounts\nand um\nof course we want to be still cost\nefficient as possible and what we did\nnow\num is also documented and recommended\nfor flights or to go for this\nmultichester setup and we have now the\ncontrol plan with the flight admin\nscheduler data catalog and\nI'm using some other service and this is\nonline\nto in working hours so it's already it's\nall accessible\num in some key Neo Central account\nand the data planes\num these are of the site propeller these\nare really just getting created and\ndestroyed\num dynamically when there's something\nscheduled and this um\nDynamic registration and gig\nregistration from the data plans to the\ncontrol plane\num yeah it's really working out quite\nwell for us\num it's really just creating a class on\neks installer to propeller on it and\nthen dynamically register it to the\nworkflow deregister and shut down again\nand\nyeah the benefits of course are just\nbeing super super lightweight on the\ncontrol plane still keep the data\nisolated in the data plans\nI'm not sure if it makes so much sense\nto connect the new data plane on Azure\nto learn\ncontrol plan on AWS but I think you\ncould do it and we are fully in AWS now\num but\nbut we definitely will do in the future\nis like having a data plan on on-premise\non-premise and just in a keywords\num\nour control plane in the cloud and the\nlittle crunches on design but I think we\nhave the time for a little\na little demo if you don't mind\num yeah because I would just want to\nquickly show you this really this\nDynamic registration degree registration\nof a\num for the data plane to a control plane\nso\nlet me quickly show you\nthat I'm connected to AWS I mean but\nyou're getting the YouTube config for\nthe control plan\nand then show you\nand hopefully empty cluster config\nenamel file so yeah that's all\nthe control plan without any data plans\nconnected to it and I have\num a code pipeline in the in the\nbackground so this is\num one resource that comes out of our\nterraform modules that this basically\ntakes care of the automated creation of\nthe data plane so first let's\num\nspeak\num basically it checks out the\num the repository enable some data dot\nmonitors and then like I said really\nimportant a cluster installed propeller\non it and yet and register the data\nplane to the\ncontrol plane and update the execution\ncluster level this basically tells them\nto slides\nadmin okay this data plane belongs to\nthis specific flight project and this\nspecific slide domain\num\nlet me show this\ntakes a while now I also have\nsome workflowing around\nit is so here we are it's it's connected\nand uh yeah now I put\nreally use it in just\nconnect to this\n[Music]\num\nor just with my\nfive flight run and it should be\nit should be there\noh and it was not\nit's um that's unpleasant\nlet's see\nuh did you see it let me know\nuh hey Jen\num so it's it's complaining about the\ncrd definition itself not in the cluster\nand that gets created when propeller\ncomes up so I'm guessing propeller isn't\nup yet\nforeign\nbut I thought this record kind of\nshould be safe in the database so I\nthought I could skip it\nbut let's give it another try if not um\nyou have to you have to trust me\nwe trust you\nyeah you trust me perfect and you just\nfeel good so here it is I was in the\npast indeed until we see all investment\nso\num we're just running\num let's start let's not wait until\nlet's finish this workflow so this was\njust uh\na little demo\num let me continue\num yeah to make our lives a little\neasier we put everything into terraform\nmodules that are taking care of the\nautomatic deployment of Control Data\nplane and what's really exposed to the\nuser to the engineer is the air Factory\nproject module we call it\num\nand this is\num yeah the idea is really to use this\nmodule and hide any complexity of\nsubmitting Workforce to fly to the\nengineers so the engineers should focus\non developing and other Solutions are\nnot\nlearn how to use my flight run or\nprivate register so Engineers can go\ncheck this air Factory project module in\ntheir discus repository developed\nlocally until the workflow is looking\nall right and provide some inputs to\njust through this module terraform apply\nand again you get to get a proper\num machine learning cicd pattern out of\nit so yeah it's also checking out your\ncode the new base image for\num for propeller let me test image\nrunning tests and yeah packaging the\nlaunch but registered so\nyeah that's the idea to not worry about\nanything as an engineer and this is\nlooking out great so far\num yeah to really sum it up now and I\ntalked a lot about a lot of successes\nalready so I'll make it quick so we are\nstill Cloud independent which is really\nnice we stayed in doing kubernetes\ndomain\num so no knowledge is lost from there\nless dependent on external tools like I\nsaid Great Expectations spark\num less costs greatly a greater\nreduction all in all we see a great\nreduction also in in code so like I said\nwe migrated from YouTube to flight and\nyeah I don't have numbers but it's just\nless than this but it's always nice\nfaster development cycles and most\nimportant thing happier engineer so the\nfeedback the internal feedback so fast\nreally\nuh really amazing\nmaybe some feedback really really quick\num we had the feeling that the\nnotification setup filed a little\nawkward tool to set up and uh we were\nalso with the biggest fan of\nhaving the notifications with a mail on\nslack we are currently kind of abusing\nthe SNS sqs of flight admin and send\nnotifications ourselves using a Lambda\num\nit would be great to just have support\nfor web talks for common messages like\nselect all Microsoft teams\num but I think this is already also in\nany pipe I've seen it somewhere on some\nchat\num if feedback from the from the\nengineers what that the flight cut error\nmessages are sometimes a little\nlong and yeah\nhard to how to identify it's mostly if\nyou forget to set the task pick writer\nor\nsome inputs value of some task is not\nproperly set then it's really hard to\nfigure out but first but mostly it's\nalso\nsomething rather small but something\nbigger is of course some role-based some\nsome system of yeah\nsome well there's like a system for five\nprojects because we have another problem\nwith\num customers that they want to\nwant to collaborate with us and then we\nhave to spin up\num on individual control plane because\nyeah they probably should not see the\nother projects\n[Music]\num\nthis is about the feedback\num\ndivision between Sun setting and keep\nflow so this is still ongoing and we\nreally don't want to support the flow\nand flight is yeah it's obviously then\ngetting rid of the last Cube flow\npattern but another big goal is\nlaunching flight on Azure so basically\nhaving the same setup that we had on AWS\nwe have also in Azure because we have a\nlot of customers that I like\nat home at the Microsoft domain and then\nyeah if they don't want to links to AWS\nwe may be able to\nyeah dynamically create and delete the\ndata planes because the way it is right\nnow is that we have to take off two\nschedules so the schedule of the actual\nworkflow and key schedule of the\ncreation of the data plane and we had\nthis idea of adjusting like admin in a\nway to\npublished scheduled runs in some queue\nand then maybe trigger decoration of the\ndata planes but\num this will be let's see in the future\nif this is really worth the timing\nadjustment let's see and last but not\nleast um flight plugins so we are not\nsure if we should use something maybe we\ncan also cover this now in a second\num because our current approach to not\nreproduce code I mean workflows is to\nuse just functions from our own pipe\nserver and then use it in tasks and\nstill that the user decide on task\nresources caching and averages or no and\nall these things and we are quite still\nnot sure I haven't figured out if my\nluggage really make our lives easier on\nthis one yeah\nto discuss it\nand I thank you so much this was it\nthat was great thank you again\nany question comment\nI have a I have one question\num\nfor\num flight I'll be stoked if I could\nconnect with you to get you know more on\nthe flight kit error messages because\nthis is one era that we've been you know\ninvesting in it would be great if we\ncould cover more cases so\nI can ping you offline it's it's all\nright we don't have to go through it now\num\n[Music]\nso hi yeah and this is Martin this is\npretty amazing to see uh what what you\nhave done in such a short amount of time\nI'm with Union here uh I have interested\nquestion so you might have answered\nearlier but I might have missed it but\nhow long did it take for you to evaluate\nI think you said like something like two\nmonths uh the prefect versus slide\nversus group flow and who was involved\nin your evaluation there wasn't just you\nor was it a bigger team can you tell us\na little bit about you know who made the\ndecision at the end of the day\num yeah\nI'm not social but the wedding decision\nquite quickly I would say maybe also\ntwo or three weeks and uh yeah it was\nbasically me and two other colleagues\nfrom the mop scene\num\num and to be fair I was a part of the\nresearch\num which was done by someone else but um\nyeah\ntwo to three weeks for figuring out and\ncomparing all the requirements and then\nthe three of us did it fantastic and\nthen uh on I mean you used um kubeflow\nbefore how much of the code were you\nable to reduce when you're brought over\nkuflow tasks to fly them it was that a\nreduction of boilerplate code that was\nsignificant to you or was it just like\nthe same amount of codes that you came\nup with writing\num I would say I think he'd probably say\noperations and things like we say tasks\nand this basically stays the same right\nso the logic has to be the same but the\njust the entry points or the pipeline\ncode in comparison now to the inside\nworkflow code\num this is almost half half of it only\nonly having for kubeflow um like I said\nwe had our own components and the thing\nwe use the mouse is just a basic AWS S3\ncopy component right so we download\nstuff upload stuff and just have enough\nfiles and that directory it's just not\nusing this one specific hipster\ncomponent makes makes half of it\nbasically\nfantastic you're gonna last question so\nI don't want to occupy time too much but\nif you would work with I mean you might\nwork with you know data scientists who\nonly just focusing on statistical\nmodeling and and you know being one of\nthe data science and on the ml side uh\ndo you think you know flight would be a\nmore appropriate tool to get into this\nbecause it's more python or or do you\nthink it's still a hard step for those\nguys to get into and workflow\norchestrate and run their notebooks and\nget their code orchestrated\nright answer but it's just much easier\nso this I mean we are still like I said\nwe are still in the face of migrating\num still migrating pipelines to\nWorkforce\num so not every engineer in the team has\nexpanded so far but the few with the few\nfeedback I got is amazing is amazing\nthey already would say yes I am I'm\nfeeling more comfortable in the cloud\ndomain and iron faster so\nit's an important question because what\nwe see is like companies who are that\nare in research and have a lot of data\nscience for them to step into an\norchestrator is always uh you know it's\na big step because it requires so much\ninfrastructure knowledge and other\nknowledge and to reduce that hurdles is\na really important thing to let make\nthem feel comfortable running their code\nthat's why I'm asking that question\nokay great thanks a lot\nall right great thank you Martin any\nother question comment\nyou know yeah well I think we're over\ntime thanks again John it was a great\nsession we learned a lot we'll make sure\nto follow up with your feedback keep\nimproving the project\nand yeah with that thank you all for\njoining don't miss the next episode\nwe'll have Dr fabiogratz from recogni\nalso sharing what they have learned\nusing flight\nwith that thank you all\nfor joining see you next month thank you\ngoodbye"
    },
    {
        "title": "Flyte Community Meeting - May 30, 2023",
        "transcript": "into the flight community meeting today\nis May 30 and David Espeon happy to be\nyour host today\num there are a couple of reminders first\nthis meeting is being recorded and it\nwill be posted to the flights YouTube\nchannel\nand second thing is that this meeting\nfalls into the Linux Foundation\ncode of conduct\ngreat\num yeah this is special day I guess we\nhave some new members and again for\nanyone who's just joining\nI'm sharing here the link to the agenda\nand hopefully\nuh very complete notes\nfeel free to add yourself to attend this\nlist including your affiliation that\nhelps us to not only know who attended\nbut probably continue the conversation\nafter the meeting\nit's native and also have any question\nor discussion topic you want to see here\ncovered in the open mic section I will\nshare my screen real quick\n[Music]\num\nthere you go so yeah I will go first uh\nyeah\nyeah good okay anyone joining the\ncommunity meeting for the first time\nI think\nyeah besides our top contributors\nI see Mike sorry to call you out is this\nyour first time here welcome yeah it is\nawesome would you mind brief entries in\nyourself\nuh so I'm Mike Morgan I work at Fidelity\nat the moment and uh we're trying to\ndeploy flight we're kind of uh poc in it\nawesome yeah thank you for for trying\nhopefully we'll be successful and yeah\nwe're here to help you thank you for\njoining\nthank you awesome\ngreat\nthat's awesome all right let's get\nstarted uh let's move to community\nproject updates uh first thing that's\nnew since last meeting is the fact that\nwe finally have in place the kind of the\nstructure and process uh for Community\ngroups uh basically if you have seen\nother communities other projects and\nother organizations having what they\ncall working groups or special interest\ngroups is\num is a it's a really nice way to\norganize a community and have a narrower\nfocus and more clear goals and\nwe were I think lacking a structure for\nthis this is the first version if you\nsee something that could be better or\nsomething that is not clear feel free to\njust give your feedback and everything\nis there in the in Community repo so\nbasically the idea is that special\ninterest groups cover project wide\nconcerns\num\nor domain specific concerns like for\nexample how to improve the integration\nwith XYZ uh component with with another\norchestrator with another\num solver piece in the in the ml space\nor in our data space so those are six uh\nthey are supposed to to outlive the\nprojects they try to tackle they are\nopen to anyone uh to join and contribute\nand learn\nand we also have working groups that are\nkind of a lightweight version of a sick\nwith a specific deliverable specific\ngoal and after uh the the original\npurpose is Achieve the basically are\ndisbanded\nso right now I'm happy to confirm we\nhave the first official working group\nThe config lover rights working group uh\nsubset of the the community join it\ntogether to implement this proposal\nand uh once it's deployed implemented\nand communicated well it will be done\nso um feel free to ask any question if\nyou want to form a group uh the\nrequirements and the responsibilities\nare described there is nothing too big\nwe try to make it practical\nuh but it's still a structure and yeah\nhopefully we'll Foster even more\ncollaboration there are many\nareas where I think this could be useful\nif you have any comment right now that's\ngreat\nall right uh next up office hours we are\nnow approaching if you remember I don't\nknow a couple months ago we moved the\noffice hours space to be on demand\nso you can use office hours to have\nscreen time with maintainers and ask\nquestions and you just need to book them\nusing calendly\nbut right now we are trying to approach\nit uh with with a team with teams per\nslot uh so you can see there in the\ncommunity page for example if you're\ninterested on anything flight propeller\num or anything regarding flight\npropeller you should try to book it on\nthe 7 A.M Pacific time and Slot where we\ncan\nsecure the availability of a maintainer\nuh that has experience in this area and\nso on and so forth uh we're trying to\nhave this uh to balance the availability\nof maintainers and also give you the\nbest support experience and\num again if there's any feedback we're\nopen to\nto here\nall right uh yeah good news flight is in\nthe hogging face dogs this is fresh out\nof the oven it's in the callbacks uh\nsections so the trainer can use the\nfollowing callbacks including this tiny\nlittle thing called flight callback\nwhich is great I don't know if anyone\nhere would like to briefly chime in and\ncomment\non why this is interesting why this is\nimportant right now\nuh yeah I think Evan I don't know if\nEvan's on the call right now but he did\na lot of the work\num to push this through I think also\nwith help on on review with uh James\nSutton so yeah shout out to both of them\nfor\nyeah working on this this is an awesome\nfeature basically I don't know too many\nof the details obviously I just got back\nfrom vacation actually so I'm like\nreading this myself but essentially a\nflight has flight decks and checkpoints\nand this integrates those two\nabstractions with Transformers the\nTransformers Library\num so as you can see from the\nthe um code example there\nyou define like this checkpoint object\nwhich is a flight abstraction you\nregister the flight callback on your\nhugging phase trainer\nand it you know automatically as the\ntrainer uh goes through epoch's uh\nrounds of training\nit will use the flight checkpoint\nto save your model state\nand so you know for example if you're\nusing spot instances or something\nand you need to resume from a certain\ncheckpoint\nthis will work nicely actually with spot\ninstances and and retries on the flight\nkit infrastructure\nawesome\nthank you Niels yeah I think there will\nbe a follow-up blog post\nfrom the very authors of these\nintegration so yeah\nkeep an eye out on this\nawesome any other comment\nnope\nall right okay next up I think yeah next\nsection is the this section is always\nopen in the community meeting is what we\ncall show and tell\nuh is a space for you to share with the\ncommunity what you have built using\nflight\nand you don't have to to prepare slides\nor something like that but we always\nhave uh contributors who go uh the extra\nmile and prepare slides and demos and\nthat's awesome\nso\num first of all again thank you and\nshout out to our top contributors of the\nmonth you will want you can glue Environ\nSue\nand we're lucky enough to have them here\nuh\nuh yeah I think all of them\nso um yeah thank you again\ncongratulations and uh they're kind\nenough to share with us details about\ntheir contributions so I don't know from\nthe three of you who wants to go first\nuh I can go first\ngreat welcome you will you mind briefly\nintroducing yourself before yeah yeah uh\nlet me share my screen real quick\num yeah\nthank you yeah\nall right\num\nare you guys have a seat\nyeah okay awesome so my name is Yobo\num I actually work at LinkedIn uh same\nteam as Byron which you all should\nalready be pretty familiar with so um we\ntogether are working on uh\nproductionized by Ellington\num so\num I actually did some contribution\num while we were just onboarding\num\nso two contributions I had one is\nsupport the union and non-types in five\nCTL so actually\num when we were so we had a lot of tasks\nthat accept types that can be Union of\none thing and none or one thing and like\nfloat and integer\num that was not supported so if you do\nflycity I'll get execution you'll get a\nfail to convert to a known literal\nand saying that it's not supported\num so basically Union type\num\na flight is actually just\num a repeated well a list of variants\num you can imagine it's just uh variance\nof uh of different literal types\num so in this case\num what do we just I discussed with the\nfights Union AI people and we decided\nthat we can just simply use the first\none so if we create a literal we can use\njust the first one first variant that\nmatches with the uh the type of that\num of that input\num and if there are multiple matches the\nfirst one would be taken so if the input\nis 1.0 and we're trying to make a\nliteral and the unit type is Union of\ninteger and Float uh integer will be\nused this is 1.0 will match with integer\nas well\num\nsince there is like no other good weight\nof like doing exact match on which type\nis but this works pretty well if it's a\ninteger and non or float and on\num this is the first contribution I had\num the second one is actually\nrefactoring the copics of keep flow jobs\nplugins so uh flight\num The Wave flight supported uh all\nthese three jobs uh pytor's Chef job and\nMPI job are all through uh doing keep\nflow plugins\num\nso when we were actually so we in within\nLinkedIn we have\num a lot of use cases uh using kef Java\nand API now we're getting into pytorch\nso we found out that um you cannot\nactually set the resources for uh for\nthe specific uh replica groups for\nexample you cannot set launcher to a\nspecific resource and worker to a\ndifferent one\num\nin MBA jobs\num\nand also you cannot set different\ncommands uh in in different work groups\nsorry replica groups so um we actually\njust to enable this we actually allow\nusers to set images resources replicas\nrestart policies\num all different replica groups and also\nsetting the Run policy for that kill for\njob\num\nso\num we had to do a bunch of refactoring\num so actually all these uh plug plugins\nshare uh some common\num\nsome some common uh properties like\nrestart policies and um and clean pop\npolicies those so they all be put into\num a single uh a single comment Proto uh\nin flight edio and\num all three jobs actually depend on\nthat\num and we spend\num some effort on supporting the\nbackboard compatibility so actually when\nI talked to Dan um then think it's\num we better not\num\nbreak the current user's Behavior\nbecause\num there are a lot of people using this\nfeature so we actually used\num the task type version which is\nbasically a version for the task\num\nso it was recently introduced uh by\ndefault it's set to zero and\nso we have we have a if else statement\nso if the\num if the task type version is set to\nzero we just do the existing behavior\nand if the task type version is one we\njust handle it handle the new idls that\nwe created\num\nso in a fly kit side I actually we also\ndid\num backward compatibility\num support so we actually include the\nold configs um into the new one so user\nif they set the new ones it will\nactually override the old ones\num but if they only set the older ones\nit's\num still working\num yeah so that's two of my\ncontributions\num so\num thank you guys\num and shout out to Daniel Uh Kevin and\nadodor\nuh Edward though yeah so um actually I\nwould like to say\num flight Community is\num is really\nit's really like people are really\nhelpful and\num really willing to explain things and\ncommunicate really well so I think it's\nreally fun experience\num working with working off light and\nwith unit AI folks\nyep thanks\nthat's amazing thank you\nright any question about use\ncontributions\nand comment\njust a uh a quick question here is that\nbackwards compatibility like task type\nflag thing is this the first time we're\ndoing this pattern or has that already\nestablished so\num I think Ben used it\num back when he was doing at Ford array\nnode jobs or some kind of other jobs\num they introduced this uh it's also for\nsupporting the backward compatibility\num and we just adapt at the same fashion\nokay cool thanks\nawesome\ngreat anything else\nyou know\nthank you so much you again and um yeah\nif there is any uh portion of the\ncontribution experience that you feel\nthat you struggle a lot and you didn't\nenjoy please let us know\nawesome\ngreat who's next\nuh if possible I would like to the next\none\nthere you go I will share my screen with\nyour slides right\nand\nyeah\ndone a little bit\nhere you go there you go\noh yeah\nokay so let me introduce myself first\nlike\num yeah uh so I'm eating like I'm\ncurrently an MSS student at UIUC and I'm\nalso a very active open source\ncontributor to virus project like\nincluding the flight and also the Google\nrace so yeah I should be very glad to\nintroduce my contribution to the fraud\ncommunity\nso uh so this contribution is really\nabout the enhanced text execution\ninsights so thinking like when you have\ntext running in the cluster actually you\nhave almost no idea about what happened\ninside the cluster like for example you\nexpected attacks to be finished in uh\none or two minutes but it's actually\nfinished like for one hours and it's\nvery for the current implementation it's\nvery hard for you to figure out uh what\ncomponent causing this problem there's\nthis a flight kit problem or it's\nactually something wrong with your user\ncode so uh\nso this PR actually like provides you\nwith a detailed time information about\nthe execution time of each component\nwithin a text including the flykick\nparts and even your user divide parts\nso giving you the opportunity to\noptimize your workflows like for for\nexample what you can do is like\nuh you just import your the time ID\ntools as as you can see in the this\npicture and also cite the disabled text\nto force and and and when you finish the\njob you can see a timeline graph and you\ncan virtualize all the time information\nabout the flykid part like uh how many\ntimes it takes to like translate a\npython value to the liter something like\nthat so you can have an idea about these\nthings and also you can do uh in your\nuser defining pattern like for example\nuh for the T T1 text you can use time it\ntime time yet as a decorator like\nmeasuring the time of downloading the\ndata from Asterix right and also you can\nuse timer here to use a contact manager\nlike you have a sub sub text I don't\nknow what it is maybe filtering the data\nmassage the data so you just name it and\nyour this component it will measure this\npiece piece of your code yeah so uh\ncould we go to next slide\nthank you yeah so basically if you open\nthe after you finish the tags and you\nopen the back you will see these things\nback there you have a comprehensive uh\ntimeline graph and you can zoom in and\nzoom out so basically it tells you how\nlong it helps measuring time for each of\nthe parts like for example if you have a\nvery large list\nand it and if you're typing is any like\nthe fly kit maybe like translate to\npickle file and upload to S rewrite so\nit may be time consuming and with these\ntools you can easily figure it out like\nyou will see like\num the time to uh converting takes a\nvery long time so you can all virtualize\nit and yeah basic of providing usability\nto optimize your workflow and improve\nthe overall performance yeah that's all\nthat's awesome any question any comment\nhow do you generate the diagram\nuh so it's used for likely component so\nbasically in the behind we when flykey\nrunning the port we collect all the\ninformation\nand at the end of point we like we we\nuse the uh that components we we\ngenerate this HTML file and send it to\nthe console\nuh is this answer a question yes\nI have a question is is the is the\nflight deck\nfile that you make is that the only\nplace where this data gets stored\nuh so\nuh could you see it again sorry the the\nthis time data this these the details on\nthat the execution time is that only\nlike essentially stored in that flight\ndeck file or do you store else does it\nthere's something else so for now is\njust storing a DAC HTML file but later\nwe are consider I'm considering it to\nstore all this information and the\nspanning the uh flight at the mean maybe\nwe first send it to S3 and find the main\ncode download it\nso yeah\nuh it it could be integrated with some\nPort information right like the compiler\nhave some information about the pause\ncreation time and poor tear downtime\nyeah it could be integrated with the\nflaget Matrix\ngreat\nthanks so much g-shink any other\nquestion\nright again thanks imagination for your\ncontribution in your time that was that\nwas great yeah also I I want to thank\nKevin so he helped me a lot with this\nuh PR yeah\nyeah thank you Kevin doing a great job\nmentoring contributors awesome\nokay so I guess uh next up Byron Zoo who\nI don't think by this time he requires\nan introduction to this community and ah\nlet's get started welcome Byron and\nthank you for your contributions\num\ncan anyone see my screen\nyeah\nyeah so uh today I'm going to present my\nuh site project so I'm in uh I build a\nsmall project with length uh building\nLantern on flight and I'll explain how I\ndo it\nso yeah so I'm Byron Shoot actually\nissue like he or she\nbut yeah and then I'm currently working\nat uh LinkedIn and I'm working on the\nsame thing as ubo and we're building the\nnext generation of machine learning info\nor a large language model infra and um\nuh the an open source enthusiasm I've\nbeen working on flight and qplay and\ncurrently\nand I was previously on submarine\nproject with Kevin\nand let's see how 5gpt performs\nso I asked a question about how is fast\nregistration implement\nand I think it answered perfectly with\nall the details in the code in the\ndocument\nso it basically says that oh flag what\nis a purpose of uh for the fast\nregistration so it allows users to\nupdate workflow code without having to\nrebuild the docker image and this is\nachieved by zipping up the code and\nuploading to the S3 so this is pretty\namazing\nI'm also surprised by the result\nand my data my data set comes from two\nsources the first one is the public\nsectional and the second one is uh other\nsource code so I basically pulled down\nall the source code from flight\norganization on GitHub\nso yeah so what is tension nation is one\nof the most trending\nlarge language model library on the\nmarket now\nand land chain essentially provides an\nabstraction to combine all all kinds of\nlarge language toolkits easily\nbut uh so for example uh for the loader\nit implements tons of loader including\nthe hello the HTML loader and read a\ndark loader and for the model side it\nintegrates all kind of API like llama\ndolly and open AI\nso actually I think the idea of pension\nand flight is\nkind of similar at some point because\nthe chain is similar to workflow so how\ncan we leverage flight to view an\nattention\nand here are the steps\nI use to create my flight to PT\nso the first step is\nto ingest the documents into a queryable\nformat and the first step is to load the\ndocuments\nand this step is using a component\ncalled document loader\nand the bot will load where loads the\ndocument using different kinds of loader\nfor example I load the stack message so\nI use stack loader and I load the code\nso I use GitHub loader and the next step\nis text splitter\nand text splitter also various\nso for example when I read the python\ncode I use Python text splitter and when\nI use the golden code I use Golden text\nspeeder\nand then after\nthe\nsplitting is done I create embedding for\nall the documents and this step I'm\nusing the hacking face model I was\npreviously using open-air embedding but\nit was too expensive so I switched to\nhugging phase 3 embedding model\nand then the last step is to store the\nembedding in your vector store and in\nthis step I'm using files from\nmeta\nYeah so basically just two steps\nand you can find all the code on my\nGitHub\nand\nthe second phase is to query documents\nso the main idea of this stack of this\nphase is that whenever you have a\nquestion and then you embed it embed it\nusing the same embed model\nas our previous step\nand then you use the similarity search\nto find a similar\ndocuments inside the embedding and then\nPaul\nK often for example for that and then\nput everything into the Open Air Model\nyeah so here's a breakdown of the\nof the step the first one is that get\nthe user question\nand then look up documents in the in the\nvector DB relevant to the user questions\nand then construct The Prompt with both\nthe contacts which is the relevant\ndocuments and the question\nand then pass the prompt to the model\nand in this case I'm using open airgbt\nand then you can get the results\nand here's the code\nOkay so\nuh actually if I can make mention even\nfaster\nbecause as I just mentioned we have two\nsteps ingest and query\nbut one benefit of fly is that it can\nuse the caching mechanism to catch the\nresult of inches so I don't need to\nembed again and again because every time\nembed takes GPU resource and in my case\nit takes about 10 minutes but if I can\nhave the cash mechanism I don't need to\ndo it every time\nso break step into test and then catch\nthe embedding\nso I have two tests here and I construct\na workflow so I can do something like a\nbulking phrase\nyeah and here's my contribution to\nflight so far\nso I have adapted flight authentication\nto Azure or on-prem use case because\nuh originally we don't have a dock to\nset up hrid authentication and also\noffer on Prime use case we often need\nsome proxy to access public internet\nand the second one is that I help\ninitiating and implement the part\ntemplate\nuh feature I guess it's pretty popular\nand usable now and the third one is that\nI drive the config override discussion\nfor quite a while and then currently uh\nimplementing the feature with a lot of\namazing folk and then also support\nHardware as a MPI plugin\nand the last is I resolve multiple bugs\nin Cube flow every\nplugging and flight kit\nyeah and that's all\nno amazing thank you any question or\ncomment\nuh just got a quick question about the\nline chain object\num that first of all that's this is\nawesome Byron\num I had worked on something very\nsimilar\nbut uh didn't get as far as you in terms\nof implementing things\nin terms of the slack\nthe slack data did was it just a\ncomplete dump of everything or did you\npick certain channels just dump uh so\nactually hence the data to me and I\ndon't need to do any preach passes there\nis a slack loader and I just use nice it\njust works wow that's cool awesome\nso the format is that\nuh lay each each channel is a folder and\nunder the folder there's a bunch of Json\nand then stack loader can handle\neverything\nso in terms of like next steps do are\nyou thinking of\nI mean I don't know like I know chat GPT\ncan get kind of pricey so uh like having\nobviously be an actual thing you know\nthere's a chance because uh I don't\nthink the prompting The Prompt is that\nexpensive each response takes about\n0.0.1 dollars but embedding is very\nexpensive it takes about three dollars\nevery time so if we only use the prompt\nit it should be just be fun\ncool yeah uh yeah let's chat um I want\nto work on this because\nthere's there's I'm working on other\nthings to like fine tune uh open source\nllms\num you know so this would be an\ninteresting kind of comparison for both\ncost and quality you know because if you\ncan fine tune yeah um because you can't\nfine tune GPT right now obviously\num but you can fine-tune an Open Source\nOne\num it may not be great quality or it\nmight not might not be as good but I\nthink that's like a research question\ncool thanks for your work\nyeah amazing thank you Aaron thanks for\nsharing the\nGitHub repo any other comment or\nquestion\nno\nwell again to our top contributors think\nyou were taking time from the thousands\nof insurance projects out there taking\ntime to contribute and make it even\nbetter thank you and thanks everyone\nhere\ntrying to set up flight using it\nconsuming it thanks so much\nall right in terms of the agenda I think\nthat's it\num yeah there are no questions I can see\nhere discussion topics unless you want\nto say something\nI think I heard someone so\nall right\num yeah that will be it for the next\nsession we will have uh Yan from\nAi and\num\nyeah that that will be great\num and yeah sorry to call you Yen but I\nsee the in the scheduling yeah I'm\nexcited to learn from you\nall right any other question comment\ntopic no\nokay that was great thanks so much for\nyour time for joining and I hope to see\nyou the next one\nhave a nice day thanks everyone bye"
    },
    {
        "title": "Flyte Contributors Meetup - May 25, 2023",
        "transcript": "you welcome everyone to the flight\ncontributors Meetup today is May 25th\nDavid is very unhappy to be your host\num this meeting has been recorded and\nrecordings will be posted to the flight\nYouTube channel\nand uh this meeting also falls under the\nLinux foundation's code of conduct\ngreat and also yesterday I became aware\nthat may is github's maintainers month\nso happy training since computers month\num right so let's get started um\nsorry hey hi mix good to see you man\nthank you for joining\nhey nice to see you thank you\nyeah I'm\nsharing here links to the agenda notes\nfeel free to add yourself to it in this\nlist\nbe great and yeah welcoming new members\nwelcome Hong Shin AKA onyx great to have\nyou here we'd like for the benefit of\nthe audience we'd like to briefly\nintroduce yourself\nin general\num I work for Spotify\num one of the first Engineers\nintroducing flight to Spotify I believe\nwhen it was still in the lift\nthank you to meet you everyone\nyeah Onyx needs no introduction David\nhe's he's a Fame Precision so he's\nawesome thank you for joining\n[Music]\nawesome great and also I see Jay joining\nfor the first time again welcome great\nto have you\nthank you hey everyone\num yeah it is my first time here\num I work at Freedom with the GU and\nGreg I think Greg is here as well\nand yeah it's great to be here\nawesome thanks for joining great to have\nyou\nall right\num yeah again the let me share my screen\nreal quick\nand we'll we'll go over the agenda so\nfirst thing outstanding rfcs\num and probably that leads me to the\nfirst uh comment here uh we will try\nunless there's any objection to use the\ntime here in this meeting to discuss\nonly new rfcs or rfcs that you\nintentionally put here in the agenda to\ndiscuss something very specific and for\nthe remaining discussions keep them\nasync because there's a huge risk I've\nseen this in the past in other projects\nat a single RFC could take the entire\nmeeting so I want to avoid this and\nwe'll we want to leave time for other\nconversations also\nso um\nyeah I don't see any new uh rfcs here in\nthe board and listen missing something\nor it's not labeled but\nuh does anyone has any new proposal to\nintroduce\nright so next up will be eager modes uh\nthere's a question I don't know if it's\nprobably from Fabio what happens to\nchild executions when an eager task is\naborted\nI asked the question\num maybe we can also because I saw that\nrender before we can also take it to the\nRCP prepare but that's a question that\ncame to me because of my suspiciousness\nthat it will dangle not be killed\nI think they will all be killed I think\nbecause\nthere's just no way right\ncurrently there is it's not possible\nright because that's like that idea of a\nfailure note like\nhow how will you do it yeah I know we\nhave to build certain things but I think\nthe expectation of the user is that the\nentire tree is killed isn't it exactly\nwe're not changing this with eager mode\nright we should not but I think\ncurrently it's not doable I think there\nis a different question probably Fabio\nthat you're asking is like is no I mean\nthe question that I'm asking is going to\nbe quite an eager task and eager task\nbehind the scenes will use flight remote\nto start other executions that are not\ntechnically part of the workflow graph\nright\nand then the workflow that contains the\neager task is now aborted the eager task\nwill shut down what the executions that\nthey were started by using flight remote\nthey will probably not be shut down I\nexpect but as you said I think the user\nwould expect that they are yep\nand and this and I think in the current\nexperimental mode\nthat may not happen but I think we\nshould probably quickly follow up with\nlike making sure the expectation is\nfulfilled because that is the\nexpectation that most people have like\noh all the downstream is cleaned up uh\nit could be done right we could we could\nhandle this as a specific either Plug-In\nor a node within propeller or we could\ndo something else completely entirely\nit's very it's totally fair in my\nopinion that the first implementation\ndoesn't do it but I think it's something\nthat users will ask\noh absolutely no I think and this is\ngreat to add as a as a thing that hey\nthis currently is experimental this will\npotentially cause\nDownstream leaks\nand let's figure out if people really\nwanted like what we really want to do is\nfirst figure out the caveats here that's\nour number one thing okay then I'll I'll\nwrite a sentence about what we discussed\nnow in the RC and from myself we can\ncontinue thank you Fabio next month\ndo you do everybody agree like I don't\nknow if Onyx even knows about has read\nthis RFC\nno I have not\nyou will you will like it thanks you\nshould read\nyeah I think I will\nthere's already something catching me\nthat try accept statement not supported\nI think exactly today we got the\ninternal user asking for this the exact\nfeature\nyep so there is a failure more node that\nis being worked on by Eduardo but this\nis like arbitrary python this allows you\nto write arbitrary python or Java or\nwhatever language it doesn't matter\nand and what we want to do is get\nstarted with this early uh figure out\nthe problems and then make the model\nmuch more replicatable across all\nlanguages Etc we'll start it with python\nuh and it and I think that the entire\nimplementation might twist a little bit\nfor this right there are some other\nsystems that do this\nthere are negative we are trying to even\ndo more right we are actually saying\nthat the the actual async eager\nthing is a task that means it could run\na spark job if you really want or it\ncould run on gpus or hundreds of CPUs\nright it doesn't matter it's like it\ncould happen and you can now do\nDownstream things with it which is\nreally powerful nobody's done this today\nso uh but we want to be careful\nforeign\ndoes this mean there is not like a type\nchecking right\nso anything can\ngo right I'm just looking through the\nRCs well it says the downside for\nlike never compiled\nwhy do you say this we still have the\noriginal type checking yeah in the in\neager mode there's no static analysis of\nthe graph yeah like there's no static\ntype checking it runtime type checking\nwill exist right it'll always exist\nyeah\nso what will happen is like you if there\nis a bug in your code you will find it\nat execution time this is the sad part\nright which it is what it is similar to\nDynamic today right\nyeah like you basically have this same\nproblem\nruntime errors yeah runtime hackers yeah\nbut this is actually little worse than\ndynamic because Dynamic can spit out the\nentire graph we statically analyze it\nand we could fail earlier here let's say\nif you're going through the graph you\nmight go 50 of the way and then fail\nbecause of like an integer to string\ncasting error or something but\num\nthis is like this is python problem we\ncan't do much with it\nand it's a trade-off right all of the\nstatic stuff is still there Dynamic will\nstill be there this is like in cases\nwhen you want that extra flexibility and\nI when we want to be careful in telling\nthe audience and the people the\ncommunity people that use it only if you\nreally need it don't use it all the time\nthese are started as a experimental like\nwe're gonna learn how to expose this\nproperly right so yeah\nright great thank you\ngreat next up in the RFC is the\nexecution tax\nI see at least three questions coming\nfrom u5 also\num not necessarily a question I made a\nPR\num in which I in PR into that PR where I\nchange the document a bit from\nthe changes that we discussed last time\num please review if you agree but\nnothing we need to discuss right now I\nwould say basically I just there's this\npart where you say hey we could do like\nthis where there are specific tags\num that have a special meaning or users\ncan arbitrate use arbitrary tags and\nlast time we agreed that basically we\ndon't want to force on users\nthat there are specific tags with a\nspecific meaning and the pr just remove\nthis part\num\nyeah and then there's this question by\nIlly\nmy bad I will reply to some of the\nquestions I've been a little busy\nrecently sorry oh briefly no words I\nmean we should\nshould have bags for all entities right\nlike there's no reason for us to just do\nthis for workflow executions and\nadding tags\nand removing them after the fact should\nalso be supported like I don't want us\nto remove tags no\nbecause removing is a very\n[Music]\nyeah maybe I have to read the comment\nbut I do agree tags should be\nsystem-wide ideally and this is what\nwhere he was talking about something a\ncouple days ago and I said like let's\nconsider tags being a system-wide\nconcept\nthen how will they look and we could\nstart with like executions as being\nfirst\nthing\nit's the most obvious need is in\nexecutions first yeah let's uh\ndiscuss would you say that yeah that's\ndisgusting RC but I think they need to\nbe removable because\num some experiment tracking starts\npeople say hey Mark this one as good one\nand then you have a better one so you\nwant to remove that like I think people\nwill expect that you can remove them ah\nI think it's a separate question\nif like we want to have like an audit\ntrail of tags you know but they should\nbe like easy to do\nthat the reason why I said whether it's\nremovable or noise because let's say I\nwe want to have a behavior on the tag so\nfor example if you want to say get me\nthe the latest\nor the the\ncorrect execution and based on that get\nme the output of that execution and I\nhave a downstream Behavior like a\nservice that's using this to pull down\nthe data let's say a model\nyou're already building a thing on top\nof like tags right\nthat maintains State essentially yeah\nbut like now then it's suddenly you're\nswitching thing under the covers without\nreally telling and you know with flight\nif things are reproducible and like you\nknow we have tracking so\nI am not opposed to the idea but let's\nyou know probably just talk and I will I\nwill comment back yeah it's my bad idea\nI do think that we should separate like\nhow easy it should be to use the feature\nversus like this idea of you know\nversioning everything like auditability\nlike\nyeah\nyeah but as I said let's go discuss ERC\nI think there's a lot of good ideas that\nand\nI already started you know discussing\nwith with Kevin how about implementing\nlike at least a v0 simple why\nyeah much of it exists but I think MySQL\nis where it all the hair is Byron or no\nthat's LinkedIn team here\nyeah yeah because you know yeah very\nmuch of an implementation detail but I\nI have\ndiscussed with having like a\nan idea for implementing this it doesn't\nrequire any extension you know oh cool\nyeah but again I think the Big Ideas\nobviously should be discussed there\nlet's not take more time to say yeah\nphonics I would also recommend you guys\nshould take a look at the RSU you'll I\nthink you'll like some of the things in\nthe proposal\nno thank you I will take a look\ngreat\num yeah there's a question for from\nBernard isn't that the same as Docker\ngit does I mean with the possibility of\nusing the hash in case you want\nsomething more stable Downstream\nyeah basically to maybe add the line to\nthat it's like with text right a lot of\nsystems already use tags where the tags\nare something that you shouldn't rely on\nbut you haven't a possibility that\nthat you could use so I think the\nconcept is around in similar systems and\nthe truth if we would like to have a\nsimilar thing\nI agree about that there's an address\ndrive that you can\nthat's what you're referring to right\nyeah for example the latest take you\nknow can be\non anything you can also like remove a\ntag and yeah it's a take right so you\ncan remove it and put it on again yeah\n[Music]\ngreat\nthank you Allen anything else in regards\nto this proposal execution tax\nnope\nokay uh next up the config overwrites\nRFC uh results of later discussion needs\nto be incorporated and what happens\nafter that\nrealm time uh so I I will\nuh finish the ideal part this week and\nthen uh penhurst and Fabio can start the\nwork\nright yeah I think there's another\ntopic here from Bernard when Heidel\nsplit up work for the config override\naverage RFC which is also related to my\npoint here on the community groups\nimplementation so uh probably I'm not\nsure if it's the answer to the question\nhere but uh some pieces of feedback come\nin from from ye and other community\nmembers around how how to ensure that\nthere's ownership for RFC implementation\ncomes to proposing that almost every RFC\ntypically should eat the when it's\naccepted should land in a working group\nwhich will result in starting a working\ngroup\nwhich remember it's a very focused small\ncommunity group with a single\ndeliverable which is in this case\nimplementing the accepted proposal and\nafter it's done and communicated it's\ndisbanded it's kind of lightweight\num so not sure if that answers the\nquestion what happens next in terms of\nthe RFC process the uh the if it has the\nnecessary approvals and there's no\nobjection yeah remember there it's in\nthe final coming period to be ready for\nfor the final comments approvals in the\nmerge I think next thing I think\nwe came to an agreement how to look like\nbut I think the latest state of the\nagreement is not part of the document\nthat could be emerged\num\ndo you agree with that or for example we\nrecently changed how you would override\nor you how you would create this\noverrides and while writing a workflow\nwe changed it from the from the workflow\ndecorator to do it like in the workflow\ncode\num I think this needs to go into the RC\ndocument and then from I said we can\nmerge and I think other people already\ncommented that they agree\nOh you mean uh merging to my uh the\nmarkdown yeah I mean when we worked with\nthem people would see we'll see the\nmarkdown file and they won't see the\ndiscussions\num\nso the result of the discussion needs to\nend up in the markdown before we merge\nbut other than that I would say we can\nI think it has one one approval already\num\nyeah\nI think there's like just a few\num comments like we're very close to\njust approving just you know moving this\nRFC too accepted\nbut um\nDavid can you explain a little bit on\nlike how we're gonna create these these\nworking groups like what how like\nyeah I think it'll be the first time\nthey're gonna do this so I just wanted\nto clarify you know what's the the next\nstep after this is in\nyeah well the everything is here now in\nthe governance document\num yeah all the processes that we\nalready reviewed in the RFC\nand uh right now the the steps\nbasically answering a couple of\nquestions that that for this specific\nRFC they are already there what's the\nproblem the artifact and\nEtc\nand uh just sending up your\nuh to this file in the community repo\nit's basically a list\nuh so we can document the group\npretty easy to lead it\num\nmeetings are not mandatory for working\ngroups you don't need to establish\nmeetings but there should be at least a\nslack channel for communication\nand uh the mission statement which is\nbasically in this case implement the RFC\nnumber XYZ\nand\num\nit should have the approvals from the\nSyrian committee and that see that's\nthat's the process\nto formalize a working group and get\nthis started\ncool and we are doing this in the emo\nfile because this is this is going to be\nfed into some automation\nor just because\nnot right now but um\nit's the next stage to fit this into an\nautomation that creates folders and Etc\nbut right now I it's kind of the\nthe fastest way that I found to document\nuh hey this working group exists and\nthat's it\num yeah if\nis there a slack Channel or something\nfor this group like for Passive Watchers\nlike me\nlike how do you observe the progress\nnot yet we're gonna follow the process\nyou know for this working girlfriend\nawesome you you being by the door\ngreat okay so um regarding conflict\noverrides any other comment question\nno\nokay\nuh next up there there's a there are a\nlot of points here in the up in my\nquestions\nat the end\num so let's get started with flight kit\nregressions on\n1.6.1 that's a standard point maybe you\nstart at a disappoint\nyeah um first of thank you for making\nthe 1.6 release it's great uh we're\nalready on it in on the flight side on\nthe flight kit side we are not um and\nwe're super stoked to get on it because\nit you know it's just because of the\nperformance fixes\num we've noticed just out you know out\nof the field uh three or four\nregressions\num I think most of them are in select\nthreads or in tickets already\njust wanted to make sure\num that you you guys are aware as\nfeedback it's not we're not in a rush\nalso to switch just to but we are\napproached on like three or four four\nthings not true here someone else has\nseen stuff\nI've seen\num\na stream of tickets yes thank you\nI'm more curious about like how\nwe should be prevent something like this\nyou know like\nbugs are happening will happen right\nit's the nature of software but like\nhow how can we do a like a beta testing\ngroup yeah yeah like\ndo you think it it's worth like you know\nfixing this this idea of like hey we we\ndo\nwe do nightlys or something and we use\nor leverage this community here\nto catch\num errors earlier\nwe can discuss more about this you know\nasynchronously but like\num\nyeah\nyeah I'm I'm sure you don't know more\nabout like your your idea like\nyeah if there's an alpha or nightly we\ndo have pretty good test coverage on our\ncode and we do continuous delivery so we\nare happy to you know just try it out\nrun our tests on it and can at least\ntell you when things blow up um so if\nthat if that's something we can help\nwith that's definitely I think\nyeah that would be amazing that over\nnightly I think if every company within\nthe group kind of helps us do one\nportion of their tests right it this is\nthe surface area so wide\nthat we sometimes may not catch\neverything\nthis will be amazing\nso I'll ping you on slack because I\nthink setting this up will be relatively\nsimple and yeah thanks for volunteering\nyou know this is\none thing that I always wanted to do\nit's great that I I think yeah\nOnyx can you can you guys do this as\nwell\nyeah I just want to add that um we also\nhave ultimate PR or something monitoring\nthe open source release and proposal\ninternal PR automatically do we run\nthrough a pawn shop integration tests\nbut not very thoroughly\nbut indeed I was thinking positive it\ncaused a few problems so sure we're\ndoing this\nokay\nand Fabio but it's not really nightly\nit's um whatever the latest release has\nbeen done we'll pick it up yeah so\nnightly we can do a release right and\nAlpha release yeah 30 years\nwhatever IPI won't complain\nall right\ncool this would be awesome yeah\ngreat awesome all right next up\nsplitting up work for configurable right\nfrom Bernard anything else you would\nlike to come in here we're North uh no\nprobably yeah maybe we should create\nourselves a selection or so where we can\ndiscuss\nwho's doing what I think probably this\ngroup is too wide for\nyeah\nright\nright thank you yeah we'll create\nawesome thanks\nyeah next up was my point and Community\ngroups implementation but you you\nalready saw most of it uh feel free to\ncheck out the governance doc ask any\nquestion feedback it's not perfect\nthat is the first step\nall right\num a related Point here the\nimplementation of privileges for roles\nin repos if you pay close attention to\nthe governance dog especially the path\nmaintenance ship it shows that the scope\nof permissions is for a specific repo or\nrepos\nand um\nright now we'll we'll like to Define\nwhat will be the exact implementation of\nthese permissions in the repository\nrepos\nuh ideally it should be different per\nrepos I mean for for a specific Ripple\nto have different maintainers\ncontributors uh inheriting permissions\num and as you can see here there's the\ngood explanation\nuh the roles and permissions basically\nthe idea was to have three levels\num what GitHub calls triage will be\nsomething like\nsomething something a role in the middle\nbetween community members and\ncontributor if you are not aware of what\nI'm talking about it's again here\nbetween a community participant which is\nanyone everyone any contributor this\nrole in the middle will have the triage\nrole that basically\nhas the ability to help reviewing vrs\nEtc but cannot improve nor merge\nand uh Next Level will be the\ncontributor we would write permissions\nthey can not only approve but actually\nmerge\nand finally they maintainer\nso um yeah again we want to keep the\nmaintain Department leadership as simple\nas possible and as often as possible and\none step for this is to actually\nImplement what's it what's in here\nin permissions in the repos remember\nthat also the kind of the community sign\nis that in the end six and working\ngroups every repo should be owned by a\nsick or a working group but that's\nthat's a different stage\nhow it's just defining permission so I\nwill put all this in written form we'll\nshare it in the channel for you think\nand review and let us know what do you\nthink\nany comment about this\nall right next one discussion about\nadding additional lock links from a task\nyeah maybe I can summarize that\num so we discussed a bit about it last\nweek and there was a threat in the RC\nincubator than another issue another\ndiscussions kind of continuing the issue\nby somebody who's not here\num the idea is that\nwhen let's say we're trying to model at\na task called train and we're logging to\nsome experiment tracking server which\ncould be whichever one you want could be\nan outflow could be want to do something\nlike that it would be great if on\nCyclone so we could click a link and\nimmediately bring us to the execution in\nmlflow or wherever so we can see the\nprogress and the question is how we do\nthat and how do we make it show up\nbefore the task finished\num currently at least what we do and I\nthink what the answers in other people\ndo is that we added to the dash but\nattack has shown that when the task is\nfinished\nand there has been the proposal to\neither\num\ndo it in the task or basically expose\naway in the past decorator to say to add\na link\nto site console an additional one that\nuses templating so for example you could\nuse the execution ID or the project as\nGinger templates\nmaybe if you scroll down maybe I should\nhave my screen because I think I can\nscroll to these things or\num I send a link to maybe the comment\nthe exact one\num yeah because there's actually some\npeople saying you know that you cannot\nreally just templatize it because of the\nyeah I'm coming to that I'm coming to\nthat so\num\nif we do so doing it in the time in the\ntask decorator which would be at\nregistration time should be fairly\nsimple\num but the caveat is that you have to\nknow at registration time how the link\nwill look like at runtime or at least\nwith variables right\nfor example for 1tb that would easily\nwork because what we do is that we with\nthe wanted the python API we create tags\nthat use information from the site\ncontext like version and execution ID\nand since we know that we will be doing\nthis in the task we could already add\nregistration time create the link using\ntemplates right\num the Assumption then is that one\nactually does it in the code so there\nwill be a brief time when the task has\nstarted where the link will already be\nin the flag console but actually there's\nnothing there yet\nbut probably that's something we can\nlive with\num for other\num David\num I put I pasted the link into in the\nchat maybe you can share that\num because it shows the different the\ndifferent two options\num the other option that was proposed by\nNiels is that it would be cool if one\ncould do that by a flight context but\nthere is no mechanism to communicate\nthat the upside if there was such a\nmechanism so that's the upper so\nbasically these two these two things\nhere either the lower one we do it in a\ntask decorator we have some link here\nthe link contains Ginger variables and\nthen if one in in the code\ncreates the respective tackles once they\nbeat this link would resolve uh\ncorrectly and would work\nthere are other experiment tracking\nservice where this won't work because\nyou um because either the links are not\ntemplatable for example\nso you can't just parameterize the\nURL because um just not built that way\nand then that won't work news proposed\nthe upper solution here\num where you take the flight context and\nthen say add lock link but there isn't\nthe kind of communication channel so\nthis wouldn't work even though it would\nprobably be the perfect solution\num\nI've been wondering I want to hear your\nopinion on whether something like that\nwould work I mean what we probably don't\nwant to do you commented on that then\nthat the task actually like makes a\nmakes a call to slide admin or something\nbecause in the multi cluster setup that\nwill be terribly complicated and it's a\nmajor change to how it works\nprobably we don't want to do that\num I'm also worried about the\nreliability aspects of this thing why\nwould you have to check and\num no no no dude like how do you publish\nlike this is like real-time events\nand you can't control because this is\nuser code so people can send random\nevents right against whatever they want\nI mean they won't flow through life\nright you know in this case that was The\nProposal one of the proposal was to\nactually send it to admin or something\nwhich is dangerous yeah that's not true\num yeah so one thing that I want to ask\nyou what I would like to have your\nopinion on is that the deck\nimplementation currently works like this\nthe user code construction HTML file and\nthen the HTML file is uploaded when the\ntask finishes right\nit put it's put into blob storage and\nthen flight console retrieves it from\nlob storage and serves it in the UI yep\num currently the deck is uploaded only\nwhen the task finishes right\ncouldn't we just trigger the upload from\nthe from the user code and then flight\nconsole takes whatever latest version is\nin the bucket does it have to be\nuploaded only in the end because that\ncould be the mechanism to communicate\nthe link once it was created\nand then follow-up question couldn't we\nuse the same mechanism for links also\nso it's not the deck\nit could it's just like you know it's\nlike misusing a blocked storage for I\nagree yeah again it can be done and I\nthink\nto be honest this is you know again I'll\ngive you a union perspective on this\nuh uh and one of the things is we've\nwe think this would be great addition to\nfly to have like a real time\nEvent Source from or event sink\navailable in flight from the task but\nit's just not like the community already\ntoday you don't see like deploying\nflight people ask hundreds of questions\nthey were like all right here you go now\nyou have to Define Kafka or like that is\ndurable cost that people are like nope\nyou don't want to do it right so\num\nI would love it because Union can\nprovide a hosted service for this to be\nhonest but but you know I don't know if\nthis is right\nokay\nI I want to put to this is the the same\nproblem that's blocking uh like flight\nkit runtime metrics as well right now\nwe're talking about how you know flight\nkit can emit spans for certain things so\nif a user wants to say here's my task\nand we'll break down the runtime into X\nand Y and Z and report that to the UI\num it's the exact same problem of you\nknow that Communication channel\num\ncommunication to admin so I'm I think\nthat's kind of a core problem to a lot\nof these extensions so may I recommend\none thing\nmay I recommend that we actually make\nflight kit have the capability of\nsending events with no op\nright as a default\nand then companies who want to do it\ntill the receiver because this is I\ndon't think we should force the\ncommunity people to actually have this\nand then use the default like S3 or\nsomething as an implementation that's\navailable in like Block store I mean\nblocks are available in the open source\nand say use it at your own risk\nI don't know\nI mean also\num to put it in perspective we don't\nnecessarily need it\num if we do it in the task decorator\nwith the experiment tracking server that\nwe use I think where we can make it work\num one thing I don't like about that is\nthe task decorators are recorded in the\ndatabase and so let's say if you change\nthe end point in some way yes\nit's like then it won't work\nso it should be like in this case it's\nlike log link right that's why we put\nlogging server configuration in the back\nend and the reason why that is even if\nyou take an older execution workflow and\nrun it the log links will be updated\nbased on your newer settings\nwhich is the right approach in my\nopinion\nbut the problem is for example that with\nwith experiment tracking stores that\nwouldn't work because not everybody\nthere's like hundreds of them\nso maybe maybe we just add this at this\ncapability\nI think in the in The Decorator it will\nmostly work because most people don't\nchange their providers right 99 load\nforeign\nyeah\nand we don't put you know admin in the\nhot path like\nI would want to keep admins table I\nthink reliability is a bigger more\nimportance for most of us I'm sure then\nuh one more long link\nI agree\nbecause I don't think postgres is made\nI know there are other some people some\ncom some communities are doing that but\nit's stable\nuh and this is just spitballing but\num right now the log links you know that\ntemplatized variables in there are are\nvery like static\num what if we can have like environment\nvariables or like things like that where\nwe include it and then if a person you\nknow runs a workflow you can set a a\nlabel or an environment variable to a\ncertain thing and we can include that\nthe long links maybe there's a way we\ncan leverage\num you know something a little bit more\nDynamic for workflow executions\nand we do have that now right we have\nenvironment variables that you can pass\ndirectly from the CLI\nright\nor even like put that behind you know\nsome code like you don't have to\nnecessarily put the\nthe URL there like I have another option\nin in propeller we do this then maybe we\ndo the same in Flight kit have a\nbuffering system for events\nand the buffer flush interval is in like\nsome\nsize like you have to off like there is\nno you put put the buffered events in S3\nlike what Fabio is saying just put them\nin a screen make make Flight Deck\nliterally dynamic\nand just have a buffer intervals you set\nthat buffer interval to be like\none megabyte when you hit one megabyte\nyou upload and I think that's okay\nso it's not immediate it might be a few\nminutes later one megabyte or whatever\nright 10 seconds\nwhat do you think about that Fabio\nand I agree with your point that we're\nkind of misusing blob storage at that\npoint\nGod\nbut if you might require it would not\nrequire any new communication channels\nright yep and that's what I like about\nit\num\nmost cases yeah I don't care whether the\nlink is a minute later\nyeah I care about the link not being\nthere 12 hours later when the training\nis finished I need to look at the loss\ncurve I don't need to look at it like in\nthe exact second but if it's if it's a\nminute later so that would be amazing so\nbasically if I could do from the from\nthe Titan side\nflightgit.currentcontacts dot upload\ndeck or something or Flash Tech yep\nyep\nso that basically an update to the deck\nAPI\nto make it like either the end of the\ntask another task should always be the\ncase or have a flush interval really\nI think the problem yeah then we\nwouldn't build a new feature for log\nlinks because then we could include it\nin the dock and there would be sorry not\nto dock the deck and there would be a\nway to make it appear before the task\nfinished\num\nand it would also like help in like Poor\nMan's version of Fast Track like\ntraining tracking Etc\nevery few seconds\nit's really poor man's but\npeople won't happen and as as we mature\nwe might find a solution in there in my\nopinion\nhow does it work currently you can take\nthis offline yeah\nokay this is great this is great but I\nthink the stripe guys propose this and\nso there's already some momentum and the\nnumber of people who replied to it the\nmoment it was proposed it just shows\nthat people want it having a solution is\na good thing\nawesome thank you\nright and next topic pedantic plugin\num is Neil's here today\num I don't think he is right\num I think I can reach out to him about\nthat and he's already aware and it's and\napprecially ask him I want to ask him\nsome questions but we can take that to\nSublime Maybe\num there's some guy who replied to you\ndid you see that Fabio yeah and he's\nright like um he's right like um\nthe current limitation will be broken by\nalong arbitrary types and I think that\nif we allow arbitrary types that will\nbasically be a re-implementation of the\ndata Class Type Transformer\nand I wonder how we can do that so that\nwe don't duplicate all the code\nthat should be doable\nit's code restructuring\nin like weird\nthe question is what exactly what would\nthe data class Transformer you know\nnobody likes the dating\nonly Kevin knows how to update it I\nthink like there's one change\num that Eli made like\nmake the data classrooms from like a lot\nsimpler and\nessentially would reuse the type engine\nfor all types which you know if you read\nthe data class Transformer code to dates\nfull of edge cases\nwe're removing all of that\nso I think we should invest in in in\nthat\nto help you know make\num identic models better supported\nforget what I take away from this\ninstead of paying bills and comment on\nstock maybe and we take it there\nand take a look at this PR that I'm\ntalking about like the it's another yeah\nthat's a current one\nyeah I'll send you the I'll find it\nyour number\nyeah I also like the idea of identity\nbecause data class also was real slow\nsome people have also talked about\nmoving away from marshmallow to what's\nthe other one tomorrow\num yeah\nthat's what um there's this PR by Eli\nkind of get a locks you know\num because you will not be tied to the\ntheir class chase the Transformer\nanymore\nbut it requires some work\nyeah\nI think there are some couple other\nthings I think these are all the topics\nbut there was a couple other things that\nsomeone is working on Pi flight run from\nremote file I just love that feature\nthere is some Community member who's\nworking on it\num so yeah I don't know\nthey are not in this meeting\nso\nsorry I don't know them I just saw the\nwork on it\ncould stop happening\nis that it David\nyeah yeah in terms of gender items\nthat's it is there anything else you\nwould like to comment share here\n[Music]\noh\nall right thank you all for your time it\nwas as usual a great one and hope to see\nyou in the next\nthank you awesome thank you this is\namazing thank you congratulations\nbut yeah but hopefully we will power\nmore in Flight we are here for you guys\nsir take care guys thank you okay thank\nyou bye-bye thank you"
    },
    {
        "title": "Flyte Image Spec Demo",
        "transcript": "yeah hi everyone my name is Kevin I'm\nsoftware engineer at union.ai today\nwe're going to talk about the image spec\nso what's inspire so like\npeople spend a lot of time to write the\ndocker file to run to run the by tags\nbut like uh\nwriting a Docker file is painful and\nlike sometimes you fail to write a fail\nto run a text because like module not\nfound you forgot to copy the code to the\nimage right so we want to get rid of the\ndocker file so we create image\ngive you ability to define the container\nrequirement in your local code\nhere's the example like you can Define\nyour like python package empty package\nand then you specify your registry\nwe will create a Docker image for you so\nlike people don't like to write a data\nfile like struggling with writing darker\nFabrics sometimes they're not familiar\nwith darkest file syntax and\nthe image spell also allow you to build\nimages faster and then simplify managed\nmultiple multiple image\nso how does supply all this flykey build\nthe image so similar to type engine we\nhave the image build engine so you can\ncreate your own image spec Builder tell\nFrankie how to build how to push your\nimage and then you can register the\nimage to the image registry in the image\nimage view engine so and then you can\nspecify\nyour builder in the image back and then\ninspect where you use your Builder to\nbuild image\nand then by default like it's like you\nonly have one\nimage Builder called empty I will talk\nabout this in next slide\nso empty is an amazing project it's\nbuilt on part of Docker and you use\nbouquet under the hole so you allow you\nto build an image with different\narchitecture and then you can\nparallelize the image view process\none of the example is that like if you\nuse darker to install python package\nuh Docker will install your package\nsequentially but if you use empty\nempty will compile your like requirement\ninto an IR or deck and then you will\ncheck if there is uh it will check if\nthere's dependency between two package\nif there is no dependence between two\npacks you will install those python\npacks in parallel so MDA has a lot of\nother features you can check out their\nwebsite to see this project\nokay let's do some demo\n[Music]\nokay can you see my terminal\nyeah okay thank you\nso let's start with a single workflow\nso here this is the best live also I\njust print hello world in attack so in\nthis case you don't you don't even need\nto build a right Docker file you can\njust run\nwrong and then it's like he will use the\ndefault image for your text\nand yeah let's see another example\nuh I think get more complex when you use\ntensorflow so\nin this case people have to rely\nthe docker file for these tensorflow\ntags so you can see you have to copy the\ncode to the root file if you copy the\ncode to other file like it will fail to\nfind your module and then you have to\ninstall plucky tensorflow\nand then you have to remove the FD cache\nif you've got to remove the case you may\nhave like gigantic darker image and then\ndue to some security issue you have to\nremove now you have to update the user\nyou have to use the real user in the\nimage\nso\nwe want to get rid of the docker file we\npeople can use image spam so uh\nDefine you you add your patch in the\nimage back and you define your uh.com\nregistry\nand then that's it you don't need to\nworry about how to remove the cache or\nto change the user and then how to copy\nthe call you only need to install the\npatch\nuh you only need to add your requirement\nto your image step and then when you run\nokay\none round\non the rest of the workflow before rush\nbefore you rush the workload so I key\nwill build the image for you\nuh you can see why\nand use empty to build the image and\nthen first you try to pull the flyky\nbest image\nand then you will start installing the\ntensorflow on top of the\nuh yeah\nso one of the benefit of use using image\nspell is that before the before building\nthe image uh slightly will also look out\nthe docker Hub to check if the imagery\nis if image now is\nuh it was building images just stupid\nbuilding range so let's see I removed\nthe\nannounce here\nand then\nbecause I already built this image\nbefore so I can let me\ndress sure this will plug in\nokay\nyou can see like\nimage image funnels the building and\nthen you can\ngo to your workflow\ntext template\nand then you can see uh Plucky just\nreplaced the image spec with the\ninternet so\nuh so\nso the benefit of using the image bags\navoid to rebuilding\nuh the Send image over time but let's\nsay like here I have a thousand dollars\nimage in my darker heart but I don't\neven know what patch I install in this\nimage so every time I write a new\nexample I create a new Docker file and\nbuild an image but I may already build\nthe image before at some point so but I\nstill have to repeat image but by using\nimage back image backward\njust look out the docker file dot Docker\nHub and then find the same image you\nbuilt before and then just reduce the\nintroduce the image\nokay let's see you're not example here\nso\nif you have a workflow you have a\ntensorflow task and then python test\ntypically people will install\nthese two dependency in the same image\nso you may have a very large darker\nimage and then you will take a lot of\ntime to pull the image\nuh to address this issue we add a new\nfunction in English\nso\nit's container so\nwhen you run\nthe T1 text Timo is a TENS of attacks uh\nit will check uh we're running the text\nin the parallel it will share the image\nusing the part it's built from this\ntensorflow image step if true if if this\nimage is built on tensorflow image spell\nthis demo it will be true and then you\nwill only import tensorflow and will not\nimport torch so you can easily to\nbuild a workflow with beauty container\nMulti Image\nforeign\nso you can see you have uh you have to\ninstall sound dependency of Spotlight\nspot jar or air you have to add some\nbinary meaning your data files\nlike sponsor me\nand then there's a lot of Demands that\nyou need to add on the photographer and\nit's hard to convert this spec to the\nimage spec so what we what we do is that\nwe in flaky plugin flux spot plugin we\nbuild a base spark image so\nevery time you register display\ninstead of pull pulling the fly keypad\nimage when you run the spot test you\nwill try to overall\nthe spark desk image\ndot com\nokay here you can see uh instead of\npulling the person like the best image\nwe pour the fly key call that as far\n1.6.1 this flykeys for Best Image\ncontain all the spot dependency so you\ndon't need to worry about Auto add spot\nemergency into your Docker file\nand then\nso you only need to install uh all the\npython events you need in the\nnamespac\nso\nto build a multi Arc image image\npeople can just specify the different\ndifferent level in image but if we use\nall the owner images is Linux and mdl64\nso if we want to build if we want to run\nthe tax on\na machine you can just change\nthe architecture here and then\nbuild image\nand then you can go to Hawaii\nuh the fact you will build a image for\nyou you can see why I have a\num image architecture image and and the\n64 image\nso in summary like\nby using image value to build image\nwithout Docker file you just Define your\ncontainer requirement in your flight\ncode and then you also support the music\nArc image and then by using emvd after\nbuilding the image you will remove the\nunused cache from the from your data\nimage so you will get a smaller and\nlinear image and then by looking all the\nDocker Hub uh you you can avoid\nrebuilding the same image over time\nand then one more future I forgot to\nmention like\nMVD also supports remote build so let's\nsay I don't want to build the image\nlocally you can\nuse those power around to build an image\nthe empty will create a local contacts\nuh by default\nhere we can see I use default contacts\ndefault contacts will\nbuild image locally every contacts have\nas a\nbuilding Builder instance so this\nexperience distributor will help you\nbuild an image so let's say I already\nhave drilled\nremote contacts and then I can check\nthat shoot\nyou know so for now\nI can\nclose my Docker textile\nokay now I already closed my double\ndesktop and then I can run paver again\nand then you can see it\nstill can start building the image\nwithout running docs desktop locally\nokay\nso\nby using image value or support and\nremote build and then you also simple\nsimplify the process of the building\nSpar image by using a\nuh Base by image conflacky and then\nfuture world like we would like to\nsupport GPU inspect but now all the\nolder image doesn't have a good app\ndriver so you can use this image to\nchannel model but we will support it in\nthe next release\nand then feel free to give it a try by\ninstall Plucky and lucky blocking empty\n1.6\nokay\nany question\noh thanks Kevin for demoing the feature\num this feature could save a lot of time\nthat typically goes into Building images\nwith darker files and it is abstracting\naway Docker which I think uh simplifies\nthe uh iteration process yeah so um does\nanybody have any questions or feedback\nyeah I only wanted to say that\nI think we are just discovering the the\nimplication of this feature for the\nbetter I mean it has the potential in my\nopinion to transform part of the process\nof house and other users\num leverage flight in conjunction with I\ndon't know cicd systems and\nand\num yeah I think it's it's massive\nthank you King\noh I saw some some questions in Reading\num dude the package getting story at the\nlocal registration time uh yeah all the\npatch\nare installed at the registration time\nso\nuh empty you stalker under the who so\nwe've already registered they will\nfollow Iran Taco Bill locally so we\ninstalled those page before in\nregistration time\nuh Hey Kevin I have a question for you\nthis is a really cool\num way to like parallelize that install\nprocess what was the situation that you\nran into that um\nI like inspired you to come up with a\nsolution\nyour question is why\nhow do I come out with this solution\nyeah like I was like the the use case\nlike that had that bottleneck that\nrequired um\nrequired this\nuh okay uh so\none of the use case there like every\ntime I ride a workflow I spend a lot of\ntime to write a Docker file\nand then\na lot of example uh flywheel for example\nlocally and then uh\nevery like every week I will remove some\nsome Docker file in some local file and\nthen I find that next time if I build a\nvirus another I write a sentence file or\nwrite a new workflow if those stock file\nuh already built before but I don't know\nI forgot the image name in the darker so\nI have to I still have to rebuild the\nimage but by using image spam imagepam\nwill calculate the hash uh from uh thank\nyou for calculate hash from image stack\nand this hash will be the tag of the uh\nwill be will be attacked after your\nDocker Docker Docker URL and then you\nwill look at it dot com to see if the\nimage is so which means I had if I\nalready built\nthe image at some point before I don't\nneed to rebuild the image again\nand then another reason we another\nanother reason is that uh MD is my\nfriend my friend is creator of empty and\nthen he really want to integrate empty\ninto fly so\nwe find it like empty it's good yeah\nbuilding the image but imply\nuh we we don't we don't really like help\ndo certain manage or probably just build\nthe image so\nhow to build an image people have to\nfigure out by themselves so with empty\nwith image star people can like easily\nto build the image without worry about\nwhat how to remove the cache how to\nhow to build how to make building image\nfaster like yeah\nawesome yeah it's an awesome like\ndeveloper experience left\nforeign\na little bit late but um how like what\nis like what is the support for cloud\nbuilds directly right so yeah the\nexample the demo that you're showing was\nit using buildac a build kit essentially\nlike locally uh with demon local cash\nlike what kind of support you know for\ncashing in for bills uh existing MD for\nyou know like running on a remote build\nkit cluster or something\ndoes that exact question makes sense\nso\nso we can do it again\nso yeah so I mean briefly the demo was\nrunning a Docker build locally on your\nmachine\num zombie as part of like the register\nprocess but I was wondering if ND also\ncaptures the use case where you might\nuse a remote you know build a build\ncluster as opposed to like a local local\nDocker demon\num I don't know if it has support for\nthat like do you do you know and can you\ntalk a little bit about that\nyeah so\num\nthe reason why we wish you remote build\nis that you can have a dedicated\nremote machine to build a Docker image\nso let's say you can have like a machine\nwith like 100 CPU and then you allow you\nto build an image much faster\nand sometimes like I don't want to run\nthe dark desktop locally because you\nusually take occupy like 20 or 30 like\n20 CPU locally so I don't like to run\num\nlocally and another reason to use remote\nbuild is that if uh\nin like for example in the company\num\noh like they can have a dedicated remote\nmachine to build the\nimage and then\neveryone in the company can use this\nremote machine so if uh if someone\nreally built uh like build an image on\nthe remote machine and then for example\nlike if someone already built an image\nfor tensorflow\nremote machine will cache your\ntensorflow package so next time someone\nwants to write another word for this one\nand this will also you have 10 support\ndependency and then you can build the\nimage on the remote machine uh you will\nuse the remote cache so you can make\nComputing image much faster yeah\nawesome thank you\nkitten you want to say something\nuh no I just want to say this is an\namazing creature I I think I have myself\njust to add to Kevin's thing I myself\nhave spent you know\num just to write examples sometimes we\nhad to write Docker files\nand this ended up causing problems for\npeople when to use the example and in\nthe in the documentation for the example\nshould include the docker files and how\nthat's in a separate place\nso we ended up writing many many\nexamples that use the base Docker file\nright which is not the reality\num so this just makes it extremely clear\nclean and I think goes well with our way\nof thinking in Flight which is two\nthings one is\ninfrastructure and code are together\nthey are declarative so this brings this\nmakes containers declarator\nand secondly\nyou don't need to use your like you know\nyou don't really need to spend a lot of\nenergy trying to understand how to on a\nmulti-container image and or\nmulti-container workflows Etc it should\njust be triggered that's uh amazing\nkudos\nokay\nyeah thank you\num so how's the docker image selected\nfor flight uh spark task you mentioned\nit's automatically taken care of right\nso I haven't understood that very well\noh yeah a question like uh\nso when you run a spa test you have to\nspecify the spa config and then we use\nevery if you specify this for config\nhe he will use like those your\nspot plugin right and then your plugin\nstop working there's a underscore on\nstore in me so every time you call the\nSmart Text in the init function we will\nreplace the default base image with the\nspark as image yeah\ngot it thank you and as the flight get\nuh plugins nvd uh installed as part of\nflightgat or should it be installed\nseparately\nuh\nyou mean the MD plugins\nyeah yeah\nuh empty plugin will install empty in\nthere like okay so you just yeah you you\njust installed the you plugging\nempty yeah\nall right um\none more thing sorry\num this is\nthat we should probably make every\nplugin have a basis yeah yeah\nyeah\nbecause then then like let's say you use\ndesk or you use even use Pandera or\ngreat expectation they just start there\nalready installed and so makes it super\nfast to start using new things yeah yeah\nso for now we only replace the base\nimage in spot plugin so we will like to\nupdate all the big and plugging Radix\nindividual yeah"
    },
    {
        "title": "Flyte Community Updates 036 featuring Show and Tell, Feedback",
        "transcript": "welcome everybody to the community sink\nmy name is samhita I'm with the Union AI\nteam and I'm your host today today's\nmeeting will be short since we only have\none talk on the agenda uh next slide\nplease\nyeah I'll go over some logistical\ndetails and uh Community updates and\nKevin will be demoing the image spec\nfeature\nthe agenda is open to the community it\nnow lives on hack MD please feel free to\nadd questions or discussion topics if\nyou have any\nand this meeting is being recorded and\nthe recording will be posted on the\nflight YouTube channel\nyeah so um\nslide 1.6 has been released uh please\ntake the release notes and give it a try\nlet us know your feedback or comments if\nyou have any\nif you go to the next slide\num\nright uh so if you have developed any\napplication using flight we'd love to\nhear from you feel free to share them\neither in the GitHub discussions or the\nshow and tell slack channel uh and\nremember it doesn't have to be limited\nto Applications we welcome any kind of\nproject or accomplishment you would like\nto showcase\nand if you have any feature requests\nthat uh that have already been filed as\nissues uh please give them a thumbs up\nreaction the more thumbs up reactions\nand issue receipts the greater\nimportance we will assign to it in our\nrelease planning your feedback will play\na crucial role at prioritizing the\nimplementation of requested features so\nwhenever you get a chance please take a\nlook at the current list of issues and\ngive them an upload\nall right\num let me go through the links and\nresources if you would like to speak\nwith the flight maintainers you can\nschedule a 30-minute session using\ncalendar we offer time slots on\nWednesdays at 7am and 9 pm PT\nand the next Community sync is on May\n30th Robin eggplant from Capital we'll\ntalk about their flight adoption Journey\nyou can use the add event link to add\nthe event to your calendar\nand yeah that's about it we have covered\nthe agenda items for today which means\nyou're going to get your 30 minutes back\nthank you so much for your time and\nparticipation see you all at the next\ncommunity\ncan I add one thing before people leave\nuh this is an open Forum if you want to\nshare your Journeys please uh let us\nknow and\nand yeah and I think this helps the\ncommunity a lot every time like Stripes\nsharing it last time or Spotify before\nthen Etc I thought right it really\nreally helps the community understand\nhow they can use uh right so\nthank you\nforeign"
    },
    {
        "title": "Flyte Contributors Meetup - May 11, 2023",
        "transcript": "let's get started okay welcome everyone\nto the flight contributors Meetup today\nis May 11th and uh well happy to be your\nhost a couple of reminders these\nmeetings being recorded\nand will be posted to the flight's\nYouTube channel and uh every as every\nCommunication channel for this project\nit falls under the Linux Foundation\ncut of conduct which includes several\nprocedures to make you feel safe\nhopefully\nand prepare prevent any kind of\nharassment\nanything like that okay\nuh so welcome and uh glad to see you all\nhere like every two weeks\namong the wave of Open Source llm like\none every week\nit's glad to see you all here\nstill paying attention\nI guess as they as they say goes\nattention is all you need okay stop I\nneed to stop okay uh there you go some\nand I will share the link to the agenda\nhere\nI'm asking you to consider adding\nyourself to the attend this list right\nhere\nincluding your affiliation will be great\nand okay\nanyone joining for the first time let me\nsee no I don't think so\ncool\ngreat next one uh let's get to it the\nreview outstanding rfcs\nso the first one the config override I\nsaw something in the agenda regarding\nthis RFC has to do with naming so I\ndon't know if Byron or anyone else wants\nto touch him yeah I'm here\noh\nso I think everyone agreed on the\ncurrent user user interface but we're\njust discussing about the name and the\nimplementation effort\nyeah the reason why I wrote that comment\nis because\nwhen we first discussed this at the uh I\nI wrote with runtime override and Keith\nand said with override Hooks and we just\nstick with the first name maybe before\nwe merge the RC we should agree how it\nshould be called it's a minor thing\nthough it's um yeah\nalso I remember someone brought up the\nimplementation things\noh let me check\nyeah yeah I think that was probably me\noh yeah\n[Music]\nlet me see oh Ben I think prepared\nroll up the implementation part right\num\nI've seen\nyeah very hard do you want to talk about\nthe implementation\nI saw your life\nI mostly\nwe would attach things so I think from\nthe ux perspective we're pretty pretty\nlined um\nand\nyeah I'm not no strong opinions there I\nthink\num someone proposed I'm not sure who it\nwas on the original\num thread\nthat we could do some matching so that\nwe\ncould make the whole system a bit more\nflexible in the background so that we\ncould for example also allow for\nlike in the future saying we match this\noverride to a bunch of node names or we\nmatched to a bunch of\nwhatever\num pretty much you can cook match it to\nto anything\num yeah\noh yeah sounds good yeah yeah also I\nwant to discuss about the word\ndistribution because I think yeah uh\nyeah I think I I might need others help\nto work out I think it would be great if\nwe can work on this together maybe\nseparate it to front end and back end\npart and we can work down the pr\ntogether\nKevin do you have any idea obvious\num\nI think we should work on the bacon\npartners\nokay\nyou mean a propeller and idea\nyeah\nunplugging liking don't like it okay\nsounds good\nyeah we can start working together after\nafter we go everyone would agree with\nthe name\nso I'm interested in helping I would\nneed some guidance where to start where\nto implement it I don't think I know\nthese parts of the code page\nbut in general I'm interested in also\ntaking part\nokay\nyou ask him here so I'm also happy to to\nhelp out um\nI think I've seen small bits\num but\nespecially on the propeller side I don't\nknow all the bits I think\nsounds good\ngreat thank you so much any other\ncomment about this proposal\nyeah we just need the opinion of the\nname maybe ask\nor some someone\nI think wait for his comment\nhey I think I think Kevin will help\ndrive this\num so like we'll coordinate uh\ncoordinate work between between everyone\nif that sounds okay but it's Byron on\nthe fabulary you mean cretan or Kevin\nKevin okay okay\nsounds good\n[Music]\nhow are you say you also wanted to help\nout\nyeah if there's something like yes\nthank you thank you very much\nsorry I need to ask a question is\nsomebody sharing a screen I think my\nzoom is broken\num yeah I think David is sharing okay\nI'll re-enter I can't click anything\nalso can't click the chat you see on a\nsecond I'm sorry uh\nyeah I'm not sure\nokay\ngot it cool thank you\num yeah the current status of this the\nproposal is still in review\nbut I see you already mentioning\nimplementation so\nuh\nit doesn't have yet the approvals to be\naccepted or not even passed the final\ncoming period so what's what's our idea\nwith this proposal do we want to enter\nthe final comment period\nuh kind of final stage for for objection\nEtc before before\nthat one um\nsorry what was that\nnevermind sorry\nyeah so in terms of status\num\nI'm back welcome value in terms of\nstatus what do we want to do with this\nproposal moving to final coming period\nyeah yeah I think we can move to a final\ncomment area\ndo we need to even agree on the the\nexact details of how it will be\nimplemented in the back end like if\nnobody has any concerns that can be done\nand could cause problems\nmy question is does it have to be\nincluded in the RFC how exactly it will\nbe done\nyeah\nthat's ideal I mean in general in terms\nof the the RFC process it's ideal but\nnot really required to be accepted\num and in the goal with the FCP with the\nfinal coming period is also to give some\nspace but update line\nuh to discuss or agree on the potential\ntrade-offs during the implementation\nphase and\nafter that you will agree you can\ngive your approvals and and it will be\naccepted okay\nyeah good question\nall right there great anything else this\nproposal\nno\nall right uh next up in review was a\nsmall thing on but it could become huge\nand on community groups\num there was a really good question from\nBernard and\num\nkind of the chicken eggs problem do we\nhave enough contributors right now to\nhave this kind of a structure\nuh probably the plain answer will be no\nbut\nthe existence of community groups\nworking groups uh special interest\ngroups will create more opportunities\ndifferent opportunities to contribute\nand so it will it will kind of bring a\nnew wave of contributions\nand I think it's helpful probably yeah\nprobably it has a bit more structure\nthan required now but I didn't want to\njust\num\nadd an ad structure just uh on demand I\nwanted to propose a kind of a skeleton\nof a structure that will help both kind\nof groups\nand the other question was if we were\nplanning to form any group right away\nI think we need to do\nquery or survey the community on this or\ntry to gauge the interest uh specific\ninterest in the community on this\nbecause I I could think of for example a\ndocumentation working group easily with\na clear goal to a clear variable to I\ndon't know improve certain parts of the\ndocs uh specific part of the dogs not\nexpecting that that the community will\nactually write in the entire thing but\nwe will help collaboratively write and\nreview\nand\num where this provide a wish list\nrequirements um\nyeah I think we can we can start\num\nput in this test on this kind and we\nmaybe count the config override as a\nsubproject under this proposal\nuh config overwrites a project yeah\nwell that that should be part of uh sake\nor working group I don't know which\nworking groups are first I read it again\nitself that they have a that they're\nshort-lived and yeah very concrete\ndeliverable attached to them right I was\nI raised my hand because I wanted to ask\nthe same thing the deliberate could be\nbasically this RFC that could be the\nfirst working group now yeah that would\nbe great because it has a clear goal and\nwill will kind of help us probably\nrecruit uh more contributors or folks\nfrom the community to help or at least\nto give their opinion so yeah it's a\nreally good idea\nthank you\nawesome yeah uh so any other comment or\nany objection to move this to the final\ncoming period\nyeah sorry I guess uh I I think my only\nno it's not a concern but\num I don't have any I like formality and\nI like to process just\num as I don't think this will reduce the\nvelocity\num of contributions that also\nI'm sure\nyeah sorry I I love you so you think it\nwill reduce the velocity no no I I don't\nthink it will okay yeah thank you\nyeah\num the goal is to\nkeep it keep keep the velocity and also\nkind of scale out a bit open more doors\nso let's see how it goes thank you\num\nright\nawesome uh great there is a proposal I\nthink it was the rain node\nthe it's\nit's accepted right now it has all the\nboats so good I think it even the\nimplementation is already on the way\num any comment about this proposal\nyeah oh okay that's a that's a very\nquestion for Keaton\num nope not coming from this\nproposal that's good that's fine\num okay before moving to the next item I\njust saw a new RFC proposal popping up\nhere so uh that's very interesting\nthat's real time\nuh RSC creation so um yeah Niels do you\nwant to briefly introduce\nis RFC yeah\noh sure let me do that\ncool so\nthis RFC has been in the incubator for a\nwhile I've kind of been cooking\nuh informally just like making a work in\nprogress\num just to see if this actually would\nwork\num but the idea behind this RFC\nis articulated here\num\nthe\npunch line is that\nkind of like we have static workflows we\nhave Dynamic workflows sometimes called\nDynamic tasks\nand we have this this proposal is to add\nan eager\nworkflow quote unquote\num and it's\nsimilar to Dynamic workflows in that\nthey are tasks kind of masquerading as\nworkflows whereas Dynamic tasks compile\nan actual workflow\nunder the hood at runtime using inputs\npotentially\nthese eager workflows are actually\npretty much fully just tasks under the\nhood\nand\nit's not it's missing here but if I jump\nto the pr\nan actual working version of this\nexample\nuh\nso right this makes it a little clearer\nso you basically like make oh this is\nbad this actually shouldn't be here so\nyou make a essentially a remote object\nand it's essentially flight remote just\nlike kicking off tasks on the Fly\num\nthere are a couple of decisions here but\nessentially\nevery on in the context of an eager\nworkflow\ntasks which are defined up here just\nregular flight tasks they're turned into\nasync\ntheir alternative awaitables in Python\nWorld\num\nand so like all the awaitable the async\nsyntax and python applies here\num and you can basically call tasks just\nlike regular python weightables\nand flight remote handles just kind of\nlike\nkick up kicking these off as you go\num so the benefit of this\nis you can do everything that you can do\nin Python you can try like try accept\nyou can use async IO\nyou know constructs like gather\nyou can do\nthis else statements\nit is basically Python and under the\nhood it uses like\nbasically polling to\nwait for these things to finish\num before wait for tasks to finish\nbefore you know passing the data the\npromises\num Downstream\num\nthis is the same code example and this\nis just a table just kind of like trying\nto articulate the spectrum of\nuh I guess static-ness and like\ndifferent properties of these things\num\nhere's an example of a workflow or sorry\nan eager workflow\nthat\nWorks uh clearly it's just a task\nnothing has you know like flight doesn't\nunderstand how this is\num the only way we know things are\nhappening is uh\nthere's a slight deck that's\nautomatically kind of collects tasks\nthat they're as they're kicked off in\norder\num and then it's I'll put it here as a\nflight deck\nso you know very very hacky but uh it\nworks\nit's cool to see it work I think we\ndiscussed this the first time\nlike a year ago in the context of trying\nto build a hyper parameter optimizer yep\nand\num\nI have two questions the first one that\nthis may be a quick one for now it's in\nthe deck to put the plan B to eventually\nhave it kind of maybe as a drop down or\nso on the main view or\nyeah I mean I I think the you how this\nis rendered in the UI is\nup for design\num but something like this uh\nbriefly run down the rabbit hole of\ntrying to construct the graph\nbut I quickly pulled out uh and uh yeah\nit was really not worth it so\num I think temporal does something where\nit just does the list view which is you\nknow kind of just like the\num\nas sub nodes as nodes are being executed\nby the eager workflow it will just kind\nof get appended to the to a list well\nthe other question that happens I think\nthe more interesting one\num when doing hyper parameter\noptimization you might figure out early\non that your task is not really going\nanywhere that you might want to prune it\nis there do you think there will be a\nsyntax to say hey we don't need to wait\nany longer let's just kill this run\nyeah that's a good question I\nI'm fairly yeah I don't know all right\nshort answers I don't know I feel like\nyeah we should come up with like an\nexample like this because I think that\nwill be very\na very common or if not useful example\nto have\num so you're talking about a case where\nwe might kick off a bunch of these right\nI mean and it departs from the async i o\nand the weight\num syntax right that doesn't have it but\nfor example if you look at what's a\npopular hyper parameter Optimizer um\noptuna for example right\num they kind of Trials and then they\nmonitor some Metric and when they notice\nearly on that the loss is going to\ncompletely wrong direction they don't\ntrain until completion they just kill it\nand\num I feel that's at least\nthat will be the if I wanted to build\nhyper parameter optimization on top of\nthis this would be the first feature I I\nwould want\nyeah\nbut why can't you kill it if these are\nour futures right I know the our weight\nFutures don't have a cancel on them\nbut these are our futures we can have a\ncancel method on them right as long as\nyou're wanting to read partial data from\nthem as long as it's an entire task that\nyou're canceling I think that's stupid\nyeah I'm not arguing it can't be done\nI'm arguing that like that there needs\nto be a syntax that um that that lets\nusers do that um yeah yeah in the end\nthese are just execution started with\nslide remote I guess right so\n[Music]\nyep exactly\nI I like the idea of like actually\nhaving a cancer so that this is one of\nthe things that I was actually telling\npeople that we probably do not should\nnot\ncompletely follow a single threaded\nmodel we should actually follow a\nmulti-threaded model for the core\nroutine right this is a CO routine and\nby that what I mean is the moment you\nactually have an async task being\ninvolved it is invoked in the back end\nand now even before calling in await you\ncould possibly\ndo a DOT cancel or something it's not\nthe eager work for them would be\ncanceled right it would be a task that\nhas started in an eager workflow\nyeah\nso this is just to be clear this is\nwe're not in the training Loop of a\nsingle model we have like a thousand\ncandidates or something\num\nor in this context are you typically\nevaluating them\nlike 10 at a time or something and then\nkind of like\nit depends this is like an amazing\noptimization thing I think it depends on\nthe hyper parameter optimization\nframework I think you know tuna they\nthey evaluate themselves actually by\ncomparing themselves to like using their\nshared database to other runs and then\ndeciding I should stop myself and start\nsomething else\nbut I think the the 1db pipe hyper I'm\nnot 100 sure now but I think you can run\nthe 1tb hyper parameter hyper parameter\nOptimizer yourself and there they have\nthe concept of just stopping runs when\nthey they did they know the metrics\nright\nand then they stop runs I think they\nstopped them by having the 1tb.log\nfunction call rate and exception the\nnext time they log a metric so that's\ntheir mechanism\nbut with my last company we tried to\nbuild an optuner hyper hyper parameter\nOptimizer task and we ended up doing the\nsame thing we just had like a python\nfunction task and that you could import\nand that one ran flight remote\nand um then we had like multiple\nmultiple threads each thread monitored\none task and then when we noticed that\nthe result it wasn't good then we\nstopped the execution and used that\nthreat to start another trial with\ndifferent parameters\num so it depends a bit on the exact on\nthis on the framework but they all have\nlike a notion of stopping something\nwhen you need to prune a run because\nit's not worth\ncontinuing because you know the results\nare not going to be good\ncool yeah I think I think it's worth\ncollecting\nwell I noted this one down but\npotentially other use cases where it's\nlike if we were to build this feature I\nthink it's worth investing and making\nsure these things work\num yeah yeah other than that it's an\namazing feature\num\nyeah so so just to uh Point people to a\nfew\nopen questions\num these are kind of I guess related so\nlike what does the ux look like for like\nat eager like if I want to do this\nyou know currently\nI have to head\nDefine a remote\nobjects the thing that's magic like\nmagically happening under here is I've\nhard-coded and eager like a specific\nsecret group and\nP that is basically this the flight\npropeller client secret\num so that's like a back end\nuh design Choice here of like how do we\nexpose that\nto make it super convenient such that\nyou don't even need to do anything extra\nhere\num so that hasn't really I didn't really\nthink too hard about that um just in\nthis MVP\num yeah that's basically these two\npoints like how do we\nlike does an eager workflow is is that a\nspecial thing that flight just\nunderstands and injects a bunch of a\nbunch of stuff\num under the hood and then the ego\nworkflow can like use those things\nyeah so\num\nyou know feedback uh would be awesome\nany questions any additional use cases\nwould be great\nawesome thank you all and thank you Nils\nany comment\nabout this proposal\nany other comment\nnope I'm sorry just one more thing is uh\nI had trouble deciding like\nthere was one design of this one version\nof this where if you just say async def\non a task it just automatically now\nyou're just invoking tasks within tasks\nright and there's just async tasks\num I kind of opted for eager\njust to kind of isolate the feature at\nfirst just to make it very clear that\nonce you decorate this function then it\nalso has to be asynced app\num\nthe little weird magic that happens here\nis like\nunder the hood this is this task is like\npatched and becomes an async entity\nwhich is defined\nuh\nhere\nso it wraps it just like looks at the\nmodule finds all tasks wraps them\nan async entity and then any task that's\ncalled within the function body is now\nan awaitable so a lot of weird things\nhappening but um\nthis ux seems like it isolates the\nfeature a little bit better than\num if I were to just async def this like\nnow everything is\nnow we can just call other async Dev\ntasks\nso that's just another decision point\nokay\nthank you\nthank you any comments\ncelebration question\nno all right thanks so much nails\nall right and the next RFC uh\nI think\nthere's uh something here in the\nexecution tags RFC\nuh I don't know who added this note but\nI wrote this comment\num so the so the RCs and the kind of\nfinal approval effect right now and\nthere are two approaches out lined out\nhow users could assign labels or or we\ncall them tags via the CLI and there's\none option where they can basically\nassign any key value pair they want and\nthe other approach would be that there\nare special key value personal group\nexperiment name\nand there's\num in the in the in the pr there's a\ndiscussion how exactly this would look\nlike\num and I feel before we merge this we\nshould agree on whether there will be\nspecial key keywords like I'm against\nthat for discussion of course or whether\npeople can just assign any\num any key value pairs\nso that's one question that I think we\nneed to answer before we can merge this\nand\num\nyeah let's let's do this one first\nthe question basically is should we\nshould attack that the user assigns just\nbe any key value pair they want\nor do we do like special tags like group\nexperiment name that have a special\nmeaning\nokay\nto me it's not clear where the advantage\nof restricting this to specific key\nvalue pairs is um you know I Envision\nflight I know your particular use case\nis typically for model training things\nlike that but people do use it for ETL\nor maybe experiment doesn't make any\nsense in that use case\nand there's a lot of different use cases\nso in my two cents I don't I don't see a\nclear advantage of restricting it yeah I\nagree that is also what I wrote on the\npr so that's why I propose that we uh so\nmy what I wrote in the pr I'm interested\nin your feedback is that basically tags\ncan be any key value pair and that\nexecution name is not a label in a part\nbut actually has influence on the on the\nmetadata.named field of support we\nactually do that already because flight\nremote allows you to customize the\nexecution name and part of the\ndiscussion in the in the pr is whether\num whether things are mutable but then\nof course the execution of the company\nmutable\nand um if we if we don't if we say there\nis basically there can be any key value\npair and execution name is not a tag\nthen I think this this discussion is\nbasically answered\num\njust just want to plus one the arbitrary\nkey value\npairs I think I think it'll be up to\nusers or dsls on top of white to like\nhave opinions about that\nyes same here I think I wrote it today\nyeah\nand\num the other thing that is not 100 clear\nto me yet is um when I say Pi flight run\nminus minus tag full bar then then\naccording to the IRC right now I would\nhave a labor Foo bar on the pod\nand we would also I guess persist them\nin the database\nand I might also want to add a tag to\nthe execution after it already finished\nso let's say the part's already gone\nbecause the note shutdown\nthen I can click in the UI and also an\nattack will it then be saved in the same\nplace in the database like we treat them\nthe same\nwhere do we store like longer text that\npeople might want to assign to net to an\nexecution there was one other discussion\ntoday\num in the discussion section where\npeople where somebody asked that they\nwant to be able to to like write a text\nfor it like a summary for an execution\nfor example telling College why it\ncrashed like we treated this the same\nway or is there something different\nit sounds like something different right\nnow\nyeah\njust to yeah the labor listed anything\nthat description is a description\nI agreed something different\nbut I think it needs to be clarified in\nthe RCM\nall right thank you Fabio\nany other comment about these questions\nright yeah thank you so much how are you\num all right eager mode check so I think\nnext up was the or is the release\nplanning\num is this something we are prepared to\ndo here right now you know nails or I\nthink\nyeah sorry go ahead uh Eduardo's at\nright now but I can give a quick update\nso we\num are hoping to get 1 6 out as I don't\nknow what actually this means when you\nsay release planning bills uh to release\nupdate is at least that we're trying to\nget one six out as quickly as possible\nand along with that\nwe will be pushing out\num 1.5.1 as well 151 is a patch release\nthat should not affect\num then we won here I don't lose uh but\nbasically it is a\nsmall small patch to handle a corner\ncase in extremely old deployments of\nlight and there's a detailed uh release\nnote in the in that patch release but\num yeah we're just waiting on that and\nthen\num\none sex groups quickly followed\noh yeah another thing just to add here\nis\num\nEduardo and I just still need to\nmove this stuff over I don't know if\npeople have access to this Google\nspreadsheet uh maybe David if you can\nvisit that link and it's just like a\nscratch Pad planning uh spreadsheet\nthat\num\nto the best of our knowledge like\ntracking all the work that we sort of\nwanted to do 160 and Beyond\num so sorry 170 and Beyond\num so some of these probably won't you\nknow so I guess the array node will\nprobably be one eight oh something like\nthat but um moving forward we're gonna\nessentially\ndo this exercise kind of Union AI among\nthe uni AI team but the idea is we then\nmove this over to a flight issue that\nwill just be like titled something like\nrelease 170.\num and then folks in the community\nincluding this group can\nprovide feedback we can update things\num\nso yeah just I think I'll be out of\noffice for the next two weeks two weeks\nbut um yeah I'll work with Eduardo to\nlike get that that um kind of like\nplanning issue out and so we'll plan to\ndo this moving forward so that we're a\nlittle bit more\ntransparent and you know this the\nitems and commitments we set out to do\num for each release is a little bit more\nyeah accessible\ngreat thank you\nthank you so\nyeah keep it keep an eye out on the on\nthe issue\nand hopefully get involved with the plan\nlet's share the the Google street the\nplanning\nyeah yeah so I'll I'll\num if you request\ncan you request them access I I can do\nthat real quick I think I'm the doc\nowner\num but as I said this will also be an\nissue so okay cool I'll just share it\nwith everyone here\njust real quick is there anything that\nstands out to anyone else not at Union\nlike that like you have a strong opinion\non or you want to help with\nsupport\nthat there's notifications I think would\nbe really nice like\nI think I saw it somewhere I think uh\nmaybe I can help on the part template\npart\nthe documentation that at was assigned\nto because I implemented from Empire\nmaybe I can help the uh row seven\nI know Eduardo's got a pro for that that\nwe're just iterating on right now so\nokay oh oh sounds good okay\nyeah\nI said before about the\num\ngot my brain I'm sorry\num okay my brain starts sleeping the one\nthat from you Byron I want to help for\nthat\nwhich one\num the overrides one okay okay yeah I\nthink we can cooperate on this one this\nis a big one I guess yes\nyeah\nmulti GPU support\ncan somebody say sentence about what\nthis means please yeah so the that is\num is going to be multi-part I think\nactually I'm not knowing maybe I'll\num\num it's investigating primarily\nuh from the base level so gcp I'm sorry\ngke and eks how the Nvidia GPU\num demon set interacts with kubernetes\nand then from there so supporting both\nlike different types so like if you have\ndifferent node groups of b100s and a100s\nand then also thinking about fractional\ngpus so everything from hell\nhow those are exposed by the NVIDIA\ndrivers to the kubernetes layer all the\nway up to how that should be exposed to\nthe user in terms of resource requests\nand in this effort we may think about\nother things like shared memory maybe as\nwell\ntemplates currently all of what you said\nwe do with part time Clips oh yeah\neverything that that we that I that\neverything is doable currently in pod\ntemplates\num but it is going to be like kind of\npulling that not pulling but like uh\nexposing it at a higher level basically\nhigher abstraction make sense\nawesome yeah I I don't think we plan to\nlike discuss these items right now but\nthis is awesome\ngreat\nthank you\nall right uh\nuh finally a couple of new ideas in the\nincubator\nI think I saw a new one\nyeah\nthere are a couple of ones so should we\nstart with this one as someone put it\nthere\nyeah it reach our reach out to Tim the\nauthor he wasn't able to attend\nuh but um\nyeah feel free to to read out the idea I\nthink it's interesting and you can kind\nof a boat here or write comments yeah\nmaybe I start because uh actually this\nweek we looked into slack Integrations\nand we did it because one day we know\num because sending it like it you need\nan email client currently either on AWS\nor send credit or something and it would\nbe really nice if there was just the\nnative like integration that supports\nwebhook and or the the app up\num\nso literally this week we look for the\nsame thing and then when like did it\nwant to be\nso I think this would be really valuable\nyeah same same for us um so we\nespecially you know company politics and\nthen we had a Sanskrit account but now\nwe don't and there's just yeah\num we are also defaulting to\nI have a question is is this useful like\nwithout like a framework for\nestablishing like a trigger and all that\nkind of thing or do is that or maybe I'm\nwrong maybe that's already existing and\nlike\nso you're referring to the part here\nwhere they say that we could Inspire\num the tech inspiration from Argo CD how\nthey do it well yeah I mean that's but I\nmean in other words in general and\nnotifications right it's like okay but\nyou know when are you gonna get it like\nhow are you gonna say when you get it\nand how do you like you don't want\nflapping notifications like firing\nconstantly and like so there's like\nMachinery or like what triggers it how\nmuch time before you send another one\nthat kind of thing\nSo currently you cannot already\nconfigure that in inside kit right\nand there is a slack notification class\nwhich on the load sends an email but\nthere is like user facing there's a\nslack notification plus I think you can\nsay which phase to send these\nnotifications on and since you you\nconfigure these in the launch plan you\ndo it very fine grain for every work for\nevery workflow or launch time here I\nthink in the in the RC uh Tim proposed\nthat this might be configured on the\nplatform side for a specific project\nright\nwhich would be cooler but if there was\njust like the current user facing API in\nFlight kit but I could just attach it to\nlike a black web hook I would be\nperfectly fine I don't need anything I\nwouldn't need anything else this would\nsolve my problem\nso this would be like bonus for me here\nwhat he brought\nokay\ngreat any other comment\nyeah just a reminder that the RFC\nincubator is you won't find fully\nfledged proposals here\njust ideas and he's trying to kind of\ngauge the audience or the interest\nso feel free to add your comments there\nuh would be great for the author to know\nthe people to get the feedback\nand uh if the steering committee and the\nmaintenance grip uh we can work with\nwith Tim to actually craft the NRC\num and start the process from there\ngreat and the next one was logging links\nconfiguration media to improve it so\nthat one from me so maybe I describe um\nwhat I tried to do this week and for the\nidea I had\nSo currently we link to stackdriver logs\nand we we want to be able to also lock\nthe link to 1tb we do that with the deck\nso the deck after an execution contains\nthe link to 1tb\nthe thing we don't like about that is\nthat the link would only appear after\nthe execution well we would want to\ntrack the like the loss curve so while\nthe task is still running we need to\nmanually navigate there so that's not\nperfect\nand I was asked a bunch of times in the\nlast weeks people said told me hey my\ntraining is hanging uh why is it hanging\nand then turned out the training wasn't\nhanging it never started because they\nrequested resources that weren't\navailable or just couldn't be\nprovisioned in the cluster and this week\nI tried to add there's this template\nlady you can give like arbitrary name\ntemplates now and I tried linking to the\ngcp console which is just an overview\npage that gives you the information that\nyou would get into control describe\nbasically so you see these bench on the\nparts and you see like some other stats\nand My Hope was that people can click on\nthis link and then see the message like\nred exclamation marks not triggered\nscale up cannot be scheduled and they\nfigure out okay I requested something\nthat the cluster cannot provide my plot\nnever actually started\num the thing why that what that link\nlike adding that link was perfect like\nworked the the reason why it isn't\nuseful is because this link is only\nappearing when the Pod is running I knew\nI would have needed this link to be\nthere to be shown when the Pod has been\ncreated but not necessarily running\num so it's not something I absolutely\nneed but I wanted to guard the interest\nfrom other people\num I think it like I could have built\nwhat I wanted to build if I had the\noption to say the template that I had\nhere which links to the Google Cloud\nconsole kubernetes engine sections to\nthis specific part show it when the Pod\nis created don't wait for it to be\nrunning this option that existed then\nour Engineers they could click on it\nthey see red exclamation mark yeah kind\nof be scheduled not triggered scale up\nand then they wouldn't ask me anymore\nwhether training doesn't start\num it's like I just documented how they\nfind the section now but if we if we\ncould configure these things more and\nalso had more information available\nwhich is the second Point like if we had\nif if we're templating we had the\ninformation of which were flight version\nis this which which execution that is\nthis then we could also provide the link\nto 1tb there uh because we basically can\nsearch for we set a tag in the run to\nlike we said the execution that is\nattacked and then you can there is a\nlike we can come up with the URL that\nbrings us to the right 1tb task\nand then we could have put basically\nmore links there that are shown also\nbefore the task is actually running that\ngive more information to people\num\nso nice to have like we live without it\nand it's fine\num but I think I could build something\nuseful if I if I had more information\nthere to template and if I could show\nbefore the Pod is running\nthis is great I've been doing a lot of\nfine tuning\nstuff uh internally and llm fine tuning\nand yeah it's like I bind the execution\nID to One One DB run name and it's it's\nlike just a lot of copy pasting and just\nrefreshing pages on One DB side so it\nwould be awesome to like this is the\nexact use case that would solve this or\nexact solution that would solve this\nproblem\ncool\nright thank you for taking the time\num any other comment\nokay yeah please\num also give the comments right there in\nthe discussion be great remember again\nthe the main question to solve in the\nincubator is do you all think\nthis idea is a good fit for an RFC\nand if it is we can help uh to actually\ngo to the RFC process but yeah thank you\nall right\num finally yeah I think that's it in\nterms of the agenda is there anything\nelse someone would like to discuss here\nin question\nso just click uh just going back to the\nlog links\num RFC proposal it Fabio did you have\nlike what what's the\nis there a story here for like an end\nuser customization of like in the flight\nkit SDK\nbeing able to say\nfor some future service right like that\ndoesn't exist yet like I want a link\nthat I know based on some flight\nvariables\nI want to generate a link for and render\nit on the UI\nis that because I just based on the last\nquestion right is like I feel like this\nwould constitute an RFC if it kind of\nrequired changes in multiple places\nhear you not sure what the criteria is\nspecifically but like if there was a\nuser-facing uh like a\ntask files are facing\nuh way of expressing this\nI mean yeah did you have thoughts about\nthat yeah so the way I've wrote it right\nnow I saw myself as the user as a\nplatform engineer\num and I would have like I this week\ntried to configure this basically in the\nhelm values file but what you're saying\nright now is also that the the user of\nthe platform could say hey I generated\nthis link now I want to I want to have\nthis displayed in the UI from now on and\nyeah I do see um So currently what rml\nEngineers do they they log to clear MLB\nthey lock that using logging.info and\nthey put like a bunch of like they they\nthey write like a sentence before that\nthey then search for in the logs they\ncopy it from there paste it into the\nbrowser and then go there so they really\nneed to go to the stackdriver logs\nextend the time range to find this thing\nor search for the string that they know\nmanually copy\nif they had a way to say flight kit dot\ncurrentcontacts.dumplink whatever that\nwould be really nice\num\ncool\nyeah\nwhich also would solve the problem that\num of course I in the platforms on the\nplatform side I can configure a link to\na 1tb execution but that then relies on\nthe user creating the run and assigning\nthe right tags right if they don't do\nthat this link Point nowhere\nit would be way much would be way nicer\nfor a 1tb if the user and from from the\nflight sorry from the python side of\nthings could could create a link for the\nPCP console I think it's something that\nshould be considered like configured on\nthe platform side but I do see value in\nbeing able to do that from python\nI'm happy to provide an RC and put this\non just like yeah\ncool\nthat's great thank you Fabio\nright it's in\nis there anything else\nsure because\nhey so with that thank you\num for your\nattendance day I there was a question\nfrom Byron and adding a feature\nintegrating with Azure ad out\num yeah yeah I I added that to the\nroadmap already so cool and I'm sorry\nByron that that's something you can uh\nwork on for like 170.\nsorry\nyeah so I I already submit all the PRS\nyeah oh nice okay\namazing\nawesome okay thank you all for your\nattendance today and hope to see you\nnext one it was great to see you\nbye thanks all bye have a nice day good\nevening\nforeign"
    },
    {
        "title": "Faster end to end ML dev lifecycle with Flyte",
        "transcript": "okay so with that is my pleasure to\nwelcome Mick\nbecause I try to pronounce your last\nname well but I'm trying to avoid\nmistakes so welcome welcome in Mick uh\nfrom stripe and I think there are some\nother\num team members from stripe here so it's\ngreat to welcome you here\nand uh yeah I don't know if you wanted\nto share with us or it's part of your\npresentation and sharing with the with\nthe community kind of part of your\nbackground and your professional\nbackground and something you enjoy doing\noutside work\num yeah so um my name is Mick I am with\nstripe for four and a half years now\num I'm now based in New York City\nand I guess for my free time now I work\nin the I intern at a restaurant\num last year I took a part-time culinary\nprogram and I've been\nworking at it for four months for five\nmonths now yeah during the weekend it's\nat the Thai restaurant called fish\ncheeks if you want to join me on\nSaturday my shift it's 10 to 5 PM\nall right\nso I will share my screen so we have\nhere my teammate my tech lead Brian and\nalso one who has been a really great\num partner team as well as my\nem I think harshita might oh Hershey's\nalso here yes so those are straight\nfolks thank you for joining us\nyes I'm sharing my slide and I think you\ncan see my slide here\nyeah we can see your your slides\ncan you see my fools right now yeah\nPerfect all right so let's get started\nso the topic today it's called faster\nend to end ml Dev with flight and I want\nto focus on end to end because I think a\nlot of time when people talk about\ntooling and focus on the specific aspect\nof it sometimes it doesn't really\nencapsulate the reality of ml\ndevelopment which is it spends just it\nspends more than just training and so\nthe outline here will go we'll start\nwith prototyping on Notebook which is\nlike a to-go solution for everyone and\nthen how I think\num notebook become an anti-solution at\ncertain point and how we can Transit\nthat to flight and show the benefits of\nlight in multiple regards and lastly how\nto Transit seamlessly to\nproductionization and the results that\nwe see in the limited availability\nwithin stripe\nso to get started so notebook it's\nobviously indispensable in prototyping\num because it gives quick cycles of\nfeedback so\nthe interactiveness of it when we get\num errors and results and we can\nincorporate that to guide our\ndevelopment and and Analysis and at the\nsame time I think the other really huge\ncomponent is low barrier to entry so\nusers open in notebooks and there's no\num the additional tooling or knowledge\nthat you have to learn is just a\nreliable shell that always welcome you\nwith open arms and you can always go to\nright and so but at certain point this\nreliable tooling will become uh\na problem for you I think\num in the sense that\nwhen you have a workflow or computation\nthat is fairly straightforward it is\neasy to choose notebooks and to be able\nto reason about the results but as you\ntry to\nsqueeze the entire ml workflow within a\nsingle notebook\nand because of the cell based executions\nwhere you can execute\nand then in a linear order or like out\nof order the results that you see from\nthe notebooks sometimes you ask the\nquestion whether this model Sterling\nresults is actually a result of what you\nsee in your notebook or rather or\nwhether it there's a different\nunderlying stage in the notebooks right\nbecause you could have rerun certain\ncells you could have forgotten some\ncells and as a result I think when\npeople release models from netbooks we\nrarely have high confidence about what\nit can do or to be able to trace back to\nthe provenons in the metadata associated\nwith that just from the notebooks cells\noutcome\nand obviously I think one pattern that a\nlot of ml Engineers would do it's just\nto you know refresh everything start\nfrom the beginning and that's we'll get\nthe fresh states there and of course the\nthe concierge is really high just\nbecause the entire\num ml work workflow involves maybe\nfeature engineering that takes a lot of\ntime and of course the training is just\nitself\nand lastly I think maybe this is a\nsecondary concern because I think new\ntechnology now like Ray or better\nIntegrations with\num big compute tooling to notebook is\navailable but a lot of time when you get\nstarted and you want to scale with\ncompute notebook is an actual isn't\nisn't the primary solution for you and\num unless you do more non-trivial\nIntegrations that you'll be able to get\nthat compute resource and notebooks so a\ncertain point\nI think node will become an\nanti-solutions and that's when I think\nwe should look for a different solutions\nto enable developers to productively and\nreliably run their ml experiments in a\nreproducible fashion\nso transiting to flight with ease\num so it is a part of flight profile\nauthoring so what does it mean when one\nwants to use flight so if you look at\nthe image Below on the left side it's um\ntask that\num\noh oh no on the left side it should be\nwithout the annotations my apologies\nhere so what I'm trying to say it's on\nthe left and the right side the code is\nexactly the same the difference should\nbe just on the right side it has a task\nannotation\num so that's how you would\num that's how you would make your\nvanilla python code into flight task and\nI think\num compared to other orchestration\ntooling\nwe use airflow and we still use the old\nversion but the\norchestration it's in in those\nFrameworks is not testable software it\nis a separate decoupled dsls that\ndeveloper has to do context switching\nbetween the business logic within your\npython code and the orchestration logic\nthat string those tests together in a\nparticular tag\nand while there's a benefit in\ndecoupling them and ability to add more\num sort of annotations or metadata to\nhelp with operations in the airflow case\nfor example in the old version of The\nAthlete we use\num but we find that having\nthe the benefit of having expressive\npythonic code to write your\norchestration logic together with your\nbusiness logic and in a way and actually\nEngineers won't actually realize that I\nthink that's a great experience when you\nactually write a software that is\nexpressive that captures your intention\nand you leave the orchestrations their\nconstructions to flight so I think in\nflights documentation you would call it\num like declarative workflow and any I\nthink that's exactly what our users is\nbenefiting a lot moving from\nboth airflow and also\num moving from notebooks where you know\nthe cost of conversion here is just a\ndecorator\nalthough I must I would like there's\nzero differences so there's some the\nsmall difference of um promise\nabstractions that um doesn't so a\nvanilla python code once you change the\nflight the results will be evaluated as\npromises but there's a small learning\ncurve there\num but most mostly your code should be\nable to run just fine by just after\nchanging this annotation\npassing at this point is there anyone\nanyone want to intercept or feedback\ninput questions\nall right\num right so once we have our code in\nFlight right\num how do we actually make it known to\nflight this is when we talk this is when\nregistration comes in and I think this\nis the core part that makes experiments\nscientific scientific here means that it\nis reproducible you're able to trace\nback what you ran and you can rerun it\nagain to give reproducible results\nso um at stripe we use\num get get shot from from the branch as\na flight version as you do as you\nregister so this gives a very natural\num experience to just to product\nengineering where they make a features\nthey would commit and those are reliable\ntraits of their work and we are able to\ngo back to flash comments and see\nexactly what the business logic was\nsupposed to do so similarly when you do\nflight registrations based on your git\ncommit here you can go through different\nversions and you can be you can trace\nback to your code in your repository\njust to understand what exactly was\nwritten there and how that could explain\nthe results you're seeing\nand in terms of resource organization\nbecause the registrations here would\narchive your code AS container image so\nthis the code is Frozen in version right\nand this is decoupled from execution in\nthe data so compared to notebooks where\nthe code and execution it's at this it's\nin the same abstractions of notebook run\nand notebook UI\num in Flight you have a separate\nworkflow and one workflow you can launch\nmultiple executions\num with different parameters with\ndifferent inputs or you can just run the\nsame workflow with the same input but\nmaybe your code by nature of machine\nlearning it's a non-determined\nnon-deterministic by Design and you just\nwant to get multiple results from this\num this registered code\nand lastly as a as a real scientific\nexperiment if you have friends doing\nlike Rich lab when they you know the\ntemperatures of the culture goes up and\npower goes out they have to transfer the\nculture into a different fridge\ndifferent lab running real experiments\non flight you will come into an\nunexpected\num outage because of let's say the\ndelegated compute system that you're\ntalking to and you want to be able to\nintervene the experiment you want to be\nable to manage retry a board begin again\nright and these are available tooling\nthat flight UI allows you to manage your\nreal scientific experiments\nthen\num so now when you have a basis for\nrunning a real scientific experiment we\nwant to talk about how we can improve\nthe speed and I think one killer feature\nat a flight enables this as a first\nclass abstractions is caching\num if you look at the um the the code\nsnippet there and the task annotation\nyou see cache is here to true and\nwith some cash version so\num imagine like we have a large workflow\nand as a good software engineer you\nwould make changes of fine tune\nuh targeted a modular component within\nyour workflow and if you were to do that\nwithin notebooks you might have to\nlike maybe you want to run just a single\ncell but I think this goes back to my\nprevious example my previous point which\nis it's really hard for you to keep\ntrack of\nyou know cell best execution where you\ndecide to change one cell and execute it\nbut are you sure that you didn't\naccidentally like change the other cells\nto be able to reuse the old States but\nhere the caching is the first class\nabstractions and once you add this\nannotation there flight will understand\nbased on the cache key which is computed\nbased on the signature of the method and\nthe version there and the combination of\nthe input variables\nthat these tasks have already been\ncomputed so you can actually confidently\nrun the entire webflow and let task\ncompute and decide whether which ones\ncan be fully cached and reuse the the\nprevious results and this saves a lot of\ntime so\num our users have expressed a lot of joy\nin running the experience just because\nthey're able to get the results much\nfaster\nnext well so we know how to iterate fast\ndo we know how to iterate together in an\nefficient and peaceful manner by this I\nmean that\num Flight Team level caching saves\ncompute so\nrelying on based on what is said about\ncaching the flight enables here so model\nteams can run the same workflow but they\ncan Branch off to do multiple to do\num to experiment with their own features\nbut the Upstream tasks that their work\nstream rely on might be exactly the same\nand they are able to rely on the caching\nresults and other team members have\nalready generated so\num this allows collaborative work and\nyou know saves compute and lastly I\nthink flight\num elegantly use communities resource\nquota per namespace very well and\nespecially where the task where the\nflight\num resource relations organizations it's\ndivided into project and domain and that\nmaps to the namespace this allows us to\nmanage the resource quota transparently\nbased on kubernetes resource quota\nabstractions\nso what does this actually mean for the\nteam is that they're able to tell us the\ninfra team how much quora do they want\nwithin their team and they're able to\nnegotiate within their team how they\nwant to use this quota\nand also if they want to increase the\nquota they'll be able to ask for\nbusiness justifications on how much why\ndo they think\num there's larger computer translate to\nbusiness insights or some features that\nthe team actually wants to achieve\nso with this so the peaceful part is\nthat you know team once we have a team\nlevel\num resource quota\num there's less problems in terms of\nresource contention across teams so as\nan inferior team I think that's more\nreally one good guard rail to support\nmultiple teams running on the same\ninfrastructure\nlastly so now we have iterated\ncollaboratively efficiently peacefully\nfast right now let's let's translate\nentrepreneurs in a productionization so\nhow that how do we Transit from that\niterative development\num into actual productionization so at\nstripe\num we\nwe schedule the flight workflow on\nairflow\nthat's mainly because of a lot of our\ndata engineering pipeline is on airflow\nand\num the Upstream data set to flight\nworkflow is actually from airflow and\nthat's why we decided that it's\nfunctionally it it suits there and I\nthink the but the benefit here is that\nthere's no code transformation between\nDev Dev workflow and prod the\ndistinction here is really you just\nregistered\num I mean you can actually use the same\ncode the same container image the same\nflight workflow apps resource but\nactually at stripe we make some\ndistinctions where\nwhen we do add uh in active that is\nreserved purely for ad hoc development\nand when you actually want to go into\nscheduling on airflow you would register\nunder a prod domain and over here use\nPython flight kit SDK\nto um write our own python right now on\nairflow abstraction for flight task so\nin this way a flight workflow is a node\nwithin airflow tag and we create the\nexecution ID prefix with airflow so\nthat's easily distinguishable from other\nad hoc workflows\nand this pattern has been good for us\nwhen people want to do auto training and\nauto\num\nauto training and auto deploying of\nmodels\nso let's see results at stripe in\nlimited availability so this is results\nbased on a single team where we see 10x\nmore offline training jobs dispatch from\nflight\num and that results in 5x more frequent\nmodel releases with sizable business\ngain\nso I think the realization here\num is that Mr productivity is not nice\nto have but it's actually a business\nrequirement and I think this is similar\nto Dan's um comment about feature is not\nnice to have it is a feature right in\nyour earlier\num slide and I think\num yeah on a similar vein I guess um\namount of productivity for us in\nparticular\num we have many use case which is\nadversarial and to be able to react to\nthat respond to incidents in a timely\nmanner it's actually critical to\nbusiness survival\nand flight does enable\num this quick model experimentations and\nrelease\nso um we talked about all the flights\num from our ml development Parts\num I want to talk about the infra board\nwhich is um I am from ml team and\nBrian's here um it's it's such a joy to\nwork with a union AI team and also with\nflight um software artifacts\num for example it supports model caters\num kubernetes clusters seamlessly the\nservices are modular decoupled and we\nare able to deploy the control plane in\nour internal service deployment and the\ncore a data plane on the kubernetes and\nobviously the critical part is the\nworld-class engineering team and giving\nfeedback to us to inform our\narchitectural design and we look forward\nto working with the team\nand Shameless blogging Minecraft stripe\nis hiring\num so go to strive.com jobs\nthank you\nany questions\nI was awesome thank you make any\nquestion comment\nthat's that was an amazing presentation\nI have a one question so that was not a\nShameless blog I think we\num I don't know David if you agree but\nwe should allow people to post jobs\nrelated to flight in the flight slack\nChannel yeah it's happened let me put\nthe link here yeah please do please post\nthem\num and one other question over here uh\nyou've seen the adoption in flight\nis there more demand how is that going\nto grow like what are you guys planning\nyeah there's definitely\num demand within flight within Stripes\nuse the product I think we are as we've\nshared with you we're migrating to eks\nand with you know security requirements\ninternally that we have to implement and\nyou know user identity and workload\nisolation that is the the blocker and we\nlook forward to working with you more to\nhelp us to deliver that\num yeah\nand people will be so ecstatic like you\ncan't imagine I spend so much time\ntalking to users that please hold on\nthat we want that our infra is not ready\nyet we still want to iterate and and uh\nand we have to sadly reject them from\nonboarding to flight so it's a great\nproduct everyone please do this fight\nthank you it that probably is it right\nno right just write a blog and it will\nhelp us a lot the entire Community doing\nthis doing this video really really\nhelps the community because we\nunderstand and we see and you put many\nof the points very eloquently so highly\nI'm obliged thank you\nany other question\nyeah just a quick uh question slash\ncomment about uh something you raised\nabout kind of like the learning curve\nthere's like what I call The Uncanny\nValley it's like it it's almost there\nit's like almost python but there's like\na few gotchas so if there's anything\nspecifically you'd like to kind of call\nout as an issue\num yeah we'd love to\nwe're like constantly trying to close\nthat Gap\num I mean there's always going to be a\ngap I think\nbut uh\nyeah if we can make that Gap as small as\npossible and raise errors or Warnings\nwhen when things kind of Veer off from\nyour regular python expectations\num yeah would love to know what those\nare we kind of know what those things\nare but uh it's always good to have\nexternal validation\nnot to noted I am not developing flight\nday to day but I think our team our user\nteam has been fantastic and giving\nfeedback Steph and Juan and Juan I don't\nwant to put you on a spot but only if\nyou have\num\nissues that would definitely get\nattention from my team here if you want\nto share if only if you have any at this\npoint\noh fondness yay\nhello uh yeah definitely a big fan of\nflight uh I'm on the team with that\ngraph where we just exploded our\nexpectation velocity each other is\nreally excited to use it\num yeah I think the the comment on like\npromises and kind of I've noticed\num it takes a little bit for me to for\npeople to kind of get used to it to them\num I think two things come to mind one I\nfound like we've gone used to some of\nthe error messages but I think initially\nthey can be a little non-intuitive\nespecially as you're starting and you\nknow you go try to interact with the\npromise and then it's kind of like oh I\npromise doesn't have a get uh do they\nhave a get a function or something like\nthat and it's like wait what does that\neven mean right like it doesn't really\nclick until you're like actually you can\nreally interact with Promises at all\num so a little bit of\num like more usability on the on the\nerrors I think would help guide people\nearly on when they're not familiar with\nthem\num and then just secondly I think we're\nsuper excited for like the I think\nthere's something chapter and eager mode\nand maybe kind of like being able to\nresolve this problem those problems is\nquicker so that people don't really have\nto think about them too much um we're\njust super excited to get to use that\nand to to try that awesome\ncool yeah yeah uh eager mode is\nsomething\nuh we're working on hopefully having an\nexperimental kind of sub module in\nflight so you can start you know using\nit and giving it a shot\num which does make me think we should\nhave like a roast white kind of segment\num\njust so we can like surface these a lot\nof these sooner\nyeah I have a lot of roasts myself so\nanybody else have any questions any\nquestions oh yeah hey go for it hey\num so I since you guys have exploded\nyour um\nthe model runs that you've done I just\nwant to know like uh\nare you able to like uh reuse workflow\ncomponents across those different\nmodeling use cases and maybe talk about\nlike I don't know if there's like a set\nof\nmodeling tools you're using like xgboost\non spark like Yeah because it anyways\nyeah that's my question\num yeah so we\nwe definitely we're sort of in the\nprocess of like I think there's a lot of\nuh kind of little utilities and tools\nthat we've built\num that some of the teams are trying to\nreuse and we're trying to figure out you\nknow what kind of ownership and like who\nshould maintain those and and that kind\nof stuff so I think that like we're in\ndeposit figuring that out\num in terms of kind of like particular\nmodels like definitely big users of like\nXT boost uh Transformers neural Nets and\nlike ensembles of all of them\num so I think that like would mostly\ntrying to reuse the\num the utilities that we've built to\nhelp us kind of interact and like\ncommunicate with different services like\nlike send like send particular data jobs\nto spark and like bring them back and\nget results\num I think that in terms of kind of\nacross different models\num we do like what we did our first\nproject was like building rewriting our\nexperimentation rails on flight and like\nall of our models are trained on flight\nand they're trained side to side and\nthey share the same like feature\ncomputation they share the same like\ntraining workflows\num and so like now we're going into\nadding new models to it and it's been\nreally nice to be able to just say like\nokay I'm going to add a new model I know\nI need to do like you know I can reuse\nthese features I can use these training\nworkflows um and and these evaluation\nreports for example\num and so it's been really great uh and\nproductive that way\nI think what's really great it's because\nflight task it's python code so instead\nof like reusing components it follows\nthe standard software engineering\npractices where if you want to like\ncreate a utils libraries and you know\npeople can reuse them in a standard way\nbut I think what I look for but I think\nwhat it's perhaps going to be for us to\nthink further it's when the plug the\nback end plaque back and plot in it's\nit's used at stripe this one that's\nwhere um you have higher level of\nabstractions of managing the um the\ncomputation how the flight code in test\nand the test is being run and how the\nback end managed the entire in the\nentire life cycle event I think that may\nrequire if you haven't thoughtfully\nabout it but I think it's it's slightly\na different like simple low barrier of\nEntry of like contributing to code so\nthat's that's where I'm curious if\nKeenan has um more thoughts to it\nyeah I think uh and that's why I wear\nthe python back-end plugins will really\nhelp you further we already have\nback-end plugins in go but\nuh the goal of backend plugins has\nalways been\ncleanly separate what the users\nof the flight system should be doing and\nwhat the maintainers or The Operators or\nthe administrators or the infra people\nright like are doing uh that way you can\nkeep on you know upgrading the\nfunctionality adding new capabilities\nswitching service providers even need be\nall with the back end and uh the beauty\nof the backend plugin is that you don't\nneed to run a pod it runs within like a\nsimple service so the cost dramatically\ndrops the performance also increases\nquite a bit and the number of concurrent\nworkflows can on a single\nwide instance can go up to hundreds of\nthousands as Spotify and like some\nothers have done and so yeah so it\nscales really really well\num so yeah so I would direct like I\nthink we've talked about this in the\npast for some of your uh systems and\nit's a great way to integrate Without\nReally opening up any of your\nIntegrations to the you know in the open\nsource because it's completely you don't\nhave to release it or anything you can\nkeep it with yourself internally and if\nyou feel that you're you know\nintegrating with let's say open Ai and\nyou want to release that out to the\nworld you can\nthat would be great\nhopefully that answers your questions\nall right well thank you Megan entire\nthree team at stripe\nor this fantastic presentation any other\ncomment question so far\noh\nI have one more question yeah so you've\nhad success at stripe with flight which\nis really exciting I was just wondering\nlike when you show up with a great new\ntool\num\nI was wondering like what if there was\nany pushback that you could surface you\nknow sometimes there's always like\npeople like whoa this is new it's doing\nsomething different\num that's always useful for us to hear\num\nyeah\num Brian only if you I don't know if\nyou'd be on the spot but I think you're\nmore qualified than I am if you are if\nyou want to talk about the initial phase\nof bringing\num flights and you know like arguing\nwith\nairflow that's right\num I wouldn't say that we were arguing\num but I think what helped make quite\nsuccessful\num was it really like early and deep\npartnership with the the users\nespecially um like juanstein\num they were a huge component in terms\nof pushing for flight as a product\num I think we went through the initial\nphase of trying out a bunch of different\nworkflow engine tools and flight just\nwon out and in terms of feature\ncapability we created a bunch of buy\nversus build type of documents uh ran\nthat through effectively our our\nexecutive sponsors and then we got by in\nfrom our partners as well as our leads\nso uh from there it was actually kind of\nlike proving out that um we're kind of\nlike collecting our muscles in terms of\nadopting\num basically third-party tooling and\nopen source tool so I think that was a\ngood exercise for us as as a whole and\nuh yeah\nthat's that's primarily how we got it\nIncorporated\nawesome thank you\nthank you all that was fantastic"
    },
    {
        "title": "Flyte Roadmap Update: Flytekit v1.6.0",
        "transcript": "all right um hi everyone I'm Nielsen\nthielen I'm I'll be standing in today\nfor Eduardo I'll give you a few roadmap\nupdates\nso let's see how Dive Right In\nyeah so there's a lot of cool stuff\ncoming out in Flight one six\num which should be coming out this week\num\nall of these are awesome so I'll just go\ndown the list\num we're gonna launch we're gonna launch\nexternal backend plugins\num shout out to Kevin Sue\num for driving a lot of the work here\nbasically you can write back in plugins\nin Python current state is that or\nbefore this release you'd have to write\nit in go\num so this just kind of opens up the\nfloodgates for python authored\nback-end plugins so just examples of\nthings that are offered right now right\nthat you could you know write in Python\nuh just some examples of querying a\nsnowflake database or running a training\njob on sagemaker if that's um platform\nyou're using\nand probably a whole ton of other things\num watch out for docs on how to do this\num we'd love to see what the community\nbuilds with this um\nyou know with this new feature\nanother you know Kevin Sue thing is uh\nobviously other people are involved but\num shout out to him for uh getting this\nout\num essentially image spec\nuh we'll we'll probably have demos for\nall of these right but um at a high\nlevel image spec is gives you the\nability to Define your container\num requirements uh kind of directly in\nyour flight code\nso you don't have to think about\nbuilding and pushing containers anymore\nyou can declare it in your task\ndefinition and flight will\nprovide this Pi flight build utility\nthat will just do this for you\num performance performance as a feature\nso this uh a lot of this work was driven\nby our very own Dan Ramer and also shout\nout to Jason Porter and the front end\nteam for getting this out on the UI but\nessentially\num you get runtime metrics so if you've\never been curious about\nhow long it takes for various parts of\nyour task\num or like what what fight is doing\nunderneath right uh in that timeline\nview we're going to provide a lot more\ndetail on that view\num and so you know we'll demo this as\nwell probably eventually but this is a\nsuper exciting feature\num the second point right is is there's\na flight deck now also that will break\ndown the task timeline\nas well\num\nimproved ergonomics this is something\nCaitlin and I kind of worked on we're\ngoing to continue working on\num so if you run Pi flight run on flight\n16 you'll notice that things are a lot\nprettier their colors and a lot more\nstructure on the CLI now\num and we're working on better error\nmessages the thing that you'll notice on\nflight kit if you\num specify a type for you know across\nhere the tasks in your workflow that\nthere's just a lot more context provided\nto you so it's no longer just that\nthere's a type mismatch it'll give you\nwhere in which workflow and which task\nand you know what types type mismatch\nwas uh was there in your code so\nhopefully that gives you a little better\num\nmore information for you to fix bugs in\nyour code\num last but not least this is a maybe\none sixth thing we're very close\num so we'll probably get you get there\nbut\num we're we have a torch elastic plug-in\num this will be part of the\num existing fly kit plugins KF Pi torch\nplugin I believe but this allows you\nbasically to run torch run which is\ntypically run on a command line\num but we have a plug-in that allows you\nto do this straight from your task\nconfig so there there'll be a new\nelastic task config object that you can\nspecify in your tasks and you know\nflight will handle all of the stuff for\nyou\num things that this enables are\ndistributed training using you know fsdp\ndeep speed probably a bunch of others\nthat um we're not aware of so look out\nfor that you know I can add one other\nthing so this the goal of this has been\nalso that you should be able to train\nrun your training code locally and make\nno changes literally to push it on to a\none GPU or an eight GPU\nare 16 GPU and so on kind of a cluster\nand it just scales that we've been\ntraining fine tuning llms ourselves\nusing this uh it works really really\nwell\nso more on that stay tuned\num this is coming soon and and we will\nalso have examples of fine-tuning\nsome of the top parallels right that we\nknow of right names yep yep we've been\nhammering away at this the one caveat\nthere is I guess if you're riding plain\npie torch you'll probably have to be\naware of local Rank and like things like\nthis but if you're using a higher level\nframework for example the gameplays\nTransformers library or pytorch\nlightning they kind of abstract that\naway so um minimal code changes\nbasically\num moving on\nwe are always trying to improve our you\nknow governance and kind of processes\nfor uh\nyou know informing our roadmap so\num for planning one seven and Beyond uh\nthis is more of like a call to action\num\nif you go to the next slide uh some\nconcrete things that you can do\nso we are going to\nbe a little more diligent about looking\nat GitHub issues and you know votes on\nit so if you\nupvote the description of an issue with\na thumbs up we're going to look at that\nwe're you know not promising that it'll\nmake it to the roadmap but we'll\ndefinitely use this as feedback to\nfigure out uh I mean if you just look at\nthis top\none right failure note support it's been\naround since 2021\num probably a strong signal\nthat we should look into that\num but this is something you can do as a\ncommunity member you can just um you\nknow they're kind of a ton of issues but\nif you like search through and thumbs up\nthings that you're interested in in\ngetting into the\nby code base the features and whatnot\nwe'd love for you to to start doing that\num\nothers another thing you can help us do\nwith relating to planning is writing or\nvoting on RFC incubator discussion so\nthis is a in the discussion section of\nthe flight repo there's a special RFC\nincubator is it meant to be kind of a\nlightweight idea brainstorming kind of\ndump\num for you to write an RFC\num it's kind of like a soft\ncategory categorization of what is and\nwhat counts as an RFC\num roughly it's like a big effort right\nand we don't\nwe we don't exactly know so if you write\nthe ideas down here we can help you\nassess\num whether it's an issue just a plane\nissue or an actual RFC\num and I think this is the last the last\nslide\num is joining the contributor Meetup\nsince just another call to action we\nhave a contribute Channel and the flight\nslack we have a meeting every other\nThursday the next one is next week at 11\nA.M PST 2 p.m EST\num so uh Dave David runs these and you\nknow these are great for you to express\nyour ideas\num rfcs that is and we'll kind of\ndiscuss and and Hammer out you know\nspecifics\num and that's it for updates\nawesome thank you so much for the\ninformation for the shout outs to the\nmany ways to\num yeah give your feedback and\ncontribute back to the project so that's\nawesome\ncool uh yeah even related to this uh the\nrecent changes we've been uh doing to\nthe governance and contribution\nexperience Etc\nuh it it gained the attention of the AI\nand data foundation and even that\nescalated to the Linux foundation and\nit's been showcased today\nuh I'm sharing the link here in the in\nthe chat\num there's an i collaborated with some\nLiang Foundation folks to kind of\nshowcase historian\nnot only what is flight but how we are\nin the process it's not perfect but we\nare in the process of improving a\ncontributor experience trying to lower\nthe barriers for contribution\nand so yeah that's that's great thank\nyou all for your support\ncool any further comment\nno\nright think emails"
    },
    {
        "title": "Flyte Community Updates 035 featuring April Top Contributors and PyData Recap",
        "transcript": "cool so let's get started if you're not\nmine okay welcome everyone to the flight\nCommunity sink community meeting\nhappening every two weeks\nuh today May 2nd and David Espejo and uh\nopen source developer Advocate at Union\nAI and happy to be your host today\nuh we from the from the last session we\nchanged a little bit the passcode so\nprobably some of you\nuh struggle a little bit to join sorry\nfor that we'll make sure that it won't\nhappen again\nuh well we're here now cool yeah a\ncouple of reminders first of all that\nthis meeting has been recorded and uh\nwill be posted to the um flight YouTube\nchannel\nand second thing is that these meeting\nfalls into the Linux foundation's code\nof conduct\njust because flight is a I and data\nFoundation project so\nyou can feel safe here hopefully\ncool so let's get started let me share\nmy screen real quick and uh reminder\nalso that the agenda document\nuh and probably it would change in the\nnear future but right now it's a Google\ndoc and it's open for anyone to ask\nquestions or post any discussion topic\nyou will you would like to ask\nand also if you want to nominate\nyourself to share anything you have\nlearned recently just put it there in\nthe actual schedule that it's at the\nbeginning of the dock and you just put\nyour name in in your topic it doesn't\nhave to be fully fledged perfect\nanything like that so anything you'd\nlike to share with the community will be\nreally helpful\nall right now let me share my screen\nthere you go\noh\nokay then\nuh slideshow cool\num all right so a bit of agenda some\nCommunity updates roadmap update coming\nand a presentation from our fantastic\nguests from stripe\nuh yeah and also in the in the agenda\ndocument feel free to add your name and\naffiliation capture the attendance\ntotally optional that would be great\nand yeah some Community related updates\nwell first of all my favorite time of\nthe month contributors of the month it's\nthe moment where we get to recognize\nsome of the individuals helping the\nproject in different ways uh for April\nwe have a team Leonard\num he not also created a series of Deep\ndive articles on the integration between\nindivity and flight\nand he also went A step above and\nsharing his experience on the previous\nCommunity sync no no codes like the\nimperative way it's already there posted\nin the YouTube channel\ndefinitely recommend it so thank you Tim\nuh next up Franco Boshi\ntwo important additions uh first of all\nthe improvements to the authentication\nstory and when I saw the pr I was\nthinking we should really have a\nauthentication a special interest group\nbecause it's a big story and there's a\nlot to do and I see\nfolks in the community with with a ton\nof experience it will be great so Franco\ncontributed an improvement to the\nauthentication\nmechanisms in flight and also in adding\ndocumentation for the datadoc\nintegration so\nthanks so much and uh finally Alex Papa\nNicolo hope I'm pronouncing well he went\nover the process of deploying flight on\neks\nnot the hard way automating a lot of the\nstuff along the way and he was kind\nenough to\npublish his learnings on a repo\nso feel free to check it out it's highly\nopinionated but it's also a good idea on\non how to automate the the entire\nprocess\nso thank you all for your contributions\nand we hope you enjoy your well-deserved\nswag\nright last week we were at Pi data\nSeattle it was a really interesting\num event\num a recap blog post is coming but we\ncan\num this goes a little bit here the\nrecordings will be posted we have uh two\ntalks one tutorial a talk in uh lighting\ntalk\nI'm not\num wrong and we also had a meet up let's\nsay a social event with more than 40\npeople showing up\nuh having really interesting discussions\nso thank you all for your support there\nand the Raffle winners will be announced\nthis week for the two one year O'Reilly\nsubscriptions which is really cool\nsome resources office hours I think some\nof you here in Nicole have already used\nor tried to use these resource is\nbasically it was moved like a month or\ntwo months ago it was moved to an\non-demand format\nuh so you need to schedule your session\nwith flight maintenance to ask questions\nusing calendly from the available slots\nright there\nand uh yeah good news the upcoming\nCommunity seeing has finally a free slot\nan open slot for anyone\nhere who would like to share their\nexperience their learning struggles Etc\nwith flights that'll be great so just\nlet us know and we'll be happy to host\nyou\num all right I think with that\num if there is no any other comment or\nquestion\nwe can call it and yeah thank you all\nfor joining hope to see you in the next\none\ngoodbye thanks everyone\nbye everyone\nbut"
    },
    {
        "title": "No Code Flyte, the Imperative Way",
        "transcript": "um okay great so uh I'm gonna do my best\nhere I don't know if this is going to be\ntoo too long or not so I'm gonna go\nreally fast in places but\nI want more than anything to be this an\nopportunity to just show people some\nsome cool ideas I've I've been stewing\non that flight has like uh allowed me to\ndo\num and uh you know if this is getting\nyou thinking about things differently\ngreat please ask questions I'm going to\nbe talking a lot about other tools and\nnot just flight so this isn't like a\nflight talk so I'm I'm aware of the room\nis a flight room so you know I apologize\nahead of time but I promise it's all\ntowards uh a a goal of just you know\ntrying to highlight a bigger picture way\nthat like these imperative uh workflows\ncan be used so\num my uh first joke of the day is um you\nknow no code flight I've called the\npresentation no code flight because\nthere's no flight code at the\npresentation so uh yeah no you'll see\nokay a little bit about me\num\nnot the DS so I'm uh based in New York\nbeen here for a while now uh I'm your\nstandard New Yorker whenever you talk to\nany of us we love the city we love food\nwe love Parks people walking running\nbiking you know you name it I'm there uh\ndoing doing the thing\num\nuh I'm also a dad so I had a had a kid\nrecently so you know I like to say uh I\nused to have hobbies but now I have an\ninfant daughter so hopefully I'll have\nhobbies again\num one day and so when I'm not\ncomplaining about not having Hobbies\nanymore uh I do like to Garden a lot\nthat's my that's my thing if I can do it\nI spend so much time at the computer\nlooking at numbers and spreadsheets and\nmoving numbers around from place a to\nplace B uh being able to get outside and\nwork with my hands and experiment and\ntoodle around uh with plants is an\nabsolute\num blast so that's just a picture of a\nrooftop garden I was helping to maintain\na couple of years ago so uh very proud\nof my dahlias\num there uh oh and uh yeah I love\ngardening also because it's great for\npipeline metaphors but I don't think I\nhave any prepared today for for you all\nbut uh if you see me again you'll\nprobably hear a gardening metaphor uh\nand then sorry did I mention I'm\nCanadian so I'll get that out of the way\nbefore I say about sorry or any of the\nother trigger words so yes I live in New\nYork but I am Canadian and I still have\nsome of those uh traits uh that remain\nand um just uh just a flag you know this\nis our new New York logo uh which um I'm\nnot a huge fan of I love the Retro look\nof the old one but uh I digress\nokay so about me as the data scientist\nlet's talk shop a little bit more um\nI've been in uh this sort of big data\nanalytics space the intersection of the\ntwo for about 10 years now I got my\nstart\num as a PhD candidate uh I was working\non multi-site neural eye movement and\nbehavioral data so we're you know\nregistering uh data from the brain at\nHigh Fidelity multi-site so terabytes\nand terabytes of streaming data\num coming in imovement data behavioral\nexperiment data this is all scientific\nmedical equipment nothing designed to\never talk to each other so uh you know\nhalf a different half a dozen different\nlogs all designed completely different\nnothing wants to be coordinated and so\nthe big job there was getting it all a\nlot aligned analyzed summarized and set\nup in a way that we can actually you\nknow write a paper and and do some\nstatistical analysis on you know uh\ndozens and dozens of terabytes of memory\ndata and so here I just want to\nhighlight some sort of chapters of my\nlife as I go through this and for me\nlike it was really foundational where I\nstarted in this world of kind of\norganizing data as structs for like\nnothing better to do it in so structs\nare kind of this Matlab construct and I\nthink it exists in Java as well I don't\nI'm not a Java guy\num of uh I think a python dictionary is\na pretty close uh similar but yeah\nanyways maybe a maybe a\na a pseudo class or whatever but uh\nreally like in the strucks zone and\nslowly it was starting to get into the\nsense that no if you organize data as\ntables and start to leverage SQL\nTransformations you can move really\nquick across a lot of data sets and\nstart to join stuff up in a way and\norganize even more importantly stuff up\nin a way that uh enhances basically\neverything so more reproducibility less\ncode uh easier to read easier to\nunderstand what you did before less\ncustom stuff it may not sound like a lot\nbut that was like a major uh jump for me\nas like a data practitioner so after my\nPhD I did what all phds do now and\nbecame a data scientist and I was very\ndriven to move into the entertainment\nand streaming space the big reason there\nis I love the lab I love data I love\ngetting uh behavioral you know\npsychology is a big part of my\nbackground but in streaming and\nentertainment you get to do that at a\nscale of millions if not hundreds of\nmillions if not many hundreds of\nmillions and so suddenly you know if you\nget into the right industry in data\nscience you've got almost on the the\nlaboratory is now the real world and you\nhave access to all this stuff at a scale\nthat never existed before so that was a\nvery natural step to me to say okay cool\nI want to do all the things I was doing\nbefore but now with 100 million people\nnot you know 10 or 5 or 1.\num\nso you know for for a couple years I was\nworking on a a platform for live\ntracking emotions across all of our user\nengagement\num this is when I was at um Viacom so we\nown a ton of different assets across a\nlot of different platforms and we were\nable to pull in comments engagements all\nsorts of social data on that and then\nprocess it all and build a live model\nand so for me they're really the the big\ngrowth was you know starting to think\nabout architecture in terms of tasks and\npipelines and not just this sort of PhD\nnotebook kind of data science stuff but\nactually being disciplined in a software\narchitecture uh you know methodology and\nreally you know data private processing\nbecomes a pipeline and really well\ndefining your tasks as part of that\nPipeline and then also you know shipping\nyour tasks to Cloud so do all your heavy\nlifting not on your laptop again this\nall sounds like super simple stuff but\nit was a major step for me again as I\nwas evolving as a data practitioner\num and then finally now I'm I'm at\nSpotify Elita uh in insights and an\nanalytics engineering team and a big\npart of that is we manage and own\nhundreds of data processing workflows\num that directly serve analytics teams\nand uh feed into dozens of other teams\nas well and you know the next step for\nme and in my learning of data\npractitioning is scale like how do you\nscale this stuff you can build all the\npipelines you want but scaling is a\ntotally different problem and to me it\nbecomes you know tasks really you have\nto you don't have just have to have a\nsense of a pipeline you have to have\nexplicitly defined pipelines\num your tasks live in the relationships\nin proper dags and then almost as\nimportantly you have tooling to manage\nall of that too so you're not doing all\nof this you know I grew up in in my\nmid-career doing pipelines by hand and\nnow I automate as much as the pipeline\nbuilding and management as possible with\ntooling so hopefully that's a bit of a\nbackground about who I am and kind of my\nexperience levels you you won't see\ncomputer science engineering anywhere\nhere I am a data science and a and a\nlover of tech but I'm not a comp sign\nengineer so please do forgive me if some\nof the language is not quite uh Landing\nexactly in a place you may be familiar\num to hearing it\nso you know uh one last thing before I\nget into it um the thoughts and opinions\nare my own I do not reflect those uh of\nmy employer this is a reflection of my\npersonal IC work the code is my own\nthere's no Spotify code examples docs\nnothing from Spotify uh here uh and it's\nimportant for me to recognize that\nSpotify is a contributor to the flight\nproject this talk is not about that this\nwork and these Concepts is independent\nof that effort so just want to\nacknowledge all of that and get that out\nof the way\nokay so um let's talk about using flight\nas a back end uh workflow orchestrator\nfor a different Tool uh that becomes our\nuser front end which would be kind of\nour workflow relationship Builders I\nlike to call it so uh we'll have a\nback-end front-end uh style Paradigm\num flight let me just say this is a\nflight room and I want to acknowledge it\nflight makes creating and managing tasks\nvery easy I can't\nbuild a better tool than what flight and\ndoes in Python for this and so this talk\nis not meant to be like we need to\nreplace the way flight does it it's\nmeant to be hey look you can also do it\nthis way that's kind of cool\num so you know uh despite flight\nstrengths in being able to you know\nDefine and do your workflows and tasks\nand run your code and all of that in in\nthe flight box you know users may have a\npreferred Paradigm for managing their\ntasks and workflows and and the\nrelationships there and and so\nimperative workflows give us a way to\nsupport that and that's really like one\nof the main takeaways I want to give\ntoday\num and so this tap this talk is an\nexample of using DBT to build and manage\nthe dag ship it to flight and use flight\nas our workflow orchestrator so I want\nto kind of use the best things of my two\nfavorite current tools but I'll get into\nthat so thanks to imperative workflows\nwe can automate most of this process and\nto the user they deal with their tool of\nchoice and the rest you know workflow\norchestration and running in Flight is\nis happening automatically so there's\nreally not a lot of flight code that the\nuser is is needing to do to do this and\nyet they get to reap all of the amazing\nbenefits of of flight as a workflow\norchestrator and it's rock solid um\nabilities there so yeah why Frankenstein\nthe two together again uh I think it's a\nneat demo of uh flight's flexibility I\nthink programmatically leveraging\nflights workflow orchestration\ncapabilities is a huge plus uh no matter\nwhat you're doing so this imperative\nworkflows give you the ability to do\nthat\num and uh in our following demo uh you\nknow the tool that I use\nI'll talk a little bit about DBT to\ncreate manage task relationships uh it\nactually doesn't give great Exposition\ninto the dag as it runs you've got some\naccess to it through docs and logs\num but you really don't get that like\namazing kind of flight UI experience\nwhen it's running its own dag so DBT has\na workflow orchestrator built in but it\ndoesn't have all that awesome stuff that\nthat flight prints the table so again\nthis is like a Best of Both Worlds I can\ndo my uh management deck management uh\nin DBT ship it to flight and have that\nuh amazing workflow orchestration\nhappening\num\nso again I want to talk a little bit\nabout this this world I'm in scale I\nneed tasks explicitly defined as graphs\nand I need tooling to manage\nrelationships uh a quick dag primer um\nand I've caveated this because I think\neverybody in this room already knows\nwhat a dag is but I made this plot and I\njust and I am going to use it as much as\nI can\num so a dag is a directed uh acyclic\ngraph it's a way of representing a set\nof tasks and their dependencies in a\ngraph structure each task is represented\nby a node and the dependency between the\ntasks are represented by edges and it's\nall uh directional there's no loopbacks\nand it's a very nice way to organize\ninformation that has a directional flow\nto it\nso as you're in Flight UI looking at\nyour graphs you're looking at dags and\nso\num they're they're fairly\ninterchangeable\num we love dags a little callback to the\nNew York logo there so uh particularly\nyou know in my opinion by abstracting to\ntasks and their dependency the dag\nprovides a high level view of the\nworkflow uh which makes it way easier to\nmanage and understand these like massive\narchitectures that we end up with\ninevitably as we're building things I\npersonally found that abstraction via\ndag is a crucial concept when it comes\nto scaling complex Data Systems as you\nzoom out uh what you can see uh your\ngranularity uh has to decrease\num you can't uh look at a thousand\npieces of sand as you stand uh nine\nmeters up above the beach the sand has\nto Clump into rocks and so something has\nto abstract somewhere and dags are a\ngreat place for that to happen\num\nI'll uh I'll just move forward a little\nbit quicker\num so\num as you start to introduce Stags in\nyour project your dad gets bigger and\nthen your dad starts to split and you\nend up with multiple dags and then your\ndags get dags and and it's crazy it's uh\nevery it's dag creep I don't think\nthere's a better word for it yet but\ninevitably uh large projects become\nmulti-dag projects and so having\nsomething to help you manage all of\nthose dags the relationships of them and\nall of those things that that like dag\nof dags uh tooling becomes actually\nquite critical as well\nso you know I've basically just pitched\nflight uh to the flight team and I think\nthat's very funny but uh there is a\nreason and there is a point there's just\na ton of overlap between you know what\nflight does what my problem set is my\nday-to-day problem set is and and what\nDBT does but let me keep going I assure\nyou I'm not here to just pitch you\nflights\nso you know everybody here already knows\nflight you know tasks you know workflows\nyou know flight UI right like I don't\nhave to talk about that let's talk about\na different tool I want to talk about\nDBT which is data build Tool uh I'm sure\nmany of you are familiar with it uh I\nbut I do realize it is a sort of a niche\nproduct but I think you know if\nanybody's in the analytics space they've\nprobably heard it by now\num it's very popular among analytics\nengineers and data teams that support\nanalytics groups\num what's critical is that DBT does all\nof this modularity and abstraction but\nit does it in a SQL first context so\nit's arguably quite easy to use if you\nknow how to use SQL\num in DBT uh a DBT model is the same as\na table output is the same as a task is\nthe same as sort of a set of SQL that\nruns with an output\nand then DBT allows you to establish the\nrelationships between these models uh\nwithin the model themselves so\neverything's explicitly defined DBT then\nparses out this information and builds\nyour dag so you get this sort of\ninteresting stuff that looks like in\nyour DBT code the um it's it's actually\njust Ginger so the curly brackets ref\nraw customers DBT then knows to\nreference that previous model and either\ninsert the table relation or if it's\nephemeral to insert a CTE or if XYZ do\nthis do that and so in that way you can\nbuild highly modular\nSQL you can establish the relationships\nbetween\num all of the tables and you can then\norganize these like you know no more no\nmore should your teams be doing 1200\nline uh SQL monolith type stuff you\nshould be using DBT you should be\nmodular you have no excuses DBT lets you\nuse tasks essentially in a SQL context\nand then um it gives you this really\nnice CLI for running portions of the dag\nwith a powerful selector syntax and so\nanalytics Engineers use this constantly\nit's a huge part of their workbench for\nyou know fixing tables back filling uh\nchecking data all of those things it's a\nlot of CLI work you get automatic docs\ngeneration this really nice stag\nExplorer that's built in but only in the\ndocs context and uh more than anything\nagain just really hit the point users\nmanage their SQL models and configs DBT\nmanages the rest so it's it's really you\ncan build really really complicated\nstuff with a fairly reasonable learning\ncurve and you're only ever touching\nconfig files and SQL so this is\nessentially you can get into python via\nGenji if you want to but you don't need\nto and that's that's really I think a\nbig appeal of DBT and how it's growing\nso fast\nokay so 10 plus slides one troll uh a\ndirty video\num but where is where do we start where\nare we where is this uh no code flight\nthe imperative way business well I'll\nget to it it's time all right the intro\nis done and the distractions are done so\nnext I want to talk a little bit about\nhow DBT runs its Stags establishes its\ntasks\num how we can leverage that via\ncontainer tasks to also run the DBT\nworkflows in Flight\num by exporting each DBT task to flight\nand then re-establishing that dag in\nFlight that's a mouthful but it'll make\nsense\num and then you know uh there it is\nimperative workflows to automate and\nscale the process so it may seem like\nwow this guy's been talking for a while\nand he still hasn't shown me an\nimperative workflow\nspoiler alert you won't see one until\nabout the second last slide so you could\njust get that out of your mind we're but\nwe're going to keep going because uh\nthat's what we ought to do to get to the\nimperative workflows\num before I'm going to go any further\nquick shout out to the DBT plugin API as\npart of the flight kit plugins there may\nbe some of the uh contributors here um\ntoday I'm not sure\num for today we're going to be using DBT\ninside of a container via container task\nwe're not going to be using the DBT API\num but this could be re-implemented\nusing that plug-in and if you're\ninterested in that definitely go ahead\nand do it\num so why did I use a container test\nthen\num because the container Paradigm is a\ngreat fit\num as a general example so it's easily\nextensible so if you've got something\nthat builds a dag and runs it uh and\nit's sort of code that can run in a\ncontainer you can like re-adapt this\nexample to work for that as well this\nisn't a DBT uh specific example even if\nI didn't use DBT for it so I love\ncontainer tasks because I love the\nability to be uh running these sort of\num\nyou know predefined uh jobs that are\nalready working well enough in a\ncontainer and it's a pretty light lift\nto bring that into flight as opposed to\nrewriting it all as flight code which I\nthink long term is the goal for us you\nknow but short term uh I I really like\nusing the container tests um personally\num so here's our setup we need a DB um\nwe're going to use a postgres container\num uh Docker I'm running this all\nlocally I was going to do a local demo\nand then I decided nah it's not worth it\nso here we are this is me simulating a\nlocal demo\num\nwe've got the docker container running\num locally and uh the only thing I need\nto do is uh Docker Network inspect\nbridge to get its uh IP address if I\nwant to do Docker to Docker\ncommunication which I'm going to want to\ndo because my uh flight CTL is running\nin a container my DBT is going to be\nrunning in a container and my postgres\nis going to be running in a container so\nI need to know that bridge address uh to\nconnect them I'm just using default\nbridge in Docker so that's why this\nworks but I could set up a custom bridge\nand do a bunch of other stuff but\nthere's no need since thankfully uh the\nflight demo respects the default Bridge\nso I can just do this\nas a side note there is a postgres\ncontainer running as part of the flight\ndemo and you can use that as your DBT\ndatabase if you really wanted to uh but\nI didn't do that because I'm trying to\nbe a good citizen but uh just to fun\naside if you like to hack stuff\num so we've got our DBT set up we've got\nthe container set up uh setting up the\nconnection profiles in DBT is super\nsimple I've got two profiles I've got\none\num to handle the container to container\num uh context and then I've just got\nthis second one here to uh handle for\nsome local testing and so DBT comes with\nthis lovely debug command which will run\na bunch of tests and so I can do the DBT\ndebug uh on my local profile using\nlocalhost it's connecting to the\ncontainer everything's great\num I also you know Skip ahead a few\nslides built a container that has the\nDBT on it and I can also DBT run that\ncontainer I can run the debug and I'm\nnot passing a profiles argument to it\nnow so it's going to use the default\nprofile uh which I've set up elsewhere\nas jaffle shop and so that uses the\ninter uh container address not all works\ntoo so lovely stuff not uh not too bad\nso far we've got our DB in a container\nand we can connect to it as we need\nso let's get a DBT project jaffle shop\nis the canonical example project\num in DBT if you've ever heard of jaffle\nshop it's a DBT thing\num I you know one day hopefully we'll\ndesign a joffle shop logo and sell\nt-shirts with jaffle shops on them but\nyou know today is not that day\num so go get yourself a jaffle shop\nproject just simple get clone and so my\nproject right now just looks like this\nI've got TBT flight demo I've got jaffle\nshop inside of it and it comes with a\nfew different models here's the Jaffa\nshop dag as it comes from the DBT docs\nso DBT doc serve and web browser will\npop up and and here's me showing what\nthat dag looks like so we've got\num essentially raw data transformed into\nyou know staging data which is sort of\nan analytics engineering and DBT data\nconstruct into the reporting layer which\nwould then go out to your uh\nstakeholders\nuh each one of these would be a model\nfile and then DBT is managing those\nrelationships and manually managing the\nrunning\nokay so a little little detour let's\ntalk about from dag to container so the\nDBT dag\num is always going to build this\ninternal dag of the project and then use\nthat internal dag to run each model task\nbased on the Run command it knows the\nSQL it needs to run it knows the\nrelationship between the models and so\nthat it knows the order uh to do it in\nand so then it goes ahead and does that\nwhenever you do DBT run\nso\num there's a little bit extra though so\ngiven a simple dag I'm just doing a\nmodel a Model B model c one line as\nsimple as possible\num DBT also offers this selection syntax\nso here I'm saying DBT run select a and\nall of its uh downstreams so that's\ngoing to give me ABC and that's actually\nequivalent to running DBT run select a\nDBT run select DBT run select C uh same\nexample so for uh DBT run select B and\nall of its upstreams and all of its\nDownstream so this is a manufactured\nexample but you know life goes on again\nthat's going to give me Model A B and C\nand and that can be broken out into its\ncomponent\num commands as well and so finally you\nknow the one other\nimagine we say C and it's down or sorry\nB and it's downstreams that will be B\nand C then that's to model One commands\nthe reason I want to sort of break this\nout is because what ends up happening a\nlot for DBT users is they'll use the\nselect command to chop up you know you\ncan imagine a dag of a thousand\ndifferent things and to chop it up into\ndifferent sections and run those\ndifferent sections on different\nschedules or coordinate different things\num you can do DBT run but if you've got\n500 tables it's going to write you\nprobably don't want to do that you want\nto think about organizing it and so uh\nwhat a lot of teams do is DBT supports\ntagging models so you add all of the\nmodels uh as part of a workflow with a\ntag and then you do DBT run select tag\nmy workflow and in that way that's how\nyou start to Define you know workflows\nin this DVD context and you can run that\nwith the with the CLI so that that\nhopefully kind of gives you a view on\nhow you would like have like this sort\nof comparison of what a workflow looks\nlike in D BT and how you can select the\nmodels which correspond to your workflow\nand so on and so forth\nso in our joffle shop dag we've got the\nfull dag here and so example run select\nuh orders and all of its children and so\nthat ends up being sort of the top half\nand then here select uh staging\ncustomers and it's uh children and\nparents and so there's different ways I\ncould have set this up I just wanted to\nbe kind of like you know show a few\ndifferent examples and so the idea here\nis um this in a in a production\nenvironment maybe you for different\nreasons the data lands at different\ntimes and you want to run\num these different processes uh at\ndifferent times or or whatever for for\nwhatever reason you don't want to run it\nas a full tag you want to run it as two\ndifferent workflows\num this is how that would uh look but\nagain scaled up to you know hundreds and\nhundreds of workflows across uh you know\nthousand endpoints\nso you know if you can imagine we've got\nthis DBT container\num given a simple dag uh instead of\ndoing DBT run you know we do Docker run\nmy project and then again DBT run select\na which then converts out to you know\nthe component you can convert that back\nout to the component Docker uh run tasks\nand so if you're familiar with container\ntasks you probably already know where\nI'm going with this if you can split out\na DBT task into its component run\ncommands and you could put DBT into a\ncontainer why not wrap up those run\ncommands in a container task and then\ntell flight what the relationship is\nbetween them and let flight run DBT\nwhere DBT is still doing all of its like\ndatabase management and all that\nwonderful stuff rendering the queries\nbut flight's going to then run each task\nwhich is again each model or each you\nknow database modification in the order\nthat the Dag specifies\num so uh you know for this example I'm\njust building a super simple DBT\ncontainer DBT publishes containers\num you can go grab that super easy I'm\njust adding in my DBT postgres so that I\ncan connect to my postgres container\num and uh here I'm I'm adding in jaffle\nshop which is my DBT project uh into the\nthe work dear which is where the DBT is\ngoing to default the container is going\nto default when it runs\nso I've basically already previewed this\nwe've got a dag uh ABC it's going to run\nuh three different you know Docker\ncommands well I can move those into\nDocker uh I can move those into flight\ncontainer tasks uh I've got my uh my run\ncommands and then from there in Flight\nin uh as part of a workflow I can then\num set up the ordering so that the the\ndag actually shows up as a dag and not\njust a a bucket of tasks without any\nsort of relationship\num so to do this uh I've already got uh\narticle up which talks about doing the\ndag extraction from DBT which is its own\nuh you know talk I'm not gonna do it\nbecause this isn't a DBT room\num but suffice to say in this example\nwe've already got a dag extracted but\nyou know you could imagine if you've got\nsome other you know software Paradigm\nthat is also establishing your tasks uh\nand the code to run inside\num that again really just has to land\ninto some organized execution plan\num which then we can input into the\nsecond half of this which is creating it\nall in flight so you know I was thinking\nabout like oh what's it what's another\nlike kind of like no code Paradigm and I\nstarted to think about like imagine you\nknow you had like a flow chart and you\njust wrote your python code in the\nflowchart and then converted that into a\nJson and then push that to flight I was\nlike that's a cool example I don't think\nit's that practical but still you know\nthe sky's the limit for this kind of\nstuff uh in terms of like how much you\nabstract and simplify for the use user\nas long as you've got you know the type\nof Paradigm that's compatible with like\nit goes in a container accepts a command\nand it has a dag\nso uh here it is uh the the Magnus Opus\nso dag comes in um each task is\nregistered as a container task so that's\nsuper simple you load up container tasks\nfrom flight we're only using a few of\nthe uh parameters there but you can get\na lot more complicated than this and\nwe're we're sort of pre uh we're\npre-prompting the command uh I've been\ndoing a lot of chat GPT so everything's\na prompt in my mind now so we pre-prompt\nthe command with what we know is going\nto be the DBT run select command and\nthen uh we already know what model needs\nto be run\num as part of each command from the dag\nhow you like manage that relationship\nbetween your dag extract and and your\ncontainer creation is obviously up to\nyou it's just doing the simplest way I\ncould\nso then we're going to go through our\nthree tasks uh just uh just some extra\nPreamble about generating task IDs and\nwhatever it doesn't matter so much\nwhat's key is we're going to build a\nflight task a flight container task\num and we're going to do a little bit of\norganization to make sure that we have\nthe flight task object we know what its\nparents are\num and then uh just a task ID which\nwe'll use later so basically we're just\nkind of building up a list of tasks it's\nnot a workflow yet\num here it is imperative workflows so\nwith the imperative workflow uh what's\nso great about it is we can essentially\nprogrammatically create you know I've\ngot this workflow which is my imperative\nworkflow and then here I'm\nprogrammatically associating each task\nto that workflow so for each of my tasks\num add the entity this task and then I'm\ngoing to use the task ID a little bit\nlater to then this is programmatically\nuh associating all those relationships\nso this is doing that like our shift on\noperator because I'm not using any\ninputs or outputs on this because I\ndon't need to I mean you could design\nsomething slightly more complicated and\ndo those things but I didn't\num and so then this is sort of that\nprogrammatic way so everybody here is\nfamiliar with setting up you know task\ndecorator task decorator task decorator\nworkflow decorator and then tasks task\nand then you know maybe a launch plan\nand so this this here is the extent of\nour flight code it's going to just do it\nall\num based on uh whatever our task is\ndefined as Json so suddenly nobody's\nwriting any flight code at all this is\njust gonna happen\nso as a single example\num I build up my container I push it to\nthe so flight I've got I I'm missing a\nslide I've got flight CTL demo running\nalready flight CTL sandbox doesn't work\nwith the container tests locally\nunfortunately but demo does so thank you\nto the flight team for helping me with\nthat\num so there's a a local Docker uh\nwhat's that called uh repo\num I guess\nyeah registry there's a local registry\nuh as part of the flight CTL demo which\nis so cool so you can push your\ncontainers to that locally and pull them\nin your flight tasks and then I'm just\ngoing to run\num my this workflow which I defined in\nthe previous one I only defined one\nworkflow one imperative workflow so\nit'll run so I have my DBT tag which\nthen uh now exists in flight and I can\nrun that all right scale that's what I\npromised right that's what it's all\nabout\nso you can extend all of this to be\nsomething slightly more automated so uh\nin this example and the the example that\nI'll share uh at the end\nI've just set up a simple yaml file\nwhich is telling you know I want to do\nthese three workflows here's the command\nhere's the DBT command that like is the\nselection Syntax for those workflows\num and then just a couple extra like\nhousekeeping config things\num DBT helper runs loads that up\nextracts the dag writes the dag import\nJson flight helper boots up loads up the\ndag import uh Json registers all the\nworkflows and that's it\num we also have a Docker build and push\nso this is this is how this looks like\nin CI CD you can imagine automated steps\nas you like build your project which is\nhow we have been setting this up on our\nproduction uh service to to play around\nwith and so what happens is you know DBT\nHelper and dag uh DBT Helper and flight\nhelper you can pack those into a python\nmodule and as long as that's getting\nimported as part of your python you just\nhave to make sure you know import you\nknow flight helper run flight helper uh\nthat's two lines of code that a user\nneeds to have in their flight project\nuh in order to uh have this full Suite\nof task registration uh and dag export\nwhich is uh pretty cool it's like\nabstract abstract abstract\num to the point where now we can set up\nlike DBT this is the ultimate goal of\nthis is like you can set up a DBT\nproject and it just looks like a DBT\nproject and there's like one extra file\nthat you never ever ever ever touch\nwhich is just the flight caller uh and\nthen in CI CD\num the flight registration is happening\nall automatically off that flight caller\nuh you you update your Docker and image\nin your diamonds so here it is here's\nour two workflows showing up uh\nuh in in remote and here's sort of what\nthat looks like when that flight helper\nis running and registering it's just\ndoing sort of it's you know okay you\nknow got demo one got demo two here's my\ntasks now I'm setting up the\nrelationships and you know shipping out\nall that information so okay that's\nthat's the main bit\num a couple of caveats so that you don't\nall go home and try and do this\num I didn't cover sharing tasks across\nworkflows there are solutions to that\nand I will talk about that in a few\nfuture article uh but in DBT it's it's\nnot uncommon to have maybe the same\nmodel be recycled by multiple tags uh\nthat's an issue uh in DBT land you kind\nof just end up running the model twice\nbut in Flight we can use caching and we\ncan say hey this already ran for this\nset of uh parameter inputs which I also\ndidn't get into and recycle and so\nessentially don't rerun it trust that\nthe data is already there so there's\nsome like pretty cool implications there\num this demo does not work on flight 1.5\nI'm running on 132. I don't know about\n1.4\num so I'll follow that up and and create\nan issue and hopefully get that that\ndrilled out\num and then um this demo where I pretend\nthat I can do multiple registrations\nagainst one imperative workflow actually\ndoesn't work in the flight CTL demo\nlocally it just kind of crashes it but\nit does work on the production flight\nserver when we run the registration\nthere so something to untangle maybe\nthis doesn't need to work on flight demo\nyou know it's a bit of an edge case but\neither way if you are trying this out\nuh last Point\num I was grinding to have a repo ready\nfor everybody to just have the code and\ntake a look at it all as a whole list of\npiece\num it is working but it's uh it's still\ntoo ugly to share so give me you know\ngive me a little bit of time to clean\nthat up and I will be sure to share that\nall out um if you're interested in uh\ntaking a peek at the hood because it's\nall there it's it's a it's a working\ndemo it's all containerized it's it's\nlovely stuff\num okay that's it with 30 seconds left\nis there uh any questions complaints uh\nanything I can anything I can say\nI have a question\ncan you is it possible do you think to\ncontribute this Into 5K plugins DBT\nitself just make it as an alternative\nway\nand um and we could make this as part of\nthe pie flight\num CLI option itself like you could get\nsomething like Firefly DVD run project\nright yeah I think all the pieces are\nthere\num except critically getting the dag out\nof DBT is actually quite difficult they\ndon't make it easy\num so I've got an issue opened on the\nDBT core side of essentially saying like\nlisten I need like on on spark you know\nhow you can just go spark give me the\nexecution plan like I should be able to\ndo that DPT and I can so it would be to\nintroduce that feature so that's step\nnumber one because you don't want to be\nmaintaining that DBT extraction sort of\nlogic as part of the flight core\num project so if flight sorry if DBT can\ndo a better job of that than picking up\nthat like uh artifact in flight and\nusing the flight API to then imperative\nregister it up yeah totally\num I think it's a pretty exciting place\nfor the the that API to go in long term\nI think the best way to get that done\nthen is to actually get a bunch of\ndifferent users you know once you have a\nrepo out share it with them and then\nthey all can ask DVD yeah I know a lot\nof people use DBT so great yeah yeah and\nthat's I think you know at the end of\nthe day my goal with like this article\nand this talk and sort of I'm just going\nto keep kind of yelling about this at\npeople is that like you know DBT is this\ngreat front end for so many like low\nuh it's a it's a low code solution it's\ndefinitely not a no code solution but uh\nDBT is like very popular it's very easy\nto learn to do very complicated stuff\num and then suddenly if DBT becomes like\na front end to all of the amazing flight\necosystem like it's such a strong\npowerful tool like there's it just\nthere's so many problems that go away\nwhen this starts to work uh for me so\nsuper excited about like you know this\ntype of Paradigm moving forward so again\nsorry I don't have more flight code uh\nto show you all but you know that's sort\nof the nature of the Beast here\nthis is amazing I would actually\nthinking of using this to featurize and\nthen train a model like within the same\npipe which is pretty amazing\nokay well I guess we're over so I'll\nhand it back uh I'll hand it back"
    },
    {
        "title": "Flyte Partial Tasks Demo",
        "transcript": "Eduardo do you wanna demo the partial\ndust feature yeah let's go um next slide\nplease\nso um first of all\num partial tasks are a feature that the\ncommunity requested for the longest time\nyou know it um\ncan you open the slide whoever is it\nDavid's driving yeah so the idea is it\ncomes from you know multiple languages\nimplement this idea you can know it's a\nfeature known by several names but in\nPython at least it's called by uh\npartial tasks this idea that you can\npartially apply a function or freeze\nsome arguments to a function if that\nmakes sense so\nnext slide please we lean heavily on\nFunk lose or partial\nand uh next slide please\nyeah the set you know you you can freeze\na portion of the test parameters\nbut again like why did you do it one is\nuh improve the ergonomics like we really\nwant to decrease the distance between\nthe pythons like native you know mental\nmodel to flights uh SDK\nso um one feature that people asked for\nI don't know how long is this idea of\nusing map tasks with multiple inputs so\num let's go see how this can be used we\nship this in in the latest you know\nflight release\nit's still tagged as a as experimental\nbut please\num the community should start using it\ngive us some feedback\num there are there's great ideas there\nsome limitations let's go talk to them\nthrough them so next slide and I think I\nwill take over can I share my screen\nyeah probably now do you guys see my\nscreen\nyes is it too small\nor it's okay\nyou should see like two tabs is it too\nsmall\nI think the terminal and the right hand\nside at least from my side looks a bit\nsmall okay let me just bump this a\nlittle bit and um\nno should also well let's see\nso um\non the left hand side we have a very\nsimple example that demonstrates how\npartial can be used you have a function\ncalled add takes two parameters A and B\nand in line six we partially specialize\nit so we are essentially freezing the B\nparameter of the function add creating\nthis other object that I hear I'm\ncalling add one\nand finally we just like run it hey I'm\ncalling this function at one passing as2\nif we run this through for example as we\ncan expect like we see\nthree which is like the result of you\nknow adding one plus two but how can we\nuse this idea or apply this idea to to\num\nflight tasks so let's go to the next\nexample\nso\noh\nyeah so I'm just extending that idea now\nI have a test called add you know\nworkflow like I'm just you know using\nthe same same method like hey I have a\npartial I partially specialize the task\nad freezing the the same parameter B\ncalling calling it and passing it as the\nresult of\num scheduling the execution of uh the\nworkflow WF\num\nhere's how this thing looks like so I do\nfive flight run the name of the file the\nworkflow and passing the\nparameter a which you know\nas we can expect we get three back\num\nbut really the sky is the limit with\npartial tasks let's go and talk about\nthe more\num\na more complicated example so again I'm\njust extending the same idea of like uh\nincrease the\nbuilding upon the you know the simple\nexample that we're we're just discussing\nnow we have a an ad function or a net\ntest that takes like three parameters\nnow and you can like partially\nspecialized a partially specialized\nfunction which is essentially what I'm\ndoing here\num line 11 where you know partially\nspecializing ad fixing a and then I I\nhave a and b bound with a another\npartial specialization of the already\npartially specialized ad\nand um you know just fighting here lying\nthe live I'm just like calling the\npartially specialized function\nand uh as one again expects if we run\nthis and I can say hey Ace one which two\nc is four we got seven as expected so\nyou know this idea that you can you can\npartially specialize different functions\nor different tasks\nso you're not\num you're not bound or limited to only\nspecializing like one parameter you can\nspecialize as many prompters as you want\nand um but this is all fine and good but\nhow how do we apply this to a problem\nthat the flight Community has you know\ncomplained about for so long so\num one idea is how do you how do we pass\nmultiple parameters to a a flight task\nyou know sometimes you want to fix some\nparameters that you're gonna pass in the\ndefinition of a map task so we use this\nidea of a partial task to\num solve this problem so let me just\nshow you how the\nold way of\num defining a map task where\nwe want some some of the parameters to\nthe function that will be mapped over to\nbe\num fixed so before let's run let's read\nthis workflow from bottom to top so I\nhave a workflow and I have this function\ncalled prepare map inputs which takes\nthe actual parameters that I that I'm\nI'll be passing to the map task notice\nhow the first one that I'm calling here\nlist Q is of type list and the other two\nlike P and S are just like floats things\nthat I want to be fixed over in the\nfunction to the map task function that\nI'll be I'll be calling so again line 24\nI'm just like preparing this boilerplate\nlike structure that will be passed to\nthe map task so the map tests for those\nwho are in the known like this idea that\nyou can\num similar to like how we we we have\nmapping function languages and in Python\nwhere you have a function and you have a\niterable and you basically apply that\nfunction to that interval\nit we have a similar construct in Flight\nwe call it map task in here let's just\ndecode this real quick you have a map\ntask which is the the flight construct\nfor for a mapping you pass the function\nthat you want to map\nand you up until now you only had the\nchance of passing one single input to\nthe to the math task so notice how this\nthis example looks like like we have\nthis prepare map inputs\num task that essentially returns a list\nof map inputs and a mapping input is a\nis an eight price and a float in this\nexample but like notice how we are we're\nkind of like forcing the user to Define\nuh some boilerplate just so they can\npass a list and a bunch of fixed\nparameters to the map task compare that\nnow to how much simpler expressing this\nusing partial tasks as so\nit's much more\num ergonomic you know we still increase\nthe font here\num\nyou still have a map mask and here I'm\nI'm partially specializing the function\nthat I am going to be mapping over so\nlet me just uh\nokay so I have a partially specialized\nfunction\nit's still like the same\num function that takes like multiple\nparameters but this time I'm saying hey\nprice now speed shipping is s I don't\nhave to worry about it I just like pass\nthe list of like things in this this\ncase uh a list of hints that I want to\nmap over as the quantity again not you\ndon't have to worry too much about the\ndetails so like just I'll just like flip\nback and forth between like the two\nexamples in this case we have a 13 line\nworkflow and paths have like almost to\ntwice as much code just to express the\nsame idea\nand prove that I'm not lying uh\nfrom\nthe new example passing your list here\nI'm for fixing um P to be zero one has\nto be 0.1 in a list\ndoesn't really matter what it does but\njust like you know wow python is amazing\nthis\nanyways um\nagain this idea that now you can express\ncomputations much more ergonomically the\ndistance between map tasks and the\npython map built-in function is much\nmuch smaller now\nbut it doesn't stop there\num\nanother uh\nfeature that people you know complain to\nus a lot about is that\num the the python building function\ntakes a function in uh in multiple\niterables it's not restricted to a\nsingle iterable you know so we in the in\nthe same theme like we want to decrease\nthe distance between map tasks and\nPython's map built-in function so we\nextended map tasks to take mood for\nintervals at the same time or also\num just keep in mind that uh\nwe we maintain a strict zip semantics\nthis essentially means the lists that\nyou're passing that you're mapping over\nthey have to be of the same size\num flight will obviously you know let\nyou know if that's not the case\nbut um yeah so again we have a map task\nwe're back to the simple example of\nhaving an ad function that takes you\nknow two parameters and it sums them up\num in this case I am passing add again\nto to map task but notice now how I'm\npassing now Xs and y s s rampers both\nare of type lists so what this is\nessentially doing is summing the\nlists pairwise so let's show how this\nlooks like in the 551\nzero five I have\num just to decode this XS is a list that\ncontains the elements one two three Ys\nis at least that contains elements 10 20\n30. if we run this you get one list that\nhas ten plus one twenty plus two 30 plus\nthree so it essentially has a similar\nsemantics as Python's map function you\nknow where we go uh and and do a zip of\ndatables and Traverse them\nassuming that they are of the same size\nand obviously you can like mix and match\nthis uh here I'm just showing a much\nmore\ncomplex example well not much\nstill like I I have aim again let's read\nit from the bottom we have a workflow\nthat does a single map task where we\npass two lists as as the parameters to\nthe map task but here I'm partially\nspecializing this function called called\nadd and I fixed the debug parameter uh\nlet's go take a look at add that takes\nlike two parameters three parameters so\na b of type int and debug of type Bull\nand if you're debugging you just like\nprint some debugging information\notherwise just like copy it and let's\ntake a look at how this\nlooks like a zero six\nI I will probably have to decrease this\na little bit Yeah so first I'm not going\nto pass I'm not going to set debug\nagain you get like the similar um so\nessentially debug is set to false in\nthis this round that we just ran\nbut if we go and Define debug we get the\ndebug information\nnotice that everything that we talked\nabout here is I I was running it locally\nbut everything that we talked about runs\nyou know in a real flight deployment the\nsame way\nand that's uh the end of the demo can we\ngo back to the slides please\nsorry there's a bunch of questions maybe\nno right awesome uh does anyone have any\nquestions I think that's a great step to\nask that\notherwise uh just reinforce what I\nalready mentioned we have a few\nlimitations one is you like notice how\nhearing I mean like\nare you guys still seeing my my screen\nor David's David's right\nyep great cool oh yeah so I'll just talk\nover this uh you cannot fix a parameter\nof type list T today\nthis is a feature that you'll be we'll\nbe working on so essentially\num when you're partially specializing a\nfunction if you're specializing a\nparameter of type list this will not\nwork flight kit will scream at you so\ndon't worry like this is not the kind of\nyou know error that you've only seen\nruntime we detect the case so that to\nhelp you not make this mistake\nfinal thing is uh the The partially\nspecialized functions all the caching\nsemantics that flight offers you know\nwhere we take the signature of the\nfunction all the parameters and like\ncreate the cache key based on that\num still applies\nso you're not you don't have to like\nthink about okay how how do I handle\ncaching now with um\nwith partially specialized functions\nit's still basically the same\ncool\nany questions\nanything that anyone wants to add Catan\nuh just a quick question what's up\num\ndoes the partial\ninvocation work with both promises and\nliteral values\nit should write it yes yeah yeah okay it\nbasically works the same way as like a\nregular\nworkflow\ncool\nI think one other thing to note over\nhere is because of the reduction of the\nrequirement of data classes or any other\nlike grouping logic you actually have\nreduced one task so the performance is\ngoing to be even higher\nand the amount of data that's flowing\nthrough that Central task of reducing\nthis also opens up other things that we\nwill definitely showcase later in the\nyear so\num coming later awesome yeah\nthanks"
    },
    {
        "title": "Flyte Community Updates 034 featuring PyData and Atlanta Meetup",
        "transcript": "welcome everybody to the community\nsaying my name is samhita I'm with the\nUnion AI team and I'm your host today\nplease feel free to introduce yourself\nin the chat you can put your name and\nthe organization you're from let's\nquickly look at the agenda now right\num I'll start by going over some\nlogistical details and Community updates\nand then Eduardo will be demoing the\npartial tasks feature and finally Tim\nLeonard will be talking about how to\nprogrammatically register workflows in\nFlight with imperative workflows so the\nagenda is open to the community please\ntake a look at it if you'd like to\naccess the notes and this meeting is\nbeing recorded and the recording will be\nposted on the flight YouTube channel\nright next slide\num Niels Chief ml engineer at Union is\nscheduled to speak about flight at the\nAI Camp meet up tomorrow April 19th the\nMeetup event will start at 5 PM ET join\nus in person if you're in Atlanta you\ncan register at the link provided here\nwhich will also share on Slack\nand we'll be at Pi data Seattle which\nwill take place April 26 to 28th swing\nby our booth if you are attending by\ndata Eduardo flight OSS lead will be\ngiving a talk about flight end to end\nand ketan the creator of flight and ye\nthe farming engineer will be giving a\ntalk about the python data ecosystem and\nwe are also hosting a flight Community\nhappy hour at uh\nlocusts Locus cider Redmond on April\n27th from 7 to 9 pm PT stop by for food\ndrinks and conversations about workflow\norchestration\nif you'd like to speak with the flight\nmaintainers you can schedule a 30-minute\nsession using calendly we offer time\nslots on Wednesdays at 7am and 9pm PT\nnext slide please\nthe next Community sank is on May 2nd\nEngineers from stripe will talk about\ntheir flight adoption Journey you can\nuse the add event link to add the even\nto your calendar and yeah thank you so\nmuch for your time and participation see\nyou all at the next Community sync"
    },
    {
        "title": "Flyte Contributors Meetup - April 13,  2023",
        "transcript": "welcome everyone to the flight\ncontributors meet up today's game\nand it is very unhappy to be your host\ntoday\na couple of reminders it's important\nfirst of all this meeting is being\nrecorded and the recording will be\nposted to the\nplight YouTube channel under the flight\ncontributors meetings playlist\nand next thing is that well flight is an\nAI in data Foundation project so this\nmeeting falls into the Linux Foundation\ncode of conduct\nbasically be nice to each other\ncool so let's get started uh again I'm\nsharing here the link to the agenda and\nnotes please consider adding yourself to\nthe attend this list including your\naffiliation that would be great\nand for the next item time to welcome\nnew faces uh so we would like to briefly\nintroduce yourself today\noh can I start or\nyeah if considering that if it's your\nfirst time in the contributors Meetup uh\nwondering\nbriefly interview with yourself\nthe community here\noh no I I've been previously to the\ncountry tomato\nthat's awesome\nthank you yeah and it's it's great to\nhave you here because it's a good\nreminder that\ncontributions take different shapes no\nit's not only purely code but also what\nyou're doing I mean\nspreading the word that's awesome thank\nyou\ncool who else for the first time here I\ndon't see anyone else that's awesome\nokay welcome again\ncool okay next item will be review uh\nweek two on outstanding rfcs\nuh let me close this thing\nand uh yeah let's kick it off\nso\num\nyeah again the the board with the status\nof the rncs it's available there\nso we typically start and we try to\nleverage this communication channel to\ngive rfcs Outdoors a place to\ncommunicate and introduce their ideas so\nwe will have here\nuh Mr Kevin zoo we have introducing the\nexternal plugin\nservice RFC\nyeah uh Debbie can share my screen yeah\nthat's better\nokay\nall right so\nwe create a new component called\nexternal parking service\nand we have office policy in fly report\nrequests so\nso for now uh Fly Parking leaving the\nwhite propeller so there's some issue\nlike for example like every time you add\na new plugin you have to recompile\nrebuild the whole propeller and then if\nwe want to add a big new backend plugin\nyou have to write the column code to add\na new plugin\nand then there's another issue is that\nthe um\nuh dependency Conflict by the\nenvironmental know that so when you try\nto uh add a new plugin for example like\nadd a new version of MPI uh plugin uh\nthey have to upgrade the kubernetes\nclient API but uh they also need to\nupgrade the propeller could\nrecommendation API so\num they might have some dependency\ncomplete in the between the propeller\nand the white blocking so what we want\nto do is that we try to move the flight\nplugin outside of the propeller here we\nhave the new component called external\nplugin system\nso in the in the fly proper side we have\na GPS plugin it's a job business client\nit used to send\nget deleted request to the external\nplugin system and all the plugin will\nlive in the standby system and then\nthe benefit of the stone parking system\nis that you can write your backend\nplugin in Python\nand then\nuh\nother like uh other deployment here is\nstill so you can scale up your parts as\nmany as you want and then\nuh to configure and then you can set out\nuh multiple external plugin system\ndeployment for example like uh here we\nhave a istanban system for production\nhere we have a default flight plugin\nhere so you can configure the one\nendpoint of uh\nsend a request to the production\nspecific type text to the production and\nthen you can have another deployment\nlike for development it include some\ncustom plugin so you can send another\nrequest to this distance\nuh I can sure I I have a question here\nyeah so uh if I have MP maybe MPI B1 and\nMPI V2 in the same external plugin\nsystem well the dependency issue is\nstill happening\nyeah but I mean you can you can run that\nlike mpm V1 in this deployment and you\ncan you can create two deployment like\norganic deployment have if you want\nanother device I'm happy too so you can\nsolve this problem or\nyeah\num\nfor now we only have one plugin so we\nideally we want to move all the plugins\nto the external plugin system and then\nuh\nwe can like we can move\num we can move all the plotting into one\ndeployment we can like each each\ndeployment only have one plugin it\ndepends on the use case like yeah\nokay and then here is a\nexample to write a big and Body in\nPython so basically you have to extend\nthe they can plugging past class and\nthen\npropeller will send uh test query\nrequires it include the text template\ninput data output prefix where you write\nI'll go to\nand then\nget delete that's it this you just need\nto implement these three method and then\nyou\nregister this begin plugin\nand then uh how to how to deploy how to\ndeploy this uh become plugin you have to\nwrite a new Docker file for your\ndeployment so basically you install uh\nyour like keep plugging here and then we\nadd a new command called pipeline server\nwhich means like\nif you want to test your big important\nyou can run the uh this Joby server\nlocally and then you can test your big\nimportance service\nyeah yeah and then\nuh you build an image and the audit\nimage for your deployment\nand then you can\nconfig the different endpoint for your\nattacks by for example like uh\nI want I want if I add a new custom text\na fly I can like\nuh I can send how the\ndifferent endpoint for my\nfor my tax flight it's an opening\nservice development and but by default\nall the requests work can send to the\nexternal Point service production like\nyou can set out a different endpoint\nuse case\nyes\nso\num yeah basically do you just\nuh historical insert system\nso does that support the kis plugin\nalready\nnow for now we only have\nbig credit plugin here\nokay\nbigquery is web API right yeah yeah so\nyou are using like uh\nuh you have a linking spot tags right\naround your Hardware yeah so yeah it's\nwritten by web API so you can add a your\nspark tags today because it's an\nimportant system\nokay got there if but if we we are\nfamiliar with golden do you see any\nbenefit that we move into the new plugin\nsystem\nand yeah good question I I think uh\nanother benefit to move the\nAuto plug into it's not coexistent that\nwe move the overhead of the running\nplugin to the outside of the propeller\nso like which because you can have many\nparts as well in the install plugin\nsystem you can easily to handle uh more\ntax things in the install appointment\nsystem okay\ncan I say something here too real quick\num yeah it is important to note in this\nthat uh you know this will be running\ncompletely externally there are a lot of\nadvantages to running some of these\nkubernetes plugins within flight\npropeller specifically we use\num\nwhy Informer watches on kubernetes\nresources so like you know on the Kate's\nplug-in we detect changes to a pod and\nthen can automatically\num enqueue the workflow to be\nreevaluated again\num so that's something that just there's\na there's a little bit of overhead in\nhaving this external plug-in system\nwhere there isn't this fast feedback\nloop um so so performance might be a\nlittle bit worse um and that's kind of\nthe trade-off here is is this\nflexibility of deployment and ease of\nimplementation of external plugins\num potentially for performance we have\nwe haven't done any extensive benchmarks\non this but something to keep in mind\nit's possible to add an inform occasion\nin Python\nadd what uh add informal cache in\nexternal parking system yeah the problem\nis that we don't have any\num feedback loop back into propeller to\nre-enqueue the workflow right\nokay oh\nbecause the communication is only only\none direction\nyeah\nthanks\nuh one question\nyeah I reviewed the pr on Saturday I\nthink there was a discussion whether all\nthe plugins should be in one image or\nwhether they should be one image per\nplugin\nand um here it says now right rebuilt\nthe docker image so it is currently one\nimage if you ask my question would be I\nremember from plug from plugin to\nplugins and go that are compacted to\npropeller one of the\ndifficulties is is kind of like version\nconflicts right when you want to upgrade\none plugin then all of a sudden you get\nto upgrade all of them and spending most\nof my time in Python I can say that just\njust the same there it's not any better\num\nso maybe I missed that part but is it\ngoing to be one image for all plugins or\none image purple yeah\nhere you can see like this this tool is\nso external operating system is two\ndeployments so basically it's diff you\ncan use different image here like okay\nthis one and one of them is for this one\nso\num those two image can install different\nplugin\nso basically we can put multiple plugins\ninto one image but have multiple images\nuh one image\nbut one image with one copy one point\nyeah okay that's good because yeah okay\nperfect then we don't have the problem\nall right\nany other question comment\nnope anything else you'd like to cover\nKevin\nuh no\nthat's great awesome yeah again feel\nfree to add your comments observations\nto the pr itself\nand keep the conversation async\noh thanks so much Kevin all right uh\nnext up was the\nReynolds RFC is there any update for\nthis proposal\nuh really no update I think you know\ngenerally everybody that's taken to look\nat it says this would be great to have\num as as I understand it so I actually\nhave a a\nMVP working right now in the back end\nwe we can talk about I I don't know what\nthe process looks like to merge this\nuh\nI would push forward with merging this I\nguess as as soon as we can generally I\nthink people are think it's a good idea\nyeah I think in regards to the RFC\nprocess next stage will be the final\ncoming period is when the community\nagrees that we are\nwe have already discussed all the\npotential trade-offs Alternatives Etc\nand we are ready to make a decision in\nseven days or less\nuh so\nif there is no objection we could move\nthis proposal to the final coming period\nwith the intention to merge\ndoes that sound right or\nawesome\nthat's great\ncool uh around this existing tax in\nmetadata proposal there's anyone knows\nif there's an update\ndon't think we had any updates since the\nlast time we discussed this yeah seems\nlike no updates let me take notes for\nthat\nuh\nMarines\n[Music]\nthere you go and uh all right next one\nwill be the config overwrite all right\nso for this one oh we're still\ndiscussing internally and we'll have a\nhold a meeting with e next week\nokay yeah\noh\nall right thank you\ngreat\nright that will be it in terms of\nproposals rfcs is there any other\nRFC that we are not\ntracking here that we should discuss I\ndon't think so right\nokay\ngreat uh yeah news couldn't join he was\ngoing to present some of the ideas in\nthe incubator\nso unless you want to discuss some of\nthe ideas here we could move to the next\nitem in the agenda\nany incubator idea you would like to\ndiscuss here\nnope\nokay so we can move to the open mic\nquestion section starting with use\nidentity from Byron\noh\nright so it's e here\nyes okay\nso yeah do you have any idea on this\nuh we're gonna do it\nwe're just thinking uh the name will\nprobably have to change the okay in the\nmessage\num uh hatham and I talked this morning\nwe're just going through all the use\ncases uh that anyone might have and for\nthe most part we think a\nstring should suffice\nfor whatever people want like\nwhat we're going to call that I don't\nknow but something smart it's cool and\ngeneral General because it might be an\nemail it might be a username it might be\na number in a string it might be a\nlike signing certificate\noh okay right basis before encoded or\nsomething like it can be a bunch of\nthings so are we going to only use one\nfield or we can create another protocol\nincluding like email ID blah blah blah I\nthink one field is fine if we need more\nwe just add to that field probably to\nthat message\nokay we need more what sorry if we need\nmore we can always just add it to that\ncurrent message the parent okay okay\nokay let's discuss another threat yep\num okay could you give us a really quick\ncontext on this\nuh me\nyeah\nsure so the context is that we want to\nadd the user identity into the security\ncontext because some of our flight\nplugins need the user identity like my\nusername to start a job\nyeah\nthat's it\nokay cool thank you\nanything else regarding this\nitem\ncool thanks\nuh next one from Eduardo database\nqueries\nso yeah uh I I just want to this is more\nlike a a call to action to the community\nI I want to have\num specific point of context I I need\nyou to run a few queries\nin the flight admin and data catalog\num queries so that we can\nwe can gauge\num how how how big the columns of some\nof the the tables should be so\nessentially I have a list of queries\nthat I want you to run and Report the\nresult to me in case they\nlook weird I'll put in the instructions\nwhat I mean by weird\nokay\num the con like the the bigger context\nhere is that um we are moving\num to a place where people will be able\nto choose which database they want to\nrun for you know admin and their catalog\nand whatnot\nright now we only support postgres we\nwant to support MySQL and potentially\nother databases but in order to do that\nwe need to run a migration and you know\nhow can how database migrations can be\num\nwe need we I we just want to be super\ncareful and not break anyone\nokay got it thank you uh do you plan to\nshare the list of queries in the\ncomputer I will\nthank you\nthanks all right next up from you will\nuh manufacturer KF operator plugins\nyeah so\nthere's a PR so\nsorry first time attending this so\nshould I share my screen or something to\ntalk about this yeah and you if you\ncould briefly introduce yourself\noh yeah okay so hi everyone my name is\nYobo\num I'm I working I'm working at LinkedIn\nuh I'm from the same team as Byron um so\num we are currently uh leveraging flight\nto build our\num uh orchestrated system\num and\nwe have been using flight and we have\nsome found some issues and we're trying\nto fix that\num yeah so this this particular issue I\nhave talked to Dan\num\nso it's because when we are using the uh\nkeep flow operators uh MPI uh tensorflow\nTF jobs\num we have troubles setting\ndifferent resources and images for\ndifferent\num for different replica groups for\nexample\num for MPI job you have launchers you\nhave workers they should be using\ndifferent resources but right now\num the only thing that's exposed uh on\nflight and is the a worker or the number\nof workers and I think number of the the\nlaunchers so that's not enough so\num but okay so let me share my screen\nso I would like to kind of can you guys\nsee my screen about this\nokay so I think the the issue is that we\nwant to refactor this and make it into\na separate uh a separate a new version\nof plugin which is not Backward\nCompatible like\num I'm not sure what people think in\nthis way like how to how do we actually\num does not break the existing users\nuser experience like because\num the IDL is not Backward Compatible\num there are two ways of doing it so one\nway is to\num basically add the new Fields into the\nexisting IDL field\num and that the problem with that is\nthat then we have then it's hard to kind\nof like refactor the code um because we\nare adding the things to the existing\nexisting uh photobuff and the other way\nis actually having a completely uh\ndifferent portable file and then\num by um in the in the back end we can\nuse this new introduce the task type\nversion so if it is text type version\none we handle it in certain way and it's\na task test text type version two you\nhave in a different way but I think so I\nthink Dan suggests me to go with the\nsecond approach but uh the problem with\nthe second approach is that so when\nusers actually\num\nwhen users actually didn't upgrade the\nthe back-end propeller but they updated\nthe flight kit then they got they're\ngonna still face this backward\nincompatible thing so\num because the the propeller is not\nupdated the plugins are not updated uh\nit's not expecting version 2 of the\nof the task in Flight kit butterfly kit\nis actually updated\nso I'm just wondering how is flight\ncurrently handling such\num\nbackground uncompatible incompatible\nchanges\nand the task template has a version\nnumber in there\nyes typically have used that to uh if my\nsorry\nthen\nI was trying to find in here um yeah so\nwe talked about this very briefly and it\nwas like this this task type version\nthat Kate had suggested\num is is kind of the direction that I\nwas hoping we would go with this\num basically we have a new set of\nmessages for the cube flow operators\nthat more closely align with how Das and\nRay plug-ins are done right now where\nthere it offers separate configuration\nfor each replica set not just the number\nof workers but it has resources it has\nimage\num some different things ultimately we'd\nlove to get pod templates into here so\nthat you can you know set pod templates\nfor your workers\num\nparameter servers anything like that and\nthen in the back end that will just be\ntask type version two and so the idea is\nthat if it's a separate plug-in if a\nuser updates you know in Flight kit to\nuse the plug-in version like 1.5 or 1.6\nwherever this would land 1.6 then the\nuser would be forced to update the back\nend but if the user didn't want to\nupdate the back end they could still use\nthe previous version of the plugin\nbecause those are installed separately\nfrom flight kit and so it kind of offers\na path for uh maintaining on their\ncurrent version or if they wanted to use\nthe new one they would have to update\nthe back end I think that's the same way\nto do it I think that's how we've done\nit in the past\num but if anybody has any thoughts you\nknow very very happy to discuss it\nyeah but there is one one problem over\nthere that it's just that if you update\nflight kit and don't update the back end\nthen you will hit an issue it's like it\nwas not using the new version number but\nthat's a compatibility Matrix that you\ncan probably\nif you want to use this thing it should\nbe flight back end greater than correct\nyep and that's exactly what we were\nsaying and then if you're not okay with\nthat you can downgrade your flight kit\nplug-in version because these are these\nare installed as separate separate\npackages\num\nand and use the old version so I have a\ndiscussion so should we make first make\na PR in the propeller or the plugin to\nadd the error message or the warning\nmessage saying hey this propeller\nversion is not going to support the new\nversion of apply kit no\nbut how would they know that\nyou're just gonna see that the task\ntemplate\nlike there's there will just be\nsomething some error message in the\nproperty\noh not proper right it will probably be\nan error message in the UI saying unable\nto parse or something okay yeah yeah\nthat error message is propagated back\nthrough the task execution so that if\npropeller says that it can't process the\nthe task type version\num then when a task of that new new type\nwill be would be executed in the UI\nit'll say uh when we can be smart about\nthis uh like you know version\nincompatibility or something like that\num I think every plugin should have that\nproblem like every plugin should have\nmaybe this should be in plug-in manager\nat some level that hey version is\ndifferent as compared to what I expected\ncurrently we don't have that\nthat's one thing that we could do Hugo\nit's\nbasically\ngive a nice message there\nbut those flights have some kind of like\nversion set system like so\nwhat fly kid version work with what\npropellers\nwe're not maintaining a compatibility\nMatrix right now I don't believe\neverything works with everything\nI'm gonna say we don't frequently come\nacross uh incompatibility between\nversions either though we do everything\nwe can so that everything works\nour goal is to keep one dot star\ncompatible\nthroughout\nbut now there will be cases like this\nwhich are like individual plugins right\nthe core is one dot star is compatible\nfeature-based right like you can't use\ngate node like it with Vivo if you're on\n1.0 yeah\nso like as you upgrade you're expected\nto upgrade things but there is\nand those usually die in like you know\nat registration time itself\nuh but like in this case because it's a\nplug-in\nit's going to be a runtime failure\nyeah because at one time I think there\nwas like one above a bug about the\ndynamic workflow\nin like at one point\n1.0 or something\nand\num I when I use it it just gives an\nerror and I have to search for like any\nissues to see oh this version actually\nlike this feature does not work for\nDynamic workflow I think I think it was\nI think\nso yeah they're like I like there's no\num like messages or descriptions on like\nindividual versions like what's missing\nfrom there\nit's very hard to\ninto the White surface area\nI think overall we should try and come\nup with a compatibility Matrix\nvery useful that\nfeatures XYZ need\nlike that that tape already exists\nwithin right now\nwe just I think we can\nspend some time like to make it more\nexplicit yeah\nand I'll be curious to to know you go\nwhat was this bug like we should\nprobably add that to the to the Matrix\nyeah yeah I what was the issue I can I\ncan I can look it up real quick\num\nhey can you share the link\num David can you share this link with\nthe Spotify folks I think some of them\nshould also be joining\noh right yep\nin this channel\nbecause I see them active\nsimple words okay\nI can't I can't remember but I'll post\nit maybe in the link or something\nabout an issue\nI think yeah I guess\nwe can\nshrink offline about like how do we add\nthis error message about an uncompatible\nportions of the propellers\nthe inner propellers\nI see a lot of bugs have kind of been\nlogged we should do a bug bash in the\ncommunity just like all of us together\nonce\nthis kind of squash of bunch of these\nsome of them are actually already\nresolved we have not done a good job of\nclosing them I feel\nbut probably\nthank you and I think the next item also\ncame from you or Global config yeah\nthere's a um so I think this is a\nquestion for the AI folks um so we are\ntrying to set uh the conflicts at the\npipeline level like workflow level\num so this conflict should be accessible\nin every task for example we want to set\nour own workflow name\nuh but could only be\num I don't think there's a there's\nsomething in light that supports that\nno I think this for me this sounds great\nactually I don't how would that look\nlike\nhow will it get passed yeah yeah like\nyou know\nthree injected in the flight context\nsomehow yeah how big of a config are you\nthinking can you give an example of what\nall are you thinking of putting in there\num\nlike what like are so basically our\nworkflow should belong to certain groups\ncertain uh AI groups and\num\n[Music]\nfor execution\nno but within the task itself we need\nthat info to send it to our metric\nsystem\nso in the task we need to access that so\nat like at execution time like when\nyou're taking up your execution you want\nto pass that object right it basically\nlike flight context you can accept\nbecause like successful within the task\nyeah\nbut flight compact I know is internal\nlike it doesn't support you to like edit\nanything to it would environment\nvariables be enough\nuh environment yeah that that's enough\nbut the thing is that's\nset at the task level right no I'm just\nI was actually thinking of also\nsupporting environment variables at the\nworkflow level and I think you can level\noverride so that's actually not a hard\npiece of work and it has a lot of\nbenefits like it's\nsort of a Reynold right now adding\nenvironment variables on the execution\ncontext in the workflow so when you\nstart an execution you can say I put\nthese environment variables on every\ntask\noh\nuh would that work\nyou both um so the exit so okay\ncan we associate this with the launch\nplan like so\nyeah\nabsolutely\nso for launch plan you specify okay\nI have this conflicts for this launch\nfor that I think so that we should be\nable to do that I I think that's\nactually the right thing to do\nit should have been in the launch plan\nand in execution context but there but\nthere's one issue with it so\nI think these doesn't affect caching\nright if I've said it as a context yes\nit won't affect fashion so we're\nthinking like is it if we set certain\nFields like can we like make some Fields\nlike\neffect cachings actually you change this\nbecause then it should be an input like\nyou are not the yeah it has to be part\nof the signature right because some\npeople have asked us in the past yes try\npast us can you remove this one input\nvariable from caching okay\nso it's like like excluding some input\nlike from here okay so but yeah right\nopposite thing yeah no no but but I do\nrealize that uh there is like a embedded\ncaching uh flag you can set right when\nyou're with the execution contacts or\nparameters\nyeah um\nso do you think it's possible for us to\nlike maybe in the pre like let's say\nwhen executing a workflow like we\ndetermine whether we want to enable that\nflag like within\nwithin some like maybe pre-execute after\nworkflow\noh at the point of workflow execution\nnot like in admin\nyes\nwe don't support that today but\nseems like a\nwhenever caching happens I find all\nkinds of dangerous situations and you\nhave to be careful it was like in launch\nplan like be able to expose I think\nlaunch band exposes already\nyou but you're thinking like maybe I\nguess what what does that process look\nlike for figuring those out are you\ntrying to launch like a pod that\ndetermines this or is that just like\nsome weird web hook thing\nuh no it should be\num\nso like for a launch plan uh\num we so if this field is like\nchanged from wait let's see\nyeah I guess not actually it wouldn't\nmake sense\nokay so basically it's just we have this\nlike uh base pipeline directory that\nwhere every single task should write\nunder this path like a prefix to the\npath and we would like to set it for\nevery single task and once users change\nthat that should change like every\nsingle\num task and the cash should be meditated\nor the retention config like we have\nthis retention\nthis is raw output prefixes already\nexists\nI uh but yes but it doesn't work with\nthe hdfs like I know like you guys have\nS3 or\nlike it should work with the DFS it can\nwork with root file system we have we\nactually tried it with uh\nlike FSX for luster or like you know RFS\nright we tried it with NFS it works oh\nyes so the wrong prefix is uh you just\nhave to choose the right plugin I think\nnow that is FS spec everything is FS\nspec hdfs should be a plug-in in there\nso if you use this plug-in raw output\nprefix will do all the right things and\nit will impact cash correctly because\nit's an input and output automatically\nyeah\nyeah\nthat will work if like if all our tasks\nare implemented in Flight way but\nunfortunately we have some like external\nyou know as a spark like you you send a\nyeah so then for you and you can decide\nto create a new output right which is\ndifferent\nuh and then cash will be different\nokay so actually that determined that\nthat invalidated the cash if you change\nif you said the output not for that task\nright because that task is running\nalready because you are in the current\ncontext but let's say in the downstream\nanybody who uses their input if the path\nis different it is a different output\nright so the next input will not be ca\nwill not be assumed to be cache it it\nwill be just a new let's say about\nretention config like how long the\nartifact gets kept\nthat's yeah that is interesting that\nthat is a whole Rabbit Hole uh retention\nconfig and I think we should probably do\nthat at the launch plan level too I feel\nat some level I don't know Dan what are\nyour thoughts on that\nhow long do we have here 15 more minutes\nit's not kind of fun crazy stuff will\nhappen but yeah it's\nnot bad but what do you think\nI'd be happy to discussing it um I mean\nwe still have all the cash\ndelete stuff uh all the PR's open from\nfrom Nick that had opened that up that\nwe we need to work our way through then\nthat you know in the UI has cash delete\nbuttons\num the goal was to like integrate that\nwith cash exploration features where you\nknow yeah we can then delete the cache\non expiration and different things like\nthat I think adding cash expiration at\nthe task level makes sense\num\nlaunch plan level potentially I I don't\nknow if sky's the limit\nwhat do you think okay all right so yeah\nI think this deserves an rfcw I think\nthis is a little dangerous\npeople bite people yeah I I guess my\ngeneral question is how like how like\nexisting flight users are using like are\nsetting these pipeline or overflow level\nperfect there's no working the system\nlevel cash exploration right so only the\nttle is system level so after 30 days or\nwhatever\nyou try to hit it again it says no I\ndon't find it and you run it the beauty\nthe way if you think about pure cash\nright that means the executions are\nuh hermetic\nthat means in worst case you rerun them\nyou result in the same output in a\ndifferent place and it's okay\nright they will not affect each other\nthat's the goal of the system it's like\nit should not impact the stability of\nthe system\nworst case you'll get an extra execution\nbut cash was only an optimization right\nright I guess that's all I do thank you\nawesome\nI'm sorry that's great thank you\nuh okay next item because well I think\nshe really had to drop\num Fabio he discovered a small bug we\nput templates I think we already\ndiscussed it in the chapters of the art\nof sectors that are reviewed\nforeign\na bit brutal way but at least our\nEngineers are happy\num and then a test what Byron buildings\nI actually said\nawesome that's great thank you\nall right anything else you'd like to\ncover here let's go here\nI had one thing uh one was about the the\ntorture and stuff that Fabio and I have\nbeen kind of working on mostly Fabio\nI've just been\ndoing a little bit for it uh Fabio if I\nam going to use this forum to quickly\ndiscuss sex so where do you think it is\nwhat should we do next\num I think it's pretty far\num the only problem that I have with it\ncurrently is that I think the Rendezvous\nconflict of pytorch is not correct\nRendezvous basically this thing where\nall the workers talk to each other\nand\num\nI have I was able to run it locally or\nlet's say just locally or in a single\npot with one worker and also in multi\nin multiple Parts with pipe with the\nfight operator sometimes the they\nweren't able to Rendezvous\nI think it's just a misconfiguration\nsomewhere\num what I would do I guess on Saturday\nmorning is that I will take the minimal\nworking example from The Keeper training\noperators repository and run it locally\non the notebook run it in the Python's\njob but without flight and then just\nprint the config and figure out where\nthe differences\num yeah I hope that I hope that there's\nkind of small difference somewhere and\nif that's the case then I'll adapt that\nand then there's a PRN flight idea where\nI introduced like an like an elastic\nconflict similar to the part one there's\na plr open in propeller graph PR where\nuh there's basically just an if\nstatement if this elastic config is set\nlet's put it into the project job yeah\nand that already works it just it needs\nto test basically but it works\num yeah on the flat good side it's also\nworking in principle apart from this\nkind of around the Google conflict thing\nso I hope that it kind of works on\nSaturday morning let's see\num yeah\nwith the Rendezvous thing\nI'll be driving only on one machine and\nI tried with alpaca\nit was it worked so now we are trying to\njust scale up alpaca but where are you\nworking but were you able to Rendezvous\n[Music]\num\nsee 10d that the local\nI haven't tried it with another one oh\nbut I think for the distributed or you\nhave to use that CD right\nno I think c10d is also fine because\nthen it opens a service for the master\none and then they all basically it's\nrunning on the master and the other ones\nthey just talked to the c10d in the\nmaster I think\nbut then you need a server like service\nExpo sport exposed all of those things\nI think it does it takes care of that\nhonestly because when you set the I need\nto take a look at how exactly it works\nbut when you set c10d as Rendezvous back\nend\nthen it sets the environment variable\nhow to code master address\nit sets it to the service for the master\npod\nand so from that that take that it\nbasically runs between the end all\nothers just use that one running in the\nmaster but I need to look exactly of\nwhat needs to be configured to work\nhappening using this Rendezvous server\nthe c10d or let's see it it says it can\nuse that CD but then you have to give it\na service account to talk to hcd which\nis kind of dangerous I feel but I mean\nyeah\ntraining operator repository there's\nthere are two examples\num and one of them I think they they\nhave a deployment that runs at CD\nand then you use that one\nbut in the torture documentation\nyourself that kind of the one they\nrecommended ctandino.cd\nbut I'm not 100 sure but from what I\nunderstood so far is that you can\nbasically\njust use the the c10d that is running in\nthe master replica\nyeah\nelastic\nthe default Rendezvous configuration is\nat CD\nI don't think it is\nyou know it is if you go inside and look\nat the code\nit's a CD as the default and czd is for\nthe Standalone mode\nokay I need to check\num so just check I guess maybe we\nprobably uh so that's one part so that's\ngood like I think we can get it all\ncombined we'll also I'll also do some\ntests and I can help you with certain\nthings as well if you want uh maybe or\nthis weekend\num but this is great I think that this\nactually the experience is so much\nbetter uh I was also then looking at\nhugging face because of the checkpoints\nand we are working on hugging phase\ncheckpoint will probably contribute it\nback into hugging phase uh so that way\nanybody who uses hugging face can just\ndirectly use checkpoints out of the box\nuh one flight but then the third one was\ndeep speed\nI answered in the chat just before the\nmeeting I'm not sure if you saw it I\nknow I didn't see it sorry yeah\nyeah\nokay I'll take a look so yeah that's\nlike let's talk about that probably it's\nlike a similar pattern I don't know\nmaybe not I can I can answer I guess\num does everyone familiar with deep\nSpeeders or should I give a quick\nsummary\ngive a two minutes so basically the idea\nof the deep speed of the paper so it was\na paper from Microsoft they introduced\nthe so-called zero zero Optimizer and\nthe idea is that you can have\num when you have n times more gpus you\ncan have an N times larger model because\nonly every GPU in the cluster only has a\nsubset of the of the model and then\nbasically at every like step in the\nforward or backward path all the gpus\ncommunicate with each other and like\nsend each other the respective weights\nthey need but in total like the entire\nmodel is distributed over all gpus which\nis pretty cool the trend is Mega models\nusing it\num and I never trained like seriously\ntrained something with it but like one\nand a half years ago the DML Engineers\nthey asked me whether we would be able\nto run it with what we had and I kind of\nhacked it together that I could run it\nin a python strawberry's job because\nultimately under the hood when you use\nthe Partridge black and the only thing\nit does it uses it wraps storage\ndistributed in a process group there's\nlike a deep speed in a process group but\nultimately it's just storage distributed\nand\num I think it's all memory optimizations\nper process that's what it really does\nright yeah yeah it does also like\noffloading to like and then you drive so\nthat does some crazy optimizations but\num let's say we kind of hacked together\nlike a minimal working training example\nin keyflow pie charts job you needed to\ninstall the Nvidia uh like mvcc needed\nto be installed and some other stuff but\num back then at least we kind of trained\na meaningful working example using a\nsimple key proprietor's job so we should\ntest it again\nbut if they didn't change something I\nguess it would have would already be\nsupported oh really with torch run you\ncan you can use I thought deep speed is\na separate entry point but maybe I'm\nwrong we didn't use it with torch on we\nuse it with vanilla torch distributor\ntraining ah okay so I I think deep speed\njust like torture now has a new just\nentry point kind of a thing but maybe\nI'm wrong so let's take I think it's\ncatching up I think these things yeah\nthey have a CLI entry point but I don't\nthink you need to use it oh okay\nthat was one and a half years ago maybe\nthings changed um right now all the big\nmodels are all trained using this feed\nright like they're all like getting\nbigger\nlike any architecture\nyeah this is infrastructure stuff it's\nnothing to do with yeah the crazy thing\nis that um\nin the paper they say there are two\npapers but one would think this leads to\nsome crazy Network traffic right but I\nthink it's the they quantify the penalty\nand it's just a factor of 1.5\num training speeches which is\ninteresting I think it's just really\ngood engineering and I don't think\nthere's any\nyeah any like\nreally good engineering and they're\nreally optimized\num he's just uh just a info guy\nso but it's a very like all the\nchallenging all the GPT foreign\nprobably at some level there is some\nwork going on in this area and there\nwill be hopefully uh popular is the real\nexpert here but uh there are a few\npeople on my team also we'll bring all\nof it together I mean let's let's get\nthat I'm happy to like an artistic thing\nunfortunately I don't have the problem\nthat I need to transfer to big model\num I wish I had\ntalk to them\nthat's cool maybe one FYI about the\ndepartment thing if you work on it\num it looks hanging for example if you\nsaid\num if you said\nthe\nbasically if you configure it in a way\nthat waits for another worker but you\nrun it locally then it will appear as if\nit's hanging because it's waiting for\nthe Rendezvous and by default it doesn't\nlog\nso it just appears to it with that like\nit doesn't it's just it appears to be\nhanging but it doesn't hang you need to\nset the lock level in for torch elastic\nuh to I guess info or debug I don't know\nbut then you will see that it's trying\nto run everyone in the sales and then it\ntries it again and it fails then if\nthere's an exponential back off\num we need to catch that in a nice way\nbecause like it takes a few minutes to\nfigure out\num yeah so maybe we should enable that\nlogging by default which we can do right\nbecause we have control the entry point\nnow so you could just probably set that\nup\n[Music]\nif you if you play with the Rendezvous\nconflict um\nand figure something out just please\nwrite it in the chat\nonce I get a chance I will try\ncool thank you\nawesome that's great\nall right anything else that you could\ncover in four minutes or less\nI don't think so right\nokay uh yeah next one probably we should\nlike make Kevin demo the back-end\nplug-in system like the pipe plugin\nsystem I don't think he is demo right\nand this group should see it earlier and\nprobably\nso we talked about it oh we did the RFC\nbut not not no no yeah not demo yeah but\nwe should probably go through the demo\nwe should probably just click to bring\neveryone up to speed because I think\nlike the contributors here probably\nwithin their company are the experts of\nthe flight yeah so uh like if we can\nhave a more in-depth demo for like just\nAdministration right like how this would\nbe installed yeah not just like user\ndemo can we also get like make from\nstripe and uh yeah I like from Tesla and\nother places join this meeting because\nactually we can get more feedback from\nsome of these contributors\nthat's great\nwell um thank you all for joining that\nit was great and hope to see you in the\nnext one\nsee you all\nhave a good have a good day\nyeah"
    },
    {
        "title": "Flyte Contributors meetup - March 30, 2023",
        "transcript": "okay great\num let's get started welcome everyone to\nthe flight from Traders Meetup\num do I have to see you all here and\nDavid Espejo I'm an open source\ndeveloper Advocate at Union Ai and I'll\nbe your host today\num a couple of reminders first this\nsession has been recorded and we will be\nposted to flight's YouTube channel\nthere's a new um playlist for for the\ncontributors Meetup and second thing is\nthat flight is a an AI and data\nFoundation project so this meeting falls\ninto their code of conduct which\nbasically boils down to be nice to each\nother\ncool right so again\num probably for the third time let me\nadd the ring for the agenda notes Here\nand uh let's quickly welcome new members\nso I see some new faces Tim will like to\nbriefly introduce yourself here\nuh sure uh I'm Tim Shiner I just joined\nUnion as a head of product design I'm\nnot sure I'm actually going to be much\nof a contributor to flight but\num\nI mean but David invited me to come to\nthis meeting and so I thought I would I\nwould come and see\nabsolutely there are many ways to\ncontribute\nthank you for joining all right uh I\ndon't know Greg is this your first time\nin this meeting\nyeah first timer for me so thanks for uh\ninviting me to this and have been a long\ntime user of flight and trying to ramp\nup some of my contributions uh so yeah\nexcited to be part of this meeting and\nthe team\nawesome\nready how are you here\nokay seems like no other new faces here\nawesome so next thing in the agenda will\nbe to review outstanding rfcs uh let me\nquickly share my screen here\nand uh there's a new GitHub project\nthank you meals for your help\nbuilding this the idea is to basically\nuh have a central place so you can not\nonly see the rfcs but this this status\nor the stage they're in\nthere's a couple of new ones I know that\nkitten has to leave early so we'll go\nfirst with Kaden kitten I don't know if\nyou want to briefly introduce this\nproposal\nDavid we are joining like Kate and\nincluding he's joining this meeting from\nthe other room so keep it you know a\nminute okay we'll be right back\nno problem\ncool so in the meantime I see Byron here\nand I see that you just in the config\noverride RFC there's there's already Yes\ninteresting discussion in the issue and\nthe pr itself but for the benefit of the\naudience would you like to briefly\nintroduce this proposal yeah sure\nsharing my screen\nwe have the link to it or\nyeah I think I have okay\nso yeah I think there are some ongoing\ndiscussion for this and\nokay I'll briefly go through it\nokay\nso this issue has been discussing for a\npretty long time and basically it shows\nthat uh for the test config the the\nconflict that is inside the test\ndecorator\ncan you hear me\nyeah yeah okay\nso so for the test config and what is\ntest config is the one inside the test\nnotation like it cannot be modified\nuh after they got registered we cannot\nmodify that them at execution time and\nfor us is because we heavily rely on\nreference workflow\nso we want to send some test config of\nthe reference Word file 3 execution type\nso we are thinking uh figuring out the\nbest way to reach the consensus from the\nopen source size and linking size and\nbasically effer a long discussion we we\nthink that this might be a possible\nsolution so basically we'll create\na fly ideal and it looked similar to\nthis so uh word uh it has workflow\noverride and under workflow override it\ncan has test workflow uh test override\nor wherever override so that it can be\nnested to many layers\nyeah and you uh API will basically look\nlike this\nso fly remote dot execute or fly CTO and\nyou can also include this in your uh\nexecution file but I think okay so I\nthink the other one bring up another\ndiscussion is here\nokay\nso the other way of doing this is that\nokay we still pass with override but we\ncan Target for\nwe can Target for some tasks with\nsomething like filter for example we\ntarget for test a and input is like this\nand then if if we match if the tasks\nmatch the pattern we'll execute the\noverride I think the idea makes sense\nbut I'm not sure how to implement this\nidea on UI to have a better experience\nyeah I think that this difficult part of\nthe whole thing is the UI part I'm\nwondering if you guys has any suggestion\non this\nyeah\nI had a question on that it do you mean\nby UI the front-end UI flight console or\ndo yeah\nI think for a fly remote and fly CTO is\nfine but I'm not I just can't figure out\na way to better do this on the front end\num yeah the main concern that I had when\nI read this is what you\num\num sorry I forgot your name I need to\nlook up your name\nno not yours\num\nwhat you raised that when we have like\nmap tasks in there or Dynamics then the\nname of the note will be very\ncomplicated and nested right like this\ncan be like\nI I can't even say it yeah but it will\nbe a very complicated string right\num and when I read that I had the idea\nthat maybe when registering the task we\ncan mark the task for overrideable and\ngive it a name there and then in the UI\nwhen you launch the workflow we will\njust have a list of details that are\nmarked as override so not every task by\ndefault\nthat's a pretty good idea but what if\nthe task reside in your very deep layer\nlike\nin your workflow of a workflow\nor something\nI mean in principle I haven't like fully\nbut\num like thought through the the how it\nwill be implemented but even if it\nhappens very deep inside and it's\nregistered and dynamic sub workflow at\none point the Dynamics happens I'm\nregistering a task now that is Marcus\noverride with the name Foo actually from\nthe console when we started this I was\ntold that whatever is called Foo should\nrun with two gpus\nI hit that case now this needs two gpus\nthen we would have I think yeah\nthat's a really good idea so you mean on\nflight Council so on each task we can\nspecify whether this is overwriteable\nand this is a test level you give it the\nname also\nyeah I'll still give it a name and I'll\nfly UI will show it up oh that's nice\nso easy to avoid the nesting problem\nyeah yeah\nthat's pretty good and for I think for\nfor that taste\nwe'll need to sell for the\nflight remote we also just specify the\nname the override test name and then\npass it so it will it won't be nested\nanymore\nokay yeah I want to hear others so yeah\nokay okay please\nyeah it was actually Eduardo yeah I just\nraised for him yeah sorry um how sorry I\nI I just want I have a question about\nthis this suggestion that five you gave\nlike how how does this work with\nDynamic like this idea of naming the\noverrides\nso let's say we have a workflow that has\na dynamic sub workflow that registers\nsome tasks in the Dynamics sub workflow\nat that point when like I have my my\npython file when I register it I know\nthat in the dynamic sub workflow I know\nthat I'm executing a task or running a\ntask at that point that at the later\npoint I want to be able to override\nso in the dynamic type workflow where I\ncall a task I I don't know the syntax\ninstead\num I could it could be dot task dot with\nruntime overwrite and then give it a\nname\nand then in the UI\nbasically when this is registered we\nwould have to show in the list all the\nruntime overrides with the name that at\none point might be used and then I can\nspecify them there and then whenever we\nregister something dynamically we\nrealize okay the user when starting the\nexecution they specified that the\nruntime override Foo I'm I'm registering\na task now let's run with runtime\noverride full I need to use the gpus\nthat whatever user specified\nbasically I mean it's not fully thought\nthrough but that would allow us to avoid\nlike this this very complicated nesting\nproblem that we might not know before\nyeah so essentially you name the\noverrides right and you pass that when\nyou kick off the execution and you use\nthat in in your like anywhere like in\nyour workflow sub workflows Dynamics we\ncouldn't reuse them like yeah even reuse\nthem exactly so is that like a separate\nstored entity in the system sorry I\ndon't get that part how do you name\nlike when do you name the our rights at\nregistration and when do you use them\neducation\nI mean that wasn't it yeah 20 minutes\nago I did not think through how it would\nbe implemented it's basically right now\nhow I would want it to be in Python and\nthen console\nbut basically you would assign the name\nof the overwrite at registration time\nand then in the console or when you\ncreate the execution specular file you\ncould basically get the list of existing\nruntime override and then you can choose\nto fill them or not fill them\nso if I had to paraphrase this are you\nsaying that basically create override\nhooks so when you define the task or a\nworkflow itself you kind of say I am\noverrideable you can override me by\nX right and I I actually think that is\nthe cleanest interface but should that\nwhat about like\nshould everything automatically have\nnames then by default but if that is the\ncase then\nno he's\nso one of the reasons why we haven't\nimplemented Byron I told you this right\nthis is just the hardest problem in ux\nit's like it is just no control yeah\num\nespecially for large workflows it\nbecomes like almost unmanageable\num but I think we probably should just\nchange I like the idea of naming it's\ncleaner yeah like this idea of relying\non node IDs was always the thing that\nlike really it didn't seem clean you\nknow but I think that should be the\ndefault that's available I'm just saying\nthat it should be a combination so if\nyou name hooks if you have named hooks\nin your workflow\njust override those hooks if there are\nno hooks then yeah then you have to know\nthe names of the nodes and so on and\nprobably nobody will use them which is\nokay right\nI mean foreign\nright but when it comes to Dynamic it's\nprobably impossible to\nto know these things beforehand or like\nvery very I'm nice to use to use that\nyeah it's like it's impossible yeah and\nespecially now I don't know if you guys\nhave seen what Nils has been working on\nasync tasks this is\nno override that overrides at runtime\nonly really right that's what's\nhappening over there um so\nI really like these names so yeah yeah\nI think\nsorry go ahead\nyeah yeah so so I think not all the\ntasks need to be over right just some of\nlike some tasks for example TF job using\na lot of resource and we wanted to know\nthe resource we can add an overwriteable\ntag on this if we have all the node\ndefaulty can be overwritten uh we'll\ncome back to the same problem it will\nbecome the UI will will become very\ncomplicated it will be nested\nyeah and and I'm sure team is listening\nto all of this and it'll be interesting\nbut hey Greg do you want to contribute\nto here or you have another separate\nquestion I think you should just step in\nyeah yeah separate question like uh\nrelating to uh this comment\num it's more about like the ux uh when\nit comes to like actually handling the\nworkflow definition and handling the\noverrides so if you like click on that\nlink or just like scroll down or like\nscroll to the comments on this thread so\nI put a link in chat if you can see chat\noh\nokay\nlike the Zoom chat yeah nice\num so like this it shows like the\nworkflow uh definition and like those\nparameters to the workflow are actually\nlike promises so like one thing I really\nwould love is to be able to like pass\nthose promises to that like override\nconfig exactly what's shown here so\npretty much just like ensuring that that\nfunctionality Works\num\nyou mean this one yeah exactly like do\nyou see how you're like providing C1 and\nM1 like those are promises right now so\nlike I never really got it figured out\nwhere I could like actually uh resolve\nthose promises and handle over rides if\nthat makes sense so I'm not sure what\nwork it would take to get there but like\nthat is the ux that I want When\ndeveloping a workflow yeah\nyeah yeah I agree\nI actually think this the my here's my\nproblem with it they are not inputs I\ndon't think we should confuse them with\ninput I think that's one thing that\nwe've kind of gotten them gotten pretty\nnice in Flight that inputs and outputs\nare clean and separate and they are what\nbusiness logic is this is config and I\nlike let's call that config right like\nand and with Fabio's idea of named\noverrides basically it's like a hook in\nyour config and you can say like oh\nthere is something that expects the\ndistinction is pretty clear yeah config\nX and then you just like pass that in\nand and how you make it work is like\nfive right run could take a yaml flight\nflight CPL run could take a Yammer UI\ncould take so you're imagining sorry\nlike every workflow would then take a\nconfig\nactually\nyeah that's how we do it like at Freedom\nwe like we actually have the workflows\ntaken overrides parameter that defines\nthe overrides but yeah doing it on the\nexecution makes so much more sense\nyeah why not but I don't agree with Greg\nthat this is a more most intuitive way\nso we can\nuh parameterize it and then passing to\nit we don't need to\nbecause if we want to do an execution\ntime we'll need to pass another thing\nwith fly remote or personal thing on UI\nyeah and this is this kind of dirty I\nagree\nbut with the async test you should be\nable to do this let's not invent a way\nwithin flight to do this through\nunraveling promises and I don't I don't\nthink you guys have seen that probably\nbut like hopefully Niels will do a talk\nsoon and like share it in the\ncontributors uh right but I think you'll\nbe able to do this completely as is as\nthis is\nI don't think users will really like it\nbecause we've used users they're like\nyou know suddenly you are saying C1 and\nthey have to Now understand C1 is used\nsomewhere else as an input and it's kind\nof confusing yes yes yeah like you're\nthinking about like two when you think\nabout two T1 and T2 it's easy when you\nhave T1 T2 T4 distributed spark XYZ\nanother spark is because the moment you\nmake this\ndeeply nested you know yeah sub\nworkflows with Dynamics like\nit will be very polluted like yeah\nhave that config to tracks in like IDL\nsomewhere like on the execution two\nwould be really cool\num\nyeah yeah\nokay what\nlike what Dan said caching pollution is\ntrue right it will catch oh yeah you're\nnow passing random inputs and then it\nwill cache pollute which is not a good\nreason probably you don't want that so\nyeah I agree\nso so so kind of what's the best way in\nyour mind is it using the uh\noverwritable annotation or just the way\nI suggest you\nI think what you suggested is the\nmechanics of how they'll get applied but\ninstead of n0 and N1 I like the idea of\nFabio to cut short using\ngo first\num I think technically we might have a\nproblem for example that Dynamic\nknowledge plants don't know what they\nlaunched beforehand right so the hooks\nmight not show up or in our nested\nworkflow that has like a I think there's\nlike some technologies where they're not\nsure of\num by like specifying those hooks and\nthe second thing\nis for like if you have the same task\ntwice you couldn't run it with different\nresources in case that's a requirement\nthat needs to be fulfilled\num so right if you have for example your\nmap over a thing and then based on the\ninputs you want to have different well\nyou could have hooks at the workflow\ndeclaration to node level right you can\nand that's what fabulous saying probably\nand I didn't say to do it in a task\ndecorator I'm meant to do it like with\noverrides when you do it in the workflow\nso when you call the Tasker twice you're\ngonna have two hooks\nokay but you'd still have it in the task\ndecorator we would specify whether it's\na variable right that would be like a\nBoolean no I would I actually wouldn't\ndo that much decorator I would say as we\ncurrently have like no dot with override\nI would say like no dot with override\nhook or with named over right\nand with at that level because then you\ncan call a task n times in the workflow\nand have n different hooks for it\nokay okay\nworking Dynamic task art Dynamic task\npassed at registration time so that they\nwould then\nshow up and you could still already\nmaybe we couldn't pre-fill the name in\nthe UI the console or the the execution\nstack file\nbut if you know just like from looking\nat the code that there will be\na task called in a dynamic sub workflow\nwith them with the name took Foo then\nyou can put that in the UI because you\nknow that it will come and then when\nit's being registered in the sub work in\nthe dynamic sub workflow\nthen uh\nyeah then then it should be detected oh\nthe user is specified a hook with the\nname Foo now we encountered this now\nit's time to use this\nlike it would be nice if there was a way\nto make it already like shown in the UI\nthat this will come up but at least for\nme if for dynamic sub workflows and we\ncan't right now make it show up\nbeforehand but we can still like add it\nand then once it's discovered it will be\nused I think that's also like fine\ncan I I think I have a suggestion over\nhere\nin the workflow when you do with\noverwrite I let we could call it\nsomething like with Dynamic config or\nsomething right and called the configure\nC uh\nin in the dynamic case\nit's impossible right we don't pass the\ndynamic workflow and that's by Design we\ndon't want to purse we have we don't\nhave the inputs materialized so we can't\nparse it but I think what we could ask\nis the users have to take the internal\noverrides and specify it and we we can\nensure that they are available when a\ndynamic task is run\nso that because actually if you realize\nwhat happens in Dynamic is when you run\nthe dynamic workflow it\nit gets compiled after that point\nso the task definitions themselves\nchange the entire tasks even though they\nlook the same\nyou can override the config do whatever\nyou want with the config and push that\nout\nso there is no run time\noverride in the traditional sense in\nDynamic this doesn't exist because you\ncan modify anything like what Byron\nshowed is an example can be done today\nwith dynamic\num like C1 C2\nand I know people do it yeah\nyeah so but then the user should be able\nto say here are four Dynamic\nconfigurations that I need\nfor this task and that could be the\ndynamic task declaration Point probably\nit's like a workflow\nthat's totally with them in Dynamics\nright it's\nI don't know other ways like at least in\nyour config that you say like XYZ you\nneed\nyeah it's terrible but what else can I\ndo\nit could be when you call the dynamic in\nthe workflow right with config or write\nXYZ\nall right then this yes yeah my only\ncomment on this was like um\nas this scales if this is the solution I\nmean I don't think there's any good way\nto determine like what a unique idea of\nwhat this override is going to be but\nlike Byron says they massively use\nreference workflows and things like that\nlike telling the I think that that if we\njust use like a unique ID that a user\ndefines on a task\nsub workflows you know nested all kinds\nof stuff reference workflows we are\ngoing to run into tons of situations\nwhere a config override gets applied\nsomewhere where the user did not want\nis going to be specified as Foo in two\ndifferent places and also they're going\nto say why is my task being done with\nthis especially if they don't have like\naccess to the task definition\num using like a reference workflow and\nthen you're going to have like\nre-registering workflows so that they\nchange the task you know override here\nthat you're using for The Unique ID\num\ndoes anybody else foresee that being a\nproblem I guess just want to bring up\nthis conversation if this is a thing\nwe're going to go with\num I think it's it's an issue that could\nhappen\nright\nforeign make them specific by project\ndomain and workflow yes\nwhich it could default to current\nrunning project or something otherwise\nmake them overattackable too yeah if you\nhave two workflows that both have food\ndefined and then somebody has a workflow\nthat they call your both of those\nworkflows as sub workflows or reference\nworkflows then food's going to be\noverridden in both of them and not a\nsingle one right\nyeah but wouldn't defaulting to project\ndomain of current execution be fine and\njust allow that to be overridden and\nmaybe even like check for conflicts at\nruntime if that I don't know\ncheck for naming conflicts if that makes\nsense\nsure yeah checking for I mean this is\nwhat I'm saying like checking for naming\nconflicts and then like warning the user\nthat they're being applied incorrectly\nor something like that it becomes uh\nvery complicated very quick\nbasically what you're referring to\nDennis when I'm importing another\nworkflow in my workflow where I'm not\nexactly sure how it looks like and what\nit does exactly and just consuming its\noutputs\nthen I might be overwriting something in\nthat workflow\nthat I'm not aware of that's what you're\nsaying right\npotentially so are are you saying that\nfor example I register a hook and\nLanding foot oh but maybe in other\nsoftware folders also another four I\ndidn't know so okay okay that makes\nsense\ngreat\nall right I'm sure you all have more\ncomments and ideas about this proposal\nand happy to see this I encourage\neveryone to keep the conversation I\nthink also in the pr itself especially I\nthink and I suggest David can I suggest\none thing I think there is no right way\nI feel that we should launch this as an\nexperimental feature\nI don't know if it's really doable in a\nway we have to as a community become\nlike allow these kind of crazy ideas to\nbe possible Right a lot of people have\nasked for this nobody's ever come up\nwith a good solution\nuh it says that I would love to\nlove to say that you know move ahead as\nlong as you don't destabilize the system\nplease do not destabilize this people\nwon't like it so if we can somehow avoid\nthat could use this opportunity to like\ntag this this feature as experimental\nsomehow you know yeah like really use it\nlike the modules that have to be\nimportant the flight kit to use this\nI think the hooker method is the best\none I I've ever seen so far I think it's\nthe cleanest way to do it\nokay Fabio says wild idea yeah like I\ncan't press shift enter and type more\ntext so the rest is coming in a few\nminutes\num\nokay\nshould we move on to a much less\ncontentious one\nI mean this is great yeah probably the\nnext one in the agenda that was your\nproposal K10 around this is system tax\nand metadatarial initiate my screen real\nquick or do you want to share a screen\num so just to quickly go over this is\nanother thing that\nwe have heard from some users uh\nuh including methane sat I think\nLinkedIn also commented on it people\nfrom no perception stripe a few other\npeople have commented on this idea\nand so the the reason why this exists is\nuh here's uh I tried to write a few\nreasons like what people think\num one of them is today if you go to the\nflight UI you see a bunch of executions\nyou can't group them you can't\nuh and and most people are running\nexperiments and they sometimes want to\ngroup those experiments uh there's no\neasy way let's say you have you have a\nsystem that is built on top of flight\nwhich some other people are building\nlike latch Etc and you know even\num you may want to add your own uh flag\nand and group things so that's one of\nthe reasons second is\nyou this is something that actually\npeople have asked in the past is is\num\nlike releasing releases in GitHub uh\nthat means you create a\num you you mark a specific getcha\nas a release right and so it it's\nnothing but a tag on the release and and\nwhen you add that it actually you can\nnow depend on it existing and you can\nquery for it you can do things with it\nso that's that's interesting with models\nright if you if your pipeline generates\na model you may mark this model as\nproduction ready or Deployable or\nwhatever right or release 1.0 and 1.1\nand you can do some work and it can\nprobably scan through the list another\none is uh one problem today is like you\nknow uh if you take a workflow and it\nlaunches a bunch of launch plans and\npeople who are doing this at scale will\nknow like for example black shark and\nthinset specifically\nthey launch about 20 or 50 000 workflows\nfrom a single workflow more 100 000\nworkflows I think it's impossible to\ntrack at some level which workflow\nlaunch which other workflows even though\nthere is like a chaining and dependency\nit's not easy so with tags you could\nactually group all of these executions\ntogether\num\nuh and as I said external systems could\ngroup and then users could name their\nexecutions without having to worry about\nthe naming limits this is like theorem\nand a few other people ask for is like\nhey I want to call my experiment\nBernard's favorite experiment one right\nand I don't have to think about that it\nis URL\nsafe etc etc you just want to give a\nname\num and then finally you know make it\neasy to simplify you know filtering so\nuh so that was the idea uh that's how I\nthought about it and the interesting\nthing that I found is that most of our\nuh protobuffs today already support\nlabels and annotations\nand labels are essentially this so I was\nlike what if we could just make them key\nvalue Pairs and use labels uh to store\nthe value uh the CLI interface in that\ncase would look something like this when\nyou do pipeline run you would pass the\nlabels uh at large time if you're slight\ndetailed you do the same\nuh you could also get the execution and\nfilter it by labels uh so you can\nretrieve and the UI interface is even\nmore interesting you could actually put\nsomething like label again I'm not a ux\ndesigner this is just a proposal and\nidea uh and I think damn or somebody\nelse could do a better job than me uh\nthe idea would be that you would have\nlabels and you would have a freeform\nsearch field instead of the current\nmultiple filters and you should be able\nto write the key value maybe the\nexecution ID the workflow name and it\njust starts filtering them grouping them\nlike that uh while I was doing this I\nrealized this is this is another thing\nsome people have asked for us is that\nevery execution that I create I want to\nadd a label I want to add some\ndescription to it like this is like\nthink about this as model card or you\nknow things like that that I may want to\nadd\num just the description part of it\nbecause flight decks has the remaining\npart so this is where\nthis idea came in where we should be\nallowed to add description\nand the same thing you could be allowed\nto do pipelite run remote x i z and you\ncan optionally add a description to it\num\nyou could also pass in a readme.md and\nboom that gets uploaded to that\nexecution\num and so now your UI shows that it's\nhere you can read the description you\ncan edit the description you can see it\nand it gets preserved with your history\nso now that we are with with the dark\nHub stuff which is coming soon which\nalready exists right in the code but\num Kevin's working on adding it to the\nUI you get a GitHub link with that now\nwe actually get descriptions so it\nbecomes like your one place where you go\nto to\nunravel what's happening right and if\nthey are model so that was the idea uh\ndrawbacks uh they're actually\ntwo big drawbacks one is this is an\noverhead like this is extra storage uh\nbut when I did some basic math if you\nlimit the total number of labels we can\nlimit it to like a couple gigabytes in\ndatabase so it's not a big deal\nuh the real big problem is Byron good\nthat you're here I think I would have\nloved Ankit to be here too uh my sequel\ndoesn't support an easy way of doing key\nvalue pairs while postgres supports it\nright you can use that store or I forget\nthe extension name\num\nso we will have to model this as a\nseparate table\nand and if you look at it what we have\ndone specifically in Flight execution\nstable is have avoided doing any kind of\njoins this has been done\nlike uh what do you say purposely and\nthe reason why we did it is because even\nthough we use a dbms table we didn't\nwant to use it like an rdbms table no\nqueries no joins so that the the rights\nare fast and the reads are fast index\nbids are fast\nEtc\nuh uh this might impact that a little\nbit even though it's only one only at\nthe top level execution which doesn't\nget that many rights\non a on an already created education but\nthis is just a problem we could do it\ndifferently for MySQL use two tables for\npostgres use the same table I don't know\nwhat the right way to do is I'm not an\nexpert go on especially government but\nthat's one one of the reasons why we\nhaven't already implemented it because\nuh the entire code already exists\nwe store the neighbors as well but\ninside the closure we have to export\nthem out into the key value pairs\nand just add a simple Pi flight run and\nboom you should all be should be there\nand then UI work\nthat's the\nRFC pretty short\nany words anything people don't like\nawesome any comment\nobservation you know this proposal\nI hope you were paying attention to\nkitten instead of watching have you\nimplement an RFC in the chat I I really\nhope so\nyou stole the show\nI was listening though yeah\noh wow this is an entire RFC here\nright a couple of questions I wonder oh\nsorry\nyou first\noh okay so take this with a grain of\nsalt because I'm pretty ignorant still\nabout how this whole thing works but\num\nI think one thing I'm wondering I mean\nto me that seems like a good idea I'm\nwondering about kind of like default\ntags or like one thing I noticed already\nin the UI is that it\nit doesn't automatically group things by\nworkflow or or task name and I unless\nmaybe a misunderstanding but I'm just\nwondering about like those things being\nsort of default tags\nuh it does group things by workflow name\nautomatically right and for every single\ntask execution it groups that's\nthe automatic default grouping it\ndoesn't Group by version automatically\nand version should be the default tags\nyou're right a version is like a default\ntag the launch plan name could be the\ndefault tag\nuh even though we have a separate page\nfor launch plans itself that groups buy\nlaunch plans already\nis an implementation in the UI but like\nwhen you go to the UI right now it's not\nvery prominent at all like with you have\nto look pretty hard to see which\nexecution was what which was which\nworkflow like so maybe I'm just\nmisunderstanding the UI\nI think it's not hard to see which\nexecution was which works so if you go\nto the project page yes but if you go to\nthe workflow page then it's group but\nyou're right and the project page it's\nnot like clearly obvious that and that's\na great feedback and I think we could\nadd all of these as default tags but\nthat limits the total number of tags and\nadds extra\ncraft we don't need to add these as\nStacks because that data already exists\nin the system so but you are right like\nthat's the idea of thinking\nbut like I guess what I would say is\nmake sure that this proposal isn't like\nredundant to the fact that I already\nnamed my workflow and named my tasks\nthat's all maybe I misunderstanding but\nI wouldn't want to have to like tag\nsomething that I already gave a name to\nyeah you wouldn't have to that would\nexist\nthe other thing I was thinking about\nwith the description part is to me\nit's kind of like I mean maybe I'm wrong\nabout this but it seems like when I you\nknow\nI guess you know register right like\nit's a little bit like making a commit\nin a way and so you know standard with a\nright you have to have a message and I\nwonder if instead of treating this\nproposal as like tags and descriptions\nis the thing it's like separate that out\nand instead there is a separate proposal\nlike there should just be a required\nmessage every time you you you do this\nthing you know you do a run like\nsometimes it's more like\nyou know you get the the description is\nis that\nthat's a great point it's just that we\ncan't make it required because there are\nmany systems automatically launching\nthings and I don't want to force them to\nadd a description uh this is like in the\ncase where people are actually using ad\nhoc behavior of executing but fabios\num I think\nthis not only supposed to be happening\nat registration time right the use case\nthat I mostly see is when people are\nrunning a bunch of experiments and they\nrealize hey that's a good one here let's\nmark that one so that we find it again\nlet's write what we did here so that we\ncan remember what it was because we're\nrunning so much things in parallel\nthat's what our email Engineers they do\ncurrently want to be so I wrote that\nalso in the IRC on every slide run we\nuse the deck to just put the link to 1tb\nand then we do all the grouping and\ntagging and describing there\npost like post execution not that\nregistration time\nto just summarize basically whether this\nrun was good or not and I think that\nwould be great to do that inside another\n1tb\num but I wanted to I wanted to say that\nin response to you Tim because what you\nmentioned is all at registration time\nright but I think the labels are like of\ncourse the workflow name is\nnon-registration time and some labels\nmaybe you can give it registration time\nbut I think the compelling feature is\nbeing able to do it after after\nexecution to find stuff again yeah\nyeah like there are people who are\nactually maintaining I met some people\nwho are maintaining log books of\nexecution ID is\nto like you know uh uh do whatever their\ninternal idea was and I'm like oh this\nis like completely\ninverted it's not what we want to\nachieve and that's what motivated this\nfeature I think\ncan you say that one more time what did\nyou not want to achieve\nso so there were people I I met some\nthere was one biotech company I can't\nname the company at the moment uh I was\ntalking to them and they're like oh we\nhave this\num\nthis piece of software\nthat maintains a link to a launch\nwith a human readable name\nfor that launch and and that was they\nmaintain it truth in in a like in some\nUI of some sort another theme that I met\nthey were actually using spreadsheets\nlike literally like Google Sheets so\nevery time they launched something they\nwould take a link put it in the\nspreadsheet and right next to it some\ndescription and some tag and I'm like\nthis is like just bookkeeping metadata\nthat you're doing and that's what we\nwant to start and Greg what do you think\nI don't know do you guys have you seen\nthis\nyeah well like we're very like notebook\ndriven in that like important readouts\noften like call flight workflows in a\nnotebook and then that notebook is saved\nsomewhere in version tracking and that's\nlike used for a report uh there's also\ntools that like wrap the notebooks like\nI don't know if you've seen quarto or\nlike Jupiter book but there's ways to\nmake like formal reports with a Jupiter\nnotebook and I'm like trying to think of\nlike how that could be relevant for like\nthe execution tagging like you showed\nthat it would like support markdown so\nlike would those reports almost make\nsense almost like tied to the execution\nor am I trying to combine two things\nlike it's probably not like we always\nhave like the execution details like in\nthe notebook cell if that makes sense\num yeah which will also like refer to\nthat as our like source of Truth\nbut again you've built things out of it\nright just to support like one simple\nuse case of Discovery and descriptions\nand tags uh did that your the notebooks\nare description but the tag would be\nwhen you launch something from your\nnotebook you might want to give your\nexperiment that you were running at that\npoint a name right yep we do like model\ngrouping we like typically use like\nweights and biases for that to like\ngroup a bunch of bottles and do\ncomparisons on them but the tags would\nbe like perfect for that use case of\nlike establishing the hierarchy of\nmodels um yeah and another tag could be\nan external system ID right it could be\nanother tag so uh\nbut\nanything else Byron you were trying to\nsay something I think\nforeign\nsorry yeah so yeah I have a second\nquestion is Fabio are you still allowed\nto multiply the label after the\nexecution is executed\nNo Labels cannot be modified after\nexecution in my opinion or maybe you can\nI think we should allow it probably for\nlike adding new labels just change\nrelease\nmore useful\nyeah like I envisioned doing things like\nin Flight remote\nuh which yesterday I saw a great\nproposal from E internal like he was\njust talking to me yesterday retrieve\nright and he was just showing me you can\nretrieve like let's say a model or an\nartifact from a previous execution\nif you could put just the label of a\nrelease version like a semware say one\ndot x dot X I don't care match the most\nrecent and pull that down it would like\nyou could start doing model deployments\nvery very easily right like automating\nthe full model deployment uh so that's\nmy like so maybe for that we've probably\nwant\nto allow editing the labels after the\nfact\ncould be a could be a V2 like you know\ndoesn't need to be in the MVP but\nprobably\nokay okay\num cool probably the last thing to cover\nhere\nwe don't have much time right now\num unless there's anything you'd like to\ncomment or about the array notes\nproposal\nuh if not next thing was real quick\ntaking a look at the incubator probably\nwe don't have time to introduce some of\nthese ideas thank you Fabio yeah keep\nthe conversation going in the pr itself\nuh yeah the RFC incubator is probably\nyou already saw the changes to the RFC\nprocess this is kind of stage zero where\nyou can put here ideas\nbefore way before writing an RFC just to\ngosh interns from the community to see\nif that would be a good idea and from\nhere you can convert this to to the\nintegral and continue the process so uh\nthere are several ideas here uh yeah\nmost of them have been authored by Nails\nI don't know if Neil steer here and you\nwanted to come in something about this\nyeah I I made a bunch of\nhalf-baked proposals\num\nI'll I'll develop them as I go basically\nthe next one that I want to put in\neveryone's radar is eager mode\num yeah I'll I'll prepare that for the\nnext time\num and we can discuss\nsure thank you yeah encourage everyone\nto take a look at this section of the\ndiscussions at your comments uh it's a\ngood place also to catch ideas early on\nin the process\num so that's that's grade one of the\nideas this one comes for me came from me\nwas establishing special interest groups\nuh probably it's something more Indie\num housekeeping side of things uh but\nkind of the idea and inspiration uh to\npropose more kind of the community\ninside the community some more focused\ncommunities that will help us keep\nUnlocking The Innovation and leverage\nexpertise from the community here's the\nidea take a look at your comments\nbefore this turns into our RFC\nall right I think we don't have much\ntime this because this is one question\nso how can I how can we add more to the\nRSC incubator just an issue or is this\nin discussions\njust a new discussion right\nyeah just any discussion there's no\nformat design for this just yeah okay\nboth here early ideas and start\ncollecting interest from the community\nyeah I would I would love to add I have\na five or six of them sitting somewhere\nso I will add them some of them are tiny\neasy some of them are bigger so\nawesome\nall right\nokay anything else that you would like\nto quickly\ndiscuss here\nI have one more thing\nso if\nso flight kit is going through\na lot of changes all the time right I\nwould love for uh so couple things that\nI would love over there is help in\nuh reviewing PRS from the team\num and second is\nhave been contributing like some low\nlevel so these are not really features\nbut like enhancements in the system and\nthose could go through an RFC processor\ncould just be working groups I don't\nknow one of them is like asynchronous\ndata uh completely async IO\nsupport for all of the data portions\nlike type Transformers and so on like I\ndon't know if you've seen but recently\nI added support for streaming\nI don't know if it's released but\nstreaming files and directories\num\nlike dynamically also he completely\nchanged the base\ndata set data layer to be to use FS spec\nand\narrow in some cases so but I think that\nthat work is\nis never going to be done right it's a\nlot of work like so we would love\nhelping like\nanalyzing performance and improving\nthose areas uh one of the like we like\nDan did a performance Deep dive and he\nfound\nthe most of the propeller parts are\npretty fast but flight kit itself is\ntaking of eight seconds to print hello\nworld from start to end and some I like\nwe don't know why and so we would love\nto help and analyze specifically people\nwho are great at Python and I know this\ncommunity is much better than me and\npython so like they can help improve\nmany stupid things that we've done like\nit like one person today was asking\nabout why is flight kit I have so many\ndependencies I want to cut down those\ndependencies please help so\nso those are the like I'm basically a\nplea for help is what I'm saying\nabsolutely yeah I forgot to mention that\nGreg has been nominated as contributor\nto flight kit thank you greth Greg for\nstepping up\nto help and uh yeah we'll we'll keep\nrefining the the contributorial header\num and yeah feel free to step up and\nand help Building light\nawesome\num yeah we run out of time thank you all\nfor attending for your contributions\nthis was a fantastic session and uh yeah\nhope to see you in the next one\nyeah\nthat's all hey I have a"
    },
    {
        "title": "Flyte Roadmap Update: Flytekit v1.5.0",
        "transcript": "right so it's that time of the month\nagain for world map all the updates from\nEduardo\ngreat so\num\npacked month we had you know in the last\nmonth um we are shipping\nflight one five today and I just want to\nhighlight some of the the features that\nare going out\nwe are finally supporting python 311\nofficially so you know for all the\npython officials out there 311 is is is\nis in\nwe also revamped the beta persistence\nlayer so now instead of our\num\nad hocode we rely on FS pack the file\nsystems back that\num essentially exists to provide a\nfamiliar API that will work for whatever\nstorage backend you you have S3 SSH FTP\nyou know gcp\num\nyou you as a user you don't have to do\nanything to benefit from this change\nand um\ncorrect Fabio you no longer have to do\nthat\nthat is wonderful yes I know right\nthat makes the image like 800 megabytes\nsmaller yeah\nand so much faster to build also yeah\nalso Fabio there is streaming directory\nand streaming support so yeah check it\nout and let us know what you think okay\nyou're stealing my thunder man I was\ngoing to talk about that but great it's\na great Segway uh we are in this release\nwe're shipping you a few um experimental\nfeatures one being this thing that\nCaitlyn just mentioned like we're\nshipping support for streaming files and\ndirectories or flight files and flight\ndirectories using FS pack\num yeah let us know how how it feels\nlike it\nwe're pretty pumped about this feature\nanother yeah uh experimental features\nthat are shipping with one five is\nsupport for partial tasks\num wait for the details in the docs but\nessentially you'll be able to partially\nspecialize a task with you know\nbasically freeze inputs to tasks and use\nthem\num in Downstream tasks sub workflows and\neverything\nawesome I I think this is I it is a\npiece of feedback that we we heard the\ncommunity you know asking about for the\nlongest time and it's finally in\num but and we we also like in one five\nwe spent a lot of time laying the\nfoundation for better perfect insights\nyou know like again listening to the\ncommunity like people are always curious\nto understand like what is flight doing\nin terms of like scheduling decisions so\nwe are\nwe're we did a lot of work to start to\nexpose this in the UI it's not going to\nbe fully out in 105 yet but\num\nwe we expect that in the next release\nwhen sex and Beyond\num you'll be able to actually see what\nis the\num what the flight back end is doing in\nterms of like when your container is\nbeing started like when when it's like\nwriting outputs and all that so again\nanother\num\nthis is\na question that was asked over and over\nyou know in in by the community we we\nlisten to it we're working on it and I\nthink this unlocks a lot of the perf\nwork that we're gonna be\num investing in in the near future so\nagain\num not a lot to show in one five but we\nwere hard at work making sure that um\nthe foundations are there for this too\nin to happen in the future\nspeaking of the future in one six and\nthen beyond\num one feature that uh we we are also\ninvesting in is what we're calling\nexternal backend plugins so\num you'll be able to write back-end\nplugins in Python and the life cycle of\nthe plugin will all be managed by flight\nso instead of you know writing Pi or\nplugins in go which was the the\nthe way up to now\num you'll be able to iterate and write\nyour your plugins using a more familiar\nlanguage python with a with a Dev\nexperience that matches\num what you expect meaning the dev X for\nthe these external backend plugins will\nbe fantastic because you'll be able to\nrun the them locally and expect that\nthey run you know properly when you\nfinally deploy them alongside propeller\nuh another feature that's coming in one\nsex is\num support for other databases you know\num we've been hard at work you know\ntrying to support MySQL\nbut this is just the entry point like if\nyou have your favorite database and you\nand you you want to make sure that you\nhave that option when you deploy flights\nlet us know contributions are welcome\nbut again this is more about like lady\nfoundations so that other databases are\nalso supported including you know hint\nhint sqlite\nbut yeah\nsomeone asked about the\nwait let me see the chat here Mona repo\num\nthat\nI\nthis is also coming Byron but it's way\nless\num\nexciting but also coming like this is a\nthing that we we want to heavily invest\nin one sex so expect\num solid news when it comes to the model\nreport in next you know a couple of\nweeks\nthank you\nawesome yeah I guess this is this is it\nthere's a lot happening and you know\njust by if you if you hang around like\nin the USS slack or the indicator of\ndiscussions like you see that\nthe uptaking the contributions from the\ncommunity this is just like it it's\ngrowing exponentially it's it's amazing\nso\nthank you thank you so much\nfrom the bottom of the of our hearts you\nknow the the flight maintainers\nawesome\nthank you Eduardo any other question\ncomment about road map\nyeah okay then please\num so I I wanted to add like I think\npeople are contributing and I know some\npeople have pinged and asked for hey\nwhat's happening with my code review we\nare trying to keep up uh it's a little\num to work with us take you know come to\nthe channel and think in the channel\nprobably we should have a there's a\ncontribute channel right uh David we\nshould probably\nyou know flag the pr over there\num we would love to add more approvers\nto PR so that they can help us take some\nload off that would be really really\nhelpful\num yeah and uh for me personally I think\none thing that I'm most excited about is\nthe entire data persistence layer\nmigration\nI think that not only reduces the size\nof the images it makes it possible to do\nfantastic things with the persistence\nlike you know completely\nstreaming is one thing I mentioned but\nalso uh reducing like you know using\nasync Etc which is we will be coming\nsoon so we need help in making all of\nflight kit I think uh anybody who's\ninterested in contributing let us know\nthis might this will help a lot in in\nton of different use cases so\nEthan is stealing all sorts of Thunder\nthis uh meeting\nyeah look out look out for the eager\nmode RFC I'll I'll um\nthink this in the channels and stuff so\nlook out for those\nall right yeah thank you all for sharing\nthat's that's great"
    },
    {
        "title": "Flyte Community Updates 033 featuring PyData and PyCon Talks",
        "transcript": "right let's get started welcome everyone\nto the flat Community sink today is\nApril 4th\nand Davis I'm an open source developer\nAdvocate at Union Ai and happy to be\nyour host today\nuh a couple of important reminders first\nof all\num this meeting has been recorded\nand the recording will be posted to the\nflags YouTube channel\nand second thing is the slide is an AI\nand data Foundation project\nso this meeting and mostly all the\ncommunication channels from this project\nfall into the record of conduct\nso feel safe\num everyone will be nice to each other\nall right\num yeah I'm I'm posting link to the\nnotes in the agenda in the chat I will\npost it an additional time for folks who\nare joining feel free to add yourself to\nattend this list there\nand\num and the next thing probably not\nexactly in the agenda but I would like\nto do right away and it's still welcome\nnew faces so if this is your first time\nattending this meeting probably for some\nof you it is\num we like to briefly into yourself\na bit about yourself I don't know I see\nsome new names here so do like to start\nso I did make an announcement there's a\ncouple uh renowned folks on board I\nthink I see like five just a heads up\nokay that's great welcome everyone from\nthe freedom\noh thank you Greg anyone else\nlike to unmute\nif not it's totally cool\nall right so next thing\nwell some updates\num I think next slide please\num yeah this is one of my favorite parts\nof the month uh where we get to\nrecognize uh the the humans who make\nflight the flight Community great\nuh the contributor of the month\ninitiative aims to provide some\num recognition in the form of Swag and\npublic shout out uh to community members\nhelping and different ways and this\nmonth we start with Martin Beyond\nhe's being helping answering questions\nto other users on his lag which is\nawesome for many reasons this is a good\nexample that there are many ways to\ncontribute to an open source project in\ngeneral and to flight in particular not\nonly maybe pushing code or documentation\nEtc but just\nhelping each other helping other users\nthat that's the whole idea of a\ncommunity\nand we really appreciate marketing for\ndoing this um\nnext up Greg edash who was also\nrepresented today uh he's been working\non on some Titanic integration probably\nyou will be able to share a bit about\nthis and what it means to users\nand also finally no other than Mr\nByron's who you've been very active in\nthe contributor community\num discussing and implementing the\nconfig override proposal and some really\nimportant backend\num work so thank you Byron and thanks in\ngeneral to all the top contributors from\nthis month you all are great\nand we hope you you and your your swag\nall right next thing uh we'll have\nseveral events this month starting with\nuh our presence are python\num and graph will be presenting a poster\non Union ml at pycon so I don't know if\num\nyou you wanted to briefly describe\nwhat's the goal of the poster and\nwhat attendance will learn here\nI think I'll show you yeah yeah hey uh\nyeah so super excited to be presenting\nat pycon us uh this year\num so we'll be having a poster\npresentation so poster presentations are\na bit different than the regular talks\nso\num the main event dates will be from\n21st to 23rd of uh this month and 21st\nand 22nd will be the regular talks and\non 23rd uh with the career fair there is\na separate uh presentation where uh\nwe'll be presenting the poster live the\nposter will also be live for the rest of\nthe days as well uh but we'll be\npresenting specifically and uh people\ncan come up at any skin command discuss\nwith us regarding the presentation so uh\nwhat we want to primarily showcase for\nwith this presentation is uh the use\ncases for Union ml as a tool that helps\nyou to not only make it easy for\ndeveloping mic services with the help of\num like you know uh for machine learning\nMicrosoft this but also at the same time\nuh how easy it is to actually integrate\nthis and deploy with various Solutions\nlike Bento ml or for that matter like\ndeploying it with S3 buckets but also\nproviding an entire ecosystem of machine\nlearning tools that can be integrated so\nwhether it's choosing the type of\nmachine learning model whether it's like\na tensorflow model or a sky sky kit\nlearning model or for that matter\nusing any of the different type of\ndatabases so super excited to be\npresenting uh at pycon so with me it's\ngaurav uh I'm not sure if he's there on\nthis call but he's actually a high\nschool student so he's super talented uh\nthis 17 years of age and already like\nyou know very involved with machine\nlearning and python so uh super excited\nto be presenting uh at pycon and uh I'm\nnot sure if it is there in the other\npresentation but I will also be\npresenting a tutorial on Union ml itself\nat Pi data Seattle so that immediately\nfollows pycon that will be on 26th of uh\nuh this month and that would be at the\nRedmond uh Microsoft Office so they have\na pi data Seattle conference taking\nplace so it's uh our long tutorial on uh\non Union ml\nwell thank you Siri that's great yeah I\ndon't think we have it uh just here so\nfeel free to share it in the events\nChannel Maybe\nuh yeah for if you're not familiar Union\nMail is an open source project\nmaintained by some of the flight\nmaintainers and it's developed on top of\nflight\nso it's it's relevant for this community\nand yeah thanks to Shiba in Gora for\nspring the word awesome\nthanks next up\num yeah now that you mentioned Pi data\nit's basically a it's a flight takeover\nat pay data uh there will be\num apart from the union ml tutorial\nthere will be a couple of flight\ndogs from poor flight maintainers\num standing with a talk from Eduardo I\ndon't know if Eduardo you wanted to give\na brief overview of what this talk is\nall about this tutorial I think\nof course yeah as you said uh this is\ngoing to be a tutorial I plan to cover\na large portion of the flight API\nyou know with a real example\num ctbd well I'm thinking you know\nsomething along the lines of like\nshowing how to maybe do fine tuning of\nan llm\nshow up there if you're interested if\nyou're curious\nawesome thank you yeah if you plan to\nattend just let us know\nmost of us will be we we're planning to\nbe there so we'll be great to connect\nthat's great yeah there are so many\nthings happening this month that are\nthey are already in the um applied\npublic calendar\num if you are subscribed to the mailing\nlist you already have access to the\ncalendar all these events are just there\nso if you just import the calendar you\nwill have reminders at Syria for these\nevents uh links and resources uh this is\nall about the um yeah next slide please\nthe\num office hours again a reminder you can\nschedule some time with flight\nmaintenance to receive guidance and\nsupport with your questions using\ncalendly\nwe be available as Lots you can see\nthere on Wednesdays\nand uh finally I think the next session\nwill happen April 18th\nwe\num two team members from stripe\npresenting also their Journey with\nflight so again\num yeah keep your eyes on the newsletter\nto be out this week hopefully and yeah\nthank you all for attending see you in\nthe next one"
    },
    {
        "title": "Notebook First Development Experience for Flyte",
        "transcript": "we'd like to welcome great gidash I hope\nI'm pronouncing your last name correctly\nfrom Freedom\num who share with us with knowledge\naround running ml workflows from\nnotebooks so welcome\nGreg and um I don't know if before your\npresentation you wanted to briefly share\nwith us about your background and\nprobably something you enjoy doing\noutside work will be for sure yeah my\npresentation will cover that uh so if\nyou don't mind letting me drive and it's\nGreg geidish by the way you're pretty\nclose it's a hard name to uh pronounce\nit's also made up so uh you know there's\nno real pronunciation but it's fine so\nyeah just want to say hello to the\ncommunity uh my name is Greg geidish and\nyeah I've been a long time lurker of\nthis meeting an avid user of light for\nthe past couple of years but it's my\nfirst time presenting at this forum so\nI'm really excited to be here and\nappreciate the time\num and yeah in a couple minutes we'll\ntalk a little bit about providing\nworkflow interfaces with pedantic uh can\nyou see the slides right now\nall good uh great\num but before we like get into the weeds\nwith that I definitely want to like\nintroduce myself talk a little bit about\nsome of the work that we're doing at\nfreenom for context and how that uh\nrelates to flight\nso\nyeah we'll do some introductions I'll\ntell you a little bit about Freedom uh\nwe can talk about improving workflow\nDiscovery and then uh there should be\nplenty of time for questions but feel\nfree to interrupt throughout uh we can\nalways chat about a of the content being\npresented\nso just about me\num I am a senior bioinformatics research\nengineer uh my background is in\nbiomedical engineering and material\nscience uh but I've done a lot of\nmolecular biology and assay development\nboth in undergrad and uh professionally\num but I transitioned to more of the\nbioinformatics side because I really\nlike handling uh the data that's\ngenerated in the lab and also I've done\nsome full Stacks golfer engineering I'm\na flight Enthusiast I tell all my\nfriends about it and my family too you\nknow Play To The Moon\num but I'm also an aspiring contributor\nso I'd like to contribute more to this\nproject because yeah I'd love to see\nsucceed and think the team has done\nphenomenal work so far\num outside of of work you know I do like\ncoding for fun I like following biotech\nI really like farming and sustainable\nfarming uh bikes and board Sports so if\ny'all surf or snowboard or skateboard or\nanything with the board and surprise\nthem on an indoor board right now the\nbalance board that I usually take for\nmeetings but thought maybe not for this\none uh but yeah any board sport really\nlove\num great so now we can talk a little bit\nabout prenome which is a place that I've\nbeen working for for I'm coming up on my\nthree year anniversary so it feels like\nforever pretty crazy\num but yeah Freedom was founded in 2014\nwith the goal to create tools that\nEmpower everyone to prevent detect and\ntreat their disease\num so since okay progressed since 2014\nuh we've actually grown from the\noriginal Founders to a team of over 500\nand will soon hit the uh 600 Mark\num which we're really excited about so\nthis 600 is you know a large group of\npeople uh comprised of you know\ncross-collaborative individuals spanning\nuh automation clinical development\nengineering bioinformatics molecular\nbiology assay development there's just a\nton of teams working at freenom but we\nall share this common goal which is to\ndevelop Next Generation blood tests to\ndetect cancer in its earliest stages\nwhen patients are most likely to be\ncured\nso cancer is incredibly complex so\nfreenom's approach involves creating\ncancer specific blood tests to try and\nnarrow that complexity the first of\nwhich will be a colorectal cancer\nscreening product so\num colorectal cancer is actually the\nthird leading cause of cancer deaths in\nthe United States uh taking over 52 000\nlives estimated for 2023 and early\ndetection technology is improving\nrapidly and if you catch colorectal\ncancer early it actually has a 91\nsurvival rate so uh really great if you\ncatch it early for later stages that\nsurvival rate plummets\num so we really think that early\ndetection is really the key to fighting\ncolorectal cancer\nand it is recommended that everyone over\nthe age of 45 get routine screening for\ncolorectal cancer but despite the growth\nof screening methodologies that have\ncome to Market in recent years many\neligible patients are not actually\ngetting screened\num where screening rates are only you\nknow 59 in the United States and there\nare a variety of reasons for this uh but\ncurrent screening methods often involve\nthings like colonoscopies uh or\ntake-home stool tests and uh not many\npeople want colonoscopies and not many\npeople actually send back their stool\ntests after receiving it because it is\nkind of a fun process to do at home that\nuh may not be fun for everyone\num\nso yeah what's been shown though is that\nfolks that may be less open to\ncolonoscopies\num or you know the the stool tests they\nmay be more open to routine blood uh\nscreening where two would have three\npeople who either have not been screened\nor were inconsistent with screenings\nwould actually prefer a blood-based\nscreening option and that's also clear\njust by the percentage of patients\nenrolled in colorectal cancer research\nwhere the majority of patients consent\nto venipuncture or you know blood draw\ncompared to colonoscopy as it's far less\ninvasive for the patient so freenom is\nworking hard to try to get our products\nuh into the hands of millions of\npatients around the world so that we can\ndramatically change colorectal mortality\nrates and hopefully colorectal cancer\nwill be a thing of the past in the next\ncouple years\num so along with the science and\nengineering expertise needed to build\nthis product uh having key opinion\nleaders informing policy is equally\nimportant so freenom was recently at the\nWhite House just last month uh to\ncollaborate with public and private\nPartners as part of the cancer moonshot\ninitiative reignited by Biden and we're\nconsistently part of the conversation on\nimproving screening methodologies from a\npolicy standpoint\num so yeah that's just a brief you know\nin a nutshell what renome is working on\nuh but we're doing a ton of other things\nand we definitely don't have time to\ndive into everything uh but yeah I kind\nof want to focus the rest of the talk uh\ngiving you like a sneak peek into some\nof the problems that we're trying to\nsolve on the research platform team at\nfreenom where we support over a hundred\nscientists across a range of fields\num and jeev who's actually on the call\nnow was just in the chat I think I saw\nuh he gave an excellent off at a\nprevious community meeting where he\ndescribes freenom's decision to use\nflight and how we use it so definitely\ncheck out that talk on the union AI\nhomepage I think it's still there\num so yeah that's that's freenom that's\nme\num and now we can go into the the\ndreaded uh demo time uh where we're\ngoing to talk about improving uh\nworkflow Discovery so wish me luck\num but yeah at Freedom we just have a\nlot of flight projects\num where we have you know over 10 flight\nprojects and hundreds of tasks and\nworkflows used across our teams so with\nthis massive adoption of flight we want\nto make sure that all teams can easily\nsee what has already been created and\nreally encourage both task and workflow\nuse and reuse we just want to prevent\npeople from Reinventing the wheel if the\nwheel has already been invented so we\nexpose flight also to many scientists\nthat aren't trained in software\nengineering and we really want to make\nit seamless uh to add to their existing\nflow as possible and and try to prevent\nunnecessary context switching so most of\nour users are using jupyter notebooks as\nthey're like standard computation\nenvironment so I thought I'd put\ntogether you know a demo showing how you\ncan launch and retrieve workflows uh\ninteractively in notebooks\num and like I said I think there should\nbe plenty of time for questions we're at\nlike the 10 minute Mark I think the the\ndemo will probably take another 10\nminutes uh so feel free to you know ask\nquestions throughout so I'm just going\nto stop share for a sec and get over to\nthe demo\ngreat\nso sweet can you all see this this is\ncoming through\nyes\ngreat\num so yeah we're going to talk a little\nbit about providing uh workflow\ninterface uh praying for the demo Gods\nthank you Anuj\num where uh I wanted to you know get\nthis nice marriage of flight and\npedantic because if you haven't used\nFinancial before it's an amazing data\nvalidation library and I thought I could\ncombine like workflow interfaces with\npedantic to give a really nice ux\num so I did mention that at Freedom we\nhave a ton of projects a ton of\nworkflows scientists use these little\nJupiter notebooks which is this\nenvironment that I'm in so what I really\nwanted was a way for users to easily see\nworkflows and tasks from notebooks uh\nand easily run them so this demo is\ngoing to show how to access and launch\nflight workflows from a notebook uh it's\ngoing to show validating of inputs prior\nto launching the workflow both how I do\nit here and kind of comparing that with\na flight remote experience which is\nhonestly my goal to get some of this\ninto flight remote so someday maybe we\ncan you get there and then I'll show\nvalidation of custom types and some of\nthe caveats of using this system that\nwe've identified so far\num so right now I have this repository\nwhich I'm happy to share the code for\nthis repository at some point but I've\nmade this pedantic demo and it has a\nfunction uh get registered workloads and\nwhat this will do is it'll query flight\nwith a given project and domain is the\nfont size okay can you all read this\nit's all right\ncool I think it's fine um so it has\nproject and domain uh we also provide\nyou know filters which is like a flight\nkit you know filter uh concept so if you\nreally want to filter it\num there's also a way to sort it by\nsomething different if you don't but by\ndefault it just sorts by the latest\ncreated workflows uh there's also like a\nMax workflows to fetch because uh if you\nhave thousands on thousands of workflows\noftentimes you only need the latest\nversions of them so we limit that so I\ncan show you what this looks like uh it\ndoesn't take any time at all because\nflight remote is really fast but what\nthis did was make a call to flight\nremote for the project and domain that I\nhave actually in a local sandbox over\nhere\num so this is flight sandbox domain is\nuh main so it just fetched all the\nworkflows that are registered there and\nthen\nwhat I can do is select one of these and\nI can show you what's nice about Jupiter\nnotebooks is you have this tab to\ncomplete so you can easily you know just\ncycle through all of the workflows that\nare registered to this given project and\ndomain and you can easily just select\none of those uh to grab it and it'll\ntell you the current version that it\nfetched which I've done down here but um\nI've also you know created a few other\nversions of this uh compute Square\nworkflow so you can see we have another\nversion registered with an old interface\nthat I'll use in a second to kind of\nshow you what this looks like\num so on top of this workflow object\nthere's a load method which loads a\npedantic workflow model which is\nsomething that I've created for a given\nworkflow we'll get to that in a second\nbut you can specify a version which is a\nregistered workflow version and by\ndefault it uh we'll just take the legit\nlatest registered version because that's\noften kind of what folks will be using\nespecially if you have you know Dev in\nproduction domains\num so this load function will query\nflight remote grab the interface of the\nworkflow and some of the dot strings\nthat are associated on flight remote or\nI mean flight admin\num but using flight remote\num and it'll transform that interface\ninto a pedantic interface\num where you can almost use a flight\nworkflow as a python function or a\npython object so I'll show that here\nbecause it's kind of a lot to take in\nbut I have this compute Square uh\nworkflow which I had mentioned up here\num I'm loading it calling dot load and\nnow I have this pedantic workflow model\ninstance where this represents the\ncompute Square workflow that's on remote\nand so you can see I have this little\ndoc string to execute this workflow on\nflight provide the required all\narguments to execute uh which takes you\nknow all the parameters of flight remote\nexecute so if you want to have you know\nan execution name or something like that\nyou can do whatever you want\num and then\nwhat's nice about this is when it\nfetches from flight remote I can fetch\nall the parameters to the workflow and\nthey are dock strings so I have this\ncompute square and now you can almost\nlike just use this like a python\nfunction except all the work is going to\nhappen with remote so I'll show you what\nlike the validation looks like because I\nthink pedantic is where like this really\nshines because since this is a pedantic\nmodel it nodes all of the like uh\narguments that are required by this\nworkflow we can see this workflow just\ntakes value expressed as an integer\num so it errors and it tells you kind of\nexactly what's going on here like we're\nsaying field is required\num and then I can try and do something\nyou know that's obviously not an integer\nand it'll say like the value is not a\nvalid integer so it just gives like\nreally nice informative uh workflow or\nvalidation errors\num\nand uh so that's kind of how the\nvalidation works and then you know if\nyou actually provided something that's\nlegit it'll you know succeed and that's\nwhere you can call uh execute on this to\nactually execute uh the workflow but\nbefore we do that I kind of want to show\nuh kind of the same thing but on flight\nremote and this is where I think we\ncould try to improve flight remote to to\nkind of have the same level of uh like\nvalidation error messages so I'm going\nto run pretty much the same thing using\nflight remote and you can see uh if we\nprovide no inputs which here I'll show\nyou just one more time here if I do\nvalue or just this it tells you that the\nfield is required which is nice for\nValue versus down here we just get a uh\nyou know flight invalid input execution\nand you may have to like read the trace\nback to to figure out uh like expected\ninputs values missing so like we could\nwe could definitely make this a little\nuh like cleaner with pandantic like less\nof a trace back uh so it's like quickly\neasier to find exactly what's wrong with\nthe workflow so I'm just going to\ncomment this out to keep this stuff\nclean and now I can show you uh when you\nwant to execute this you can just\nexecute this right on flight\num so I do have the local sandbox over\nhere and uh everything's set up for\nrunning on flight so I can just you know\nsay execution name is it's\nit's demo time\num\nlet's see let's see\nuh okay I have an invalid input it's\nnever good here like no one sec\nuh let me actually I think the execution\nname I may not have actually figured out\nby that but\num we can see we're running this on\nflight with execute and we can always go\nover to the console here where we can\nsee you know this is queuing up it's\ngetting ready to run\num and then I have you know some\nprogress logging here where we're you\nknow waiting for the workflow to\ncomplete uh before returning the output\num and I log every 10 seconds as it's\nrunning uh but now you can see the\noutput of 4 squared is 16 and what I\nreally like about this is like now you\ncan easily just like use this value in\nyour notebook do whatever you want\nsquared again\num but it allows for like an interactive\nkind of computing where you're running\nthese on flight retrieving their outputs\nand getting them right back to the\nscientists to do their additional work\num and sometimes you may have a workflow\nthat takes some custom input so let me\njust show you this uh custom input where\nwhere a lot of people use like data\nclass Json to create you know a config\nto start their workflow and you may\nthink that maybe like challenging to get\nfrom you know flight remote and handle\nthe config properly and and how do you\ndo that all in notebooks\num but it's actually really nice\num because you know that config has a\nschema associated with it so I can show\nyou what pedantic does to try and\nvalidate that schema but if you just\npass you know an empty dick to this\nit'll tell you that this config class\nyou know is missing list events and\nweight which if we go back to here\nthat's exactly what this like schema\nlooks like right like if it has two\nFields required list events and weight\num\nso if we actually provide those\ncorrectly that's when you know the\nworkflow validation succeeds and I can\nactually execute this workflow on flight\num so we can again you know see on the\nflight console that this is working uh\nwe're you know it's view all the inputs\nand outs but the config looks pretty\nlegit flight's doing all the fancy stuff\nbehind the scenes to handle those uh\ncomplex inputs and they can get\nincreasingly complex and what's really\ncool about this is like you can launch\nworkflows\num with inputs that are specific to that\nproject outside of that project because\nyou're just using a schema so like you\ndon't need any of like the dependencies\nof that project to launch so that could\nbe really nice for task for use where\nyou want to keep your environment really\nsmall but still allow for like some\nreally cool complex inputs uh that\nworkflow all it did was like take a list\nevents and multiply them by weight so\nyou can see the output is just\nmultiplied by weight\num flight runs pretty quick so we're you\nknow running this on kubernetes uh and\nit's actually a local kind cluster so\num that's kind of what's going on here\nand then you can also so\nlet me just show you uh like we have\nthis get file length workflow also so if\nI do like uh get file length this just\nyou know takes a flight file and Returns\nthe number of lines in that play file so\nvery simple workflow\num so if I show you what get file length\nlooks like\num you know we can again see the doc\nstring input file is of you know flight\nkit types flight file\num but it also can be a path string or\npathlib\num that's something that I had to like\nadd so that like you know pedantic would\nbe happy with any kind of type that\nflight would take\num and then for workflow parameters you\ncan see we just have input file which\ncan be a remote or local file\num and then the output will just be the\nnumber of lines in the file so uh what I\ncan do here is just oh geez sorry\num too many keyboard shortcuts uh so I'm\ngonna run this with the input file as\nyou know this demo file uh because I\nthink that would be fine and uh what\nthis will do is take that local file and\nactually you know upload it to a uh like\na could be ephemeral location whatever\nyou want to do\num and it'll use that as the input so we\ncan still see uh that flight console is\nchugging away getting the file length\num which I'm going to print here when\nit's finished so it should take you know\nanother five or ten seconds and we'll\nhave it\num so yeah the file length is 44.79 so\nreally cool way to do interactive\ncompute and I'm happy to show you like\nhow this code Works uh behind the scenes\num but it does have like some\nlimitations so flight kit uh sorry chat\nis blowing up and I have not uh did it\nupload using flight script upload system\nuh it just passed a string path to\nremote and remote did the uploading so\nyes I think okay yeah that's the same\ncool\num so there are some caveats that like\nnot all types are supported at present\nso if you want to do something crazy\nlike pass a data frame into an input\nworkflow\num we'd have to like explicitly encode\num like cases for that\num and some other complex types and\nunless we start representing the types\nwith uh like\nformal python types in Flight IDL\nsomehow which uh that's probably beyond\nmy area of expertise I don't really know\nsomeone can comment on it but I can just\nshow you what happens like if you try\nand run a workflow that takes a data\nframe uh like flight actually represents\ndata frames as structured data set so uh\nwe're not actually able to like run this\nwith data frame unless we were to do\nsome fancy Transformations before\nhitting it with or before starting to\nget option play remote but I think play\nremote would have the same issue if you\ntried to submit it I'm not entirely sure\nbut this is an issue that we could work\naround can you get a question\nnot a question I think you can do the\ntransformation because structured data\nset can automatically form\nthe right data frame type I it might\nyeah so this this board might just be a\npedantic thing that needs to be changed\num yeah just basically adding some\nspecial martial around marshaller for\nfor pedantic when we could try it live\nor we could uh always yeah you can try\nlater but I maybe it's possible is year\nround I think we might not work\nyeah you have a comment on that\nI think he's sleeping yeah it might be\nmy demo put him to sleep sorry guys uh\nbut yeah does anyone have any questions\nabout this like happy to show the code\ntoo if that would be helpful but like we\nare just grabbing the interface from\npolite transforming that into a pedantic\nmodel and that pedantic model is\nultimately what's handling all of the\ninput by validations it also is the\nthing that's like putting the doc string\nhere to like grab the doc strings from\nflight remote\num so there is some work going on behind\nthe scenes but it's it's really just\nleveraging flight kits uh like amazing\nyou know capabilities as well as uh\npedantic\nwhat's what's one other interesting\ncaveat that I can show you is like if\nyou've ever used pedantic you'll know\nthat there can be some oh I don't think\nI showed that like when you load a\nseparate version uh you can like load\nany version you want but you can see\nI've like modified this compute Square\nworkflow to be an older version and like\nin this case it's saying that it wants\ninteger to be you know Square as a\nstring but if you look at like the\nlatest compute Square uh it's taking the\nvalue as an integer so and it's you know\nan integer to square uh you have value\nstring so like all this is like always\nquerying flight remote to get all like\nthe the source of Truth\num so it really is you know somewhere\nthat you can run any version but I do\nwant to show you one like technical\ndetail where like this says that the\nvalue is an integer but I can always do\nsomething crazy like this\num and pedantic One Core issue in\npedantic is it does type coercion\num but like thankfully in pedantic 2.0\nwhich is in Alpha right now they have an\nability to run it in strict which I\nthink we'd probably want to uh when\nwe're doing stuff like this\nso you can see like right now pedantic\nactually like coerced that type uh so if\nyou look at the inputs it coerced it to\nan integer before actually sending it\nback\nand I can like prove that by just doing\nuh like this dot value and we can always\njust like type this uh and it's an\ninteger\nso type coercion's happening on pedantic\num but yeah that's that's my story\num I hope you all liked it and yeah I\nwould love to spend some time you know\nanswering uh any questions that you guys\nhave I don't know if I showed chip and\nJoanna but uh you guys don't watch\npicture uppers you should\num but yeah happy to answer anything\nfrom the community I would love to get\nthis into play remote at some point um\nyeah\nwell that was great thank you Greg there\nany question I think there are some\ncomments in the chat\nuh not sure sorry not sure if you\nalready addressed the question from Nils\nif you implement it\nidentic type Transformer to get this to\nwork\num sorry could you elaborate on that\nnews\nyeah I think um actually jeeve answered\nthis so it's dynamically creating\npedantic models\num yeah exactly okay because we had\ntalked about tech Transformer some time\nago yeah so it actually it creates the\nworkflow model by like fetching uh the\ninterface from remote Loops through the\nparameters in a science defaults\num and then with those parameters uh the\none interesting thing is like it has to\nget\num\nthe python type like this is where it\nhas to guess the python type that's\nassociated with the interface and this\nis where some like interesting things\ncan happen like that structured data set\ncase uh and this is where you can see\nlike you can put you know some custom\nhandling for this where like if it's a\nflight file I want pedantic to be able\nto take a flight file or a string or a\npathlib so we may have to do like the\nsame kind of things for like if it's a\ndata frame\num but there may be some better way to\ndo this uh but you know this is a start\nand still pretty excited about kind of\nthe usability improvements this gives\npeople the notebooks\nyeah this is this improves the ux of\nflight remote like crazy so we should\ndiscuss\nhow we can what the approach there is\nfor upstreaming\num that makes sense\nyeah it should be like should we keep it\nas a separate thing maybe so that the\ncore API remains as is just\nuh for like\nthe extreme sites or other users it's\nfunny because at this point there are\nlike two levels of abstraction right\nthere's synchronous client and then\nthere's slide remote\nso we'll need to decide whether this is\nlike another interface or there is uh\nyeah we can always yeah I think getting\nthe first thing which would be the\ninterface part the async part I think is\nis less critical for now like right now\nyou can make this uh like we do what I\nhave for like the retrieving of outputs\nis just like on execute I have like this\nwait for outputs which is a bull uh\nwhich you can always just say like wait\nfor outputs is false and then it'll just\nlike execute and return right away\num and give you the actual like\nexecution back\num so we can talk about some of my\nthoughts there uh because I'm not sure\nwhat what is better you know to have\nsomething that's like a jack of all of a\ntrades and kind of have the bullying for\nwafer outputs or have the user work with\ntheir own executions but I know for our\nscientists they'd prefer like the\nsimplest case possible to be the default\nand then always allow power users to do\nwhatever\nyeah yeah because flight remote has the\nweight argument as well I feel like the\nbiggest value out of this is like\nyou know the log outputs the validation\nthe kind of immediate feedback and not\nhaving to read grpc errors as uh someone\nelse split it up above\num yeah so this is amazing work\nyeah it's amazing uh I had one thought\non that like basically the synchronous\nflight client I just want to get rid of\nit I don't think that it's I know it has\nsome other apis but it exists it's most\nvestigial right uh with the new grpc and\nput above stuff that we have I think\nthose clients themselves are pretty\nusable on their own\nbut it exists and people have embedded\nthem into their code bases today our\nproblem today is like you know some\nthings you just can't remove because\npeople have used them in code bases\num it's not a bad thing it's just extra\nmaintenance extra code\nuh but I I like this a lot because it\nsimplifies that but I'd love to like a\nfollow-up for this is like you kind of\nlike the logging right now but I think\nthat's like a week point in this and\nthat like having an ipy widget that\nwould show you something similar to the\nconsole with exactly what's currently\nhappening how many of your Dynamics\nsucceeded failed you know and everything\nlike that is my next little fun side\nproject idea\nand that's actually doable because\nflight console is splittable you can\npull out certain parts it's just that\nyou know how do you I'm doing it all\nwith play remote so I may need to sync\nwith you guys and talk about what could\nbe possible yeah authentication is\nanother interesting hard part over there\nbut\num\nyeah if you all have uh any like\nfollow-up questions feel free to slap me\non the flight slack uh we could always\ntalk about it and like I said trying to\nget this Upstream uh if you think this\nwould be useful for your uh company or\norganization let me know\nhey Greg I had one more question on that\nwhat do you think about actually\ndeveloping Sometimes some of the tasks\nwithin Jupiter and then\nrunning them directly from jupyter do\nyou think that's a valid use case do you\nguys use it absolutely\num because there are folks that uh do\nyou want us\nscale on kubernetes a simple function\nthat takes you know very structured\ninputs uh and scale that across you know\n100 uh positive notes\num so 100 I think that would really be\nenabling and that on top of like this uh\nlike logging and everything would be an\nincredible ux\nyeah that's doable as we discussed\nyesterday in the contributor channel so\nyeah the cloud pickle is gonna do some\ngreat things\nawesome that was great thank you so much\nGreg and the entire Freedom team I was\nthat was really useful for the entire\ncommunity"
    },
    {
        "title": "Flyte @ Shift",
        "transcript": "um next up we have Sam Cox staff data\nscientist from shift Technologies to\ntalk about how they have been using\nflight\nSam the floor is yours\nhey there how are you all doing\nyou don't mind if I share my screen\n.com\nyes we didn't do anything like too out\nof the ordinary for what I've been\nreading and stalking on the the flight\nstuff I still like to go over how we got\nit set up and then hopefully some ideas\nto make it easier\nuh there's one thing that I haven't seen\nin the flight uh slacks that I think\ncould help other people but we'll get to\nthat in a second\nbut yeah so\njust flight adoption at shift uh I'll go\nthrough some of our shift overview what\nwe actually are and whatnot if you\nhaven't heard of us data science at\nshift our Legacy ml architecture if you\nwill or lack thereof\num then I want to go over like our\ninfrastructure team and how they have\nall of our shift-wide infrastructure set\nup and how we were able to fit flight\ninto that and why flight was chosen uh\nsome small improvements honestly this is\nso much better I don't I feel guilty\nsaying improvements for y'all but there\nare a couple that would have made my\nlife easier and then question answer\nyeah so shift overview we are a used car\nuh dealership focused on e-commerce just\nlike carvana or vroom\nuh we want to our Founders started it in\n2013 because they went through horrible\ncar buying experiences uh one of our\nFounders the CEO grinder now other one\nis an advisor on a lot of different\ncompanies so they're uh what do you call\nit pretty pretty heavy in the Silicon\nValley space uh\njust due to the economic environment\nwe're actually starting to switch to\nmore of an omni Channel experience to\ngain in more customers so now we have\nin-store processing purely e-commerce\nand a hybrid of the two to try to make\nit easier for people\nso our company has gotten more complex\nand with that all of our data science\ntasks have gotten a lot more complex as\nwell and flight has we've just now\nstarted I think we're at two months in\nactually running it in production and it\nhas greatly simplified a lot of our\ndeployments so appreciate everything you\nall have done\nyeah data science it shifts there's two\nmain areas you can think of for us we\nhave Acquisitions that's where you go\nand get a quote online so we have offer\npricing optimization or we run a lot of\nlarge-scale optimization tests where\nmachine learning models feed in you can\nthink of you know basic elasticity\nthings things like that you also have\nmarketing market analysis so whatever\nwhat are our competitors doing need to\nCrunch all that data\nand web scraping and stuff from that uh\nwe also have a ton of forecasting things\nso you can imagine forecasting isn't the\nbest right now uh but uh\nthat is very important to see like what\nare depreciation rates for car is going\nto be and whatnot and those just need to\nbe scheduled and churning constantly\non the sales side obviously we have our\nsale price what is the best price to\ngive for a car to get to sell fast not\nkeep it in inventory there's a lot of\ndifferent things we need to take into\naccount\nalso all of our F and I products uh we\nlook at inventory anomalies quite often\nand that is another perfect flight fit\nit just needs to be constantly churning\nand seeing what our inventory is doing I\nalso have a lot of recommendations as\nyou can imagine for marketing campaigns\nuh so\ntalk a little bit about our Legacy stuff\nbut basically it was kind of a custom\nhodgepodge of things uh sagemaker was\nour main platform but we coordinated it\nwith step functions on AWS also had a\nlot of Athena tasks uh AWS batch and\nthen we had airflow for like the\nhardcore data processing thing so it was\nvery disparate some of our stuff was at\nairflow some of our stuff was in this\nhalf-baked workflow that we made up with\nstep functions\num that was coordinating sagemaker tasks\nand Athena tests\nbasically it was not easily extendable\nor scalable\nuh the complicated deployment I would\nsay uh\nml models were housed in a mono repo so\nwe could have all of our common things\nwe couldn't separate it out\nvery easily we couldn't genericize it\nvery easily uh we basically had like the\noption of using like a five-step process\nMax\nand you could use some of them or not\nbut it was very constrained\nuh it was difficult to change it was\nawful if you wanted to add something or\nadd another step or something like that\nbecause it would affect every other\nmachine learning workflow and\nbig one is because of the complicated\nnature they basically just made us our\nown account\nso we didn't really have a develop\nbetter staging environment uh I do want\nto preface this was before I got there\nwhenever all this was set up so but uh\nmy personal mission whenever I got to\nshift was to get rid of this\nbecause I remember trying to onboard it\nwas\nquite interesting but\nI want to talk about our infrastructure\nteam a little bit we made uh as we're\nlike decomposing a lot of our large\nsystems and smaller microservices\nkubernetes was an obvious choice our\ninfrastructure team has done an amazing\njob setting up the whole company on\nkubernetes\nuh is to manage all of our applications\njobs uh we use flux CD for our\ncontinuous deployment\nprocess so everything including the\nkubernetes state and the app state is\nmanaged by flux CD\nwe take advantage of just a lot of\ndifferent crds that make our life easier\nuh\nto connect we even like started using\ncross plane and things like that started\ntesting that out so we could actually in\nyour app you could Define like Aurora\ninstances S3 buckets all that good stuff\njust basic\nkubernetes yamls\nyeah but big thing is all of our cluster\nStates and add-ons are fully defined in\ngit\nthank you\nso just a little diagram I'm not a very\ncolorful person so I apologize\num but yeah so you can see we have a\nkubernetes git repository and that\ndefines all of our Helm add-ons all of\nour customization add-ons you can\nprobably see where I'm going with this\nfor flight all we had to do is just do\nanother hell mat on it was pretty easy\nto get that up and running uh the\napplication States it's the same thing I\ndon't know if anyone knows how flux\nworks but basically it monitors your\ncontainer registry for new tags and then\nit will update your git repo for the new\nstate\num what not and it will continuously\ndeploy all your apps in the the get\nstate that you want\nso why flight uh we need a scalable and\nimportantly scheduleable and reusable\nworkflow tool\nuh we looked at kubeflow I've used\nkubeflow at previous companies I've\nnever set it up so that was a eye-opener\nuh it was pretty\ncumbersome I would I would say uh it\ndidn't fit into how they wanted to keep\ntheir kubernetes state defined and\nwhatnot uh so it was actually the our\nmanager senior manager of infrastructure\nwas like have you heard a flight and I\nwas like I haven't so I started spinning\nit up locally playing with it and one of\nthe other senior data scientists who was\ninvolved with all this uh she just\nabsolutely loved it so we decided to go\nfor it\nand get a full POC up\nbut yeah the deploy there's some\nconfusing bits in there uh but for the\nmost part it was seamless besides\nbasically hunting down through y'all's\nflight or slack channels what to do and\nI really want to thank all y'all for\nthat uh\nand then yeah easy workflow deployment I\nreally like the fact that I mean some of\nit can be a little complicated sometimes\nbut we can put that all in one place and\nall the other data scientists just have\nto worry about the code itself\nso yeah we basically on our team\ndesignated two people to know exactly\nhow flight works and whatnot everyone\nelse just needs to Define code so we can\nset up that really easily and fast for\neveryone the visibility is great I don't\nyou all know the AWS console is lacking\nin a lot of things for sagemaker kind of\ntests so flight\nUI you can just click on it and go you\ncan even scale it out if we need to uh\nbut yeah main part is it really\nseparates the infrastructure parts from\nthe workflow code which I really\nappreciate\nuh yeah and more on that multiple\nworkflows and one repo that was one of\nour difficulties for us is like\nwe can define a training workflow is our\nmain thing and we can actually start\ndoing feature store ingestion in the\nsame repo for that model or models we\ncan do drift monitoring uh we can do a\nlot of different things so it takes a\nlot of our stuff from airflow it\nactually just goes ahead and puts it\ninto the flight system\num yeah data validation that is\nabsolutely key instead of just random\nscripts running everywhere and it breaks\nrandomly really like the complete audit\nTrail just out of the box something\nhappens we know exactly the lineage of\nthat\nand a big one is the local sandbox for\nme because I hate having to go to the\ncloud immediately so we made kind of\nHalf Baked for our own needs uh kind\ndeployment with flight so that users can\nget it spun up exactly everything\ninstalled that they need and whatnot and\nthey can just work as if they were doing\non the cloud\nuh yeah and also it enabled us to have a\ncompletely separate Dev stage in prod\nenvironments our infrastructure team\nfelt more comfortable with us being\ninvolved with the rest of the company so\nthat was good\nyeah and then obviously we haven't\nimplemented any of this yet the\nnotifications and even in the loops I\nknow samita just\nshowed one of those things that is very\nkey for us because some of our models\nare absolutely critical need to be\nlooked at before we actually go the full\ndeployment route\nyeah our flight deployment how we have\nit set up and our kubernetes repository\nwe just defined the helm itself flux CD\ncompletely managed all of that\nand we could just have the complete\nflight project State defined also in\nthis main kubernetes repo so if you\nwanted to add a new model or project or\nsomething like that we kind of came up\nwith a way to check these specific\ndirectories and update it if needed\num as far as the app repositories itself\nhave all the flight defined in there and\nwe deploy it through GitHub actions\nand activate and deactivate based on\nthat\num flight workflows usually just at the\nend of it we dump to S3 depending on\nwhat it is or we can upload a redshift\nas well\nand then we have a model serving\ncomponent that's also defined in a crd\nmakes it pretty easy as long as we have\nthis S3 thing there flux can actually\nrecognize to update the crd tag and go\nahead and deploy it\nafter the training is done\num one thing we did that was unique I\nhaven't seen anyone do yet\nI know on the official docs it says to\nit recommends the AWS RDS management\ninstances but we're one of our\ninfrastructure people found this\npostgres operator he's been playing with\nbefore and I would highly recommend it\nso we were actually able to deploy this\nuh to our Cloud it has replicas it has\nexternal storage we can recover from\nanything uh manages all the postgres\nsecrets and we were able to integrate\nthat with flight as well since it's not\na very noisy database\num it is a lot lot cheaper a lot easier\nto manage I think than the AWS RDS\ninstances and yeah would highly\nrecommend if y'all have questions or I\ncan post this in the chat\nif you are interested in it\num\nbut yes it was actually kind of fun we\nbasically just had like 10 workflows on\na schedule\nand I was really cranking it out every\nfour minutes they're like hello world\nit's not very process intensive and we\njust started whacking the database over\nand over again and it was very very\nresilient\nwell it yeah so how we're actually using\nflight our scheduled marketing campaigns\nhave gotten a big boost uh we used to\njust have it like on\nuh basically Cloud event watch uh\nschedules now we can just easily Define\nan encode and not have it 80 different\nplaces to find a scheduled forecasting\ntasks\nwe actually doing data processing for\nETL like our simple ones that don't\nrequire that much we also use DBT but we\nhaven't integrated those two yet\ndata drift components is great we can\nnotify things uh it's easy to add we\nhave like datadog annotations is how we\nscrape metrics for datadog and whatnot\nwe could just add that on a pod template\nwe can do notifications through that or\nthrough the native notifications through\nfight\nand a big one is as you can imagine we\nhave different stores different regions\ndifferent characteristics it is very\neasy to parameterize workflows to make\nseparate forecasting things\ndifferent regions the worst vehicle\ntypes even just out of the box very easy\nand our big one is model training\num I would say the biggest improvements\nI would say would maybe be some\nmanagement crds so instead of especially\nlike the projects itself\nthat would be very beneficial because\nthen flux could actually manage it we've\nbeen thinking about making our own kind\nof Half Baked crd using chaos where we\nwould literally just use flight control\ncommands to recognize on update on\ndelete or on ad\nuh the configuration that can be kind of\ncomplex we're using the internal auth\nright now since we're like on complete\nVPN or not vpns but it's closed off from\neveryone else uh that was a little bit\nconfusing and I had to go through some\nGitHub PRS to figure out what was going\non\nyeah and then the Ingress we use traffic\nso\nI have found some examples of people\nusing it on flight slack but I didn't\nsee it documented anywhere and I am not\na very\nyou know Ingress oriented person or\nnetworking person so it was a little\ndifficult for me to get that set up\nbut\nI was basically hunting for improvements\nbut this the actual management crd one\nwould be very very beneficial for us\nany questions\nwent through that a lot faster than I\nmeant to so I apologize I think you did\nan excellent job presenting this\ncongratulations on your success with the\nproduct can you\num help me understand a little bit about\nhow big your team is and what the level\nof effort was to actually set this up\nfrom you know from not knowing anything\nabout it to having it fully functional\nbeing able to present your work yeah uh\nso our team's pretty small we have five\ndata scientists at the moment we have\nsome software Engineers on our team too\nsince we manage all the pricing our\ninfrastructure team is small they have\nthree\npeople on there uh as far as setting it\nup and this was like me doing it with\none of the infrastructure Engineers on\nour own\nit's a very like asynchronous if you\nwill\num on and off it took us about two\nmonths to get it fully working and\nautomated so it's really not that bad I\nthink if we'd spent just a week on it\nlearning it and whatnot it would have\njust taken that out that long\ngreat thank you very much\nso besides Sam this is Martin I'm with\nUnion suicide seeing the productivity\nengagement uh improvements there on on\nyour side\num what has been the feedback you've\nshared some of the feedback from the\nmarketing folks but what has been the\ncompany feedback about the impact of you\nrunning you know flight now and then and\nactually\num do you know that you have an impact\non their work and if so you know what is\nit yeah a lot of our Junior data\nscientists are more Junior I should say\nreally appreciate it because they can\njust focus on it I mean what used to\ntake us several weeks to make a new\nmodel add it into our mono repo make\nsure everything's working what not now\ntakes I think\nher name's yushwin she's one of the\nseniors she did a project in less than\ntwo days and had it fully deployed\nwell so the feedback has been good just\nspeed to delivery\nthe testing is easy I mean the data\nvalidation is key because we can deploy\nit and then it'll fail and that's you\nknow better than what used to happen so\nas far as like the company-wide stuff it\ndoesn't you know we're we're the DS\npeople always in the corner kind of\nplaying with their own stuff so they\ndon't fully recognize it but the speed\nof delivery people have noticed and\nImprovement\nperfect thanks\nRussian yeah so sorry Matthew also had a\nquestion let Matthew go first\nyeah\nuh he's asking are you running any GPU\nworkloads\nat the moment\nso we do have like a custom pipeline\nstuff for image processing we don't know\nif it will come back or not but we will\nput it onto flight or try to if that day\ncomes\nyeah Matthew also what questions would\nit be on GPU happy to you know solicit\nfrom the community happy to help in\ngeneral\nuh but Sam mostly fantastic presentation\nand thank you for taking the time for\nsharing it's really\nI think I've told you this in the past\nit helps the community a ton like just\nknowing like oh people are deploying\nusing flux and how many how much time\ndid it take to get deployed because uh\nsome folks have asked me oh is it going\nto take a long time for us to\nI hope not yeah but it's not but you\nknow but without having practical data\nit's hard\nuh I had a couple\nquestions and one suggestion uh one is\nthe postgres operators I think that's\nfantastic I think even\nblack shark folks are using is what I\nlearned in the channel\num a Blog would really help like you\nknow for people who who want to do this\nthey probably don't know a Blog would\nreally help uh so and and we'll be happy\nto promote it and share with others\nbecause I think the cost saving that you\nwould get from that are pretty high so\nyeah yeah like Aurora versus that is\nlike almost one fourth I think or once\nwith the cost uh another bit would be\num so it's like you know uh you you\ntalked about a couple things like\nconfusing with\num in the in the infra Parts Etc I would\nlove to learn more about them please\nfeel free to share opal issues\nyou know or an RFC or any anything right\nlike where we can comment and we can\ncollaborate we would love to learn\nuh we've been trying to do a lot with uh\nwith the uh you know the Ingress setup\nis it's hard it's just every day I find\na new interest like yesterday somebody\ncame up with like I want Kong I'm like\nokay\nI don't know what song is now you have\nto go and learn and we try to improve it\nbut it's possible to keep up that's what\ncommunity really comes in if you've done\nsomething with let's say traffic\nsharing that more generally helps like\nuh the community at large uh so thank\nyou for possibly sharing this but yeah\ncontinue sharing more so that you know\npeople are able to do it faster and\nbetter\num\nand uh one thing so you were using\nsagemaker Etc and all of that was for\ntraining or was that something else you\nwere doing in sagemaker you said Athena\nsagemaker step functions airflow so\nthere's a lot of different Technologies\num uh how big are your workflows and\nwhat are you moving lots of data are you\nuh\nare they all single node or are they\ndistributed I would love to learn a\nlittle bit more about that\nyeah so most of the vast majority of our\nworkloads are very small I would say\num\ndo you have some large ones that need to\nget distributed and whatnot uh stage\nmaker was we kind of use it just as\ngeneral for processing and training and\nserving we still use trade uh sagemaker\nfor serving but we're thinking about\ntrying binto or other things like that\num hopefully it'll make it a bit easier\nand cheaper yeah uh but\nyeah so yeah we are like hardcore\ndistributed stuff but we do have a\ncouple things out there\num the biggest thing are just\ndistributed stuff we used to run\nserialized like region one region two\nRegion Three and now fight yeah that\nmakes us be able to paralyze that part\nyeah that's that's actually when we\nstarted at Lyft the number one use case\nwas train a different model per region\nnumber one use case and so some of the\nPrimitives in flight are basically built\naround those\num that was 2016. so\num that's that's great uh thank you\num I again one I am always amazed by the\npricing of you know older cars and so\ngreat to see the flight kind of powers\nthat are shipped so amazing we would\nlove to learn more but that's probably a\ndifferent topic yeah\nhopefully the economy will calm down a\nlittle bit on the used car prices so\nfingers crossed yeah I lost a lot of\nprice on my car real life games\nany other\nso I had a question for you another one\nSam so obviously it looks like you said\nyou run a lot of smaller tasks and and I\nwould love to learn a little bit more\nabout what your data scientists what\nkind of like looks like there's a lot of\nregression stuff going on on the\nmarketing side and uh can you tell are\nthey using it for fabasi and stuff as\nwell is it just classical frequenter\nstuff do you know what's going on on\ntheir site what what their main tools\nare on Main Frameworks and approaches to\nsolve there yeah it's mostly frequency\nstatus up they do we are getting into\nsome Bayesian type of workflows because\nwe're starting to do like stochastic\noptimization and fancy things like that\nuh so that helps a lot but\nby and far I would say yeah we're mostly\nstraight regression classification on\nthe frequent aside got it got it okay\ncool I mean it's really amazing to see\nthat you know you don't have to have\nreally you know complex deep learning\nmodels to use an orchestrator you know\nit benefits one you know if you have\njust like uh teams that that need to get\nstuff done and and you know deployed and\nput forward quickly and and reliably and\ncan roll back and you know it doesn't\nmatter how big the task is even a\nsmaller task you get an advantage out of\nusing an orchestrator 100 percent\nthat's awesome okay thanks\nyeah also you mentioned or models uh do\nyou what kind of libraries do you use\nbecause I think we have had a few people\nwho have been asking us hey do you don't\nhave an example of trading an or model\nright and I'm like it should just work\nit's python code but you know anything\nthat you can shed light on for people\nwould really I think appreciate this or\nmeaning operations research yeah\noperations I'm assuming you have them\nyeah we usually stick to pieoma so we\ncan make it general you know we don't\nwant to get stuck with just one yeah got\nit and so you do you do like offline\npredictions or like what kind of use\ncases with or for YouTube\nmovement\nuh yeah a lot of like our pricing\npolicies are dictated by that depending\non\ncompany goals it's a nice middleware\nbetween you know it will try to optimize\nstuff but you can add constraints and at\nthe same time if you know\nI guess like there was like a\nsituation where car prices started going\ndown and the or model would be like hey\ndon't buy any cars and we're like well\nthat's not possible so you know\nokay cool\nall right that's it"
    },
    {
        "title": "Flyte + DuckDB and Banana Integration Demos",
        "transcript": "I'll now briefly discuss two\nIntegrations that I have recently worked\non let me share my screen\nall right\ncool\num so first I'm going to show the flight\nbanana integration\nfor those who haven't heard of banana\nit's an ml inference solution that does\ninference on serverless gpus I just want\nto go through the process of\norchestrating a fine tuning Pipeline and\nenabling on-demand scalable model\nserving with flight and banana\nrespectively you can either use the same\npipeline or build your own ml pipeline\nso the first step involves fine-tuning a\nbird Pipeline with flight this pipeline\ndownloads the data set Yelp dataset from\nthe hugging phase Hub tokenizes the data\nmeaning applies the tokenizer to the L\ndata set and stores the result in a\ndirectory and Returns the same\nand then the pipeline splits the data\nset into train and evaluation subsets\nit then fine-tunes a bird model so this\ntask\nfetches a pre-trained model from the\nhugging face hub\ndefines a trainer\ntrains or fine-tunes the model on the\nYelp data set\npushes it to a hugging pushes it to the\nhugging face Hub and finally returns the\nhugging face model Shah the flight\nexecution ID and the name of the hugging\nface model repo\nand then in the end the pipeline sends\nthe model metadata to GitHub there are\ntwo reasons for doing this one is to\ntrigger the banana deployment since\nGitHub push even triggers a banana\ndeployment and the other is to ensure\ndata lineage throughout the pipeline now\nthis task stores the model metadata in a\nfile and pushes it to the uh to a GitHub\nrepo and you can reuse the stars for\nyour flight banana deployment as well\nand then you can run the pipeline\nlocally uh this workflow includes an\napproval task so here's the approval\ntask now the retrained model gets\ndeployed only if the user approves the\ndeployment\nand then to serve the model on banana\ncreate an account and Link it to the\nGitHub repo that has the model metadata\nbecause that gets uh updated whenever\nthere's a retrain model and this\ninference code\ninitializes the model by fetching it\nfrom the hugging face Hub and then\ngenerates predictions on a GPU if\nthere's one available\nand then you can test the end point by\nspinning up a banana server locally\nhere's the code for that\nand uh you have to deploy the model on\nbanana you need to prepare a Docker file\nand it can be identical to the one that\nI'm using over here\nand then to run the pipeline on the\nflight backend prepare a Docker file\nregister your tasks and workflows and\nexecute the pipeline on clicking resume\nthe deployment on banana gets triggered\nnext\nyou can trigger the deployment on so you\ncan test the deployment on Banana by uh\nhitting the end point and you know\ngenerating predictions uh and yeah\nthat's about it this video captures uh\ngenerating predictions with banana let\nme just go to the flight console\nso in here let me just refresh this I\nhave triggered the workflow already now\nthe workflow is waiting for the user\ninput so let me just resume this\nand hence the deployment commences and\nthis pushes the model metadata to GitHub\nand it also uh deploys the model on\nbanana let me just go to the uh banana\ndashboard drop before that let me just\ngo to the GitHub repo\nuh yeah I'll just open one of the older\ncomments so here's how the comment looks\nlike the Shah got updated and the\nexecution ID got updated as well\nand this is the banana dashboard so the\nmodel uh gets deployed again and you can\nhit the end point to generate\npredictions\nso yeah um that's about it let me now\nopen the duck DB blog post\nall right so many of you must have\nalready heard of uh duck baby right so\nfor those who haven't the baby is a SQL\ndbms that's designed to handle\nanalytical workloads it stands out for\nits uh speed and the capacity to handle\nsizable data sets now I'm gonna walk\nwalk you through some analytical queries\nI ran on a few gigabytes of NYC taxi\ndata all within a flight workflow so\nthat you can get the gist of how you can\nuse stuck PB within your own data and\nanalytics workflows and then the first\nstep involves fetching pick up and drop\noff hours over a span of four years from\n2019 to 2022\nso here's the task for that it Loops\nover uh two lists of years and months\nand runs Doug DB queries on uh the\nSparky data\nso here are the definitions for the WB\nqueries so here's uh this is how you\nneed to instantiate them uh give it a\nname a query or a list of queries and\nthe input that is to be sent to the\nquery in here the input is a parquet\nfile and it gets replaced over here so\nthis is to fetch the pickup R and this\nis to fetch the drop of r\nand the pickup uh query and the drop of\nquery tasks are triggered simultaneously\nin the dynamic workflow which results in\nexpedited executions\nnext the coales saw DFS task at uh\nmerges structured data sets for a group\nof years\num and then it uh\nyeah so it basically um\nuses wex data frames because wex can be\nhelpful to run queries on larger data\nsets and we have an existing plugin for\nthe flight kit wex plugin you can\ninstall and use it directly and then the\npipeline renders uh renders a graph to\nvisualize the pickups and drop-offs from\n2019 to 2022 using Flight Deck\nand then\nit retrieves the pickup Peak and of peak\nhours\num so yeah here's the dynamic workflow\nto handle that after I run the workflow\nthis is the output I've got the pickup\nor the peak and the off peak are for\nevery ear\nand then these are the Doug DB queries\nI'm calling from within the peak and of\npicar's dynamic workflow\nthis uh these Doug DB queries accept\nArrow table as inputs so I'm sending an\narrow table to this query and if you\nlook at this parameter it's the same\nover here as well so this retrieves the\npeak R and this query retrieves the off\npeak r from the arrow table\nand then yeah I ran the workflow and\nthese are the execution uh bags I've got\nLet Me Now quickly go to the console I\ntriggered the workflow already a while\nago so here's the first Dynamic workflow\nyeah so\nand uh this is the task that generated\nthe plot let me just refresh the page\nall right\nso here's the plot\nand then this is the final Dynamic\nworkflow that retrieves the peak and of\npeak hours\nand here's the graph for that\nlike for the peak and off because and\nthis is the fetch strips uh data graph\nand yeah that's all I have\num if you like a comprehensive overview\nof the Integrations I recommend you to\nread these blog posts any feedback or\nquestions\nyeah very quick question about the um\npush to GitHub task that's something\nthat uh can be generalized right\nyeah\num so as long as you have some GitHub\nworkflow that\nresponds to a pull request or whatever\nyou know whatever event we could\nbasically use that as a little web hook\num okay thing for flight right\nyeah\nwell yeah and I actually proposed this\nidea you know to use yeah to encapsulate\nall of this in a flight task to\nencapsulate the GitHub thing in a flight\npass yeah yeah\nyeah that'd be awesome to have a guide\nfor that\num\nit was like one one little task right it\nshould there should be like just a\nlittle bit of setup and configuration\nbut seems pretty simple\nyeah since it's a public repo it was\nquite simple I mean it you know you're\nnot really configure anything you know\noh because you can push to a public\nReaper is that what it was\nI mean even if you could wait yeah yeah\nyou could add Secrets right yeah that\nwould be a good good uh addition I think\nMartin\nhey samita here Martin Stein I'm with\nUnion I just wanted to ask you uh an\nexperience question about banana email\nabout the earlier overview what\nobviously this is really great to have\nserverless you know hosting endpoints\nand so on uh how did it feel uh to put\nthis together were you excited is that\nan exciting uh platform to use I mean\nwhat is your personal sentiment that's\nwhat I would want to know\noh yeah um I enjoyed building the\nintegration banana was great it was\nquite simple to set up and integrating\nflight with banana was simple as well I\nmean I spent like two or three days and\nI got the integration up and running so\nyeah it was good and then as as a\nfollow-up question from people\nconsidering obviously production\nenvironments and having endpoints\num we always think about you know having\nthose really you know very\nthe platforms that are out there for a\nwhile but serverless is an interesting\nthing is that Enterprise ready I'm just\na very naive question but I just want to\nask you what's your take say hey look if\nyou have a large scale deployment and\nyou have a model that is actually\nmentioned critical latency wise has to\nbe good uh is does that get you a thumbs\nup too\nyep it is production ready it can be\nused for large data sets and large\nmodels yeah fantastic I'll try it thank\nyou so much thank you"
    },
    {
        "title": "Flyte Community Updates 032 featuring New RFC Process, Flyte Architecture Diagram",
        "transcript": "all right um welcome everybody to the\ncommunity sync my name is samhita I'm\nwith the Union AI team and I'm your host\ntoday please feel free to introduce\nyourself in the chat you can put your\nname and the organization you're from\num let's quickly look at the agenda now\nyeah so I'll start by going over some\nlogistical details David will elaborate\non the new RFC process and present a\nlightning talk on flight architecture\ndiagram next I'll demo the duck DB and\nbanana Integrations finally Sam Cox will\nwalk us through how shift Technologies\nhas been using flight\nso um the agenda is open to the\ncommunity please take a look at it if\nyou'd like to access the notes quickly\nif anyone's interested in jotting down\nkey points from this community center\nplease feel free to do so you can leave\na message in the chat so that we all are\naware of who the note taker is and an\nfii this meeting is being recorded and\nthe recording will be posted on the\nflight YouTube channel in a couple of\ndays\nuh David do you want to take over the\ncommunity updates\nyeah thank you samitan hi everyone I'm\nputting here in the chat the link to the\npr for what is the proposed\num update to the RFC process\nuh for context RFC stands for request\nfor comments it's a kind of process\nthat's been out in open source for for\nquite some time when a project needs to\ndiscuss uh major changes uh technical\nchanges governance changes anything that\ncould affect the user experience and uh\nconsidering that flight is a graduated\nproject and it's it's growing every day\nwe needed to kind of reflect also the\nrecent changes in governance\nuh to\nput emphasis on the role of the steering\ncommittee try to maintain again the es\nor or the rapid Innovation but also\nintroducing some order in how to discuss\nand whether how to approve or reject\npotential major changes uh to the\nproject\nso feel free to comment we are hoping to\nmerge this uh this week\nand this was already discussed at the\nfirst uh as a first step in the\ncontributors Meetup last week next we\nwill have the second contributors meet\nup we are all welcome to join and by\nthen we hope to have this merge and keep\nimplementing this process it will take\nsome time before we are used to working\nthis way but your comments are are\nreally appreciated for this\nright any question any comment\noh\num yeah so next thing I think it's also\nme yeah it's a if I can share my screen\nreal quick please\num but yeah it's a really lighting talk\nlike like it through lighting real quick\nuh just basically to let you know that\nI've been working on\nwhat's happening with my screen okay\nI've been working on since uh several\nweeks ago I've been trying to wrap my\nhead around the light components and how\nthey interact between each other and\nbecause that I I strongly believe that\nunderstanding the pieces helps with\ntroubleshooting helps with\ndesigning Solutions Etc so um\num I will put this into an issue so\neveryone can comment uh it's a work in\nprogress as you may see and the\nintention he is probably not going over\na finished product but just to let you\nknow that if you ever have felt the need\nto have some visuals in the docs please\nlet us know in the issue I will I will\nalso post this in the in the slack\ncommunity\nuh because uh probably some folks out\nthere are like myself were more visual\nuh we need more visuals to understand\nthe pieces and uh what I've been trying\nto do with this diagram is to try to\nkind of follow a logical it's it's a\nlogical kind of diagram and turns that I\ntry to illustrate the management playing\ncontrol plane data plane and what are\nthe components there what are their\nroles of each component\num and I I I I want to give thanks to\nthe engineering team to the core team\nthe maintenance team who has helped me\nbuilding this it's not finished I'm\nstill working this because sometimes it\nmeans just reading code to trying to\ntranslate that into a diagram\num but yeah again it's it's meant to be\na community sources resource if you have\nany questions\nEtc\nuh please let us know in the issue that\nI will share after the quote\num but yeah just\nsharing a bit of progress in what we're\nthinking about this I don't know if\nthere's any comment or question around\nthis\nsee let's take care\num\nyeah okay then see all speaker videos\nPink okay well I don't know it's not\nlike that my end\num\nprobably some kind of filter\nnot sure cool um but yeah thank you for\nlater then let us know okay\nno comments or questions\nI will send the issue I have during the\nmeeting and yeah thank you\num let's go through the links and\nresources now you can schedule a\n30-minute session on calendly if you\nhave any questions we have slots on\nWednesdays at 7 00 am and 9 pm PT\nnext slide please\nthe next Community sink is on April 4th\nEngineers from stripe will talk about\ntheir flight adoption Journey you can\nuse the add event link to at the even to\nyour talent calendar\nso yeah that's about it thank you so\nmuch for your time and participation\ntoday see you all at the next Community\nsync\nthanks all thank you awesome thanks\neveryone thank you thank you thank you\nSam everybody bye"
    },
    {
        "title": "Flyte Contributors meetup - March 16, 2023",
        "transcript": "you hey how are you doing\nfine thank you thanks for joining I know\nit's really late for you\nthat is but\num\nI'm currently in\nin so I work remotely I'm currently my\ncompany another city so\nokay later in the office\nthank you so much for joining\ncool lead\num\nprobably a minute and get more folks\njoining Daniels welcome hello\nhey David\nit's a nice uh what is that like a\nperbola in the background there it's\nlike wooden\nabsolutely like arches oh that's\ninteresting\nand we've got a new office the other one\nhad rooms and I prefer the rooms to the\nwooden things honestly yeah I don't know\nthe whole open Office\nstuff I I yeah rooms rooms are nice\nrooms are nice I agree yeah you can\nchoose to collaborate with someone but\nyou need to be heads down\nit's good to have no no distractions\ngreat to see you Bernard\nawesome\nall right uh let's get started let me\ngive up\nbrief reminder\num the channel for anyone wanting to\njoin\num\nthere you go all right so let's get\nstarted in the\nchairman screen real quick\nall right\num yeah well thank you again for joining\nwelcome to what is now the inaugural\nflight contributors anything uh the goal\nof this space is to\ntry to have\ndifferent\nor better but at least different\ncommunication paths between every\nCommunity member wanting to contribute\nback to the project\nand also this is the one of the best\nopportunities to gather with the\ntechnical steering community and in the\nmaintainers team\nto discuss basically the ideas that\ncould\nchange the project for the better\nand yeah it's helping for anyone wanting\nto\nget started or discuss ideas\num I mean sharing let me share that\nagain\nthe link to the agenda doc\nagain it's\nopen it's free for anyone to\nadd\nespecially in the open mic or question\nsection\nyou can add whatever you want to discuss\nhere so a couple of reminders first this\nmeeting is being recorded and the\nrecording will be posted in the flights\nYouTube channel\nand second thing is that flight is an AI\nand data Foundation project\nso this meeting basically is covered by\nLinux foundations code of conduct\nwhich in summary it's be nice to each\nother\nyou can add yourself to the attendees\nlist uh\nincluding your affiliation totally\noptional but again it's nice especially\nfor folks who are just joining\num so probably we'll start with that\nwe get to know with most of you here on\nthe call but\nuh just just for the sake of the first\nmeeting probably we will run through a\nquick round of Interest\nif you're online so I'll start\nhey\ngood morning welcome great to have you\nawesome so right on time to start with a\nround of introductions so I'll start\nright away uh Davis\nAI I'm an open source developer advocate\nso I kind of try to sit in the middle of\nOpen Source Community Support\num in basically trying to be an advocate\ntry to listen to the needs of our\ncommunity members and work backwards\nwith content creation processes Etc oh\nsorry where can I add the RFC link\nor yeah\nyeah there's the\noutstanding RC yeah that's that's part\nof it's also my question because right\nnow I see it\nrfcs and two different places\nso everybody if you want to discuss a\nspecific proposal you can add it to the\nopen mic question section right here\nokay it's a new item and yeah feel free\nto to do it awesome\nall right who's next\nKing on X\num I'm Eduardo I'm a I'm the engineering\nmanager for the OSS team at Union\nyes this is it\nmight as well go next uh I'm Dan Ramer I\nam a back-end engineer with the Union\nteam as well so everything like plugins\npropeller a lot of changes IDL standard\nlib usually get point in my direction\nbye\nI mean hey we're done like it mostly\nawesome welcome uh could you I don't\nknow I see the camera is a bit blurry\ncan you check I don't know if it's only\nme but for a camera\nready is it only me okay no one's\nblurring\nyeah\nokay cool thank you thank you who's next\nI can't go next um I'm Fabio I am I'm an\nadult engineer at a company called\nrecognize\nand I run or my teammate I run\num platform for ML Engineers to develop\ntheir models and part of the platform is\nflight\nalso introduced I've introduced it to\nthis company I've introduced it the\ncompany that I worked with before\num\nlike one and a half two years you have\nto do the month\nthank you\num\nhi my name is Bernard\num I work at pajama\num I'm a data engineer there and also\nwork in the platform team so very\nsimilar providing platform for other\npeople to run stuff on\num I was a flight to use it for I think\nroughly the past two years or so\num before it was a black track now I'm\nnot my pashama yeah\nbasically using flight before it was\ncool that's awesome\nthank you who's next uh I can go next\nso hi I'm Byron and I'm from linking\nmachine learning and protein and we are\ncurrently working on migrating uh flight\ninto our as our new orchestration system\nand uh I mainly contribute to uh until\nnow I mainly contribute to part template\nand then I'm currently working on the\nconfig operating so I provide the\nprivacy below yeah and I'm glad to share\nin this meeting\nyes\nawesome great to have you\nsecond go next\num hi everyone I'm Niels bentylon I'm uh\nbasically like the in-house ml engineer\nperson in Union\num so I do like a lot of dog fooding and\njust like uh for the OSS project\nuh recently kind of started doing more\nproduct management stuff so just\nlike\nlooking at the backlog helping Eduardo\num\nwith uh just like direction of the\nproject\nawesome thank you thanks everyone\nokay it's anyone okay welcome yeah just\njoining um so I'm I'm still on um I'm\nI'm working I'm a software engineer\nworking at the\nservices\nand so we've been using flight for\nyeah very early on since it was open\nsourced um and uh for all kinds of\nmachine learning stuff\nand\nI actually\num\nshowed what we were using flight for to\nGPU with gpus um a few weeks ago at the\ncommunity think so yeah glad to be here\nawesome thank you for joining\ncool yeah getting purple to just join\nI'm sharing here the link for the agenda\nand the notes feel free to have all to\nthe ad in this list and also have\nanything you like to discuss right there\nin the open mic questions section\nall right next up in the agenda it's\nreview outstanding rfcs uh this is the\nthe first meeting of this kind\nand um\nyeah kind of well yeah we'll kind of the\nbackground for this we recently\nintroduced in an update into the\ngovernance model\nand uh\nwe have a formal technical steering\ncommittee in this case we have Fabio\nBernard\nEduardo and Nils in this school they are\nall part of the steering committee you\ncan find not also their uh their\ntheories and their uh the scope of their\nsupport for the community but also you\ncan find the uh the reminder list of\nmaintainers and TSC members right there\nuh uh we are trying to Define in a very\nclear way for everyone to contribute\nwith specific requirements but also\nprivileges so um feel free to check it\nout\nso review and outstanding rfcs\num I see here in a folder for four rfcs\nin the in the repo and most of them are\nkind of bold\nuh nine months or even two years old so\nprobably that was my first question\nwhat do we want to do with these\nproposals uh we want to go through each\none of them and validate their status or\nor just discuss the ones that are new in\nthe form of PRS\nDavid generally I think everything\nthat's been merged into this flight\nbranch has already been taken care of\num so I don't I don't know if there's a\nlot of Merit in kind of diving\nthe existing version PRS\nin my understanding anything that was\nnot merged would be yeah\nyeah probably the next thing is that\nicprs but we I don't see it RFC label of\nsorts so I'm just searching with the\nname\num and AC just is four proposals uh\nprobably just a couple of them recent is\nthat right is there any other proposal\nthat's missing\nthis is all I know about I already know\nthe override became like an official\num RFC but I know that\nthere's been a lot of discussion like\nByron has been like pilot in the\neven like some of the implementation but\nyou know input from the community like\nRahul who's not here but\nyeah not in a photography at least not\nin like out\nin this format\nokay\num yeah what do you think about\nintroducing a code owners file that kind\nof matches the the files that the RC\nfolder so that some people that are\nexpected to to leave a REV you get\nnotified that they need to review this\num I think that would could have looked\nof course like I could have looked at\nthese PRS before the meeting but it's it\nwill be convenient to get an email when\nthere is something that requires\nattention\nthat's reasonable\ngreat good action item\num\nyeah that's a great idea\nI'm just thinking notes um yeah and it's\npart of uh the work on improving the RFC\nprocess which is something that we're\ngoing to discuss but yeah it makes a ton\nof sense so\nyeah I'm kind of\num getting started with this what's\nhappening okay\nwhat's happening with my\nscreen there you go sorry having some\nvideo issues\nfinally there you go\nokay uh so the first thing uh right now\nso probably will be to see the uh the\nperformance burn benchmarking RFC do we\nwant to\nbriefly discuss this one introduce this\none\nso this audience\nI can talk about it very briefly I don't\nknow half of it's already implemented\num so the idea behind this was that uh\nwe wanted to\ndo a lot of work improving the\nobservability of performance of flight\nand that kind of put into two separate\nbranches of you know observability into\nmetrics and then developing a\nbenchmarking framework as well so that\num between different feature requests\nand things like that we can say this is\nobjectively faster than the other and\nit's because x y and z\num so\nthe work that was done on this already\nis uh exporting runtime metrics if you\nread through the rfcs it's pretty\nlengthy but it breaks down metric\nreporting into runtime metrics where it\nbreaks down like execution of a workflow\nand categorizes each node execution so\nyou have a good idea of where flights\nspending its time and then the\norchestration level metrics is more so\nfrom a flight flight side so when it's\nscheduling nodes when it's you know\nprogressing through the workflow where\nis it spending its time and that had an\nintegration of open Telemetry into\nflight propeller and and we've talked\nabout like uh adding that into other\nflight components as well so we have we\nhave better observability into what's\nhappening in the back end\num so like I said I think we're we're\nplanning on merging this any day now I\ndon't know what the process looks like\nnow\num but half of this is already\nimplemented as it progresses through\nmight be working with the benchmarking\nstuff as well\n[Music]\nokay cool\num write any comment about this ongoing\nwork\nthis question\nI can only we have um\num we have seen multiple members of the\nUSS Community asking for\nso um I'm\nextremely happy that we're finally\nfunding this work\nit might not come in the next flight\nrelease which you know I think two weeks\nbut definitely for the one after that\nincluding the changes to the front end\nalso which you know will expose most of\nthese metrics that then we just talked\nabout so\nokay\nthere you go yeah I'll try to take notes\ndirectly in the pr\nso yeah so everyone has information\ncool oh can I go next because I need to\nleave at 1 30. oh yeah sure for sure\nyeah I will share the location of my\nokay or do you want to share your screen\nokay for sure\noh I don't have the permission\noh I have now yeah\nokay oh yeah so yeah the problem I'm\ntrying to solve is the config I'll be\nright at execution time because for now\nsomething like the uh catch and the test\nconfig are fixed at registration time so\nwe are trying to find a\na good solution for this so let's break\ndown some user scenario so this is what\nI currently have we currently have so we\ncan pass a constant with a with override\nto override the config on the task\nbut this is what we can do now we cannot\noverride the reference launch plan\nconfig like if you want to change a test\nconfig inside of a reference launch plan\nwe cannot do it because we don't have a\nfunction body here\nso my proposal is that we can also do a\nsimilar way used with override and then\nspecify yeah I'm still deciding about\nthis what's the best solution because So\ncurrently what I'm thinking is that past\nthe node name and then pass the test\nover object to override it\nyeah\nso yeah here's the registration time and\nnext is execution time because sometimes\nwe want to override the things or config\nat the execution time\nso um here's the API I'm currently\ndesigned so we can pass a workflow\noverride\nobject into flight remote so we can\noverwrite the workflow\nyeah\nand we can also pass this by Phi CTO\nso I think the difficult thing is that\nhow we can design this on the UI\nlike\nyeah so we we should have a maybe drop\nmenu or we can just click on the task\nand then change the config there yeah\nthis is a the thing I want to discuss\nand while we currently cannot do is that\nwe cannot espouse the config as the\ninput for the override because it need\nto be\na constant yes so I really need some\nsuggestion from you guys\nif I can bring up as well right off the\nbat\num\nusing node IDs is is might introduce\nsome complexities here like that Dynamic\nworkflow is like if you're iterating\nover a list the node IDs aren't going to\nbe deterministic depending on how you\niterate over we've run into this\npreviously with like recovery of dynamic\nworkflows and things like that\num\nand so that might be something to keep\nin mind here is that if node IDs aren't\ndeterministic you might be overriding\nthe wrong task\nyes I agree\num for us internally we use a walk\naround I can also show the work around\nhere\nso we are currently\nuh where's the code\nover here\nso we Define some special data class\nlike this one is for Python and this one\nis for TF job\nand as long as we pass the\nthis special data class\ninto the as an input into the TF drop\nthe five plugins propeller plugin will\nstart overwriting the test template by\nthis override\nand I think this is Fork\nsorry do you have a fork\na fort flat plugins or\nuh flight okay no like my question is\nthe normal flight plugins TF job doesn't\ndo that right yeah yeah I have a forge\nokay\nso basically just overwrite the test\ntemplate by Young seeing this\nyeah and I think it's kind of usable in\nour case\nbut maybe not generic to the flight\ncommunities\nyeah environment we've also worked on\nlike\nhow this idea works on the UI too like\noh so basically we can just for example\nthis is our workflow\nand then we can\noh this is just a data data class so\nthis can be shown on DUI\nand it's quite convenient to change the\nconfig test config at execution time\nand I think the implementation effort is\nmuch less than this one\njust like this works for like a single\noverride or if you're passing like the\nsame override across all tasks right\noh what do you mean\nlike in the case where you want to like\nfor in your example here ref\nwf1 you want to set number of CPUs like\nthree and in the next test you want to\nlike set it to four\nI just need to pass another CPU integer\nyeah I mean like like my input like I\nlike the idea of having this like uh you\nknow more generalized you know you know\nkey value pair and stuff like this\num if you do like my workflow like this\nif you know say you have three sub tasks\nwhere you want different task overrides\nfor all of them you need to now have\nthree parameters in the workflow right\nis that am I understanding that\ncorrectly yes I think it gets\nunmanageable like if you have a sub\nworkflow that then takes some overrides\nlike reusing that between workflows\nmight be a little bit you know hacky and\ndifferent\num\nalso it kind of makes this workflow and\ntask arguments with past conflicts that\nyou typically set in The Decorator right\nyeah this is just the uh to work around\nin the short term right Byron like yeah\nI mean they're fully fully got that\nyeah this is just a walking around for\nthe IFC no\nbut I don't think I would yeah sorry go\nahead\none step step out we use reference task\na lot between teams basically these are\nthe the artifacts that we share and for\nus a reference task is kind of a black\nbox so knowing about the internals of\nwhat's in the black box kind of The\nProposal in the pr would introduce\ncoupling between the teams so I am\nwondering what the use cases\nbecause I've kind of not not seen it\num\nOh you mean why do we need the reference\nuh why do we need to overwrite the\nreference workflow\nI'll because in our case we are we have\na bunch of components in different uh\nbetween different things and we can we\nwill write the uh reference referral\nusage as a document so they will now\nwatch what each parameter means so they\ncan it is not a bad company we'll have a\ndocumentation about this\nokay so the uh the usage or the\ndemonstration of reference workflow here\nis ancillary like it's probably you can\nimagine this applied to like a\nnon-reference workflow right initiative\nvery gracefully to reference and\nhonestly also we should add a remote\nworkflows so yeah it is incumbent on if\nyou think about it there's no like easy\nway to get this ux right like at some\nlevel you if you're going to be\noverwriting specific notes you are going\nto have to understand the structure of\nthe thing that you're overwriting\nyeah\nhow can they understand the no structure\nlike\nwell that's what we were talking about\nright like they you either the user\neither has to uh the the user who is\ndoing the overwriting has to learn about\nthis somehow so right we were talking\nabout on the uh runtime side like making\nit potentially interactive or something\num whereby we query for the nodes\nbeforehand\num and interactively prompt the user or\nsomething or the users has to go in and\nlike look at the code\nyeah\nyeah we I think they should either\nsee it on UI or I don't see why we\nshould prompt something yeah I need to I\nneed to go to the oh sorry I need to\nleave now I'll talk about this more next\ntime\na good Next Step sorry before you leave\nis you you could send this as a PR but\nalso everyone can just comment and you\ncan continue a conversation there okay\nI'll do it thank you bye thanks to you\nAaron thank you cool maybe one more note\nabout this topic so this week I try um\nor one of our Engineers asked me whether\nthey can use the task that somebody\nmaintains internally in a patent package\nand can override the cache because they\ncan override the resources and then this\ndidn't work and I'll maybe on the\nweekend have a fun time I'd make a PR to\njust expose this one too\num\nso if all kind of everything that you\ncan set in a task decorator could also\nbe set in the overrides I think that\nwould be cool\num including like the cash version yeah\nlike basically everything that you can\nset at The Decorator should also be able\nlike one should also be able to set them\nthey would overwrite they would say\nyeah absolutely\ncome on\nso that is basically but still at\nregistration time right because you need\nto modify the code for that\nand if I if I can continue the thought\nof what it would mean to do this at not\nthat registration time but it basically\nwas starting an execution in either with\nflight control\nor in the UI\nthen as a user I would probably\nwhen I get the execution spec file maybe\nthere could be like a way to provide\noverrides there\nbefore you create an execution from the\nexecution spec file\nor when you launch a workflow maybe\nthere could be a drop down that for all\nthe tasks or that would be complicated\nright because the workflow can be very\nbig and then you would have a lot of\nNASA drop downs to do all these\noverrides like that totally tricky I\nmean it could be opt-in right you could\nsay you could override a b and c in this\nmassive workflow yeah\ngood morning\nlike\nit's overridable there's the front end\nis a fair amount so uh from my side it\nwould be awesome if all things that can\nbe set in that after grade cash\ndecorator could be overwritten at\nregistration time because then we can\njust share tasks in Python libraries\ninternally\nI'm also happy to help there I'm not\nsure if I would need personally the\nfuture to do it in the UI\num not sure the other one is more\nimportant\nyeah for us for us too it's it's\nimportant to to generalize this and\nactually there's some\nsome issue I I couldn't find it right\nnow but there were some discussion on an\nissue about this\num\njust the general override\nso far for us it's interesting uh\nespecially for for gpus\num so we have the flexibility right now\nwe're using podcasts a lot and we can\nactually override at runtime switching\nswitching to a different\nkind of GPU for instance this is quite\nhard we have we have to\nre-register the task basically with a\nwith a different pod spec right and if\nthis if if we had a more General way to\nto override things not with the uh with\nthe Pod template I think\num\nthat would be quite quite helpful for\nfor this use case\num but also yeah I don't know the UI\nsupport is isn't that that important for\nfor us more the\nin general the way to to to be able to\noverride these\nuh to override these things\nalso talking about there was someone\ntalking about\nwhat was it called\num the custom resource types in\nkubernetes is that a thing that we were\nthinking about right we were discussing\nchanging the resources from an enum to\nallow free form text because case as of\none two two now supports it\nand resources are already into override\nso that's another way of doing that we\nrefer to do that change\nyeah\nresources are already I'm sure I I\nreally got it\nuh resources are already enabled yeah\nthat's that's possible yeah but you can\nonly override the enum resources that we\nexpose yeah exactly exactly so and um in\nin our in our case that's that's not\nenough because\nyou can specify like the GPU type just\nthrough resources\nyeah there is a competing\num maybe solution or something worth\nlooking at at least uh which has\nextended resources\nexactly yeah yeah and also one thing\num\num um\nwe we just had in the um\nin the in the RC was this this mix of um\nkind of a workflow\nparameters and\ntask config and of course it's\nproblematic in in one sense but on the\nother hand if uh if you change for\ninstance if you change the GPU type yeah\nand you have an arcade and you have a\nmore powerful GPU uh you also want to\nlike change the batch size and because\nyou have more memory or less memory and\nthis also effects essentially other\nuh like regular task parameters right so\nthis is also quite interesting I don't\nreally know if there's a good solution\nfor this but\num yeah this is still this is something\nwe are thinking about too\nwith the at the moment\nkind of adding a parameter to the\nworkflow would work with a dynamic task\nafterwards right because then the\ndynamic task is override and then build\nthe workflows back um so kind of that's\nI think in the talks at the moment I've\nnever tried but\nI think it might work yes yes it does\nbut yeah okay\nyeah okay but good call out yeah it just\nconflates like what there's nothing\nstopping you even with the new solution\nthere's nothing stopping you from doing\nthat so\nyeah\nso using a dynamic task\nwe could actually already\ndo that then right yeah\nI think at the moment you can't\noverwrite the template maybe because I\nthink there's no with override spot\ntemplate but\nonce that is in I think you could use a\ndynamic Tasker\nyeah okay you still need the the\ngeneralized way too overwrite yeah okay\ngot it\nwe should be making this easier through\nthe API because uh\nDynamic that's bad template overrides\nisn't the right way to do it but yeah\nagents anything else about this proposal\nno\nall right there let me share my screen\nreal quick\nuh no and we'll go to the next one not\nsure if we want to discuss this one as\nthe authorism here okay then live\nexecution tax and metadata\nany comment about this one\nyeah so I think the main use case\num\nthis this can be used for a lot of\ndifferent things the main use case we\nhad in mind for this is for things like\num you know ml experimentation where\nyour results are not localized into one\nworkflow execution so say you're running\na bunch of tests\nacross a bunch of different executions\nand\nyou know imagine a future state where\nyou can like visualize the results of\nthose executions\num somehow so this would be step one in\nsupporting that sort of use case\nwhere you can specify a tag somewhere\nand after the fact you can grab all the\nexecutions belonging to that tag\num I actually haven't read this in a\nwhile so I don't have too much context\non things like the proposals for example\nfrom like an element the quantity for\nexample you can tag or run with\nsomething and then filter based on tags\nso I think that's kind of a natural we\ndo that basically by now linking to 1tb\nand then grouping by text there so it's\na kind of a natural extension to just do\nit in the orchestration engine\nyep\nwe've also built the work around so it\nwould be nice also\na similar texting in Neptune for that\nmatter but\nyeah\nso uh just your use cases here does that\ndo they reflect the like ml use cases or\nare there any others that you can think\nof or you've seen ours are certainly ml\nexperimentation like just\ngrouping runs together that belong to a\ncertain trial\nyeah same here um\ncool\nactually we were the workaround for\nnaming executions because that was an\nimportant thing to us\num we have our own kind of registration\nscript that does something very similar\nlike pipeline judges and drum combined\nbasically we copy the code that's run in\nthere which then but we for example\ndetermine the version automatically\nbecause we don't want people to mess\nthat up and also like the project\nand part of that is that you can\noverwrite the execution name\num because when we have I don't know\nlike 20 Engineers that all run their\npods\num a few name spaces then it gets quite\ncluttered to find yours so\noverriding execution name so that you\ncan just find your pod easily something\nwe already do\nso I would argue that's an important\nthing\nawesome got it thank you anything else\naround system decks\nnope\nokay and the next one on the list is\nyeah well recent the array node for\nfunctionally complete math task\nyeah I can talk about this one a little\nbit\num everybody that used a map task has\nbeen upset about it at some point or\nanother\nso right now map tasks if anybody's\ngiven in the code are implemented as a\nseparate plugin in the back in the case\narray plugin\num this has kind of been a legacy thing\nthat was never meant to scale and then\npeople started using it and added X and\nY and Z and so the fix to this is to add\narray node uh as just a core component\nin Flight workflows where basically\nhere's a node and there is a subtask\nthat is defined by another node type and\nthen we handle this in propeller uh\nlevel so the motivation outlines at a\nnumber of different things that\ndon't work in the current map task\nimplementation that people frequently\nrun into or ask for\num\nimplementation detail details all that\nstuff is not very sexy but kind of the\nthe key takeaway is if we do this map\npast school work how people think they\nshould work\num you can then map over any plug-in\ntype not just like pods containers any\nkind of kubernetes and additionally you\ncan map pass over any node so if you're\nfeeling a little bit wiry you can map\ntask over Dynamic tasks or sub workflows\nor anything like that so functionally\ncomplete you know can you do anything\nwith a map task that you think you want\nto be able to do\num that's kind of the idea\nall right thank you for the intro any\ncomment question\nI guess just a minor naming thing is I\nguess if it supports mapping over\nanything should it be called map\nsomething else besides map task\nanother loop over over sub workflows\nwith it\nyeah you could use a sub workful then in\nthe map task and whatever else\num yeah I mean like Legacy naming we're\nnot going to change flight kit right\nyeah yeah\nbut like we could\nexpose a map there right map you know\njust to make it things more like\nintuitive yeah sure yeah map test there\nsure yeah I I don't feel strongly about\nthis I mean it's the same kind of thing\nas Dynamic tasks which are Dynamic knows\nbut they start a workflow you know so uh\nuh certainly happy to change naming to\nwhatever we agree upon\ncan you map over a map task you could\nyou could map over another map task\nokay\ncool\nI would have one Loop small question\nsorry I've also not read it I'm also not\nwell prepared\num we'll do next time\nwill it will the interface change so\nthat you can like map over\nwill the interface be the same or is it\nbecause a lot of the stuff that we do at\nthe moment is we have a prepare map task\nkind of thing that like groups inputs\nand then\num\nit does does a lot of stuff to get the\nmap task to work um whereas sometimes\nlike a partial or like like a partial or\na zip functionality kind of in flight uh\nwould be would be cool well it's hard to\ndo though I know\nI think Eduardo you might be able to\ntalk about like like partials a little\nbit more there's been a lot of\ndiscussion internally here about that\num as part of this proposal for like you\nknow the array node stuff\nmultiple inputs is something that you\nknow a lot of people ask for frequently\nlike you have you know two separate\nlists that are of equal length you have\na function that takes a and b and you\nyou know take index Zero from both of\nthem and iterate over that so that\nshould be included into here\num and then I know like I said you guys\nprobably talk about more but the idea of\nimplementing a partial into flight kit\nso that you can have like a constant uh\nvariable as well so if you have you know\nthree parameters\ndouble list and then a constant can be\nthe third one\num not unsure if that's going to take\npart of this RFC implementation as well\nbut the idea of a partial for constants\nyeah so I think the idea is to match you\nknow Python's map semantics tool you\nknow map or map task so that it it\nbecomes more ergonomic to use you know\nyeah fully agree we we actually switched\nto\nDynamic tasks and at least one use case\nbecause it was api-wise it would it was\njust\num better to understand with just the\nlist comprehension you know it's on uh\nyeah\nno that that's pretty cool um we are\nfunding the work to you know both add\npartials\nor partial support for for map or map\ntasks and also this idea of having like\nmultiple lists or multiple parameters\nso that you know if you're coming from\npython it will look or you feel exactly\nthe same\ncool\nawesome\ngreat right uh it seems like that's it\nin terms of open or in Flight part of\nCIS unless there is anything else in\nthat record\nwe can move over to the next item\nlike related\nthe proposal or the yeah the proposed\nimprovements or updates to the RFC\nprocess let me show you this way\nuh basically uh what we are doing right\nnow\nit needs even more structure\nand the goal with this updated process\nis to\nuh maintain the kind of the lazy\nconsensus structure\ndecision making but also giving the\ntechnical steering committee uh the\nresponsibility and the privilege to vote\non proposals\nand also to Define when to RC will be\nnot to RC\nuh kind of the process\non how it should work\nuh all these takes inspiration from the\npython enhancements uh process back\nprocess basically and the broth RFC\nprocess and all others\nand the acceptance criteria and what\nshould happen after an rfc's accepted or\nnot\nso um\nfeel free to give it a read at least I\nhope you like the Leslie Lambert quote\nI'm sure that Eduardo will like it and\nuh thank you\num but yeah in any case it's again an\nattempt for different corner to keep the\nuh The Innovation speed but also trying\nto grab the structure and make sure that\nthat there's enough\njust enough discussion for a proposal to\nget to the project or to be rejected or\nwhatever\nso feel free to check it out because\nprobably uh with the resulting in\nproduct we will be also adapting what we\ndo in these meetings\num yeah Andres is there any comment\nright now about this one\ncan you take myself as a reviewer and\nalso thank you\num\nthanks I will need to read it after the\nmeeting\num\nokay\ndo you think\nyeah\num\nuh I I think the only comment I I think\nI would change maybe is like I write a\nlot of the maybe initial stages of the\nRFC on Google Docs and then\num for comments there because I find\nthat concept is easier and then only\nPort over when like basically all said\nand done so I don't know how yeah well\nwhat we're doing right now here is uh\nbefore creating an RFC PR uh is using\nfor example the RFC incubator that Nils\ncreated it's a GitHub discussion\num yeah I know there's Google Docs is is\nalso there for comments\nEtc but the discussion probably could\nhave an a bit lower barrier\num for anyone to just go there and add a\ncomment or ask questions about the the\nidea before it\nturns into an RFC\nand uh there are some places where we\nshall discuss that those ideas before\nuh recommending hey just that now it's\ntime to open an RFC or we don't think\nthis is uh a good feel for an RFC\nbut thank you\ngreat awesome\nand yeah a couple of things before we we\ncall it\num\nI'll be creating an issue so anyone can\ncome in there the idea is to establish\nto define a set of a short list of\nspecial interest groups\nprobably that that comes first way\nbefore working groups I believe a couple\nof links there to like the big deeper\nbut um basically six is a way to create\na kind of a community inside the\ncommunity with a more focused uh\ninterest\num on different things Kate and has\nmentioned some ideas on six large model\nsix integration\nEtc\nand also working groups are a bit\ndifferent because they tend to be\nshort-lived I don't know a quarter with\na specific goal a group of people and a\nset of Milestones that's it so it's\ntypically used for documentation\nrevamping and more stuff that is\ntraversal to a project\nuh both that comes in a later stage but\nright now it will be interesting to\nstart defining six because we know that\nout there in this community there are\nfolks with with experiencing different\nuse cases verticals here Etc so gather\nthese these people probably will unlock\neven more ideas\nso I will create an issue and post it in\nthe contribute channel so anyone can\ncomment about this\nand unless there is anywhere coming\nright now\nand uh a PR Spotlight this one uh I\nthink it's really interesting because\nit's an effort from the from community\nmembers outside the maintenance team\nuh to improve the uh the user experience\nso basically adding\num GPU support for this sandbox uh\ndeployment options there there are some\nchallenges there as Kate and mentioned\nfor example that the lack of gpus and\nyou know CI systems Etc\nuh but it's it's again interesting feel\nfree to go there comment Etc because I\nthink it's a good example of of the\ncommunity thinking ownership and trying\nto help where they can\ncontributors\nyeah one small comment on this uh we\ndiscussed this internally you know this\ncould be as\nslight touch as just like maintaining\nthe docker Dev files containing a GPU\nand like the couple scripts but like\nreally leaning super heavily on the\ncommunity it's like why is it you know\nrun it\nforeign\nanything else you want to comment and\ndiscuss here\nI would have a short technical question\num I think there were talks about\nswitching to a model repository at some\npoint is this\nhappening or just also to know like in\nterms of contributions no no totally\ngreat question still in the cards it's\nhappening but really the expectation is\nthat this is not going to affect\npeople in USS\nlike at all we're not like being we're\nnot going to be\nopinionated about like the\nthe build system like they're not\nadopting bazo like it'll be really\na couple of make files and that's that's\nit\nbut we should we should probably talk or\nnot like if you are\nyou probably have good ideas about this\nthis memorable or what are you thinking\ncurious whether for example I don't know\nI was thinking about maybe contributing\nlike like a test tip or something and\nwhether that's like too big of a change\nto do before this happened it's or after\nlike kind of more of that like timeline\nset of things\num\nthank you Bernard\nright anything else\nlike I want to external the point that I\nbrought up in the beginning\num the code owners thought should we\nmaybe create\nteams and GitHub that reflect kind of\nthe teams in this um\nforgot the name of the document I'm\nsorry I'm really tired like the events\nyou know what I mean I'm sorry my brain\nstarts to shut down already but\nbasically you create GitHub teams and\nthen have these teams in the code owners\nfile because I think um what I need to\nfigure out for myself right now is how\nto like navigate this stuff besides work\nand having notifications that work well\nI think will do like half of the job for\nme and\num\nyou know we use that at work it makes\nsense to use it here I would say\nwith the caveat that like we we don't\nhave a lot of movement yet\nunlike the number of rfcs or even like\nATL discussion so\nbaby creating these things prematurely\nlike now\nmight not help you a lot\nwhat\num\nbecause yeah probably right now we don't\nhave much in terms of proposals but it's\ngood to\num prepare beforehand and also yeah we\nhave some uh practical implementation of\nwhat's in the maintainers file\num in terms of privileges permissions\nEtc that that could certainly speed up\nthings when we need it\nawesome\ngreat any other question\nLove Point\num\nis there any chance we can move this to\ntwo hours earlier\nyeah well I was I was thinking yeah\ngreat great point\n[Music]\nI was thinking of having even on\nyour name is scheduled because yeah in\nthe past I've seen this for for\ncommunities with very wide time zones\nprobably what will work is to have the\nmeeting a week in this uh time zone next\nweek in a more you friendly\num time because yeah for the trying to\nmanage the schedule for everyone who's\nhere the only slot that have the the\nmost boats was this one but I know it's\nlate\nin Europe so yeah probably I would\ndisabilities to vote\nin the channel to see what what we want\nto do we want to move this one two hours\nlater or we want to try to have\nalternate schedules\nI mean um I think like I like the group\nof people here I think the discussions\ncan be interesting and from for me\npersonally if it was two hours earlier\nthat would basically been be the end of\nmy work day and I'm just happy to like\nsay another or the computer\num so 7 PM would be perfect for me it\ndoesn't have to be alternating then even\num but that's just speaking for me um\nother people here need to of course\nspeak about that too and they also\nunderstand that this is like the middle\nof your work day so that there are\nconstraints from that obviously\nyeah good point yeah we want to make\nsure that\nthe majority feels included and yeah\nwe'll discuss this I think a bit more in\ntown to see\nI can totally agree with Fabio here but\nyeah I'm also in the European Time Zone\nit's the same for me basically yeah yeah\nsame same here\num\nbut work for us-based companies so I\nhang around in the evening a lot anyways\nyes let me also work for you as\ncompetition\nbut I mean we're lucky right because we\nlike I think we're all like West Coast\nand all central European times so\num we also do this at work and kind of\nin the morning your time evening our\ntime that I guess that will will only\nbecome complicated once we have somebody\nin Asia then we'll be like tricky but\nright now I think\nthink we can make this work without an\nalternating schedule\nokay\ncool\nyeah we'll check it out listen thank you\nthank you everyone for joining for your\nquestions your contributions I think it\nwas it was a great meeting I hope to see\nyou in the next one awesome thanks\neveryone everyone thanks thanks bye bye"
    },
    {
        "title": "Taking off with Kubernetes and Flyte workflows to automate and scale ML training at Roboception",
        "transcript": "all right so yeah first of all things\nfor inviting me to talk a little bit of\nour\nlight journey into perception so um\nyeah happy to talk about why we chose\nwhite and hopefully this is of interest\nto some of you and um of course if you\nhave questions during my talk feel free\nto\npost it in the chat okay I cannot see\nthe chat while I'm sharing\num but yeah if you have some questions\njust feel free to interrupt me in that\ncase or otherwise we can also discuss\nquestions at the end\num also if we have questions later you\ncan always\num give me online slack or something\nlike that\nall right so let's get right into it\num so I'm gonna briefly talk about our\ncompany so what we do our portfolio so\nyou get a rough idea of\nwhat we use flight for\num that would be our cut match program\nso I'll briefly\nintroduce that and then I'm really gonna\ntalk about what we've been using so far\nand why we needed something a bit more\npowerful obviously a bit more flexible\num we're really gonna talk about our\nlocal kubernetes cluster setup and I\nhope that this is useful someone\nand um yeah how all site workflows for\nour heterogeneous local GPU cluster\nCommunity looks like and then sort of\nlike some to-do's that we have to do on\nour side to fully leverage what's\nalready there in flight\nand then some things that we would like\nto see\nin Flight in the future\nall right let's start with us\num yeah so\nI'm with full reception we're about a\nsmall company founded in 2015\num as a spin-off from the German\nAerospace Center and yeah we make 3D\nsensors and mostly software based on\nthat certain idea is that they simply go\nfrom Pixel to action using perception\nand they're based in Munich in Germany\nso just briefly that you have an idea of\nwhat we what we do so we have a hardware\nline of three interior sensors this is\nthe also visited sensorflaminine that\nyou see in the middle there\num that's basically a smart camera so\nthat it has a\nprocessor on board with a\nvdrt grant GPU on it so we can do some\nprocessing on the camera directly and\nyou don't need an extra external\nindustrial PC\num and also we have a focus of things\nthat run directly on the camera that\nranges from like very simple things like\ndetection of tags April tags VR codes\nand so on\num to sort of like very simple picking\nscenarios using itemic that just detects\nbasically services that are graphable\nsection paper\nto the more complex at the very end of\nthe Spectrum which is uh cat match and\nthat actually is\nbasically what we use flight for to\ntrain our models for CV match and that\nactually then\nneeds more resources than we have\navailable on the smart camera at least\nin the situation and will run on this\nextra industrial education that we have\nand yeah we also have the Standalone\nstereo sensor that has a bit more\nresolution\nokay it's a briefly why stereo I guess\nthis is quite interesting in terms of\nimage processing might be obvious to\nmost of you but traditionally in\nrobotics and especially in Industrial\nAutomation a lot of stuff is done either\nwith 2D cameras or replacement scanners\num but of course if you have a stereo\nsystem or any system where you have a\ncamera image and depth information at\nthe same time that makes it um rather\nnice to sort of like combine vesical\nalgorithms\nthat can certainly shine into this video\nrobustness in some scenarios with\nmachine learning in one system\nso on the bottom here you can see this\nis not a camera image of our monochrome\ncamera and sort of like the the color\ndepth images for us to look at as humans\nand then also for every pixel in the\ndepth image we have a corresponding\nconfidence image and also an error image\nnot shown here so for every pixel we\nknow how confident we are that the arrow\nthat we report is actually uh through\nthe electrical parts\nand yeah on the right side you can see\nlike a 3D reconstruction\nof how the spin would look like\nokay so now where is flight coming so\nfor our CD CID match program\num basically at the ideas that usually\num\nwe have parts that you know before so\nfor example manufacturing\num\nin machine tending where you need to\nrecognize and open Parts usually lying\nrandom in the bin\nfor example insert them into a machine\nor somehow process them put them\ntogether or something like this\nso the idea is that the customer\nprovides us with a cfd model and then we\ntrain\num\nwe're trying several machine learning\nmodels\nthe modeling networks and we have a\nclassical\non top of that and in the end basically\nwe have a very easy to use a web-based\nUI that the customer can use to sort of\nlike set up the system Define points how\nthe object should be grasped and yeah\nsorted and so on\nis that sort of like the background\nlet's see here how this looks\nunfortunately this is I guess the\n[Music]\num\nit's supervision that used to be a give\nan animated gif I guess it rewards rule\nin the PDF\nbut we have basically two main stages\nso first we detect objects where they\nare roughly so this is a simulated image\num so that's usually a bounding box\nthere could also be a segmentation mask\nand then in the second step we have a\nrefinement that is usually done on\nwith classical algorithms to get\nsomething more precise out\nso this is a bit more detail so in stage\none\num basically we do responding boxes\nsection and then we have a neural\nnetwork model that sort of like\nestimates the rough size of a wrap\norientation\nof the object and then in the stage two\nwe use edges the depth image that we\nhave and all the extra information with\nclassical algorithms to refinance\ntogether more accurate and more robust\nresult and also do some safety checking\nand so on\nso basically we use flight for stage one\nto train our modules\nand also for the evaluation\nso this is basically the things that we\nneed to go through so we get a cre model\nfrom our customer then we do it like\nsome synthetic data generation in the\nsimulation environment\num then we train our\nneural networks and then we run\nevaluation and in the end basically we\nget what we call a template so that\nbasically contains our LAB Works\nparameters for the\nfor the classical algorithms for the\nrefinement also\nstuff like the CLD model to show in the\nweb-based visualization and so on and\nalso a simulation report so basically\nyou can pick\nuse cases that you want to have\nevaluated in like from far away or\nwhether it be with the camera how large\nwould the objects be is it a random bin\nor we have some some fire knowledge that\nmight make this a bit more\naccurate or so so we can get a pretty\ngood idea beforehand even before the\ncustomer got the first camera or you\nmissed it somehow and they're real setup\nto give a pretty good estimate of yeah\nwhat the expected working range would be\nhow accurate it would be how fast it\nwould be and so on\nand this is basically this generation is\nwhat we want to represent in flight\nso a bit further so here\num our simulation environment this is\nmostly the blender but also some direct\nbit faster opengl rendering and so on\num and we have a very large material\nLibrary\nyeah basically background scenes that\nare common in Industrial Automation\num and yeah we use that to simulate a\nlarge amount of data so we use that for\ntraining and also for the processing and\nevaluation\nthat also means here on the left side\nyou see the two left and right inches so\nsince we have a stereo camera we have\nelectric and right image\num then we also simulate uh some sensor\nmice based on yeah how we know how well\nour Mrs work what the expected most\nlevel would be\num and then we run our actual stereo\nalgorithm on these simulated images\nthat's basically the image that you see\nthe third image from the left\num then you also get sort of like the\nnaturally all the occlusions that you\nwould have or the Shadows where you\ndon't have any any real\num data points just if you would see\nthem from the real camera just because\neither there's overlap or yeah some\nother um\nand then on the right side you can see\nsort of like our\nevaluation so we detected the parts in\nthere and then since we have the real\num\nground truth data for where the object\nactually is from our simulation\nenvironment we can easily create a\nreport and yeah judge how good this\ntemplate so the whole of the two Network\nmodels including our classical\nrefinement in the end Works\num in a specific set of the customer\nsite\nall right now for the interesting part\nfor most of you hopefully\num\nso how did this look before we chose to\nload on the kubernetes and flight route\nwe already had our individual tasks\nI'm sorry training of one specific\nNetwork model containerized\num all the data lives in Arena or\nclusters so that's an S3 compatible\nObject Store that we run on premises\nand we have some very simple scripting\naround Docker compose and some queuing\nscripts and so on which is sort of like\nwhen these steps one after another one\num different GPU machines that were\ncurrently available\num and yeah we basically then coordinate\nit amongst us with a small team so this\nis still still possible of who's using\nwhich regime to train something or\ngenerate some templates and so on\nyeah so the pain points as you can\nimagine\num of course we have several GPU enabled\nservers here\num and this sort of like manual\ndistribution choosing where to run what\ndefinitely results in sub-optimal\nvisualization of these machines it\noverall takes longer if you want to get\nsomething done quickly for a customer\nand\num yeah some of these jobs need much\nmore GPU memory than others so there is\nalways a bottleneck in here to take care\nof somebody's already using that machine\nthat you have to wait for that and in\ngeneral sort of like this manual\nscripting stuff and so on\num yeah we'd like a good overview and\nwanting to use some proper workflow\norchestration tools and some uh yeah\nworkflow tooling on top than to build an\ninternal self-service platform where\nalso our non-technical\nfor example sales could just sort of\nlike Drop in one\na Cod model and in the end there pops\nout the report including the trained\nmodels that are in this template\nOkay so\nbefore we chose to go with flight and\ncheck that out\nthe question in the beginning was I mean\nwhat kind of orchestration solution we\nwould really want to use basically we\nwant to kubernetes or you want to\nkubernetes I mean as we all probably\nknow if you've done this yourself it's\nquite powerful but also quite a complex\nsystem in\num yeah if things go wrong and you don't\nknow what's happening that's usually\nworse than having a very simple system\nthat has some other limitations\nso we basically looked into some\nagent-based systems which basically\nmeans you have an agent running on its\nown machine somewhere that will follow\nor get sent to jobs and we also briefly\nlooked at Nomad which is a\nyou know an orchestration tool\nwhich is quite a bit simpler than\nkubernetes but not that many projects\nthat are like are based on Nomad\naudience that is a it's a lower level\norchestration it would and then of\ncourse uh yeah what kind of workflow\ntools that run on top even this agent\nbased or some or inspirational tool\num will fulfill our requirements which\nare pretty basic so we somehow need to\nrun remote workers in different CPU and\nGPU machines and schedule those and\nsince we already had everything\ncontainerized so\nfirst we're not looking for something to\nwrite our tasks\nat a very granular level directly in\nPython for example because we already\nhad it\ncontainerized such that this kind of\nlike fits our needs and sort of like\niterating on a specific model for\nexample right now we handle via our cicd\nsystem\nof course there's room for improvement\nand obviously since we want to build a\nself-service platform around it we want\nthat to have an API so we can launch\ntasks and workflows and so on and\ncompare them for the state\num so that you don't need to have sort\nof like the developer access\nand yeah even if it has a decent UI we\nwanted to have in the end a bit more\nsimilar UI to just drop in a model and\nsee what the outcome is at the end\nso briefly we looked at some\nnon-insubernetes so it was maybe not\njust like this native Solutions like\npremium for example that looked at first\nglance quite nice\num also strong and sort of like how to\ntrack your experiments and so on it's an\nagent-based solution but basically I\ndidn't really have good support for\ncontainers as a first class object and\nso on and felt that it clumsy to use it\nthat way\nthen also Dexter looked quite nice but\nalso so like that you're not just\nintegration in terms of resources to use\nwhat task will run where seemed a bit\nlacking in updates for that yeah at\nleast At a Glance not so nice then of\ncourse I guess what most people will\nfirst find if they Google for something\nlike this is queue flow but that was\nalso on one hand way too much much more\nthan we needed and also didn't seem to\nbe lacking momentum and yeah just in\ngeneral didn't look that appealing from\nsome technical perspectives\num one thing that looks quite promising\nat least if you look purely at sort of\nlike this workflow container\norchestration thing uh would have been\nArbor arguments\num but also then we didn't want our\ndevelopers to write their workflows in\nin yaml and so on there's some um\nabstractions around that\num but that seemed a bit too lower level\num yeah we looked at kodiacs on and then\nactually rather at the end while already\nevaluating some others I came across\nflight and I was quite surprised that I\ndidn't come across it earlier but was\nsuper happy because that seemed to be a\nreally good fit so then I didn't tried\nthat out\nokay\num so but first I before I go into how\nwe use slide itself maybe a brief into\nMexico on our local kubernetes setup so\nfor me that was quite a fun learning\nexperience because I've never been\nworking in kubernetes or anything like\nit before and setting that up from\nscratch locally\num\nyeah involves a lot of reading and\ntrying things out and seeing what what\nworks nicely for us in the end here now\nwe have in my opinion it is so far works\nreally well a quite nice setup\num so we lose k3s which is a very small\nbasically single binary distribution\noriginally from venture\nin a highly rated setup\nand we run three muscle nodes svms in\nour existing proximox cluster that we\nusually have for the actual other\nservices\nand just to make this a bit more uh I\nwould say reduction rate and sort of\nlike a standard p3s installation\num I set up a cube bit which basically\ncan provide you with a highly available\ncontrol plane so a virtual IP for that\nin case one of the notes goes down and\nyou don't have the IP for the control\nplane anymore\nso this is taken care of and then\nusually SBS comes with an integrated\nload balancer\num\ncalled service lbs which ties right into\nthe lighting tool which basically is\nreally just a very simple load balances\nso to speak for services but for our\ncase we wanted to have something a bit\nmore flexible so we have natural LP\nrunning which is quite simple to set up\nand works really nicely\num for Ingress then we actually disabled\nthe so k3s already comes with traffic\nbut I've referred to sort of like\nmanagers on my own so\num we just deployed it as with our\ntraffic and then\num install traffic again for better\ncontrol uh yeah\num and then also usually k3s comes with\num\na local path provisioner for business\nstorage since I mean like for example\nfor the applied database we need the\nsystem storage in the cluster and that's\nsomething that you cannot easily be\noffered to an object store obviously so\nyou need something there and the local\npath provision is really just meant for\nsort of like a very small single\nmachines that are definitely explain\nacross some Integrations but thankfully\na lot more here\num also from Ranger is something that\nhas been working really well has\nautomated backup integrated and so on\nthat's easy to set up so I can hide it\nin that\nand then of course to our actual\num machines that handle the workload so\nwe have I think right now six servers\nwith various gpus that we had from\nprevious projects and so on\num as worker nodes and then there's also\nsome nice plugins\num so that in your device plugin which\ndoes not feature Discovery to the bins\nand all your nodes will be labeled with\nCPU and one property for us GPU\ncapabilities so that they're just\navailable as standard kubernetes\nlabels and you can use those in notes\nand lectures or in activities and so on\nwhich is what we wanted to use to tell\nwhich foreign\n[Music]\nthat will automatically\nadd tolerations for airplane so if you\naddress the GPU usually you add contains\nto nodes that only new workloads or\nbasically something that has this\noperational run there so this addition\ncontroller can do that for you so that\nyour users don't have to click here to\nspecify that important place or\nsomewhere else correctly\num and then of course you need to\nsomehow respect what your jobs are doing\nand have some metrics and so on and when\nthey're on they're up front of the\ngrafana agent works really nicely to\nship metrics to Prometheus that we\nalready had running here anyway\num and it also locks to low key and then\nyou can basically link that up so you\nhave a link in the site console and then\nwe'll take the final dashboard where you\ncan view the logs uh you know live or\njust for any previous job and makes it\nnicely searchable and so on and without\nall the overhead because I don't know\nputting it into elasticsearch or\nsomething like that\nall right now so\num how does our light workflow protein\nlook like\nso\nit's rather simple not very large\num this is basically just our existing\ncontainerized setup voltage to flight so\nin US container tasks since uh yeah we\nalready have it containerized and some\nof them don't run python so we run for\nexample some\num yeah synthetic Dave generation\num Denver and so on so yeah we just\nbasically have two through workflows one\nto generate the training scenes this is\nthe synthetic data generation in one to\ntrain the models\num and we put them together on a larger\nworkflow and on the side to that we\ngenerate some more test scenes that are\nused for evaluation\nthen we have a small task that just goes\nover some content files that we have in\nour\nall of these various use cases that we\nhave as test scenes and\nyeah we'll get a report out and of\ncourse the finished template and this\nalready is so much better to use in\neasier to use so we can just go to the\nflight and also start the workflow with\nsome free\npopulated default values for some of the\nthings of where to find the config files\num and it's already nicely schedules it\non three new machines so we have to take\nless care of who's using what\num\nsince can\nuh they're really attracted if your\ntasks are properly annotated with the uh\nrequired or the requesting and the\nlimits of your resources\num yeah of course this is not done yet\nso this is just the initial step\num we have a few more stuff like the\ncurrency it's not in slides or possible\nair is as well and then we want to use\nthe API instead of like build a very\nsimple\nweb page where you can just upload the\ncloud model and you can see backend it\nwill just take your step running into\nlight so you don't need to have Safelite\ndeveloper like access and not really\nwant to trigger when\num yeah we're also looking into\nleveraging Dynamic workflows and some\nmap tasks I think there's some issues\nopen there to support container tasks\nbut I guess so just figure that out soon\num\nyeah and then the one thing that is very\nimportant or interesting for us and\nthanks to them for a lot of work on hard\ntemplate support\nso as I mentioned earlier we have some\ntasks that we did give you some small\nones or none the none is easy to handle\num and since we don't have a very large\ncluster it's not in the cloud and we\ndon't cannot spin up stuff on demand we\nwant to really schedule this um so we\nvisualize our existing other resources\nwell\nand just running everything on the\nbiggest GPU possible that we have is not\nreally an option\nso the idea here was that we use we\nsomehow in such a node Affinity so this\nis basically like a not selected\nintelligent which not run but it has a\nmore granular ways to select which norm\nand it doesn't need to be a requirement\nbut you can also say at refer to run a\nspecific node and the idea is since we\nalready have kubernetes labels telling\nus for example how much GPU memory is on\na specific node we use that to sort of\nlike cell\non small tasks or faster needs less GPU\nmemory to prefer running on small gpus\nand some of them others will then\nrequire to one bigger GPU\num and also we need to set the runtime\nname for GPU tasks so if you run want to\nrun GPU tasks at least look at the cloud\nbut on your own so just like in Docker\num for\num whether container runtime you need to\nspecify with runtime to use and you can\nuse this between a toolkit this will\ninstall a very long time called Nvidia\nthat will take care of mountain drivers\nand stuff correctly exposing which you\ncan go into the pods\ninitially we set that as a default but\nit turns out that some workloads that\nalso run on the cluster now I would\nreally like to run the Nvidia runtime\nfor example one horn was behaving a bit\nweird\num so we've said it better yeah stay in\nthe safe side environment normal\nworkloads on the normal valency runtime\nand with only one want to run our actual\nGPU tasks using the media runtime and\nthat is now also possible and then once\nportable support has also completely\nvalid for Consumer tasks\num if somebody wants to go down the\nroute there would be an interesting\nAlternatives namely so I talked about\nthis custom resource operation or\nextended resource television admission\ncontrollable Force so that's basically a\nmodifying Network that will change your\num\nhot yummy essentially\num to inject or modify things\nso that it injects generations and of\ncourse it would be nice here also to use\na write a custom admission controller\nbut if you're requested GPU that will\njust automatically set the runtime name\nbetween media so you don't have to take\ncare about that in the site level at all\num\nbut the red steamed for me right now in\nthe beginning to just try to have some\naddition controller and\nso the idea was that we specified that\nit's important things\nall right then\num so we have still a couple of things\nto do to fully Leverage\nwhat flight already provides and so\nfirst of all it's of course to finish\nthe content integration we have a VR\nopen for the effective part and thanks\nfor a year\nthen to help out there to test that\nright now we don't have any\nauthentication set up so that's also\nnext in the scene will launch twitch\ntasks right now it's only a bunch of us\nwe're not very very small Engineers so\nit's not super important yet but of\ncourse that is very useful\num we're gonna see how we can leverage\nby text to show some intermediate stuff\nthat we need right now is new for\nexample biological notebooks of stuff\nthat is written to our cluster\num and I really really appreciate that\nthere's already support for signaling so\nduring the new tasks since we have one\nstep that is not in the flight workload\nwe have sort of like a conclusion step\nso that somebody demandingly server over\nit before it's sort of like that\num and then a lot of other nice things\nlike Android tracking Integrations\nand I'm really interested to see\nespecially how well now\nthis works it's sort of like the default\nkubernetes scheduler\num with our limited amount of service\nthat we have and as often we sort of\nlike\nput a couple of workflows in the queue\nbut we only have five to six machines\nthat we run them so that will take like\ntwo days until everything is done\num and we'll have to see how how well\nthis now works and we can use no\ndefinities or anything maybe you also\nhave to look into a bit more\nadvanced kubernetes schedules or how\nthey interact with the flight schedule\num\nthere's some nice proposals on the\nkubernetes side that might be worth\nchecking on how well this this works out\nof the box so we get good utilization\nand sort of like intelligent scheduling\nof what runs where without\num\nto look like blocking for example a\nserver with a large degree with a small\ntask and then just delay in everything\nelse\nall right and then maybe for flight\nfutures for speaking some things that we\nwould really like to see inside and we\nneed for some information you can also\nhelp them contribute a bit\num so far notifications would be super\nhelpful for us so we would really like\nto see support for web books that was\nalso already proposed and if I'm not\nmistaken right now notifications are\nalways important on the child platform\nso educating\nAWS\nand yeah also really looking forward to\nthe observability\nimprovements that are starting to pour\nin so we can dig a bit deeper on\num\nyeah how long is that huge um yeah so\nalso it's better get an understanding of\nwhere we need to treat things that we\nneed to split some tasks up into smaller\ntasks to run more of them in parallel so\nwhat's curing time and so on so that\nwould be\nreally looking forward to that part\nand then\nalso in relation to this super nice to\nstill see for example that the class is\nsort of like scheduled from the flight\nside but it's actually due in kubernetes\nright now you can actually see this in\nthe site console so you need to look\ninto your cluster to see that\num yeah and just some small items that\nwould really help us about to Max\ndevelopers that's another engineer a\nbetter idea of why some things are still\nnot the mute and the way they are cute\nI think there's also some\nuh PLS already there from again\num\npopular request was also that we can\nterminate individual tasks in the\nworkflow since we basically grab\nconflict excellent from flight from our\ncluster and sometimes just maybe we\nstayed there\num and you would like to just cancel my\nindividual task but not to cancel the\nwhole works over the course that would\nmean like several hours of tasks that\nare already running being canceled as\nwell we want to cancel that specific\ntasks\nlike update our content in the back end\nand then if I can rerun or another like\nwe covered the whole workflow again\nlater\nall right\nthat would be from my site\num\nI hope I wasn't going to this hospital I\nthink I'm wise is kind of okay\nso I'm happy to take any questions\nwe have a\nwell that was awesome thank you Felix is\nthere any question or comment\nuh I have a quick one but I think\nfirstly my name is Ethan by the way uh\nPhoenix great to see you in person here\num\nfirstly thank you that that the entire\njourney of how you discovered it and\nthen how you reached uh since it was\nreally helpful for the community because\nthis is a question that many people ask\nus and every time people share this like\nsecond is schedulers actually when we\nwere at Lyft we had a scheduler that\nworked with\nlike per se\num we never ended up open sourcing that\nuh\nbut I think that there are better\nschedulers in open source at the moment\nand I think we can create a\nsystem that works together much better\nso happy to collaborate on that that\nsounds amazing\nand web hooks I\nhave you looked into the event egress\nfrom flight and tried to see if you can\nuse that to create a Web book on the\ncustom side not yet that's on my to-do\nlist to look a little bit more um\nyeah first I want you to get our\nworkloads actually running properly and\neverything in and then it will work on\nthe notification part\nyeah and also upcoming in the next\nrelease is python like kids type\nback-end plugins so you essentially you\ncan write the backend plugins in Flight\nkit itself\nso you could have you could make a web\nhook as part of your workflow as a step\nif you really wanted which would it\nshould be written in Python and invoke\nany service center that you want so\nlet's let's discuss what would be the\nfastest way out love to see how you use\nthis\nyeah it would be interesting to see\num\nawesome thank you any other question\ncomment\nnope\nyeah thank you so much Felix I I think\nI'd like to collaborate with you to\nproduce a blog post or something similar\nto uh document\num your deployment because I I feel that\nwe have instructions for for local uh\ntest environment deployment with sandbox\nor production in the cloud but but not\nsomething in the middle that will be\nproduction in local kubernetes\nenvironment\nso definitely your your deployment it's\nit's very interesting so\nI will chat with you to see how we can\nexpand the message but you what you have\nalready done in this session is very\nhelpful thank you\nmy pleasure"
    },
    {
        "title": "Flyte Roadmap Update: Flytekit v1.4.0",
        "transcript": "all right the roadmap updates by Nils\nall right thanks David\num yeah so I'll be giving a few roadmap\nupdates on the flight OSS side of things\nnext slide please\ncool so we just released flight kit 140\nsuper excited for this release you can\ncheck out the full changed log in the\nlink provided over there and the flight\nkit releases section\num just a few highlights there are\nplenty of bug flip fixes and docs\nimprovements but the main ones here are\nthe ductibly integration the docs for\nthose should be coming in shortly\nanother huge one uh pod template task\nconfiguration\num big shout out to Dan\nhere in the union AI side for taking\nlead on a lot of this effort\num\nwe will be following up on this by\nadding support for pod templates on\nspark desk and Rey there is a PR PR up\nfor container tasks\num so uh keep your eyes peeled for that\num yes also a huge shout out to um Byron\nfrom LinkedIn on this effort\num\nwe made a few uh performance\nimprovements on flight console UI you\ncan look at the the PRS and the full\nchangelog there for more details and\nthen finally a community contribution\nby Steph lindahl for supporting\ncheckpointing and local local mode\nfrom cached tasks\nso that's 140 you can install it today\ncheck it out if you're interested\nnext slide please\noh the emoji for this didn't show up\nokay so for 150 the theme for this uh\nwhich is actually a common theme\nthroughout our releases is to improve\nthe fight flight kit\nauthoring experience\nso\num\njust calling out some these five items\nthere there's uh many more so you can\nsee the full roadmap and the link\nprovided here\num by the way I'll also be kind of\nposting the this these slides or the\ncontent of these slides up on the flight\nslack as well\num probably under announcements or\ncontribution the contribute Channel\num\nso 4150 we are going to focus on partial\ntasks this is something that will\num be useful building blocks in and of\nthemselves but coupled with map tasks\nthis will help improve the ergonomics of\nof both\nright uh reusing tasks partially binding\ninputs to tasks and reusing them in your\nworkflows but then also partially\nbinding inputs to tasks that you want to\nmap over so that um\nyou can avoid having to write like that\nextra task\num when preparing inputs for your map\ntasks so that should improve things by\nquite a bit\nthis third issue here we'll also need\nsome community help kind of compiling a\nlist of error messages that are perhaps\ncryptic or can be improved so\num yeah we'll spend maybe a week\num\nmaybe more uh depending on how how you\nhowever in the in the community\num\nlike how quickly these come in but if\nyou go to this issue 3404 I'll add a\nlittle bit more description on the the\nissue itself but um yeah we'd like to\ncrowdsource\nerror messages as I said that could be\nimproved and so\nthe goal here is to make error messages\npart of the kind of education process as\nwell so this will help especially new\nusers to flight if for example you try\nusing map tasks within a task things\nshould error out right so um\nso a lot of things like that giving you\nhints as to to how you should use\ncertain flight constructs\nI'm super excited for these next two so\nuh\nwe're not sort of committing to\ncompleting these things this release but\nwe are we just wanted to call out to the\ncommunity we are starting to work on\num first improved map construct with\narray nodes this is something that we're\ngoing to create an RFC for we'll post\nthis up on the flag the flight slack for\nfeedback\num\nyeah and we're so we're curious for your\nthoughts on this the the scope of this\nhasn't been fully specked out yet but\nroughly speaking at a high level uh\nimproved the way map map tasks are are\ndone\nand the last last point is improved\nsupport for name Tuple\num this is something that we've wanted\nto improve for a while but it's\nit has like\nkind of cascading implications\num\nin the back end as well so\nwe're going to write an RFC for this and\nand see what the the potential Solutions\nare\njust a called out call out here\num there's a the full roadmap you can\nsee in this link over here under flight\norg projects and I did want to announce\nthis RFC incubator section and flight\nthe flight discussions\num\narea so if we go to the next slide\nI'll\nactually yeah I'll talk about this first\nbefore describing what this RFC\nincubator is is all about\num so for flight 150 release there's you\nknow just calling out things uh General\nand then more specific things that we\num are\nuh asking the community you know if\nyou'd like to help out\num so The Source One there's this issue\nuh 2686 for support for pedantic type\npedantic type Transformer I understand\nlike there are many people in the\ncommunity who are sort of interested in\nthis and have maybe\nworking implementations of something\num on their side but yeah we would love\nif you could provide feedback and if\nyou'd be down to make a contribution for\nthis uh for this pedantic support that\nwould be awesome\num feel free to reach out to me or\nEduardo or yeah\num\nawesome Fabio yeah that would be awesome\num so let's let's talk in the issue or\non slack however you want to discuss\nthis\num yeah yeah if we could all put our\nheads together and like figure out what\nthe um the Upstream implementation is\num as I mentioned earlier the Pod\ntemplate support for desks spark and Ray\nthere's a PR up for the container task\nbut there um there are these three\nplugins that would benefit from the Pod\ntemplate the new pod template feature\num so there's an issue up for this\num\nwe will probably get to this on the\nunion AI side at some point\num soon but if you would like to\nadd your own contributions to this that\nwould be very much appreciated as well\num and then the final three are kind of\nmore General points\num or sorry no the third one is the the\nimproved flight could error messages\nthing that I mentioned before\num so as I said yeah if you can\nlook through like the comments see if if\na certain error message hasn't been\nmentioned yet it'd be helpful you know\nto add a code snippet of how to\nreproduce it what the error message is\nand you know ideas on what you would\nlike to see and that's the ideal error\nmessage\num and then the last two are\njust general\nhelp wanted things around documentation\nand good first issues\num\nwe will do a little bit of work to like\nmaybe clean these up and maybe\nprioritize them\num but if there are any of in the that\nthese buckets that you would like to\ntackle\num we would very much appreciate that\num\noh I forgot to remove this part so but\nin the last slide next slide please\nyeah so if you have a idea for a big\nfeature\num big being subjective here but uh\nif it's kind of like a core feature\nrequest that seems\nmeaty\num yeah we just started this new section\nunder the slide discussion section for\nthis RFC incubator\ncategory\num so as you can see here there are a\nfew here that I've written\num\nin the section so if you have if you\nhave any kind of idea that you think\nhave implications across you know flight\nkid and flight IDL Etc\num we kind of consider those to be those\nthose bigger features\num please feel free to to articulate\nthem here as a post\num will help you also promote it on the\nflight flight slack you can also promote\nit yourself on there\num\nDavid and I will be working on sort of\nlike an updated RFC process but roughly\nspeaking you know we'll have a deadline\nfor for comments and then we'll you know\nlock it down and probably convert these\ninto issues on the flight board\num and then they'll kind of like be\nadded to our Open Source process of\nprioritizing\num and so on\nand that's it for me I believe thank you\nawesome thank you Nils\nany question or comment around roadmap\nbecause I have one this one for Greg\nsorry to call you out but I'm curious\nhow a rastified error message will look\nlike I don't know if you yeah I just\nlike rust has very explicit error\nmessages and tell you like exactly what\nyou're doing wrong even like point to\nwhere you're doing wrong Python's trying\nto go this route but like that's just\nwhat I meant like I love using rust\nbecause the compiler tells you exactly\nwhat you're doing wrong and sometimes\nlike it can be a little cryptic that's\nwhere I'm in like rustifying meaning I\ntake inspiration from them because they\nhave done a phenomenal job there\nyeah it's funny I was I was talking to\nEduardo yesterday it was exactly this\nthat that Russ error messages are are\nlike the\nthe gold standard I guess\num not sure how far we can get on on\nthat with flight but yeah I think we'll\nwe will try our best there"
    },
    {
        "title": "Flyte Community Updates 031 featuring Flyte Governance Document",
        "transcript": "right welcome everyone to the flight\nCommunity sink today is March 7th\nand I'm really glad to see you all here\nI'm David Espejo\nI work as an open source developer\nAdvocate at Union AI\nand uh well happy to be your house today\num again a friendly reminder that the\nagenda\nand document it's completely open feel\nfree to add yourself to that in this\nlist including your affiliation\nuh it's totally optional so that's in\nthe logistics\nsize\nfor today's session does anyone want to\nbe the note taker\n[Music]\noh\nokay cool\num yeah I think we can we can handle\nthat with a recording meanwhile\num\ngood okay uh great thank you next one\nplease\nright some Community updates uh by\nmyself next one please\num great I mean I love this this portion\nI mean contributors of the month as\nevery other uh open source project\nuh pretty much is defined and sustained\nby the contributor community\nand uh from the past month we had really\nawesome contributions from Berner from\npajama\nhe basically in collaboration with\nflight maintainers he wrote the desk\nintegration and uh also he authorized\nblog posts covering all the details uh\nwe will make sure to share the links\nwith the recording also with the\nnewsletter\nso thank you Bernard I hope you are\nenjoying your swag\nall right next up the Dr Fabi Gratz from\nrecogni he also wrote a really great\nblog post covering uh the way the way he\nattached a visual debugger for ML\nworkloads uh with Ds code and other\ncomponents and and obviously flight\num and thank you thank you again for for\nsharing your knowledge\nawesome\num next up Nick Mueller from black shark\nuh he author what we think is the first\nuh RFC the first proposal that comes\nfrom the community around catch or\nwriting and deleting\num thank you so much for caring a lot\nabout the project and submitting a\nproposal\num so thank you for your contributions\nand finally Steph flindel from stripe uh\nthe author of PR adding support for\ncheckpointing local mode\num thank you thank you for your\ncontributions every form of contribution\nmatters so we really appreciate you\nthank you\ncool\nokay next up and yeah friendly reminder\nthat I've forgot to mention at the\nbeginning this meeting is being recorded\nand it will be posted on social media so\num three you don't mind about this but\nit's good to remind it right so a couple\nof things I wanted to share with you all\nuh probably three and one that is\ncompletely relevant\num I I always tried I sometimes I have\nto clarify why my handles are mirror I\nthink um the thing is that mirror is\nEnglish for my last name is\nSpanish for mirror so I'm I'm also David\nmirror it's completely accurate so okay\nnext up please uh okay now uh I will\nshare my screen I forgot about this real\nquick here\num there you go\nyeah\nokay there you are all right I'm really\nhappy to share this with you\num it's a new governance document for\nflight\nso what is governance in open source\nit's basically a place where we document\nthe roles\nresponsibilities\nand in general the way a community\ncommunicates and makes decisions that's\nbasically the kind of the design or the\narchitecture in general\nof an open source community\nand with flight we had some\num design principles in mind to try to\ncome up with with this uh proposed\ngovernance model uh in general it's how\ncan we\nensure rapid Innovation but at the same\ntime how to handle chaos right because\nas as a project evolves and grows chaos\nbecomes bigger and more evident and cool\nhampering Innovation so how how can we\navoid red taping or adding unnecessary\nbureaucracy but at the same time we\nprovide order structure and a sense of\nownership a path to\nhigher responsibilities and higher\nprivileges in the community\nso the result is this one\num as you can see in the community\nReaper under flight org\nand here you will find the roles defined\nhere the responsibilities ways to\ncontribute how we envision or Define a\ncontributor a maintainer and finally the\nsteering committee who is basically the\nleadership\nor the governing body who who will make\nthe decisions that will probably change\nthe direction of the project and and\nwill ensure the sustainability of the\nproject in the future\nso I'm really happy to see\num in this case we are in the process of\nof building the technical steering\ncommittee but I'm really happy and and\ngrateful to see Dr fabiogratz in Bernard\nwho have stepped up to be part of of the\nsteering committee alongside with Folsom\nUnion Ai and\num hopefully for from other\norganizations because what we want to\nhave is a leadership\num or a governing body that is diverse\nenough to represent the the interest of\nthe wide majority of the community\nright so all these sound all these cool\nsound as politics\num but it's necessary to ensure the\nagain the scalability it's probably it's\none thing to a scale up a community\nadding more members Etc but what a\ndifferent total different thing is to\nscale out to to be able to make sure\nthat in the future we will be able to\nmeet the needs and and the requests from\nfrom the community so feel free to check\nit out this is work in progress\num probably the next steps the couple of\nimmediate steps will be first as Neil\nsaid review the RFC process because one\nof the main responsibilities of the\nsteering committee will be to vote on\nproposals and the RFC needs to reflect\nthis make it more transparent and make\nit clear for anyone's millionaire\nproposal what to expect and when to\nexpect it\nand the other immediate step probably\nwill be to\num Define what we will call special\ninterest groups or working groups uh to\nhave more Focus work for example Gather\nin a single place everyone thinking\nabout Integrations the flight ecosystem\nof Integrations\nuh or any other use cases\num will be in the process of with the\nconsensus of the community defining\num these these structure so\nagain uh if there's any question feel\nfree to ask\num\nand um yeah again it thanks anyone who's\nstepping up to help\nlead this project any question any\ncomment about governance\nnope\nright oh\nwell next up\nwe have a new section in this community\nsink that we introduced this like a\nmonth ago and we used to call it\nin-flight conversations because we\ndidn't have a better name uh we agreed\nto call it lighting talks because it's\nmore standard when we say lighting talk\nprobably everyone here knows what to\nexpect something that is short and sweet\num it's totally open for anyone out\nthere in the community wanting to share\nsomething even without slides you you\nprobably know something that someone\nelse in the community needs to learn I'm\npretty sure about this so I'm I'm just\nkicking off this section and if you want\nto get rid of me in upcoming sessions\nplease propose your own sessions in this\nplace is totally open for you\nso in kubernetes Ingress yeah and the\nparty value proposition of flight is\nthat we abstract away many of the\ncomplexity or the details of kubernetes\nconfiguration that means there is a very\nopinionated platform but there are some\nresources that are exposed one and that\nare exposed and one of them is the\nIngress resource uh loved by some Hate\nby many uh but um it's necessarily\npurling less than 10 minutes I will try\nto introduce why this resource exists\nhow does it work and I will expand more\ndetails in an upcoming blog post for\nsome of you this will be basic and sorry\nabout that I will I'm just trying to set\na baseline of knowledge for anyone out\nthere\nright so imagine that you have a\nkubernetes cluster with a single note\nsingle worker node a single path\nremember the part is the fundamental\nunit of execution in kubernetes and\ntypically there's a single container in\na path right pretty simple\nuh what happens by default is that first\nkubernetes will assign an IP address to\nthe path that's first thing and if you\nhave an another node in the cluster and\nanother path with another container\nthere\nkubernetes will not only assign a\ndifferent IP address in the same network\nbut will also enable these parts to\ncommunicate with each other even\nthroughout different notes that's the\nrole of the\ncontainer native interface contain\nnetworking interface or cni Plugin and\nEtc but basically that's what happens\nyou will assign IP addresses and it will\nenable this communication between ports\neven throughout different notes\nso if that's uh something that happens\nby default why do we need something else\nwell the reality is that you will most\nlikely will have different applications\nwith multiple parts and uh at that point\nit's it becomes\nimpractical to connect to the\napplication using an IP address because\nremember that parts are designed to be\num short-lived so when a pot dies or is\ndestroyed and it's really instantiated\nredeployed it will receive a different\nIP addresses so it's not practical to\nuse IPL racing anymore so enter Services\nservice is a core kubernetes resource\nthat basically sits a layer above of IP\naddresses and it will provide a central\nlocation for anyone out there trying to\nconnect to specific Bots which Bots will\nbasically the pots that will match uh\ncertain criteria here and in the right\nhand side we can see the spec of a\nreally simple service that will have a\nlabel as selector so any pod labeled as\nmy app\num at that kubernetes at iOS like name\nequals my app uh will be uh let's say\ncapture recovered by this service\ndefinition why does what does a service\nit's basically a load balancer or um or\na resource that will receive incoming\nrequest through a specific protocol and\nspecific port and it will forward those\nrequests to the pots matching the label\nin this case in this case uh connecting\nto a specific Target Port right so the\nservice in this case is exposing uh the\nad Port HTTP port and the pods are\nexposed in the 93 76 Port so that's it\nit's it's I mean it's good\nuh it comes by default with kubernetes\nso again if it if it works if it's a\nload balancer why do we need anything on\nanything else well again probably you\nwon't have a single app in your\nkubernetes cluster you will have\nmultiple apps so for for each app you\nwill have a different service\nand this could create a couple of\nlimitations for first one\num as I said the service is provides\nbasic routing functions basically\nforwarding incoming requests and that's\nit and if you're you're running this\ncluster in the cloud for every service\ninstance if you're using the load\nbalancer type of service probably you\nwill incur additional cost for every\nload balancer instance doesn't come from\nfree\nso it can quickly become cost\nprohibitive while not providing Rich\nnetworking services for applications so\nyeah we need something better something\nthat sits a layer above again uh not IP\naddresses not TCP ports but at the\napplication layer and to be able to\nprovide the connection points for users\nor applications so enter English\nso the Ingress uh the the in in fact the\nterm Ingress comes from networking\njargon from several years ago that\ndenotes incoming traffic there's another\nterm for outcoming outgoing traffic\nwhich is egress\nso the Ingress resource handles incoming\ntraffic purely right so\num it's different from Services because\nit seats the layer above and and how\ndoes it look like here's a simple\ndefinition and here you can see that we\nare not uh capturing requests coming to\na specific\nTCP Port but through through a hostname\nright in this case bookhotels.com so\nanything coming through booked\nhotels.com slash whatever will be\nredirected to a service so we we don't\nget rid of services we still use\nServices as the back end component of\nthe increased resource but this is why\nwe need an uh qualified name in this\ncase bookhotels.com is is a domain name\nor a qualified name because Ingress\ndon't operate layers below by using IP\naddresses or anything like that it\noperates at now the application layer\nusing host names\nright so um there it could seem like\nit's very similar to and to a service\nexcept by the protocol and that's it but\nthere are some differences because you\ncould use what what is usually called\nsub domain routing uh you could have a\ndifferent\num a different domain for example uh in\nthe same Ingress resource you could have\nbookflights.com a different domain and\nthat will be redirected to a different\nbackend service which is flights a\ndifferent service and everything is\ndefined in the same Ingress resource so\nyou were able to centralize uh the\ndefinition of your forwarding or routing\nrules in a single resource totally\ndifferent to services\nuh so that's great but Ingress is one of\nthose resources that the kubernetes API\nuh will will fulfill but uh not entirely\nuh for for the Ingress to actually work\nyou need an increase resource definition\nbut you will need an Ingress controller\nwhich is basically most of the times a\npot drawing your cluster that will\ncapture uh the configuration that will\nwatch the configuration in your English\nresource and it will translate it into\nreality right it will become the data\nplane in some sorts the Ingress resource\nwill be your control plane and the\nIngress controller will be the data\nplane who will in the end of the day\nwill forward or route uh the request to\ndifferent Services you can use different\nIngress controllers this is not\nmaintained by kubernetes you can use\nnginx very popular the one that ships\nwith with the flight um\num single binary by default it's nginx\nbut you could use some others uh I I've\nmet users who even have\num service mesh which is even a layer\nabove or with a wife coverage much more\nthan ingresses but here and we are just\ndiscussing English so you will need an\nIngress controller this is why you will\nneed them um to be able to fulfill or or\napply in reality what is defined\nincreased resource\nright\num so yeah that's it I think it's time\nfor me\num what I\ndo awesome what I I just saw the chat\num what I plan to do is to extend this\nuh into more detail in a blog post also\ncovering some recommendations on how to\ndo it in different\num clouds I see that there are some\ncommunity members that could collaborate\nand bring their experience totally open\nto write this blog post with you all it\nwill be great\nand probably will help some other\ncommunity members to find guidance on\nhow to\num make decisions on how to set up and\nget up to speed with Ingress\nright any question or comment\nawesome that's great I I haven't read\neverything that's in the chat so\num\nall right awesome\num okay any question\noh\ncool uh we will just end up with links\nand resources uh heads up that office\nhours\nuh move to an on-demand format so if you\nwant or need\nlife support from flight maintainers\nfeel free to schedule a session using\ncalendly these lots are the usual ones\non Wednesdays but it's now moved to an\non-demand format so feel free to\nschedule a session with with them\nall right and for the next one two weeks\nwe'll have some from sweep\nTechnologies and some other updates for\nsure\nso with that thank you so much for\njoining this session and hope to see you\nnext one\nbye all right thanks everyone thanks bye\nbye have a good dresser today"
    },
    {
        "title": "Flyte Community Updates 030 featuring Blog authors Dr. Fabio Graetz and Bernhard Stadlbauer",
        "transcript": "good morning welcome to the flight\nCommunity sink my name is Martin Stein\nI'm with Union Ai and I'm your host\ntoday today is February the 21st time\ngoes by fast and we have a packed agenda\nlet's take a look at the agenda and at\nthe housekeeping notes that I have to go\nthrough to make sure that we're on the\nsame page first of all housekeeping\nnotes last week actually two weeks ago\nwe have been on Riverside and after\nreceiving some feedback we made a\ndecision to go back on soon\num assume is just a lot more interactive\nand allows us to to really engage with\nthe community and that's why we want\nzoom that's number one number two\num this Zoom call is recorded uh we are\ngoing to share it online on YouTube and\nthe YouTube channel flight YouTube\nchannel and\num this is what we see here from\nAndrea's updating slides\nokay we're gonna get to this in a second\nand we have it on YouTube so you can\nactually go to YouTube on the flight\nChannel and always get an update about\nthe latest recordings and see actually\nprevious recordings as well\num last but not least it's an\ninteractive format if you have comments\nplease chime in when you chime in\nintroduce yourself obviously I need\nyourself introduce yourself and ask you\na question in a precise and concise way\nand to make sure that it's clear to each\nand everyone on the call\nwith that let's take a look at the\nagenda so we have\nlogistic updates today Community updates\nit's going to be very interesting we\nhave Fabio and Bernhard speak about the\nlatest blog posts I think they're both\nfantastic we have David espario we'll\ntalk about the flight governance and the\ndocument and also David is going to give\nus a lightning talk with that let's take\nstep four look at the logistics the\ncouple of things next slide please\nbut a couple of things we want to talk\nabout on the logistics side first is we\nhave an open Agenda and meeting nodes so\nthere's a document we put a screenshot\nright in here\num that is open and there's a link\nso now you can access this document\nbefore the meeting after the meeting\nyou can act yourself as an attendee\nwhich is very helpful if we go back and\nhave conversations you know which\nmeetings you attended also for us as a\ncommunity it's easy to see who's in\nthere but it's completely voluntary to\nfor you to add yourself to the document\nthat's number one number two is\nwe would like to go to a\nCommunity Driven agenda at least have\none item on the Community Driven agenda\nas well which basically means that you\ncan go in there and suggest\nuh topics uh in the open Agenda we don't\nhave an area right now where this is\ngoing to happen or could happen but we\nwould say just you know add an area for\nyourself say suggested topics we're\ngoing to review them most likely David\nespeka will reach out to you and clarify\nwhat the agenda item is and then we get\nit set for the next meeting and last but\nnot least I want to talk about volunteer\nnote takers it's always good to have\nnotes of each of our each and every one\nof our meeting attendees to go back and\nsee what happened and so today we start\nwith volunteering note takers and my\nquestion for the community is who would\nbe our first volunteer today to take\nnotes for this community sake\nsee not so many\nI guess if nobody does then I will ask\nDavid can you be our Note Taker for\ntoday David\nyeah sure I can do it okay fantastic\nlove it okay cool\num so let's go on to the next slide\num so we're going to talk a little bit\nabout some of the blog posts that have\nbeen put forward one of the very very\ncool blog posts and the pieces of work\nthat have been done uh is by uh Fabio Dr\nFabi press I should always say uh your\nfull title pop you appreciate it so\nPablo please\num introduce yourself\ntell us uh you know what your\naffiliation is with Rick hockney and\nwhat you've done on flight and why\npeople should actually take a look at uh\nyou know debugging ml training runs uh\nwith the technique that you're proposing\nI think it's very interesting go ahead\nso yeah um thanks for the introduction\nMartin I'm Fabio no need for full titles\nhere\num I've been a flight user for\nand one and a half two years I would say\num use it at my previous company where\nwe introduced it after doing comparison\nbetween different orchestration\nFrameworks\num and now at my current company\num recognize builds perception systems\nfor autonomous driving\num very fast accelerator chip and we\nhave perception teams that develop kind\nof perception models for for this chip\num and they are my client I'm an email\nOps Team and I run a platform or my\ncolleagues and I run a platform for them\nthat they use to develop these\nperception models\nand um yeah one thing that\nI've seen often in like not not specific\nonly to to flight but in general when ml\nEngineers move their trainings to the\ncloud that they revert back to like\nprint debugging and when developed\nlocally they put a breakpoint in their\nIDE\nand use visual debugging but then some\nthings you can only debug when running\nin the cloud for example when you need\naccess to gpus which you might not have\nlocally on your development machine\nor maybe access to data that you can't\nhave on your development machine like\nthere's some scenarios where you do need\nto debug the actual job or workflow\nrunning in like the actual environment\nand then something that often happens is\nthat you put a print like reach this\npoint we just reach this point that's\nthe value started then realize how damn\nit failed because of that and you put\nanother print statement to start it\nagain\nand\num of course things like Fast\nregistration or so they make it make\nthat quicker but still like it's kind of\nan iterative process that's that's\nannoying that is necessary and\num I wrote a blog post about how this\ncan be done faster using a library\ncalled debug Pi which is from Microsoft\num can I share my screen here or can't I\nyeah totally I can then um maybe I'll\njust show quickly that makes it clear\n[Music]\num\nyou can't see my screen now yeah\nOkay cool so in the blog post I\nbasically show how to get a simple\nflight setup\num but I think you all know how that\nworks I'll show it like a quick workflow\nthat uses some sqlr and data set to do\nvery simple training and then how to run\nthat and then\nto have kind of a like a setting where\nwhere we can debug something\nand um you need to install this library\nfrom Microsoft called debug by for this\ntechnique to work and then in your\ntraining at the point where you want to\nattach your visual debugger if you\nimport this library and you start a\ndebug server you wait for for a client\nto connect and then set your breakpoint\nand then your your flight workflow will\nstart a little like run to this point\nand then it will wait for the client to\nto basically attach\nand then in the blog post I explain how\nin vs code you can attach to this debug\nto this debug server by like\nthe PS code users probably know how this\nworks um by creating like a launch Json\nfile\ntelling it to attach to remote process\ninstead in here and where this process\nor where this debug server is reachable\nso it's reachable and localhost and the\ndefault part of the bug Pi so you need\nto make\nuh this debug server available on\nlocalhost but you can do that simply by\nport forwarding\nthe next thing that we will do is after\nstarting\nthe workflow is to figure out the name\nand then port forward\nand then you can in vs code just press\nthe like the start button to debug\nand your visual like your visual\ndebugger will attach to the running\nworkflow in the kubernetes cluster which\nis pretty cool\nbecause then you can inspect the objects\nthat you have here you can step through\nyour code\nand don't have to rely on putting print\nstatements and then pivot run again and\nthen putting another print statement and\nthen do python run again\num and like there are situations where\nthis can really save time where you\ncan't debug locally because there are\ncertain things that you can only debug\nin the cluster for example when you need\ngpus or some other reasons and this way\nyou can have you can have the benefits\nof visual debugging that you normally\nonly have locally but the thing actually\nruns in the cluster which in some\nsituations has saved me quite some time\num one thing that I want to point out is\nthat there's a library from Google or\nit's a plugin for vs code called cloud\ncloud code I think\num that takes care of this port\nforwarding part but it relies on the\npredecessor of the bug pi forgot the\nname of it\num\nin the future maybe they support this so\nthis becomes even easier\num but I think also part for awarding\nthe debug server is not not a big hassle\nfor the benefits that you gain\num so yeah that's the the blog post in a\nnutshell if you have any questions about\nthat about the method\nawesome anybody having any questions\nanother thing Andreas likes it I like\nthe Clapping interest\nwho's actually just uh somebody's\nclapping there certainly like it good\nstuff so I think\num\nwe should probably make sure that we\nhave David a link on on the flight\nCommunity page Caitlyn yes you have a\ncomment real quick\num uh David we should probably link to\nto that uh very important blog post I\nthink this could save ml Engineers a\nwhole lot of time on the debugging\nprocess so you should really make sure\neverybody can get to this okay then you\nhave a question\nyeah\nthis is great by the way when I saw this\nthis was amazing uh\nI\nI had a quick question or a comment uh\nfor the folks who are not as adept\nas finding out\nI guess you have to find out the part\nname Etc\nwould you recommend us adding something\nin in Flight kit remote that could\nsimplify this\nyou mean to figure out the pot name yeah\nand so that like you know even you could\neven completely port forward\nautomatically but I I don't think you\nshould do that because that kind of\ncomposed you with the back end but at\nleast you know simplify some of them\nyeah so if you understand your question\ncorrectly is\num whether it should be simpler to find\nthe plot name so that you can port\nforward than having to use to control\num I mean the so our Engineers they that\ndon't use Cube control they use the\nstackdriver loggings that you can reach\nfrom from Flat console and there in the\nquery you also see the Pod name okay so\nthat's a quicker way to get to it\num and honestly would say it doesn't\nhave to be easier than that I can it\nstill relies on you being able to port\nforward\num yeah which is in that part so for\nthat you can like for that I always use\nflip control I think there are ways when\nwe have code specifically to to port\nforward from pods only by using some\nplugins to bring these plugins\num\nI don't know if it has to be simple than\nthat I think the Pod name you can figure\nout rather easily from going to the like\nstackdriver logs if you have them\nconfigured\num and also first from the execution ID\nwhich is in Flight controller so Dave\nhas an idea so we actually can put links\nright for log links I don't know if it's\npossible to put a code server link uh\ninto the task you don't have to even be\nin the task limitation that's where it\nachieve the log links are completely\nconfigurable and they're arbitrary\nbut I don't know if code server has and\nlike a deep linking system\nI don't know Steve do you want to\ncomment maybe on mutants\nhey no I was just kind of throwing this\nidea out because like you can run\num so this so Fabio's running like vs\ncode on his laptop or you know his local\nmachine assembly uh you can actually so\nwe run a code server which is like this\nopen source uh like browser-based like\nIDE\num and we're we're actually you know we\nread your blog post off you were like\nsuper impressed by it and we were\nplaying around with some ideas locally\nand since we already have like code\nserver running on\nuh our Jupiter Hub like platform uh we\nwere looking into ways of kind of just\nhooking this up directly into like a\ncode server uh debugger essentially and\nso then like you know\num like you're not support because\nyou're running on the cluster and you\nhave access to like the Pod IP already\nbut I was wondering if like there's a\nway to you know like have a link that\nlike will start from some portal a code\nserver instance that allows you to kind\nof like debug this but obviously you\nknow it has all the shortcomings of like\nnot having your local IDE set up and\neverything right but uh just uh just a\nwild thought\nit's pretty cool\nfantastic you know let me ask you one\nthing probably uh I like your medium\nprofile bespoke shoemaking uh what's\nwhat's easier coming up with Solutions\nlike this for debugging or bespoke shoe\nmaking I don't need to answer but I\nthink this is pretty cool awesome\nactually making is very tricky I learned\nit for years doing my masters and as a\nhobby it's very difficult okay good I\nlove that we have the easy lights and\nneutrals here then instead we've got to\ndo a separate talk about if you smoke\nshoe making in the future okay cool\nawesome thank you so much Fabio this is\npretty cool keep it coming I think we\nall love your contributions and your\ncomments and um you know just gonna say\nthank you in the name of uh from the\npoint\nawesome let's move forward uh let's\nswitch back Tyler if you can go back to\nthe screen we're gonna go to Bernhardt\nso bernhoftauer how you're here I assume\nuh yeah I'm here awesome hi Bernhard so\nyou wrote a pretty uh interesting uh not\nonly blog post but I think you also\nworked on the integration with with desk\nuh so tell us a little bit about why it\nmatters and obviously you'd ask us is a\nhas been a big thing but you know tell\nus what motivated you and what people\nshould know about the integration and\nthe blog post\nso hello my name is Bernard\num I've also going to use a flight for\nthe past honestly two years roughly\num and also compared to bring against\nother other orchestration Frameworks and\nI liked it and still still do\num\nbut lately our\ndata scientists and machine Learners\nhave been wanting to use use tasks for\ncertain workloads\num especially to work in the geospatial\ndomain and and dusk has gotten a lot of\ntraction there with a few projects\ncoming up that manage large spatial\nworkloads and they mostly related task\nas a backend\num so we figured why not try and some of\nit has been working quite well for us\num\nand then having seen that there's\nalready a smart integration array\nintegration and all of them having a\nvery similar compute model\num yeah I thought why why not add a task\nplugin and the tldr of it is\nusually it's very tricky to manage\nmanage task clusters similar to How It's\ntricky to manage spark classes or a\nclusters because you want to have the\nsame python environment at the same\nDocker image everywhere and ideally for\nproduction use cases you want to spin up\nthe cluster run a workload and enter it\ndown again so that you can make sure\nthat all the dependencies are the same\nso\nthat's why we figured okay if there if I\ncould handle all the infrastructure\nconcerns for us spin up the cluster\naround the workload tear the cluster\ndown again that would be pretty nice and\nwe first started off with the flight kit\nplugin that had some issues in terms of\ncleanup so if certain pots went out of\nmemory that that's the cluster didn't\nclean up nicely and so we we figured\nwhere I figured I could contribute that\nI can plug in and go in that's basically\nwhat the blog post is about the blog\npost is on the task site so it focuses\nmore on the benefits of flight rather\nthan on the benefits of task um because\nuh I want to get people familiar with\nthe flight\nbut yeah we chose a small example of how\nyou how easily you can spin up a a task\nquestion\nfantastic we love it any questions or\ncomments for Bernard\nuh hi uh this is Steve again I just had\none question so correct me if I'm wrong\nit sounds it sounds like it's this\nstarted off like as a flight kit plug-in\nbut it's now a back-end plug-in as well\nand so like flight will fully manage the\nuh like the cluster and everything at\nthis point\nexactly so so this started off as a an\ninternal project uh plugin that we just\nput together in half a day or so\num and it didn't do much right there's a\nkubernetes operator for for tasks so we\nalready leveraged that in the\nuh in the the fight kit plugin but we\nhad some some issues\non on OMS and another like edge cases\nthat the cluster wouldn't clean up so it\nwouldn't remove the the custom resource\nfrom\nfor the last cluster\num and now the the backend plugin does\nthat nicely\ncool Ethan you have a question too\num hey thank you hi Bernard the thank\nyou for the contribution it's amazing\num quick question so just ask need gang\nscheduling\ndo you know that excuse me what was that\ndoes desk uh to complete the work does\nit need gang scheduling or are there\nlike a resource automation concerns uh\nlike for example the worker group if you\nhave an arbitrary sized worker group\nwhat happens if all the workers are not\nallocated is that okay is that that is\nokay uh that is okay so basically the\nthe way the compute model works is uh\nyou have to bring up a scheduler in our\ncase we also have to bring up a client\nso basically we bring up two parts one\nsimulated client and one scheduler they\ncall it a desk job in their terminology\nand then you can bring up different\nworker groups and whenever a worker\nstarts up it connects to the scheduler\nand it's resilient to when a worker dies\nit's not great sometimes because\nsometimes the worker has some State\num so that's why they also don't rely on\nkubernetes auto scheduling but in\ngeneral it's fine if a worker dies and\ngets restarted somewhere else and also\nif a worker like in case of multiple\ndesk jobs or desk and Spark and bats can\nthey all of these things working now in\na flight cluster right\num it may be possible that you may not\nget a resource and in those cases\nthere's some starvation concerns that\ncan happen right yes there are some\ncertain timeouts we also ran into the so\nwe run into those\num there are certain demos you can you\ncan set we might want to change the\ndefaults in Flight here a little at the\nmoment it's somewhere around dumping me\nto half a minute a minute or so and of\ncourse if you don't get any workers or\nespecially if the scheduler doesn't come\nup\num then you get issues because then the\ntasks can connect to the scheduler\num and so we might want to tweak a few\nthings there we're currently ramping\nthis up in production and so far it's\nbeen okay but then especially on high\nworkloads we have seen some some\nflakiness yeah there are a couple of\nthings that we have been working on uh\nessentially within the scheduler\ntweaking so uh with the kubernetes\nschedulers right so we can think of\nshare notes absolutely uh and one last\none so you mentioned you started with a\nflight kit plugin I know if like it\ndoesn't have the terminate handle today\nI think a lot of people have requested\nand I think we should be implementing\nbecause we do send us again\nEtc would that still have been a\npreferred method out or would you would\nyou do you prefer now the back end plug\nit was a lot more work definitely so I\nspent quite quite some time I also have\nnever worked with gold before so it was\na kind of a personal project that I just\nwanted to get get into something new\num but\nit just feels way way nicer and cleaner\nbecause\nthe resource cleans up where you should\nclean up um I feel like that's that's\nthe place it belongs to I'm also not\nsure when you get an out of memory for\nexample on the\nthen on the the client basically which\nis what we've used for the if I keep\nplug-in I'm still not sure if things\nwould clean up nicely probably not we we\ntried a couple of things but we couldn't\nreally get it to work actually yeah yeah\nyou're right actually so we are working\non uh I think I don't know if you've\never seen the RFC I would highly\nrecommend Fabio or not everybody look\ninto the RFC for out of core plugins\num these are language agnostic plugins\nthat are coming into flight soon\nhopefully in the next two months or so\nuh you should be able to write them and\nwith the python first course native\ninterface and the goal there has been to\nmake everything run locally as well as\nremotely and for people who are not\nreally familiar with Google use python\nor any other language to write their\nplugins in and drop them really without\nimpacting profile another important\nthing over there latency and making sure\nthat propeller is safe uh but you know\nplease comment on it you would love to\nhear more thoughts and ideas and\nquestions Kevin from the union team has\nbeen doing performance benchmarks\ninternally to make sure that it works as\nwe expect but I think he should have\nmore to share soon\nokay cool\nawesome thanks Caitlin Fabio you had a\nquick question let's see yeah I have one\nquestion like to UK um\ndoes that mean that also the the plugins\nin propeller and go have their own\ndependencies then\nbecause there are many plugins that\nmakes dependency management a bit\ncomplicated sometimes right\nthat is absolutely right this is one of\nthe reasons why we wanted to do\nout-of-core plugins right but when we\nfirst started out-of-core plugins is an\nover engineered Overkill right yeah as\nthe number of plugins has increased we\nstill like I don't think the\ndependencies have been that bad to be\nhonest like I you would ask Bernard any\nother task but I think you know where we\nreally see the biggest problem is the\ncube client dependency management system\nis like people some of them use really\nold ones and don't upgrade Etc and that\ncauses a problem so that's where we want\nto move finally all plugins to an out of\ncode model awesome uh but we wanted to\nmove quicker way and that's what we did\nis the web API plugins but let's take it\noffline happy to discuss and share more\nbecause the kubernetes dependency also\ncustom headaches for me when trying to\nlike upgrade some some of that\num yeah the auto score is just a service\nso completely independent processes cool\nthanks awesome thanks Jason for the\nexplanations here and Bernard I think\none one request that we have when you\nget feedback from from actually the best\ncommunity it would be fantastic to\nconsider a potentially a session for\nflight for desk users uh we can actually\ndo this here as well I would love for\nyou to let us know uh you know what the\ninterest is and we can talk offline but\nlet's make sure we get the task of folks\nand those communities a little bit more\nengaged with flight awesome great stuff\nthank you very much okay cool links and\nresources real fast just want to make\nsure we don't forget about links and\nresources uh you know you know same same\nprocedure every two weeks we show you\nthe links and resources you can connect\nwith Eduardo Katrina samita take\nadvantage I think this is really what\nwhat certain we're speaking about in the\ncommunity and also the contributors here\ndo this it's really super valuable\num next slide is uh then the next\nCommunity thing I think March 7th with\nFelix from reception it's going to be\nsuper interesting again we're back on\nZoom as you noticed and last but not\nleast there are a couple of things you\ncan do check out the YouTube channel\nlook into the newsletter flight monthly\nand if you're not on slack now join us\non slack so with all of that I just want\nto say thanks to all of the community\nmembers and the presenters today this\nwas fantastic have a wonderful day and I\nsee you in two weeks from now bye-bye\nyou too have a great time thanks bye\nthank you"
    },
    {
        "title": "The perfect GPU for you - GKE node-autoprovisioning with Flyte at USU AI Services",
        "transcript": "perfect perfect thanks\nyeah so um I'm seven\num I'm working at um\nUSU AI services and\num yeah I just want to show you what we\nare how how we are using flight and how\nwe leverage gpus on\ngke\nso\num\nlet's get started\nso just uh real quick what we are doing\num\nso\njust use a company that does\nEnterprise service management\num so a whole bunch of\nServices mostly for other corporate\ncustomers managing\ntheir software assets and their their ID\nservice ticketing systems and so on\num\nand all these things\num for all these things we are we are\nmaking them more intelligent we apply\nmachine learning mostly to\num to automate all the the boring tasks\nand to make these things\nnicer to use and\neasier for\nthe customers\nand we actually\ndiscovered flight quite early on\num\nshortly after it was open sourced and we\nwere looking for a um\nworkflow engine at that time for a\ncustomer data platform so we tried\nflight and were looked quite nice even\nthough\nthe documentation at that time was the\nnot in the shape of this today\nEarly times but\num yeah we were convinced it's the right\nthing for us but we for for different\nreasons not nothing to do with flight\num that actually The Artisan plant\nactually didn't pan out but we have to\ncreate the first version of the of the\nhelm chart\nand got to know uh flight and\nthen realized wow this is really cool\nalso for our in-house use cases and now\nare using flight successfully\nfor all kinds of machine learning use\ncases\num\nso but how do we actually use flight so\nwe have essentially two\ntwo branches\num one is\nlike yeah really the the Opera Opera\noperationalizing\num site\num getting\num\nml training done and making it robust\nand\nyeah really making sure that things are\nworking in production uh\nand this is where flight really really\nshines and really great and it's working\nvery well for us\num so the other the other thing we\nactually\nstarted trying\nyeah not so far ago it's really\num trying flight also for\nfor the earlier steps essentially for\nthe for for\nmachine learning and data science\nexperience\nand um\nwe are using flight there also\nsuccessfully um\nand I'll show you a little bit later on\nhow how we use it and what's what are\ndata scientists scientists say about\nusing flight for experiments\nso\num for\num the the upper rationalization\num I want to just show you one example\nwhere we're using flight\num for training\num so this is um\none one of the services used provides\nthis software Asset Management\nand basically\num\nin a nutshell it means\nmanaging all your your software assets\nand knowing what kind of software is\ninstalled on on all your organizations\nmachines being in the server or desk\ndesktop machine\nand um this can help you do all kinds of\nthings of course manage licenses is one\none important thing\num manage and optimize so you don't pay\ntoo much or you comply into to any\nlicense and so on but also reducing\nsecurity risks\num because you have some old version\num you don't want\nand do Audits and so on\num and the challenge or one challenge\nwith software asset management is that\nyou need to\ncollect all this data about software so\nusually you have some some agents\nrunning on on your machines that collect\nthe data\nuh and um put it into some database and\ncollect all the\nwhat kind of software it is of course uh\nwhat what version what which Edition uh\nand so on all everything that's\nimportant for for instance licensing or\nother things in software Asset\nManagement\nand as you can imagine although it is\nusually structured data and\num it's quite heterogeneous and and\nsuper messy\num because yeah basically\neverything can be in there and it's also\nof course different depending on the\noperating system where\nthe data is tracked or is it really\ntracked at all in a structured way what\nwhat software is installed on that\nmachine\nand of course it's a lot of data\num millions of data points especially if\nyou have\nlarger organizations and of course\ndifferent organizations\nand the challenge is really to to map\nthat that messy data into something well\nstructured to some some kind of catalog\nthat you can actually use\num to get the information you want right\nand um\nyeah you can imagine if you have\nmillions of data points trying to do\nthis manually or even with some some\nsome rules doesn't doesn't really scale\num so that's where we come into play\num uh helping\num\nto to automate uh this thing through\num deep learning so what we're doing is\nwe we essentially take that that messy\ninstallation data and\num we filled a\nquite large very specialized uh\nTransformer model\num\nthat Maps this this data into into\nsomething nicely structured which we can\nput into the catalog\num so the nice thing is this is a\nwe have a lot of lots of training data\nso\num because there's already\nthere's already this this catalog and\nthis allows us to\nessentially\ndo a classic supervised approach but\nyeah we need this specific model to to\ndo it and work it works quite nicely\num but of course having a large\nTransformer model comes with its own\nchallenges in\nyou need\ngpus and training is not cheap\nso for instance\n[Music]\num\ntraining\num a specific kind of of model for first\nfor for this task\ns\num eight Tesla gpus with combined\ncombined memory of\nuh 320 gigs and\num still takes\n25 5 hours of training even though this\nis a pre-trained model already\num\nso this is of course\nexpensive and\n[Music]\num\nyou don't\nwant to lose State you want to make sure\nthis works this Branch doesn't crash and\nor if it crashes\num\nyou can you can resume and of course you\nwant to make sure you you have the right\nfast two kills so one thing here where\nflight really is helping us is the uh\nthe intra task check binding feature\num because\num when we're doing this large training\njob when we're running this large\ntraining job on using flight\nwe can easily write checkpoints how to\nto buckets\nand\num\nyeah we can also switch to\num\nto\num\ncheaper machine types and the spot\nmachines and if the training crashes or\nsomeone or the spot machine goes goes\ndown because someone else\nuh wants to use it\num\nwe can just spin up another machine\nwith the flight and\nresume training without losing a lot\nso this is really helpful to to\nessentially make this more robust robust\nand also also of course to save save\nsome money\num\nin this case in this use case we're also\nusing flight for predictions because\nwe don't need like a real-time\npredictions and we can just do some some\nbatch processing\num we get new data uh run it lightly and\num\nand put it into some some database\nand this is where flight is of course\nalso very useful\num so we also do the\nthe prediction on flight itself uh also\nrunning on on gpus with essentially the\nsame benefits you'll get for for\ntraining\num so this is an example of incremental\ntraining\num\nbut all we can also where flight is also\nuseful if we do like a full training on\non\num all raw data this of course is going\nto take a lot of time even though\nprediction is faster than training\num if you have enough data points it's\nstill going to be slow with a larger\nmodel and\num what we are leveraging here is\nessentially\nDynamic tasks with sharding so we can\nsay\num\nokay I want I don't know 32\nmachines with each with one or more gpus\nand then with with a flight workflow\num we short the input data run\npre-processing tasks and\nprediction tasks all in in parallel so\nthis is really really nice to speed up\nthe whole process and also make it more\nrobust\num if one of these tasks fails you have\nthe right settings in Flight of course\nyou can also resume without\nuh avoiding the whole operation\num so how can we actually use gpus in\nflight so\num\nbasic GPU of course you need the support\nfrom your cloud provider or in the\nkubernetes cluster you need the hardware\nsome somewhere\num for sure but\num if you have it if you have gpus\navailable in kubernetes\njust configuring a flight to use it is\nis quite simple because it just you can\ndo it through a resource requests\num\nbut if you have multiple GPU types\navailable\num right now it requires a little bit\nmore effort\nand\num we also\nwe are also using a\nfeature in gke\nI'm not sure probably there's something\nsimilar in um\nand it it WS or something but\num I'm not familiar with this with that\nbut in gke you can actually\num\num\nspin up a specific machine\njust through kubernetes\nsettings so you essentially say\num hey um you configure your part to\n[Music]\num\nand and you say okay I need this machine\ntype\nthrough a node selector and colorations\nand\num\nif you've enabled this feature and in\ngke\ngke will essentially\nspin up an on-demand node tool\njust\nsatisfying your demand so this is really\nnice because what we can do now is say\nokay\nI in my pot\nin my pod specification with currently\nwe need podcasts to do that but you can\nsay okay I need\nfor\nNvidia gpus Nvidia a 800 gpus\nand\nif I've configured my task this way\nit will just spin up\nthe right machine for this task and of\ncourse clean it up afterwards again\nand this makes it very very flexible so\nwe can\ndepending on what we're using what what\nwe need we can just say Okay\num\ngive me this kind of machine type and so\non and of course it's not only for gpus\nwe mostly use it for tubus but it's also\nyou can also use this feature for other\nspecific Hardware\nif you need I don't know the most uh\nrecent\nmachine type AMD gpus whatnot you can\nyou can also use that\num\nso this is has been super helpful for us\nthe only\nthe only issue right now is that\n[Music]\num\nyou essentially have to\num\nset that value during registration time\nso because we we\ncan over override\num\nthese settings at runtime right now but\nthere's an open issue\nthat's now being worked on again it's\nand\num\nhopefully very soon we'll be able to\nactually\ndo that um\nduring\nexecution time so saying okay now I need\n[Music]\num\nonly for or I need eight gpus or\ndifferent kind of GPU if you can do if\nwe can do that during a registration\nthis would be of course yeah even more\nawesome\nOkay so\num\nnot to the next to the next topic and uh\nflight for\ndata science and experiments so\num and uh sorry before we move on a\ncouple of comments that I want to catch\nup\nso I think um Fabio you were referring\nto the parts templates coming soon could\nmake that a little bit easier\num I think it's a great comment I don't\nknow if you want to elaborate on that\nFabio no I can so we use the same trick\nfor non-distributed training but for\npytorch distributed training you have to\nsay task config equals to python you\ncan't say task config equals to pop so\nthere you can't use this trick but we\nneeded for distributed trainings because\nwe want to switch between a100 and B100\nuh so we had discussions with Dan\nand there will be a really cool feature\nthat I'm looking really forward to in\nthe I think probably the next release or\nthe one after I don't know uh that\nallows you to to provide a pod template\nor the name of a pod template in the\ntask decorator and that will then be\nmerged with the rest of the task config\nand the great thing about the Pod\ntemplate mechanism is that it works for\nmost of the plugins right that will work\nfor\num I don't know I don't know the list\nbut it will work basically for many of\nthe plugins but right now you can't\nprovide a pot pack\nand that's really cool\num because then we can also do this for\ndistributed training with like\ntensorflow distributed turning pointers\ndistributed training and all of the\nothers\nyeah just to chime in over there I think\nwhat essentially happens is uh pods and\nyou know non-bought container tasks are\nboth first class citizens within flight\nand flight actually supports them\ninterchangeably uh uh but like Kate\nthere was a decision made at some point\nwhich is like oh containers and pods are\nin a different that's the combination\nthat's happening that should simplify\nlike you should be allowed to add\nsidecars Etc whatever you want to any\nkind of job now going forward so that\nshould make it I think super powerful\nsuper powerful\ngreat thanks a lot\ngood feedback okay sir\nokay\num\nyeah so\num\nthe other thing\num I've I've mentioned before is that we\nare we've started to\nuse flight\nfor for experiments\num so basic idea is to yeah to to\nalready use it earlier in the machine\nlearning development cycle\num not only when things go to to\nproduction\nand of course benefit from from all the\nall the flight niceties\num\nlike like caching especially\num the on-demand Cloud resources\nincluding the the GPU stuff I've just\ntalked about\num parallelization\num of course also representative\nreproducibility and um yeah all the\ninsights and transparency basically we\ngain through\num through through workflows where you\ncan look uh into intermediate results\nand so on\nand of course also narrow the the Gap\num\ngoing from from ml development to\nproduct because if if we already do our\nexperiments in flight and we have a good\nresult it's very easy to just switch\nover right\num and of course also do something like\ntuning or trying different type of\nparameters\nthe parameterize our tasks and workflows\nit's quite easy to to try try different\ndifferent things and just run it and fly\nit and\nlater collect the results\nand yeah this is essentially what we\nwe've started doing\num and\nwhat I\nyeah also for mostly for\num for deep learning for Transformer\nmodels for for different use cases like\nuh text classification and\num and so on\n[Music]\nand what what I've done is I've just\ncollected a little bit of feedback from\nour data scientists who are are using\nthis\num I I just want to\nbriefly show that to you\num\nand yeah maybe we can we we can discuss\nor or whatnot so so one first the the\npros um of using flight what they're\nsaying is it's really great for\nfor sharing experience with the team you\nknow because of course it's multi\nmulti-user and you can run your things\nand yet then you can say to your\nco-worker hey look I've I've done this\nexperiment and here are results look it\nup and someone else can can\nhave a look at at the results and also\nof course check out the source code and\nbuild their own experiments on top of\nthat or so on and so on uh also it's\nquite easy to write workflows so it's\nalways nice feedback\num and the the caching mechanism can be\na huge huge Time Saver so\nI've seen that also firsthand for\nif you split up pre-processing and\ntraining for instance and if you iterate\non on your training\n[Music]\num\nit's really nice\nif you don't change the pre-processing\nit will just be cached right you run\nyour workflow again with some changes\nbut\num usually the pre-processing is just\ndone because yeah it's already done and\nflights caching mechanism makes it kind\nof kind of transparent easy\nuh and of course if you're running\ncluster it's all parallel so it's also\nnice for experiments\num\nand\nyeah the transparency is like I've just\ntalked about that too right it really\nhelps to to be able\nto inspect inputs and outputs and just\nmaybe maybe there's some issue in\nbetween\nand you can just\ndownload the for instance the parquet\nfile and load it into pandas or whatnot\nand and um\ncheck check the intermediate results and\nthis is really really helpful\nand also once you have a workflow yeah\nusually it's quite reliable so it's\nreally simple and convenient to use and\nand try and try different experiments\nand if you parameterize it can easily\nwithout further code changes just just\nrun it in the cluster with different\ninputs\ndifferent data sets for instance and so\non\num and so what's the what's what are the\nthe challenges or what could be improved\nthis is not necessarily something that\nneeds to be improved in Flight Maybe\num\nMaybe\nthis needs to to be some library on top\nor so it's just the feedback I've\ncollected from the data scientists so I\njust just want to show that here here\ntoo so\num well I think most most of my\nco-workers ran into early on was trying\nto to run some like\na regular python in in inside a workflow\ndecorator and of course uh that doesn't\nwork because yeah that's just the DSL um\njust have to get used to it um but this\nis something\nif you start you you think Eric this\npython can do anything but of course you\ncan\num so sometimes the the type system\nand the serialization\num\ngets in the way so if you're used to if\nyou if you used to just be able to\num\nyeah with normal function calls to to to\nuse any type\num sometimes of course you when you\nconvert it to uh to to tasks it\nuh\nit's more restrictive and sometimes so\nif you use for instance Union types or\nor I think one one example was lists of\nnumpy arrays which were difficult\num\nand especially especially return values\nwe have\nto use name tuples and sometimes we want\nto use data classes that can because\nthen you can extract the um\nthe\nvalues inside a workflow in in a\nworkflow but if you use name tuples it's\nhard to\nput the whole Tuple again as a parameter\nto the next task and so on just\nyeah some some something you have to to\nget used to how\nhow to to actually use the API here\num then if you're if you have caching\nenabled\num which is usually quite use useful in\na cluster and you do local testing\nsometimes you you don't really see that\nthat things will be cached locally too\nand you run into into issues because you\ndidn't know that it was cached locally\nand and they wonder why why it's not not\nexecuting even if you change things\num\nand then yeah of course overriding the\nresources like execution time we just\nhad it with uh with the GPU examples\nthat would be really useful\num\nand yeah what's also quite interesting\nfeedback but that maybe\ndue to the way we use flight for\nexperimentences that people say\nsometimes they need more granular\nversioning or something like like\nbranches because if you work on the same\ncluster and same project same domain\nwith multiple team members and some\nsomeone registers the new version it\nbecomes a new default right and\num\nsometimes you have kind of conflict\nRight Flight is very good you can always\ngo to the to the old version\nbut um\nuh yeah people\nnot not used to it sometimes they say\nhey I want to do something like get\nbranching and just do my own stuff\nwithout even interfering with somewhere\nelse\nso that was kind of interesting\num and also\num\nwe are for the experiments we were doing\nbenchmarks and um\nthen\num of course you don't want to use\ncaching you usually want to enable it\nbut for benchmarking of course you want\nto make sure that the thing actually\nruns so maybe you say okay I'm\nrestricting this to two CPUs I want to\nknow how fast it is and\num I think there was some restriction\nmaybe it's already listed\num but you couldn't when they tried it\nwasn't it wasn't possible to disable\ndisabled caching the UI\nthat's very interesting sir and I think\num you know some of those things sound\nlike data scientists getting used to\ndifferent environments just at Jupiter\nnotebooks and you know I'm coming out of\nthat camp and I I know all the free\ngames that I have and I'm moving into a\na little bit more stricter environment\nwhere we talk about repeatability\nreproducibility and Engineering style a\nwork it's it's a change for us but on\nthe other side there's also um\num you know there's also changes I think\nCaitlin do you want to talk about async\nWorkforce to your python real quick\nyeah I'll talk about a few things so\ndisable caching is supported in the UI\nnow so okay so maybe this was just the\nold version so great yeah I I I I I we\nwill try again with yeah yeah yeah\nactually kudos to Black sharp for adding\nsupport for that so they did\num add it in\num more of a fine fine granular\nversioning actually that's a fantastic\nRFC in progress and Linkedin is working\non with us and that is\num uh what is it called Tags and\nactually you should be using\nauthentication and the UI will\nautomatically start filtering if you\nwant based on the username only awesome\nand so that's like that Auto Branch\nright like it's not really a branch we\ndon't want to build gate in here yeah\nsure but would love to hear more\nfeedback on that\num and then uh I think the local testing\nis very interesting the last one is\nasync workflows uh coming soon in on a\nflight near you which is essentially a\nway to run\narbitrary called in Python uh which will\nalways become a task in the workflow\nand if you are interested in\ncontributing to that please\nplease Jump On In in the contributor\nChannel\nuh there is there is a RFC work in\nprogress\nthe basic implementation of that is\npretty trivial but we are trying to make\nit more robust because you know flight\nis known for robustness you don't want\nto go vacant by doing something sure\nthat's what so happy to get more\nthoughts on right awesome thanks thanks\nyeah no no this is great I love this\nstuff this is the best part of my\nmeeting\nall right\num yeah so\nthat's it for for the data scientist\nfeedback uh just want to like kind of\nsummarize\nthe\nthe the key benefits for us using flight\nto go yeah\nit's it's been awesome\num\nuh I think most of the points I've\nalready\nI've already mentioned but uh just just\nyeah just want to summarize so\num I think one really really important\nthing is that we have these yeah that\nflight makes this\nthese things are transparent and we can\nhave this Insight in in historic runs\nlook at inputs outputs this and of\ncourse intermediate results\num everything that makes\na bit fast caching parallelism\nbut also makes it\nrobust of course reproducibility\nand um\nwhat has been really really great for us\nof course the the the the kubernetes\nintegration and the the way to to\nutilize Cloud resources but just on\ndemand and scale scale up and down\ndepending on what you need\nespecially for us the GPU Support also\nfor because we have quite quite a lot of\ndeep learning use cases\nand um also yeah spot machines this is\ngreat um\nin combination with inter-task\ncheckpointing because\num you can you can save and still still\nrobust and last but not least I think a\nvery important point it's is the the\ngreat community\num this is so important\num\nbecause yeah people helping each other\nthe uni team is always has always been\nsuper super helpful\num so this is\ngreat so thanks for that\nit's not good now this is a fantastic\nsir and I have a question about uh\nobviously the support for spot machines\nand trust checkpointing you mentioned\nearlier on slide I slide eight that you\nwere able to to save uh time and money\ndo you know how much uh you were able to\nsave by by leveraging spot spotting\ninstances\nI\nI don't I don't really have have hard\nnumbers but um\nusually\nuh\nGPU spot machines are\nI think\nyou usually pay half the price or even\nless of the regular machine yeah if you\ncan imagine if you're running\nthat much GPU hours that's that's quite\nsome savings\nyeah\num yeah I think we heard this before on\nthe savings that that's very significant\nI don't know if Chief you had comments\nthere or Fabio\nI have one question my current\nunderstanding is that it trades once on\na spot in terms is that correct or is it\num the default configuration yes\num so if it fails it will spin up a\nregular instance but you can configure\nthat you can not not per task but\nglobally so I think what we did was\nchange it to\nthree or something like that so we try\ntry spot instance three times okay and\nif it still doesn't work just it will\njust spin up a regular one that is\nwhere's that I think it's a propeller\nconfig yes I'm adding Young's like for\nnow sure sure do that thanks awesome I\nthink it should actually so if you\nspecify five retries on a task you\nshould do four on spot and then finally\none on on the\non the on-demand instance that's like\nthe fallback Behavior so that yeah you\nknow it kind of ensures completion or\nproblems\nif you try it please please update the\ndocument\nit's always a hard challenge of course I\ngot it\nthanks\nawesome sir and I don't know if you have\nmore slides yep yeah just just uh one so\nthere's some German in here sorry uh\njust\num so what's next for us\num\num yeah\nall right help getting the uh General\nresource over where it's done also yeah\nwe just heard about the the Pod template\nthing so um we're happy happy to help\nthere\nuh also I also want to document the node\nAuto provisioning setup because this is\na few like non-standard settings and\nit's not in the documentation right now\nbut\nuh I'm I I'm planning to to edit so\nothers can benefit there too\nand also\num\nnew things to try\num\nI haven't been able to yet uh human in\nthe loop Works flows look super\ninteresting for\nthings like labeling tasks and\nalso Union ml I haven't been able to to\ntry it but that looks like for the you\nknow for the extra paramutation use case\nmaybe it's something something\ninteresting\nyes okay that's that's it for me so if\nthere are any more questions otherwise\nthank you\nfantastic job sir and we love it this is\nreally fantastic so let's uh keep this\ngoing uh for I mean you're also in the\nslideshow so people have questions\num they can probably hit you up on the\nslack channel right\nsure sure anytime\nawesome thank you so much"
    },
    {
        "title": "Flyte from PoC to Production at Nokia",
        "transcript": "okay time to move to the next item in\nthe agenda will be a presentation from\nsujit Samuel\nhe joined us today to chair in the Nokia\nJourney with flight from POC to\nproduction welcome to Jeet and thank you\nfor\njoining\nright thank you thank you David uh you\ncan hear me right\nyeah good so thank you once again uh for\nhaving me here uh to talk about uh how\nwe\nuh did move from POC to production with\nflight yeah so\nuh to give an overview I am the chief ml\nengineer here at Nokia\nand I work for the uh Nokia standards\nunit which is like one of the one of the\nuh business verticals here in Nokia and\nwe have a we have our own machine\nlearning platform uh which we started\ndeveloping around six months back\nincidentally I joined Nokia six months\nback and\nprior to that I was in Ericsson doing\nthe same thing so I just gained one like\ngained some experience and then try to\nimplement whatever you know I learned\nover there so\nover the past six months we have been uh\nwe have been trying multiple open source\nsoftwares uh trying to put them together\nglue them together and try to provide a\ncomposite experience to the final\ndata scientist so that they have to just\nlog on to a single UI and do all their\nactivities so\nwe have Jupiter for uh\nsmall experimentation we have we have a\nvs code ID wherein you can you know\nbasically put all your code\nthen our workflow orchestration is\napplied so we basically submit our jobs\nto the flight and flight takes care of\nrunning the training pods and we have\ntrainings that run\nuh from three hours to anything between\nthree hours to five days so we have\ntrainings that run for a long long\nperiod of time\nand we have around 100 users who are\nusing the platform currently and\nat the start of it we had like uh we did\nPOC with around 10 to 15. yeah users and\nit it was good for us\nso prior to this I had worked with Argo\nyeah and now work with flight so\nuh\nokay next slide you can go to next slide\nyeah\nnext slide please\nso why flight yeah\num I mean if if you look at my\nexperience with Argo right it has been\nkind of like you have to build a yaml\nand data scientists if I ask them to you\nknow have a deployment ml have a build\nml or you know have a have their\nworkflow as the ammo many of them look\nat me like what is ml why are you asking\nme to do all this yaml activities on the\nother hand if if I if I just ask them\nyou know just use a decorator put it on\ntop of your python function and just be\ndone with it\nuh they are they become kind of more\nfriendly towards me in that sense so I\nbelieve one of the primary reasons was\nthat it is very closer to the actual\ncode yeah I mean I mean I didn't have to\nask the data scientists to actually make\nan image you know create some entry\npoints then create a yaml and then you\nknow submit it to the you know\norchestrator and all that kind of\nactivity so flight took care of all that\nso I think that was fine I think it is\nit is like one of the big pluses that I\nhad with uh with flight\nuh it was also very easy to set it up\nwith key cloak you know and actually I\nhad a lot of help from the flight\nCommunity I mean I I raised a lot of\nqueries on the flight slack and then\nthere were a lot of help available\nit is easy and also a lot of help is\navailable so that that basically is like\nthe primary reason why we thought okay\nlet's go with flight and let's see how\nthe experiences and and so far it has\nbeen really good so we we didn't have\num we didn't have any stability issues\nwith flight we didn't have any major\ndata scientists dissatisfaction with\nflight basically they come they code\npython they added recruiter and then\nthat's it they're done with it yeah next\nslide please\nso this is how it looks like we have a\nAzure ready then the key cloak then H2O\nis the service mesh manager and then we\nhave a flight and flight is the only\nthing that I'm talking about here so\nhence I have put only flight as the big\nbox but actually we have a lot of other\ncomponents like ml flow Jupiter you know\nand all these are directly integrated\nwith each other so you you submit\nsomething in Flight we have an mlflow\nintegration that actually\nlogs everything that happens in Flight\nall the experiments that go to ml flow\nso so that's how we have a title In This\nTogether uh to provide this composite\nexperience and flight is like the\ncentral part where we run all our batch\ntrainings uh in flight and then our\nmodels are getting stored in backend\nstorage from where we pick up for for\nyou know inferencing with Selden or\nother Frameworks so\nthat's how it has been yeah next slide\nplease\nI also generally how it is like we have\na workflow\num how how does how does it reach flight\nyeah so user codes in vs code then we\nhave given them a plugin which basically\nuses their their code we have a kaniko\nbuild process that kaniko builds the\nentire workflow uh entire image and then\nflight packages that image and sends it\nto its UI I mean sends it to the back\nend like wherein all the it basically\nsegregates into workflows each workflow\nsecond into task and each task has its\nown you know\nProto buff things that it tries to put\ninto S3 and all that so that's what\nbasically works so we have a process\nfrom where a user can just use his code\nand push it to flight via kaniko build\nimage so that's a flight kit then there\nare multiple tasks on the workflow and\nthen it generates an image that runs on\nthe execution engine yeah and the\nexecution engine is something that I\nwill talk about in the next slide\nyeah so here I have explained the\nprocess here so we have a flight\nworkflow yeah\nso what happens is uh\nif you look at our uh the the way that\nwe are structured right we when we\nstarted off we did not have gpus on our\ncluster yeah on our own cluster so we\nhad we had to submit jobs to an external\nsystem yeah and that external system is\nnot flight yeah external system does has\nno idea what flight is yeah so external\nsystem basically is like a container\nengine you can you can think that think\nof it as a container engine knock\nrunning system yeah so you submit the\njobs there and then it is up to you to\ntell the container what to do so we\nbasically hacked into the system by uh\ngiving entry point as a pi flight\nexecute so we if you look at flight the\nback how it triggers each node is it\ncalls the image then it provides and our\nthe arguments like pipelate execute and\nthen it provides the argument as steps\nas argument and internal project ID and\nall those kind of things this provides\nenvironment variables so what we did was\nwe took all those environment variables\nwe trapped them and then we sent it to\nthe external system them and then we ask\nthe external system to actually execute\nPi flight execute commands with the same\nenvironment variables and it worked just\nfine so and how we did that we actually\nused python decorators so if we have a\nThe Decorator what it does is it\nbasically traps all the environment\nvariables it understands whether I'm\nexecuting on flight or I'm executed on\nthe external system so first the\nworkflow when it executes it actually\nexecutes on flight and then and then The\nDecorator realizes oh I don't have gpus\nwhat should I do you know I should go to\nthe external system so at that point in\ntime The Decorator package is everything\nand sends it out to the equation system\nasking so in our case the actual system\nis the lsf cluster lsf is an IBM\nsoftware load sharing facility which has\nall the gpus so we ask lsf to run the\nimage with all the environment variables\nand the entry point and then it runs and\nthen we have a we have a watcher on the\nlsf which basically sends the\ninformation back to flight and then\nflight understands that okay this step\nis done then the flight triggers The\nNext Step so that's how\nwe set this entire system up\nuh now we have on cluster gpus so we\nhave the capability to execute you know\njobs on\nour own gpus on the lsf cluster as well\nas on any Standalone VMS which might\nhave gpus or not which might not have\nGPU so\nthis is the entire system then\nand again I stress here that it is\nbecause of the ease of with which we\nwere able to customize all this that we\nare able we were able to implement this\nas a platform and uh it is kind of\nimpervious to the back end or to the to\nthe user as to where his job is\nexecuting hourly sees is a flight UI and\non the flight UI he sees that his steps\nare executing one by one but under under\nthe uh under if you look under the hood\nthen it will be like a lot of different\nprocesses that look that you know hack\ninto multiple things in flight and then\nsend the job outside then trigger you\nknow watch the job which are happening\noutside and send it back to flight and\nthen flight knows that okay the job is\ndone and then it triggers the next step\nthat's how it goes yeah next place\nuh what do we need to do now uh we\nactually we had a discussion I think uh\nI also you know put it on the slack\nthere that we need to eliminate\ndecorators and start creating backend\nplugins so lsf doesn't have a back-end\nplugin right now because\nwe thought it is like uh as of now we\nare not investing too much time into\ncreating back-end plugins for lsf but in\nthe future when we try to move on to\nmultiple execution systems then we\ndefinitely need something that is easier\nyou know not so much hacky as what we\nhave so uh we would definitely want to\ncreate some back-end plugins yeah we\nalso want to scale up flight to more\nnodes so right now we have around uh 40\nto 50 nodes which is which is for a team\nof around 100 120\nnow people executing experiments you\nknow around the Around the Clock we want\nto move it up and see what is the limit\nlike I I have seen blogs where in flight\nhas been running with thousands of notes\nI've seen some blogs so I'm hoping that\nthere should not be an issue\nuh but that is our plan that we want to\ndo you know and we also have uh plans to\nmove our platform as a product and when\nwe move it as a product then we would\ndefinitely like to have some user\nisolation between\nmultiple teams like now if you see that\nwe have a single flight UI all the\nexperiments show up on the flight UI\nyeah how do you ensure that you know a\nteam can only see its own experiments\nI'm not so sure how it happens right now\nin Flight I have not explored it but we\nhave to understand how to how to isolate\nthe different flight experiments and\nmake it visible only to the team that\nhas done the experiment\nuh next slide please\nuh yeah and finally uh a big thanks for\nthe excellent Community Support yeah I\nmean I have never had to wait for more\nthan one hour I mean one hour is also\ntoo much yeah I have never had to wait\nbasically once I post something on slide\nyou know the the response is kind of\ninstant yeah and uh everybody from ketan\ntill you know everybody just is it shows\nexceptional willingness to come onto a\ncall and resolves the issue yeah and\nyeah like like I said thanks to him and\nlast but not least entire flight team\nhas made such a great product like I\nsaid earlier you know uh just python a\ndecorator uh annotation and that's it I\nmean we are done with it we don't have\nto really worry too much about uh yamls\nand you know uh yaml syntaxes here\nbecause yaml is one thing and yaml\nsyntax is one thing you know because if\nyou if you miss uh if you know if you\nmiss a space you know yaml will not run\nbut it will not tell you why it not run\nso so yeah I mean that is that is\nsomething that I have faced and it's\nit's a good thing that you know Nokia\nuses uses flight and I have learned a\nlot about it and I have I'm thankful to\nthe flight team for making such a great\nproduct so\nthanks guys thank you once again\nI think next slide and I should be done\nwith this yeah yeah thank you thanks a\nlot\nthanks to you sujit\nthat's awesome any question any comment\nfor sujit\nyeah sujit the last point you made about\nviewing only the experiments that a team\nis responsible for this is something\nwhere\num\nwe'll we'll make a RFC for this but\nwe've been talking around some ideas on\nyou know execution tags\nthat's the key phrase there basically\nyou can tag a set of executions with\nwhatever you want and then kind of\nfilter them in the UI or\nprogrammatically after the fact so\nthat's something um\nwell you know we have a lot on our on\nour plate but uh that that's that's\ndefinitely on the top of our list\nso\nwould we love feedback we'll share this\nwith the community\num and you know if you want to help kind\nof design\ndesign what this looks like in the end\nyeah we'd love your feedback\nyeah definitely uh yeah I mean like I\nsaid this is a like this is a\ncentralized platform and now initially\nwhen we started off it was only our team\nyeah but now there are a lot of other uh\nbusiness units who are showing interest\nin this and uh because of the ease of\nthe user right I mean everybody's kind\nof okay to use this and with that comes\nuser isolation uh project isolation\nexperiment isolation so the way that we\nare doing it in ml flow is we are\nactually having multiple ml Pro\ninstances for multiple teams uh but the\nflight Is We I don't think I can deal\nwith flight I mean I can do it but then\nit becomes too big for me to manage\nmultiple instances\nuh so yeah it's it would be a great\ngreat thing if you know this could be\ndone somehow\nthank you okay cool great\nawesome any other question comment"
    },
    {
        "title": "Roadmap Update: Flytekit v1.3.0",
        "transcript": "okay with that further Ado we will move\nto roadmap updates by Nils bentylen\nwelcome news\nthank you David hopefully everyone can\nhear me okay\num\nyeah hi everyone I'm Niels bentylon I'm\nthe chief ml engineer at Union AI\num and the role I serve in Flight OSS is\nbasically product manager slash engineer\nat the same time but I've been working\nwith the team and Eduardo specifically\nto\ntry to bring you more transparency into\nthe flight open source updates\nso if we go to the next slide cool so in\ncase you missed it Flight 130 that was\nthe most recent release we just put out\num\nback in January\nuh calling us five major\nfeature enhancements here so there's the\nthe gate nodes node type which enables\nsignaling API for human interloop slash\nexternal signal use cases\num he gave a great presentation of this\nso we'll\nadd a link there in the slack Community\nif anyone's curious\nwe also added three Integrations ml flow\nintegration for tracking logging metrics\nduring your experiments that might\nhappen in a flight task\nthe dasc integration and the databricks\nintegration\nso\nwe'll hear about the desk one pretty\nsoon\num but uh Kevin our on Kevin Sue gave a\ndatabricks integration so you can look\nback at our recordings there if you're\ncurious and last but not least uh\ntensorflow types which adds direct type\nsupport for intensive flow records and\nyou know directories of examples\nmoving on to a 140 which should be\ncoming up in the end of the month\nuh just these are just you know six\nhighlights so a duck DB integration\nwhich will allow you to\num\ndo basically in process SQL\nanalytics data transformation type use\ncases\nin a flight task\non the next one which is pretty big is\nexpanding support for MySQL so\nthis specifically is using mySQL\ndatabase\nfor the flight backend\nfor the two components mentioned here\nflight admin and data catalog\nthis is something that people in the\ncommunity have wanted so we're gonna\ngive it to you\nuh the third Point here is the Pod\ntemplate task config for contacts here\nbasically flight kit the python SDK has\nbeen kind of designed around abstracting\naway case abstractions but you know\nuh the community has wanted access to\ncertain abstractions here like volume\nmounts and other other things\num so this pod template configuration at\nthe task level will give you access to\nall of those check this out underline\nconstructs\num\n0.4 here is uh one I'm excited about\nthis is a flight deployment reference\nimplementation\nso uh basically this is a an easy but\nopinionated way to spin up a cloud back\nflight cluster so this is beyond the\nflight demo sandbox use case\num so this will be\nsomething will provide that the union\nwill\num update and maintain\nand you can then take that and you know\ncustomize it obviously to whatever your\nuse case needs\num second to the last here is also an\nexciting item the external web API\nplugins\num so if for anyone of you who has\nwritten a back-end plug-in you currently\nhave to do this in go\num\nand this item will actually allow you to\nwrite plugins in Python so if you need\nto access some kind of external service\nlike say sagemaker imagine that that\nback-end plugin doesn't exist then you\ncan write one in Python\num and last but not least\num we're planning a bunch of performance\nimprovements and fixes on the flight\nconsole UI many of which are based on\nyour feedback and Bug reports\num okay\num last but not least just some pointers\nand links here so the roadmap we're\nhosting on GitHub projects\nso you can go to github.com\nprojects three and that'll take you to\num our roadmap which is you know kind of\ngrouped by milestone\nso if you want to check out\nthe full full plan for the 140 release\nand then maybe looking forward to 150 we\ncan take a peek in there\num as always you can always ask\nquestions and ask community in our slack\nand there's also the um\nin-flight conversations the new one that\njust came up\num and hang tight for ideas or like a\nprocess for proposing discussing and\nlike maybe voting on ideas\num so that will I'm working with David\non that and we'll give you some updates\non what that looks like in the next few\nthings\nand that's it for the roadmap updates\nthank you"
    },
    {
        "title": "Flyte + Dask Integration Demo",
        "transcript": "right next one we have the dash demo by\nanother day by that Bernard from pachana\nso welcome Bernard and thank you for\njoining\nhi thank you very much for for having me\num I will make this fairly brief I hope\num as other plugins in this realm\nalready exists the spark plug in the\narray plugin and I just want to give a\nquick overview of basically what the new\ncapabilities are\nso\num yeah basically this is a short\npresentation of what the the task plugin\nflight is about\num as for the\nagenda uh a quick why on because a lot\nof plugins already exist why why another\none um that is fairly similar\num a short intro on what tasks compute\nmodel looks like um because I think\nthat's going to be helpful for the for\nthe demo\num\na quick note on how to set up things a\ndemo then questions uh in case there are\nquestions throughout the talk feel free\nto just just interrupt me\num I said this is this is fairly\ninformal\nuh on the why\nyou might think okay\num task orchestrates tasks across\nmachines right similar very similar to\nflight\nor spark so it's why why another plugin\num and I think the main reason is that\nthat's comp tasks compute model allows\nyou to run\nway smaller tasks\num as the nodes are constantly up so for\nthe Das cluster you have nodes that are\nconstantly up versus with flight you get\neach task is a kubernetes pod so the the\noverhead of starting a task is a bit\nlarger in flight so similar to for\nexample spark does tasks are just way\nsmaller and allow for different\nscheduling\nthen why not spark I think task is is\nnice in regards that it's written in\npure python so I think that\na lot of data scientists and machine\nLearners like that it also tries to\nabstract the or tries to aim for a\nsimilar API as X-ray and is\nnumpy arrays and\nthat's a painless data frames so I think\nthe the apis are very similar to to the\ndata science Community which is quite\nnice and\nthere's really great Integrations for\nfor certain domains for example I work\nin the geospatial domain at pajama and\nthat's just a big ecosystem of tools\nthat uh build up on dusk so so it's nice\nfor us to have a very simple way to spin\nup a last cluster around those tools and\nthen\num shut the the cluster off again\nwith one of the big benefits being that\neven with multiple teams right you could\nhave last classes up all the time\num but then with big teams they want\ndifferent dependencies right one team\none sensorflow the other one wants two\ndependencies whatever and to not get\ninto dependency how quickly it's nice\nthat flight can just spin up without\ncluster on demand run the job tear it\ndown again and you have full control\nover all the dependencies and whatnot\nwhich is which is quite nice\nfor the\noverall compute model you\nit works I said fairly similar to to\nwhat you would get in a spark job you\nhave a cluster with a scheduler which is\nthe main entry point that schedules the\nworks at the the work then you have an\narbitrary amount of workers I think\npretty much as many as your kubernetes\ncluster can handle and maybe one special\nthing is that in in the case of this\nplugin we have a chop Runner it's\nbasically you can think of it as a pod\nthat submits work to the scheduler um\nin a regular use case rights that the\ntypical client you could have multiple\nclients working one with one scheduler\nuh in the case of this plugin we have\none client with one scheduler and a\nbunch of bunch of workers\nand I think that's\nsomewhat important to understand uh how\nthe plugin works\nso demo time basically what you will\nneed to do to enable the plugin is first\ninstall the task operator I will I have\nincluded a link at the at the end of the\npresentation uh\nit is an operator that can manage manage\nDas clusters if you set it up with Helm\nit's usually in the minutes of work kind\nof order of minutes\nthen in your flight config in the config\nmap you need to enable the task\nintegration and you need to enable you\nneed to map the\ntask task type 2 the task plugin so you\ndefinitely need those two lines you\nprobably have already have some some in\nthere and you want to keep the ones that\nyou have but these are the additional\nones that you will need\nthen\non the flight level the way this looks\nis you have your regular regular flight\ntask\nthe\nas always the differences in The\nDecorator where you have a config which\nconfigures\nthe three parts that we just talked\nabout so the job Runner the scheduler as\nwell as the\nas the workers and we don't necessarily\nneed to set things here I just set some\nfor for demo reasons but you can set\ndifferent limits requests etc for for\nthose parts\num\nexplicitly for the scheduler and the\nworkers you can specify how many workers\nyou want and all the things that are\noutside of this task config will apply\nto the job Runner so think of the job\nRunner as your regular pod task that you\nusually have in Flight\num it's it's just the client and all the\nthe config that you apply outside of the\ntask config will just apply to the the\nchoke Runner\num\nthe plugin supports all the things that\nyou would usually expect interruptible\ntasks passing on environment variables\nsecrets\nand all of that\nand\nit also supports running locally which\nis quite nice for for testing right for\nlocal iteration and then as soon as you\nregister you can just unleash the full\nfull power of your kubernetes cluster\nand we've scaled up quite large with\nthis and has been has been working well\num\nI included a small demo here the way the\ntask works is when you don't have an\nexplicit client it will\nget the implicit one\num so when you run this locally we'll\nget a local client implicitly you could\nalso create one if you need a special\nlocal cluster or whatever and\non the in the cloud or in when you run\nthis in Flight you will get the client\nthat is connected to the schedule of the\num this this task decorator creates so\nthere's very few lines except most of\nthem are not required actually I think\nyou don't need any of the ones you only\nneed this task last Clan here\num I can run this whole thing locally\nand it hopefully works as expected let's\nsee the demo goes are good yeah so you\ncan run this locally you can integrate\nthis into your Pi tests you can you know\ndo all the the nice regular stuff and\nthen when you register this\noh\num\nwhen you register this right you need to\nregister it with a Docker image a Docker\nimage needs to have task and flight kit\nas well as the plugin installed so\nthat's what this is doing here\nand\nthen when you go to the UI you should\nhave\nthis workflow here\nthis task workflow you can launch it in\nthis case it doesn't take any input\nparameters it could and when you launch\nit in the background when we look at\nlens here\nit will create a drop Runner it will\ncreate a scheduler in this case we only\nspecify one WordPress it will create a\nworker pod\num\nit will\ndo the work as you usually expect it and\nsee\nyeah then the work succeeds this also\nsucceeds hopefully it usually takes a\nwhile and the output is is the same so\nthat's the the full round trip\nasset setting this up is\nshouldn't be shouldn't be too much work\nI think that was mostly it\num I added a few links here I don't know\nif there's a good place to share this\ndocumentation there will be a blog post\non the desk site that has all the links\nand and a bit more in-depth description\nof how this works\num in the back end uh this is mostly\nmostly in overview\nyeah\nare there any questions\nthis is awesome Bernard\num\none question I had is I guess what I\nheard that you went into the go\nland and uh helped implement the back\nend part of this how was that experience\nand you know\nwhat what are things we can do to move\nthat along\nI think yeah it's part of so that what\nyou've seen currently is basically\nrunning on my my local setup so I do\nhave a local local kubernetes cluster\nwith an installation and everything and\nI want to say just setting things up to\nbeing able to develop was a bit of a\nchallenge and I've deployed fight a\ncouple of times in in production so\nat least the new you know which part\nwhich parts to touch\num\nthat being said I've also never asked on\nhow all of you guys do this so I figured\nI can just you know put the things\ntogether that we wanted to\num maybe for reference I did have a\nlocal propeller running that I could\ndebug into\num yeah and then just started but the\nspark plugin is very similar so I I\ntried to to mimic that and I want to say\ndon't be afraid of go at least the parts\nthat I've touched uh were quite quite\neasy to to learn it was also enjoyable\nto see something different\num\nbut yeah I think the development setup\nwas probably the trickiest bit um\nespecially given\nwhen you change something flight plugins\nyou want to make sure that that's in\nyour propeller right then your propeller\nprovides locally and use stuff like\ncluster I I think that wasn't\nbut I can't imagine that being tricky\nespecially if you're not so familiar\nwith kubernetes and Covent and all the\nall those bits\nokay awesome yeah I think\nwill well I think we'll you know we'll\nmake that contribution experience better\nbut then also with the external web API\nplugins\npeople in Python can and sort of\nimplement their back end\nBehavior\nmore easily I suppose\nokay if anyone needs something I I'm\nhappy to to help out there so if anyone\nin the community needs to implement the\nplugin I'm helping to help set up help\nset up things um and also thank you for\nthe help that I've gotten throughout\nthis has been a\nsix month project not because he was too\ntricky but mostly because I did this on\nmy weekends and they're\nfull of a scarce resource so thank you\nvery much and we are using this in\nproduction at the moment we just switch\num\nso there might be one or two bug fixes\ncoming but but so far it's running our\nworkloads and it's good\nthat's great thank you so much Bernard I\nmean this is awesome\nany other question or comment regarding\ndesk integration"
    },
    {
        "title": "Flyte Community Updates 030 featuring New Segment: In-Flyte Conversations",
        "transcript": "right so let's get started well welcome\neveryone to the flat Community sink\nthank you for joining\num I'm David Espejo and I will be your\nhost for today's meeting a friendly\nreminder that flight is an AI and data\nFoundation project so this meeting falls\ninto their code of conduct which\nbasically boils down to be nice to each\nother\nas you always do\ncool so next slide please\nwell we have a bunch of community\nupdates then and an update on the\ncurrent status of the roadmap for the\nupcoming flight release\nby nails\nthen a demo on the recently released\ndesk plugin for Flight by Bernard from\npajama\nand finally the journey from POC to\nproduction with flight from sujit\nsomewhere\nfrom the team at Nokia awesome\nright for convenient updates probably\nthe first one next slide please\nprobably the first one and if you're\njoining this meeting you already noticed\nthat we moved to a different platform\nand the reason is to fold first I think\nthat we want to make sure that no matter\nwhich Communication channel you use\nuh you feel safe\nso riverside.fm offers better privacy\nand security controls compared to soon\nuh that's one of the reasons and the\nanother one is that it will help us\neventually uh to stream this kind of\nmeetings to broader audience\nokay thank you yeah\nuh strange to a broader audience so\nthat's probably the two main reasons we\nare moving out from soon so if you want\nto discuss something ask a question\nusing camera and or Microsoft but\nmicrophone you just need to hit the\nrequest live calling button the bottom\nright corner of the screen\nand if you want to ask a question or\ncomment just drop it in the chat at the\nright hand side of the screen\nalso uh yeah you just want to make sure\nthat we don't have Echo so if you can\nmute yourself\nunless you're talking that will be\nhelpful\ncool\nyeah I hope this this change is smooth\nfor anyone out there and you have any\nquestion just let us know\nnext up there is a new post and the flag\nblock this time from our good friend\nEvan seller from HBO you probably\nremembered a couple of meetings ago uh\nEvan alongside with Kevin share about\nthe new flight and databricks\nintegration and how they use it for the\nHBO Max service so there's a full\nexplanation demonstration in the blog\nthat you can catch up now\ncool\nnext up please right yeah\num we are kicking off a new section in\nthis meeting in the community think that\nin the lack of a better name we call\nin-flight conversations is designed to\nbe a space for anyone out there in the\ncommunity to share something that you\nhave learned\nthat's it it's designed to be Assurance\nSuite space 15 minutes or less in a\nLoosely format you don't need to prepare\nslides if you're not able to do so but\nI'm sure that anyone here in this\ncommunity knows something that could be\nuseful for someone else\nso the goal is to get this started today\nI'm sharing real quick a couple of\nthings that I learned last week after\nattending Cloud native security con\nthis is a small conference well kind of\nsmall in the hundreds of people it was\nit used to be collocated to kubecon and\nthis was the first time that uh it was\nrun as a standalone event\num and it goes all about the issue of\nopen source security\nmore especially supply chain security\nproblems that if you're not aware there\nwill be a blog post to come where I will\nshare more details on the highlights\nfrom this conference and what we think\nare very relevant problems both for the\nregular software engineering space and\nalso for the mlaps world\nso I felt like I was in the middle of\ntwo big problems with a common challenge\nuh which basically boils down to\ntraceability the ability to trace back\nthe results of being a software\nengineering\npipeline an artifact or being an ml apps\nworkflow\nand the output of them all being able to\ntrace that back to the actual data and\nthe configuration that that produce that\nresult it's a big challenge right now\nand there are new attack vectors now\npowered by AI why we what we what the\nworld called AI that is basically llms\nin in some other popular models uh that\ncould really create a new family of\nprograms for open source security out\nthere and by extension for any\norganization\num using open source\nso this probably will open up a new\ndiscussion we created a new GitHub\ndiscussion uh hopefully you can chime in\nthere and let us know how how are you\nsecuring or planning to secure your\nemail or data workflows\nthere's a new there's also a new slack\nChannel where you can propose any topic\nanything you would like to share with\nthe community here and for sure you will\ndo it better than myself cool\nthat's it next one please\nall right next up links and resources uh\nprobably most of this will be shared\nalongside with a recording\nas usual we have office hours\num you can use the add event links to\nadd it to your calendar you can come\nhere to ask flight maintainers your\npressing questions\num around the project\nall right and next one\nfor the upcoming Community sync we will\nhave among many other things we will\nhave sorry Brunk from yusu AI Services\nsharing also their Journey with flight\nand also yes tune stay tuned for the\nnews later I will go out very soon and\nuh yeah the YouTube channel where you\ncan find all the recordings\nall right\num seems like that's it for the agenda I\nbelieve\nand uh\nif no one else has anything to comment\nor ask\nwe'll call it\nall right thanks everyone for joining\nsee you in the next one"
    },
    {
        "title": "How Alectio Migrated from Airflow to Flyte to Build the First MLOps Pipeline for Data-Centric AI",
        "transcript": "now Jennifer and Josh will be talking\nabout how electio migrated from airflow\nto flight to build the first mlops\npipeline for a data Centric AI Jennifer\nis the founder and CEO of electio a\ncompany created to automate the cleaning\nand optimization of trained data sets\nand Yash is an envelopes devops engineer\nat electio and he has been a core\ncontributor to the flight migration\nJennifer and yes the floor is yours\nvery good introduction I appreciate the\ntime you guys are allocating to us right\nI mean so look I've never seen such a\nfantastic transition like going to her\nfrom talking about like data leveling to\nwhat we're going to discuss now because\nlike uh we're doing at scale like\nbasically like a real basically\nempowering users to uh annotate their\ndata more efficiently so we're gonna\ntalk about Active Learning and whatnot\nbut so I will start by saying that you\nknow Alexa\nuh we we migrated to flight because like\nour resolution especially airflow was\nnot giving us the the the horsepower and\nthe capabilities to basically like\nperform this kind of operations where\nyou have like a human in the loop\nelements where you have like a\nrepetitive tasks that are supposed to do\nthe same thing over and over again right\nI mean so uh without further Ado write\nthem into basically and going to share\nmy screen I have a couple of slides I\ncan share so the goal is to just see my\nscreen maybe switch to\nslide mode right everything so you\nshould see my my screen on or my invite\nslide on my phone on your screen right\nso uh Sunita said right and so we are a\nrelatively small company we've been\naround for a couple of years now right\nand so basically like we are pioneering\na new space which I ended up calling\ndata prep Ops which is a sort of like a\nan adjacent field to ml Ops which is\nbasically like you can see this as being\nlike a ml Ops for a data Centric AI\napproach to training a machine learning\nmodel or you can see this as uh\nincrementally improving your data set as\nopposed to training your model right and\nso what you're going to see today has a\nlot of similarity with machine learning\ntraditional pipelines of course there's\na lot of like things that can be reused\nand learnings with the news from\nbuilding a traditional machine learning\nplatform there also lots of challenges\nthat we felt like a typically airflow\nwas not helping us solve right and so uh\nso basically before we some people like\narchitecture conversations and like uh\nissues with like uh you know like a like\nbuilding the system or whatnot I'd like\nto explain a little bit more like\nbasically where you need to see the\ninter apps on the market right so\nbasically the way I explain the market\nis like ml Ops has been like around like\nbasically became a popular field like uh\nabout like what we eight years ago and\nso historically what happened is like uh\nml Ops was born out of necessity because\nwhen a machine learning became popular\namong companies and companies started\nhiring like data scientists everywhere\nusually you would have people who were\nexcellent mathematicians who could build\nmodels but they had no idea how to\ndeploy right and so the original Mission\nof ml Ops was making it easier to First\ndeploy pipeline connector production\npipelines I think because what was\nhappening like basically went on stop up\nwhen I started my carrying machine\nlearning is that I could build an\nawesome model but I had no idea how to\npush this to production connect to data\npipelines or whatnot right and then\nthey're still matured and little by\nlittle we started seeing other companies\nthat would support us in other aspects\nof machine learning or model they are\ndoing greatly such as automatically\ntuning a model automatically choosing\nthe architecture of a of a deep Learning\nNetwork or whatnot right I mean so I\nmean I'm not going to say that ml Ops is\ncompletely overcrowded but it's getting\nthere and you know like you're probably\ngonna see some consolidation because\nlike uh as a user basically I want to\nhave everything in one place where\nbuilding the model becomes super easy\nfrom here uh of course at that stage we\nknow that the problem is the preparation\nof the data right and so we are kept our\nusers are kept I would say even\ninvestors are kept under the illusion\nthat data Ops is a closed field right\nbecause like you have snowflake you have\ndata breaks you have like a beautiful\ndata Lake database like a basically like\nTechnologies out there but if you're\nactually messing with a natural machine\nlearning model of course you know that\nthe state in which your data set is when\nyou collect it no matter how large it is\nit's never good enough to be injected\ninto a model right I mean so think of an\nobject detection model a segmentation\nmodel or whatnot you can have as many\npictures as you want those pictures can\nbe like a lower high quality but even if\nthey are high quality you still need to\nget it annotated right and so what do\nyou have right now on the market you put\nthat in some storage somewhere you send\nthe uh you send a message to scale AI to\nLabel Box you wait for them to get back\nto you and tell you that your data has\nbeen annotated usually to discover that\nthe whole thing is a disaster it's not\nbeen done properly right so basically\nreally shows like the necessity today\nthat in America you you still have like\na human involved in that transaction\nwith those labeling companies right I\nmean so so those labeling companies\ndefinitely fall within the data prep Ops\nmarket right I mean not basically like a\ncategory of companies but the goal of\nwhat we're providing is the end-to-end\nwhy because uh we have continuous\npipelines for building and deploying and\ntuning maintaining like a monitoring a\nmachine learning model but the moment\nthat you need to retrain and you need to\nre-inject data into the pipeline you\nhave to hit pause and you have to go\nback like to basically like figuring out\nlike a who's annotating your data what\ndata needs to be annotated whether you\ncan trust the annotations and that's\nthat's just talking about annotation\nlike what we didn't discussed earlier is\nlike uh you might have to cleanse the\ndata automate the like augment the data\nor whatnot okay so that's that's\nbasically like what we're trying to\nbuild when we talk about like ml Ops\nplatform for data preparation shot it's\nreally like a an end-to-end workflow\nthat covers the point where you can feel\ndraw the inter into it and basically you\nget like data that's machine learning\nready ready to be fed into your\nMachinery model right so the other like\nbasically like uh we we talked to all\nyou guys talked about actually running a\nlittle bit earlier right so um you know\nbasically we're very heavy on activity\nbecause my position is the reason why\npreparing data is so complicated is\nmostly coming from the volume right I\nmean so if you could work with data sets\nthat have a size of like 5000 records\ninstead of like 5 million records right\nI mean technically it would be easier to\nvalidate make sure that the annotations\nare right decide whether a specific\nrecord is worthy or not right but the\npain points mostly come from the volume\nand so do the cost and the trading time\nand whatnot right so the there is a huge\nvalue proposition in curating the data\nthat goes like a little bit counter\nTrend with what most people are used to\nright because we are used to working\nwith large data sets because we believe\nmore data means more information which\nis the case but whatever that means to\nunderstand is that information is\ntypically not uh well located or\nbasically it's not well distributed in\nthe data set right I mean so that means\nthat even if you have 10 million\npictures there is no way every single\none of those pictures carry the same\nlevel of information in fact many of\nthem are going to be containing similar\ninformation or whatnot correctly so so\nthis is actually good news because it\nmeans that you can get the same amount\nof information by training on the subset\nof the data uh the question is like what\nsubset of the data and so one of the\nanswers is basically like a Active\nLearning and a more sophisticated\nversion of active learning which is\nbasically what we do happen so I hope\nthis sets the context so uh let's let's\nmove on to that right I think so so I\nstarted talking about\nnot super necessary to understand you\nknow like the the pipeline or whatnot\nbut I think contextually speaking it's\ngoing to help right so the way the\ncommunity is typically building machine\nlearning models is to iterate on the\nmodel right I mean so basically like\nnormally like if you are an entry-level\ndata scientist you don't know much like\nthe way you've been taught to do machine\nlearning in school is basically like a\nyou prepare you you collect your data\nyou prepare your data set and then you\nstart building a model and then you\niterate on that model you go through\nphases of like training validation\ntraining validation until you're\nreasonably happy with the results right\nduring that validation process you're\ngonna tune your Hardware parameters you\nmight change your mind about which is\nthe best like uh architecture type of\nmodel you're going to use and then\neventually when you reach to a point\nwhere you're satisfied then you move on\nto the test phase and you're ready to go\nwith your model right so we we are used\nto the iterativeness of machine learning\nbut the iteration typically was never\ndone on the data set it was always done\non the model side of things right so\nthis is what another Centric AI approach\nlooks like and it's like and it's like\nthe the vanilla machine learning kind of\napproach right our supervised Machinery\napproach what data Centric AI does right\nI mean so it's getting off the bus term\nlike for you know like uh several\ncompanies and like institutions and you\nknow using the term quality processarily\nlike an AI store I still think like a\ngenetic people don't understand quite\nwhat what it means right uh so basically\nlike um instead of doing this you trust\nthat your model is in a different shape\nright I mean which you could do if you\nalready use that model before you're\njust trying to retrain like for example\nin the case of like a mL of the\nobservatability or whatnot right I mean\nso in this case you sort of do the\nopposite you freeze the model you want\nto iterate on the data right and so when\nwe say like you want to iterate on the\ndata like you have many things you can\ndo to iterate on the data right I mean\nso you can actually curate the data so\nselect which trackers are going to be\nused you can annotate the data obviously\nfix The annotation uh change your minds\non on how the annotations are made\nchoose to augment parts of the the\nentire the entire data set in fact data\nCentric AI would also include like the\ncase where you remove records right and\nif you realize like some of the data is\ncorrupted by means of like analyzing the\nreaction of the model there is no reason\nwhy you wouldn't also iterate by taking\nstuff out instead of adding it\nafterwards right I mean so so\ntechnically like really what makes like\ndata Centric yeah usually people assume\nis necessarily like a human Centric\napproach it necessarily means like human\nin the loop it necessarily means like\nselection of the data that's actually\nnot the case it's just basically like a\nnow you're treating your data set as as\na flexible object that can be\nmanipulated I can be modified that can\nbe improved right and that can be tuned\nright I mean basically like during the\nprocess so so it's interesting because\nlike we all talk about tuning usually we\ntalk about tuning the parameters of a\nmodel but like now we need to start\ntalking about tuning the data set itself\nright and so this is what data Centric\nAI is right and so of course your\ntraditional machine learning pipeline\ndoes not support that readability\nbecause it requires more more\ngranularity you have to act on the\nrecord level so basically you're gonna\nface like tons of scalability issue you\ndo need to include the possibility for\nthe intervention of a human user and\nwhatnot right I mean so so uh just to\nexplain quickly for those who are not\ntoo familiar with active learning what\nactually learning actually does right I\nmean so Active Learning is just one\npiece of the data Centric AI puzzle\nright and typically the idea is like you\nhave a very large data set and the goal\nis to\num basically like either prioritize or\nselect the data that's the most\nimpactful to your model right and so\nsomething that you know like uh is quite\ninteresting is basically like a it's\nsort of like doing data preparation with\nthe output of the model or the knowledge\nof how the model has understood things\nso far right a minute so that's a\ncomplete shift in Paradigm with\ntraditional labeling companies because\ntraditional labeling companies think of\ndata preparation and data labeling as an\nindependent step right and so basically\nlike they annotate your data without\nhaving an explicit knowledge of Which\nmodels you're gonna use right I mean\nbasically which difficulties this model\ncan use or whatnot right I mean so what\nI love love love about Active Learning\nis definitely that you know like you're\nsort of like inputting the feedback of\nthe model itself as you create your data\nso it's not the full answer but it\nbrings us like much further right I mean\nso so why simplistic way of seeing how\nactive learning works is like it's\ngrowing a a good data set from the raw\ndata set right I mean so I like the idea\nof growth because like uh I'm gonna\nexplain later but so you iterate many\ntimes on the data that you have on your\nuh on your your original like pool of\ntraining uh Records right and you\nbasically grow this into like uh you\nknow by selecting the data that's the\nmost uh the most useful\nso uh that's that's basically like the\nworkflow that actually really uses right\nI mean so let me explain a little bit\nmore let me so if you want to have like\na vanilla example let's assume now that\nI have a hundred thousand pictures\nfor let's say uh like um you know like a\nimage classification binary\nclassification problem or whatnot right\nI mean so basically and uh I don't have\nenough money to annotate my entire data\nset or I don't have the experts so I\ndon't take my data set so for the case\nof cats and dogs that wouldn't\nnecessarily be a problem now if you're\never taking like a X-rays of like a\nspecific body part that requires Like a\nSurgeon or a doctor doctor to annotate\nthe data then getting someone to\nannotate or a group of people who can\nannotate a hundred thousand records can\nbe a problem right so your goal here is\nyou know like uh why should I annotate\neverything why should I use everything\nwhy don't I just like figure out like\nthe data that contains the most relevant\ninformation that's truly going to help\nmy model learn right so the way you do\nthis is actually incredibly simple but\nit's incredibly difficult to implement\nright I mean so the way that you start\nthe process is like out of those 100 000\npictures you select More Sample let's\nsay 2000 you're going to annotate this\ndata and then you're going to train your\nmodel with it assuming you already have\nyour model right now you're not crazy\nyou know that when you train a model\nwith a thousand records especially a\ncomputer vision model chances are it's\nnot going to be that great for it's\nprobably going to suck right uh so you\nhave no hopes of pushing this to\nproduction this is just a temporary\nstate but the good news is like your\nmodel you have a starting point right\nand so you have a model in a certain\nState and then the name of the game here\nis to try to analyze the state of the\nmodel to try to assume a predict what it\nis that you should select next right I\nmean so at that point you have a\nthousand a hundred thousand pictures\nminus the 1000 you just used so 99 000\npictures which you haven't used yet\nwhich are not annotated yet right uh so\nbasically like now in order to trade\nguess what to use next typically what\npeople do is like run that model and\ninfer on the rest of the data Super 99\n000\nand try to look at the output to make a\na good decision into like what helps and\nwhat doesn't happen right I mean so so\nnormally you would be tempted to say I'm\ngoing to select the data which was\nmispredicted because that's a signal\nthat the model doesn't know about that\ndata yet right you cannot do that\nbecause you don't have the uh the\nannotations right so what you do instead\nis rely on metadata such as confidence\nlevel right I mean so very popular\napproach for this Active Learning\nselection is basically again so you call\nthat the querying strategy is to say now\nI'm gonna look for the pictures that led\nto the predictions with the least\nconfident score right the minute so this\nis called the least confidence approach\nfor activity then you take your thousand\nrecords you put that back in the\npipeline so in my little like\nillustration here you go from uh using\nthe information of the info step back\nand you feed that back into selection\nfor the next step right and so you use\nthe output of a inference unit process\nto basically like make a decision what\nto select and then you keep labeling\nwhether you added you retrain right so\nthat comes with a bunch of problems\nincluding you need to version the model\nyou need to repeat you need to reuse the\nsame processes many many times right uh\nyou you need to be able to immediately\nreuse the model you just trained okay uh\nand and all sorts of things like\nassociated\ntypically not what you would do through\nor with a machine learning model right\nthe other part of the challenge here\nwhich we also sold over the years is\nbasically like uh this happens in a\ncontinuous fashion so you also need to\ncontinuously label right I mean so\npeople usually think of labeling as like\nan asynchronous step in our case like\nwe're really trying to build something\nwhere uh it happens on the Fly and it\ncan happen on the flight it should\nhappen on the flight because now you're\ntalking about much smaller volumes right\nand so going back to my 100 000 pictures\nif you're lucky maybe only 10 000\npictures contain the right information\nwhich would mean like by picking a\nthousand you would stop after 10\niterations right uh and the English\nbatch is going to be a thousand records\ninstead of being a hundred thousand\nrecords which basically like makes it\nmuch easier for a smaller group of\npeople or even your own team to do that\nthemselves or if you use a third party\nbasically they should be able to get you\nback the results very quickly so this is\nbasically like I came up with this\npicture on the data prep\nanyway so this is basically like what\nwe've built right I mean so basically\nlike so it's by no means like the you\nknow like uh like I've been like the the\nthe perfect solution there are lots of\nother pieces that need to happen but\nbasically just to give you a sense of\nlike the how the system is built and the\ndifferent pieces how they need to\ncommunicate to each other right I mean\nso basically you go back to that\nworkflow before it the way that we've\nbuilt the process is basically for one\nwe don't host the model we don't host\nthe data we just coordinate so it's\nalmost like a federal Federated learning\nuh process with just like one instance\nwhere the data and the model leaves with\nthe customer for the sake of providing\nthem privacy right I mean so basically\nlike they don't have to share the data\nif they don't want to right uh and uh\nfor a second reason basically because we\ndon't want to host the data the crazy\none large data sets of our customers too\nso that SDK process over here right in\nbasically like it's uh it's it's built\nin a way where we trigger the training\nprocess and we can basically like\nretrieve so that's part of the pipeline\nlike we can record what happens during\nthe training process including like\nlooking at the loss function the\nactivation functions or whatever Clues\nwe might be using right and so that's\nthat information gets piped back into\nour system right and so this is where it\nbecomes important to have like a solid\nscalable workflow that can basically\nlike retrieve like a relatively large\nlike sets of data that you know\nbasically relate to like what you could\ncall that the log size of the trading\nprocess right so the piece in the middle\nhere is our back end right I mean so\nbasically like in here you have like uh\nseveral like micro services including\nlike uh an active learning microservice\nso we don't call that Active Learning\nbut it follows like an active learning\nworkflow because you're gonna do this\nrepetitively but the logic on how data\ngets selected in our case is much more\ncomplicated than just relying on the\nleast confidence level right I mean so\nin fact we look at all dimensions of the\ntraining process so that what gets\nselected is actually what matters right\nI mean it is actually like what is uh\nsupposed to be the optimal subset of\ndata that's the most likely to impact\nthe training process right I mean so you\ncan see this as being a machine learning\ndriven or a smart version of activity\nwhere we don't use arbitrary rules right\nand then when the data gets selected\nbasically of course you need to annotate\nus for my workflow earlier right I mean\nso you select and then you label right\nand again because this is this is\nliterally like curating and preparing\nyour data on the fly as you train you\nalso need to be able to annotate the\ndata on the fly as usual right so uh the\nthe rest of the pipeline for us is\nbasically like a we can route the data\nthat was selected uh from the user from\nthe user or the customer to the labeling\ncompany which we selected right and so\nno need to go in the details we have\nrecommendation systems to like a uh\nfigure out who is available among the\nlabeling companies we work with how\nthey're like how many people can can\ntake care of this and whatnot right uh\nbut so basically like so the data goes\nthere so there's some new pipelines that\ncan uh securely like share like the\nindices that were selected the data like\nthe expectations from the user and\nwhatnot right then uh it gets annotated\nso in fact we have options to do that\neither with the human annotator or with\nan auto labeling model right and then\nthat comes back on the platform\nhopefully like within like less than an\nhour because the batches are smaller\nright and then it goes on the UI of our\nplatform and people can play with it\nright I mean so they can say like wait a\nsecond I asked you guys to annotate uh\ntrucks as cars but you annotated pickup\ntrucks as trucks or whatnot right I mean\nso uh the user has the option to like\ntune the data that comes back or whatnot\nand so depending on whether they're\nhappy with it like if now for example\nthey say like a 20 of my data is\nmisalutated they can send it back so\nbasically like we have this uh pretty\ninteresting workflow where we deploy\nlike an auditing system or we look for\nanomalies we help like a sister user in\nfinding problems uh if there are\nproblems they can re-annotate if there\nare no problems they can move forward\nwhen they move forward basically like\neverything needs to be versioned that's\nobviously a big problem because if you\nhave like a an original data set of like\na uh hundreds of thousands of Records\nbasically you need to keep track like\nwhich ones were were the right version\nfor the annotations right we have a\nsecondary workflow that's also\ninteresting where you know like um uh\npart of the Unica if the annotator made\ntoo many mistakes or the user made too\nmany Corrections basically we can mark\nthem as an inappropriate like vendor for\nthat space use case or whatnot and so\nboom everything gets done goes back into\nthe system we send the signal and\nbasically like the\ntraining process retrains right I mean\nso basically like so so we're talking\nabout a workflow which is like very\nheavy in terms of like repeating the\nsame operations over and over again with\ndifferent versions of the model with\ndifferent subsets of the data where you\nneed a ton of versioning and you need a\nlot of stability and so God forbids but\nthe biggest disaster that can happen is\nlike you run seven loops and then your\nprocess freezes for Loop 8 and you have\nto start everything over right and that\nused to happen a lot with airflow right\nuh so basically like historically going\nback to like the times when I I started\nthe company so airflow was known as the\nnatural kind of uh you know like option\nfor uh data engineering data data\norchestration or whatnot right and we\nconstantly used to run into problems\nwhere you know like a process that's in\nskill or like uh there is an error which\ncannot be tracked or repeated or\ndebugged easily or whatnot right I mean\nit's all coming back from the fact that\nthe intensity of the process because now\nyou're you're looking at\nmulti-dimensional aspects of the model\nof the data or not like basically like\neffort was just not doing the job\nanymore right so we wrote a little blog\npost like a couple of weeks ago like all\nof the problems that you facing we Face\nstraight a bit so I don't necessarily\nwant to go in every single detail right\nuh but so we had problems with\nversioning capabilities the tasks were\nnot reusable the problem we have in our\ncase is like this selection process this\nquerying strategy like that powers the\nactive learning workflow depends on the\nmodel so you need the same workflow but\ndifferent variations for different use\ncases different customers so basically\nthe careful was not enabling us to do\nthat without like explicitly copy\npasting the tasks right I mean so that\nwas a big problem right uh for whatever\nreason like airflow doesn't seem to play\nnice with like the communication of data\nfrom one block to another and so\nbasically four data Centric AI approach\nthis is an absolute disaster uh there\nare other like secondary kind of\nproblems where it wasn't necessarily\nplaying nice with like our microservices\nthe other parts of the system or whatnot\nthe plugins that we need to use because\nin our case like every block is not just\npassing gate it's basically like an\noperation that requires like either to\nwait on another process to be complete\nor whatnot right uh and we had a huge\nhuge huge problem with horizontal\nscalability so basically they got we\nwere sort of doing okay with like large\nvolumes of data to curate right and to\nprepare but we were doing terribly when\nyou had more than like a three four five\ncustomers running the process at once\nright uh and So eventually we reached\nthe point where uh the engineers were\nlike so be the customers were in your\nfirst and then the engineers are like we\ncannot keep doing the doing this because\nlike car it's impossible to debug\ncustomers report like a specific errors\nthat they try but basically when we try\nto reproduce the error it just doesn't\nhappen the same way on our side or\nwhatnot right and then so a decision was\nmade to move on to you know like\nbasically like other uh like explore\nother\npotential like workflows or whatnot\nright and so we we explored like a\nprefect we explore like a cube flow and\nwhatnot and eventually we felt like the\nbest option for us was flight right I\nmean so uh you want the details so\nbasically the ash was on the call as\nwell is basically like one of the people\nwho led the migration so I'm sure you'll\nbe happy to discuss longer until like uh\nwhat really made us feel that uh flight\nwas not only just better than uh airflow\nbut it was better than any other option\non the market at that time\nso anyway so we're fully migrated on\nflight nowadays right and so basically\nyour first like data prep-ups like uh uh\nend-to-end like workflow out there is uh\npowered by flight so if that matters\nright then hopefully it matters uh and\nso basically like so uh uh of course\nit's a small part of our system because\nour system is like really a collection\nof like like different like uh uh\nlabeling labeling validation processes\nselection processes but so the backbone\nis really flight for us writing because\nit's a it's really something that helps\nus scale and reach the level we're at\ntoday right I mean so yeah I mean\nbasically like just a couple of examples\nof our architecture the full\norchestration is powered by by flight\nI've never had a problem since with like\na integration of other you know like\nbasically like a processes or the\nTechnologies and whatnot like we have\nlike a much fewer complaints than ever\nbefore right I mean so yeah so uh if\nsomebody is trying to do something in\nthat space I strongly like suggest that\nyou know like uh you you don't even try\nto get started with the old Legacy like\na orchestration systems that are on the\nmarket\nall right that's it for so so I hope\nthis was interesting like we're happy to\ntake any questions before anything comes\ntomorrow\num thanks Jennifer just an FYI I now\nhave a full understanding of active\nlearning thank you so much for\nexplaining in detail I appreciate it\nCollective yeah I like this mission is\ncommendable\nyeah anybody questions\nall right okay okay\nthat's the question yeah I was gonna say\nif you don't have questions I hope that\nthat speaker is super clear not because\nit was boring but\nI always have questions so I let people\nunderstand I'm always asking questions\nuh but I think it's more than a question\nI think what I wanted to say like uh\nEcho what samita said I did not know\nyou know active learning as crystal\nclear as now I do so thank you I think\nfor a community this is part of the\ncommunity thinking also the community\nkind of learns and grows from each\nother's experiences so thank you for\nsharing that we will do our best to\nspread your message to the community and\nBeyond because I think it's needed\num as I have in my prior life I have\nfaced the problem but like how do you\nlabel this data and this is a lot of\ndata how do I enable it right\num so I completely understand uh\nwe what what was amazing to understand\nis that they kind of laid the groundwork\nof why you needed something that fight\nwe would love to dive more into details\nI know some of it could be proprietary\nbut like how did you actually end up\ndoing it what the system looks like\nbut I think I at least I get a basic\namount of idea but\none question that I would like to know\nis how did you hear about flight\nand uh so look look I read a lot we read\na lot it was just like one thing I knew\nat some point is like I'm done with\nairflow right that is so basically like\nso so interestingly the first option was\nlike you know like Hill flow became\npopular like basically a few years ago\nso basically like that was like one of\nthe main like Alternatives you had like\nespecially if you were working with uh\nkubernetes but so uh uh yeah she's still\non the call right I mean you guys talked\na lot and basically like I think there\nis a quite a lot of prototyping done and\neventually like one day it was clear\nthat it was flight and nothing else\nright\nwhat technical like decisions were\nexactly made like it's more like I'm in\nfact from my point of view it looks a\nlot like uh like the quality of uh you\nknow basically was easier to use for us\nlike there was more support from your\ncommunity for busy political solving\nissues or whatnot right and basically\nlike so generally speaking like a\nstability scalability or we're not\nwriting so basically we we faced other\nproblems like uh when when using\nworkflow where we felt like even if you\nmigrate that or there are a lot of\nproblems that we were facing with\nairflow we are still going into the\nFacebook yeah\nyeah if you're still here you want to\nadd something if you're free tools\nI mean those like basically my\nengineering team fell in love with light\nat some point and then basically like\nwhen I saw how quickly we were able to\nmigrate and be with political cover all\nof our problems I was a no-brainer thing\nyes you were saying something\nno no we are going to sorry\nI\nuh\nokay I'm sorry for that\num no so I think you answered the second\nquestion also because you know\noftentimes when people come to the\ncommunity they are like we are\nevaluating\nbecause today this is the world where we\nhave to evaluate for a few Solutions uh\nbut\nI think sometimes some some really\nlarge companies of being me have had\nquestions about how long would it take\nto buy constricted and that's like the\nconcern usually because they are so\nentrenched in our Legacy ecosystem at\ntimes\num so hearing this and that you were\nweak and that's why I wanted to know\nlike when did you learn and how much\ntime I'm gonna say right I mean so you\nsee the complexity of your workflow\nright I mean it's not just like a\npassing gate or whatnot like you have\nlots of like moving blocks or whatnot\nright I mean so I'm gonna say like the\nthe main migration of course they're\nalways like you know like tricky details\nfor like you know like different types\nof data what not but like uh the whole\nmigration was done in less than a month\nright I mean basically like the the\nactual creation like of the workflow\nwait a minute so uh you you guys were\namazing for you know like basically like\nuh like we're using GCT basically like\nfor most of the storage and the hosting\nright and so uh that's that that was\nlike apparently like that was not like\nthe natural like decision or like the\nmost common obviously people your user\ntend to use more AWS or whatnot so on\ntop of the technology itself like we\nfelt really supported by people\nproviding answers relatively quickly\nright and basically like we had a couple\nof like uh you know like basically like\nuh correct me if I'm wrong but basically\nlike a you know like a basically they\ncan a garbage collection issues that\nwere not like uh you know supported for\ngcp or whatnot and same thing like you\nguys fixed up like in just a couple of\ndays\nwasn't an issue at all\nthank you Jennifer that's it from my\nanybody else to ask\nI have a just a quick question about\num how you think about uh data as code\num the whole\nidea of you know machine learning and\nyou know whatever software 2.0\num where the ml model is simply a meta\nprogram that optimizes the mapping\nbetween data so\nif data is the first class citizen here\nlike what what are some constructs and\nhow has flight helped you kind of Reason\nabout and handle but the complexity of\njust like human labels right before uh\nbefore going into Data are scored right\nand basically like so like look I think\nall we're trying to do now and social\nGPT is sort of the proof of this is like\nin power like uh the Regular Show to\nbasically use the ER on their daily\nbasis right and so uh of course we want\nthis to expand into like not just using\nlike a a standard application because of\ncourse GPT is like a chatbot so uh\nyou're like basically like it's it's\nkind of a general purpose but like\nbasically like a if a person in any\ncompany who's not a data scientist wants\nto build a model for whatever it is that\nthey are doing they should be able to do\nthat that's the purpose of low code no\ncode like whatever like a whatever first\nstep you take in that direction right\nand so that that was exactly like it's\nit's a big part of the mission of what\nwe do because you look at what you have\ntoday you have like data robot for\nexample sample that pretty much\nautomates like a the choice of the right\nmodel like the choice of the features\nyou have like lots of Technologies and\neven open source software that help you\nlike tune the model or whatnot but the\ndata is the bottleneck and so it we all\nspend ridiculous amount of times as data\nscientists basically like figuring out\nlike uh what should I use what should I\nnot right in fact in fact there is this\ndanger that basically if you carry the\ndata manually you induce biases and so\nlike you know like basically like we've\nseen that problem recently with a charge\napt that you know like a any strong\npolitical opinion is being being cleared\nfrom the uh at the content moderation\nStep at the labeling step or whatnot and\nthis is like you can see that as a good\nthing because you're protecting the\npopulation from like strong like a\npolitical like uh anti antagonization or\nwhatnot right but you can also see this\nas being a bias threatening and so of\ncourse it's a bias uh so the fact that\nwhat we're building it's really like\ntaking the response of the model to an\npower it's almost like empowering the\nmodel to pick its own data for its own\nsake right I mean so it's like a it's a\nit's giving the opportunity to like\nbasically like take the same stuff this\nis why you started my my presentation\nlike this right I mean where ml Ops\nreally brought us to the point where you\ndon't need somebody with a PhD anymore\nto build and deploy your Machinery model\nright you don't need to a team of 20\ndevops engineering that that was the\ngoal right so it's like empowering the\nsingle person even though at this stage\nyou probably still want to have an\nexpert right but so the same move and\nthe same transition needs to happen for\ndata because like at this stage it's\nsort of ridiculous that basically you\nhave pipelines and sophisticated\npipelines and automation for tuning\ncreating and deploying a model but you\ndon't have anything like this for uh\npreparing the data right I mean so this\nis like really a lecture is also trying\nto go towards like a local data and So\neventually by plugging both you're gonna\nend up in a situation where it's like\nand then to solution\nawesome thank you\nhey Jennifer this morning I'm with you\nand then how are you today\nhey Martin sorry to step in but I think\nsaid that other question before you oh\nI'm sorry yeah let's say that now and\nthen yeah that's all right that's all\nright thanks\num I think uh my question is not\ndirectly related to uh the platform as a\nwhole but rather one of the components\nwhich is inside so that's why I'm not\nreally sure if uh you know Jennifer can\nclarify this maybe it's something\nproprietary but I just wanted to take\none small example of object detection\nand data preparation for object\ndetection especially in the scenario of\nlet's say retraining the model yeah so I\njust want to think of like I just want\nto put this question out and I mean I'm\nI would really love to know how you\nhandle this so let's say for example\nthere was an image which was trained on\nthe models trained on that image and it\ninfluences on another image\num let's say it it defects it detects a\nuh object which is a a you know a cup or\nsomething of that so some sort of object\nonce the detection is done and it comes\nback to the user in your case and then\nmaybe he finds out that oh it may be I\nthink the model only saw the partial\nmaybe a half of the cup it didn't see\nthe entire cup so now the bounding box\nof the cup it has to be you know\nretrained or sort of like sent back\nright like this is your feedback right\nand maybe that's one situation the\nsecond situation is it saw a red cup\nwhen it's actually actually an orange\nCup right so what I'm trying to get to\nis there may be many factors where the\nuser might have some edge cases maybe he\nhis bounding box is not complete maybe\nthe color is different and so on right\nso I was wondering not as a plot maybe\nas a it's a two-part question as a\nplatform how do you address this and how\ndo you internally keep track of all\nthese feedbacks that the user is giving\nso yeah let me so it's a multi-part\nanswer and obviously and you're right\nit's an incredibly complicated problem\nbut so basically like so our current\nworkflow the purpose is basically like a\nuh you you mentioned basically like the\nperformance of the model on the test set\nright I mean so basically like so right\nthat like we're not we're real to some\nextent the basically we're not directly\nlike the purpose of like the end-to-end\nworkflow the automated part is concerned\nwith the selection of training data and\nthe annulation of training data right I\nmean so to some extent that is your\nquestion relates to that as well where\nbasically like so what's really cool\nwith active learning is like Active\nLearning basically like uh measures\nwhere the model is weak and typically\nthe model tends to be weaker in places\nwhere uh you have like a lower density\nof information so let's say I do I do\ncats and dogs if out of right chance I\nselected a lot of dogs my model is going\nto be weaker on cats right it's gonna\nit's gonna know it but you don't even\nneed to know it's a cat you know that\nthere is a fraction of the data where\nthe confidence level the entropy\nbasically like all sorts of signals\ncoming out of the model are going lower\nso it automatically rebalances right\nregardless of that so you select that\ndata you send that to your labeling\nprocess which can be automated or a\nlabeling company then that comes back on\nthe platform on the platform we offer\ntons of options for people to evaluate\nmanually to use like a normal anomaly\ndetection Frameworks or whatnot right\nbut at that point the data hasn't been\nused yet right I mean so basically there\nis an iterative process where we\npractically don't allow the user to\nproceed with the next batch until the\nthe validated some quality right over\ntime we're adding more and more so if\nthat problem happened with the cut where\nthe cup has been annotated but it's not\nfully annotated uh basically like I'll\ngive you an example of something we do\nright I mean so we check like the\ndistribution of like um uh the object\nsize right I mean so basically let's say\nmost of your Cups have a height or a\nMass Spectrum of like this much by this\nmuch if your cup was not fully annotated\nright I mean basically like\nthe ratio would be different that does\nnot mean it's wrong but we're we're\nwarning the user please cross check and\nthen we can decide to fix it or they can\ndecide to send it back to either the\nsame labeling company or another\nlabeling company right then in a\nseparately separately what we do is like\nyou have reports where we show the\ndistribution of everything so again your\ncap example you would see all of the\nannotated caps that you will see like um\nwhere they fall on average on the images\nor whatnot right and so basically like a\num it's it's allowing you like a way to\nlike identify like the harder objects or\nthe ones that are like right explanation\nor whatnot right I mean so what is the\npotential next step for us is basically\nlike you could also like use the\nannotations and the predictions coming\nout of the uh test set right I mean and\nbasically like also feed this in the\nsame reports and that you would see like\nwait a second how come uh all of the\ncups in the process are on all the cups\nin the data sets are always located on\nthe bottom left of the images\nit might be a future might be about at\nleast you bring up\nthanks a lot\nMartin\nyep so I I just have a very simple\nquestion about synthetic data uh\nJennifer obviously we have been talking\nabout synthetic data for quite a while\nand so I was just wondering how this\naffects you have you actually worked\nwith I mean I saw in your chart earlier\nyou have synthetic data how does this\naffect you know the overall outcome\nusing synthetic data versus not and I\nassume that's dependent on you know what\nkind of cognitive service you do if you\nuse each or whatever else yeah your\nfirst four seconds okay I think I got\nyour answer your question so so look so\nbasically like so the next generation of\nwhat we do is like so the regular Active\nLearning is basically like you have a\nlot of data but you want to select the\nbest subset right but you don't make up\nadditional data right I mean so\nbasically this assumes that uh somewhere\nin your existing like collected data set\nyou already have the answer uh it's just\nthat there is a lot of noise on top of\nthis right now the one thing no one ever\nthought about which is kind of like the\nfuture effect of active learning Within\ntwo is basically like a if you trust\nyour active learning process you have\nlike a really like scientific way of\ndoing Active Learning uh not only you\nprioritize the most useful data but you\ncan start looking at correlations right\nI mean so basically like one of the\nthings that we're capable of doing is to\nsay like look when like the active\nlearning process is called\num a higher density of trucks first\nright I mean so for example I've seen\nthat situation where all the trucks we\nhad in the data set were called First\nand prioritized and then you run out of\ntrucks and then the learning curve and\nthe the accuracy for trucks stopped\ngoing up which makes sense right but so\nsometimes you have crossed the effects\nwhere you run out of cats and then the\naccuracy for dogs stop going up which is\nbasically what's going to happen when\nyou are at uh you know basically for the\nthe the boundary between cats and dogs\nis sort of like a clear rate so you can\nuse that sort of feedback to say like\nokay I need more dogs right you know I\nneed more trucks or whatnot right uh and\nso that leads you to potentially like\nhaving like and so we have that workflow\nthat it's not deployed in Project\nrightly basically it's like a uh based\non that kind of material uh you can\naugment and you can synthesize data that\nyou have identified for being useful but\nyou don't have enough of in your data\nset right and so it opens the door to\nNext Generation basically like of data\nproblems where you start with natural\ndata but basically like you use\nsynthetic data as a as a compliment\nright and so on usually like today when\nyou look at the ecosystem you have\npeople using synthetic data you have\npeople using natural data and usually\nthey don't talk to each other they don't\nsee a way to you know like work together\nand I think this is the this is the\nperfect so I see I I think I take it\nthat this way today you look at it as\nmore a uh a method to treat and balance\nuh and then without recepting under\nsampling and so on according to that and\nthen down the future I think it's it's\ngoing to replace I feel like you know in\ncertain areas if you see how synthetic\ndata has already replaced to replace\nit's like really like you know basically\nlike making it play nicely together\nright I mean because basically like so\none of the the thing nobody thought\nabout is like people use Active Learning\nin a way to like prioritize that data\nbut they don't think that the process\nitself provides you feedback on what the\nmodel means how the model learns in fact\nyou can even use that as an\nexplainability framework right and so\nit's a it's it's a completely new area\nwhere people don't don't you like do you\nthink of explaining ability you think\nabout like which features were\npredictive of a specific alcohol but\nwhat not but in our case it's basically\nlike uh you know like a a real\nopportunity where you can say like huh\nthis five percent of the data was really\nuseful why was it useful what was\nspecial about this right basically is\nthere a higher density of information in\nfact we found lots of things where uh\nmodels start to struggle when the\ndensity of information is too high which\nis an interesting takeaway because like\ntechnically what I'm saying is like uh\nthe distribution of the data the\ntraining data does not need to budget\nthe distribution of the test set which\nis like uh like uh you know like light\nyears from what we are used to thinking\nright and so basically we have had\nsituations where you can train with a\ndata set where the maximum number of\ncars per image is like three four five\nbut the model still does an excellent\njob at predicting like uh images in a\nsituation where you have 20 cars in the\nimage right\nthe best you have a lot more questions\nI'm gonna hold offline\n[Music]\nwe have gone beyond the allotted time\nfor the meeting but I have a quick\nquestion\num Niels mentioned earlier that Active\nLearning usually requires some kind of\nmanual intervention during the labeling\nprocess so how beneficial do you think\nflights signaling or human in the loop\nfeature can be in your case just wanted\nto bring it up because we demoted\nearlier so it's true I'm not sure the\nsymptoms so basically like so the way\nthat we set up Active Learning is like\nso look again I don't want people to\nconfuse human in the loop and active\nlearning right I mean so technically you\ncan have a setup where Active Learning\nis you use this programmatic approach to\nselect the data and then you round this\nto an auto labeling model like a Euro V8\nthat's going to generate the annotations\nand it comes back right uh of course of\ncourse like if you're going to use human\nintervention this is the place where you\nwant to do that because you can either\nvalidate the annotations that come back\nor you can add your own like take online\nlike man I don't trust the curation\nlogic in there what not right I mean so\nbasically like so uh so definitely like\nbasically like the fact like flight has\nmore flexibility for basically like a uh\neven workflow interruptions right I mean\nbasically like an imposing and whatnot\nbecause like again going back to flights\nuh I cannot tell you the number of times\nwhere you have a process waiting for the\nannotations to come back they come back\nlike maybe two hours later by then like\nbasically like the the workflow has\nexpired right I mean so basically like\nthis is a very very important feature\nover here right I mean so and I I think\nthat's like flight is the only like a\nworkflow that you like I know is not\ngoing to have that problem or not not\nthat obviously right uh and uh yeah I\nmean so basically that's a while but I\nmean I would say flight gives options\nright I mean to use like uh because like\nthe same problem that applies though\nbecause we're talking about human\nintervention as being like basically you\nneed to pause the process and basically\nhave something else coming in so\nbasically like almost like uh the real\nvalue is like connecting to different\nworkflows which can be each asynchronous\nright I mean so basically like a and you\nwould have exactly the same problem if\nyou used an external Auto labeling like\nmodel anyways right I mean and so\nbasically like a yeah yes flight does\noffer a lot of flexibility in data in\nthat sense\nalso key to point out that signaling the\nthing that sends the signal back to the\nflight workflow can also be a flight\nworkflow so important thing to note\nawesome\num thanks again Jennifer for joining the\ncall and sharing your journey with us\ntoday love the discussion would love to\nhave these kind of discussions at every\nOSS sync"
    },
    {
        "title": "Flyte Signaling Demo - Human-In-the-Loop Workflows",
        "transcript": "now he is going to demo the most debated\nfeature of flight and that's signaling\nhey\ncool everyone can see this\nyep all right let's start here\nall right so uh I'm gonna hear I'm here\nto talk very quickly about uh human in\nthe loop workflows with flight this is a\ntopic that we\ncovered uh some time ago I think Dan\ngave a pretty in-depth design and\nimplementation discussion back late\nsummer early fall last year it has since\nbeen implemented and is uh has been\nreleased in 1.3 Earth flight so welcome\nyou all to try it let us know how it\ngoes and report back with any feature\nrequests or issues that you might find\nthat said we\nquick agenda for today we're going to\njust rehash uh refresh for everyone what\nit is we'll discuss the ux a little bit\nremind everyone of the design and\narchitecture and then we'll jump into a\nquick couple demos that hopefully\nhighlights some of the things that\npeople can now do with this new feature\nso what is human and the loop first off\nhuman in the loop is just\na human that can a workflow where the\noutcome of the workflow in the course of\nthe flow of the workflow is controlled\nby uh a person after the after the\nworkflow starts so typically like you\nonly\nuh put in inputs at the very beginning\nthis will now allow a human to intercept\nsomething and either redirect the flow\nto different tasks or add inputs or\napprove an output of an earlier task and\nthis will also bring in the Sleep\nfunctionality which is uh not a human\nbut is related to the implementation\nand why might you want to do this here\nare some classical examples with this\nguy uh\nlabeling so if you are if you can think\nof like labeling cats and dogs a\ndeployment pipeline so maybe this is not\nso much in the services sense but maybe\nin like the modeling sense\nyou can think of human in the loop as a\nway to control engage deployments of\nmodels and experiments\nsupervised Active Learning where a\nperson has to go in and approve or\nannotate some outcome of some model\nuh model running data curation so\nGathering data\nand canonical classic use cases that are\nlike more business sense like expense\nreports it's a classic one uh where\nthere is an action that someone\ntypically higher up would need to look\nat before\nuh before the working can proceed\nand\nthe developer experience that we wanted\nto go for is uh the same as what we\nalways strive for in slight and slight\nkit so should be\npretty pythonic native issue leverage it\nshould be easy to understand it should\nleverage flights existing type system\nyou should be you should be able to\ncompose it with all the existing flight\nkit features and functionality and it\nshould be simple to interact with and\nyou can do so either from the UI or from\nthe flight remote experience inside of\nlike kit or generic grpc HTTP apis if\nyou Associates\nand what does that look like it comes in\nthree different flavors so the first\nflavor is waiting for user input so you\ncan imagine that a workflow proceeds and\nthen at some point it stops and it needs\ninput from a human because it was not\nfor some whatever reason available at\nthe beginning of the workflow execution\nso this is how you would do it you call\nin wait for input you give it a type and\nthen with this the the execution will\npause with some time out wait for\nsomeone to enter in what the input\nshould be so you get a little box like\nthis and\nwe'll see this again momentarily when we\ndo the demo\nthe second flavor is waiting for an\napproval so this is basically like if\nyou look at this construct we're just\npassing in the output of a prior task\nexecution\nagain it has some timeout after which if\nnot approved it will fail and you can\nuh you can\napprove it you can disprove it or you\ncan just support your workflow\nand lastly is the affirmation sleep\nthis will just\nuh basically do exactly what you expect\nand this might be the example that we\ncame up with was waiting for something\nto waiting for a pipeline uh just to as\nlike a last step validation in a\ndeployment pipeline\nas a reminder so this is roughly how it\nlooks\nthe signals themselves which uh these\nall these three variants get turned into\nare stored in a database there is a new\nIDL service\nin Flight IDL that is implemented by\nflight admin\nand users will send signals sorry users\nwill fulfill signals either through\nconsole or through to your PC rest\nto the signal service and flight\npropeller\nin its operation will periodically check\nfor the fulfilled signal and then move\non or time up the node so\nthe demo will look something like\nthe following so uh let's talk with the\ncode first so yeah this is\nthis is our example of a demo pipeline\num obviously this has been simplified\ndrastically to mock out any ml routines\nthat might actually be present so\npretend that this function here will\nevaluate a model the model is either\npulled from a wall notification or you\ncan pass it in as an argument\nthis function will basically take the\nmodel uh let's say you\nreturn some score and then let's say you\nhave another function that is capable of\ndeploying the model to a certain\npercentage of your Fleet\nso you might construct a workflow\ndeployment sorry a model deployment\npipeline workflow that looks something\nlike the following so first see if it's\ngood and\nyou have a chance to approve whatever\ncomes out of it\nthen deploy it to 10 percent\nwait to see if that behaves as expected\ndeployed to say 50 of your Fleet and\nthen just wait for an hour and then\ndeploy to the rest of your pipeline so\nat the end if you do this it'll look\nsomething like this\nwhich I can probably examine they're all\ngood uh but yeah just straightforward\nlinear pipeline let's relaunch this guy\nand we can see\nit in action\nprobably should have done this earlier\nuh this may\nwell let's see how long it takes\nthis might take a second while this is\ngoing let's talk about this one\nthis is\num\nuh another input another workflow\nshowing the the wait for input style\nthing so the the example here is for\ninstance if you were doing some labeling\nand you have a task that will generate\nlet's say digits in the form of a numpy\narray and you can either choose to\ninspect those whatever is generated by\nthis using the flight deck feature\nwhich is included here or you can\ninteractively just pull the output of\nthis task from like the Jupiter notebook\nwhich we'll do in a second and\num\nlabel it manually and then inject those\nscores or the interject those label\nvalues back into the workflow\nso in the interest of time I will kick\nthis one off as well\nfor a relaunch and then here this should\nbe now paused so this is the first pause\nthis is just an approval node you can\neither click on the little arrow here or\nthere is another arrow in the graph\nand\nyou're going to prove it it also shows\nyou the value that was popped in\nat some point this will now so gate\nnotes when they're running\nwill show are special and that they show\npaused so that's a like an alias for for\nrunning basically\nand same thing again here\nthis shows this is the value that was\npassed into it you can resume\nuh it will structure it that way instead\nof the initial value so that it would be\nit would look like a completely linear\nthing and not depend back on the\noriginal score\nand less sleep takes an hour so we'll\nnot do that but sometimes just to say it\nwill hang there for an hour\nor 58 minutes I think it was just a\nlittle set to and then uh move on to the\nlast last step\nso here let's say you have a task let me\nget this a little bit bigger also\nlet's say you have a task that produces\nsome images you can inspect them through\nI mean this is the article scikit learn\ndata digits I just pulled in a random\nset of 20 from some somewhere in the\nmiddle of the range\nand\nlet's say you want to interact with this\nhow would you do that\num\nI think\nso working again with flight remote\num\nsome code to\nthis I didn't do\nsome code to get the value\nvalues from the prior task and\nplease hold\num\napparently I forgot to\nrun through this again\nlet's try that again\nforeign\nhopefully this will go better this time\num so again\nyou this is just to show that you don't\nhave to rely on on the flight deck the\nflight remote objects will be able to\nlist signals So currently here it only\nhas one\nif you're more it will show them as a\nlist of course\nin terms of labeling rather than going\nthrough we're just generating some\nrandom numbers and\na random set that just coincides and\nthen you can send the signal send the\nlabels that you can annotated back\nthrough flight remote into the running\nexecution and they will show up as\ninputs to the to the signal which are\nthen passed down to whatever whatever\nhas dependency on that node\nso and here whatever was generated is\nnow going to be should show here\num as the outputs of this guy and then\nlet's say your uh whatever testing\ntesting tasks will will take that as an\ninput\num yeah I think that's it uh let us know\nif you try it out running to any issues\nand\nany other feedback we might have thank\nyou\nI have a good question yeah so my name\nis Martin I'm with Union uh ye this is\nfantastic so for for\num as this is a service and stored in a\ndatabase I assume that somebody could\nactually do a report and use logging and\nsee what approvals were in the past and\nso on\nI don't I believe so yes\nokay so that would be a very valuable\nfeature because specifically in labeling\nif you go through and you run through\nyou know not approval but actually if\nyou do an approval and and that is\npotentially the wrong approval and\nsomebody has to go back and double check\nuh that would be good to have this lot\nand obviously if there's a ability to do\nso that would be fantastic yeah I think\nit will probably I think we will\nprobably add an API on top of that uh\njust to discourage raw querying of the\ndatabase but yeah it is persisted so the\ndata certainly there and it is possible\nto get back okay great thanks\nstop sharing\nany more questions\nso I just want to say this is super\nawesome\num\ne so basically with what you showed you\ncould\ntheoretically\nlike iterate through a batch\nlike in batches\num iterate their entire data set and\nlabel it\npurely using flight and then some UI\nthat uh has access to\num enough data for a human to be able to\nlike say what the label is yeah for sure\nit is possible I'm gonna let Keith and\ntake this one actually but uh it's the\nthe flight console itself may not be the\nit depends on the size of what you're\ndoing but if you imagine like\na team of 100 people working to label\nlike a hundred thousand things that's\nprobably not the best thing to use uh\nbut it's certainly possible to use this\nfor smaller data sets\nwell Active Learning is the perfect use\ncase because that the whole point of it\nis the model picks some handful of\ndata points since the that it that it's\nlike hard like right along the decision\nboundary for the two the human to\ndisambiguate so\num\nyeah I can I can see some kind of like\nHigh script\nhacking going on here uh in a flight\ndeck itself so that you can send you\nknow the you can do the grpc call\ndirectly in a flight deck\nnot suggesting we do that but you know\nin theory that that is possible\nand I think for for more serious uh\nworkflow uh use cases like when you do\nlike you you set it large scale labeling\nuh then I think it's it's it's probably\nyou know you need an interface to do\nthat uh but I think this is you know\nwell prepared to to surface such an\ninterface down the road as well uh so I\nthink I just want to say this is\nfantastic also in the context of\num as we have seen you know what\nhappened with chat GPD and you know\nmodels have been trained in in a very\ninteractive way I think this is a\nmassive step forward first and I think\nthere's a lot that I can see from the\ncommunity coming back to you know\nexploring this functionality so I just\nwant to say uh to the team who has\ndeveloped this then and me and so on and\nwhoever else was involved this fantastic\nthank you very much super exciting\nyeah\nI just uh the goal of and I think my\nname is Katelyn\num I uh mostly do nothing much I just be\npart of the community at the moment uh\nbut what I\nuh when we started this project and we\nwere discussing we what we wanted was it\nto be the canonical infrastructure piece\nthat you don't have to rebuild again and\nagain like this that was the goal of uh\ndoing this and when we did it you're\nlike okay for many basic use cases\nyou should not also have to rebuild the\nUI that's what the goal was and so\nflight console is a simple UI that money\nmany companies already run\num and they should be able to use a\nsimplified interface to approve right\nwhich is also which is like just a click\nof a button it's not even data but in\ncase of having data that you wanted to\nlabel\num we realized it's very complicated and\nthat's why you and I yesterday kind of\nwanted to do a typical notebook demo to\nshow that\nin the absence of a fancy UI you could\nuse your notebook to essentially fetch\ndata and probably we could write a\nsimplified\nif they end up flight kit SDK or an\nextension on 5K test SDK that could\nallow you to do that better\nuh and Jupiter notebooks but this\ndoesn't we are again light is never not\ncompeting with the labeling systems and\nso on it's just enabling a more\nend-to-end flow for people who are\ntrying to label as well as train as well\nas and we learned this by working with a\nlot of companies that deal with massive\namounts of data like mapping data lidar\ndata\num 3D data as well as like genomics Etc\nand they all wanted to do such a thing\nin a simplistic way\nand so there is available please use it\nI think it is still early we are going\nto find some potential issues in the in\nthe API surface Etc definitely surface\nthem uh but we've been using it it's\njust it feels amazing it's like uh it's\ninteractive now that's amazing\nthank you\nawesome love this feature\num to reiterate what he said signaling\ncan be pretty useful in cases where\nhuman intervention is required if you'd\nlike a deeper dive into what signaling\nis and the preliminary details please\ncheck out the preceding presentation by\nDan that was given a few months ago you\ncan find the video on YouTube channel"
    },
    {
        "title": "Flyte Community Updates 029 featuring Upcoming Flyte In-person Meetup and MLOps Blog Post",
        "transcript": "welcome everybody to the community sync\nmy name is samhita I'm with the Union AI\nteam and I'm your host today please feel\nfree to introduce yourself on the chat\nyou can put your name role and the\norganization you're from\num let's quickly look at the agenda now\nyeah we have a packed agenda today I'll\nbegin by providing the community updates\nand then you will do a signaling demo\nfinally Jennifer and yeah yes will walk\nus through their journey of migration\nfrom airflow to flight\num I'll get started with the community\nupdates\nflight lead Eduardo will be talking\nabout flight on Jan 26th at Microsoft\nreactor Redmond Seattle the Meetup event\nwill start at 6 PM PT join us in person\nif you are in Seattle or tune in\nvirtually you can register at the link\nprovided here which will also share on\nSlack\nSandra wrote a blog post that covers the\nrecent mlops webinar and panel\ndiscussion if you haven't had a chance\nto attend the webinar I highly recommend\nyou to read this blog post\num David do you want to quickly talk\nabout the cloud native security\nconference you are attending next week\nyeah sure hi everyone\num Jane Davis here open source advocate\nuh for the flag Community yeah I'll be\nattending next week uh Cloud native\nsecurity con\num\nI'm aware that's not probably the most\ncommon kind of\nconference in the data in ml Community\nbut but it's it's a topic that is\nit's key especially if you're using\nbuilding or adopting open source\nsoftware you probably have heard about\nthe log4j vulnerabilities or the\ninfamous or Wings attack Etc it's a big\nbig topic right now uh how to secure the\nsupply chain the software supply chain\nfor open source and I'll be attending\nthis conference this is just a snapshot\nof my agenda if you happen to be\nattending would be great to connect but\nin any case I will come back and share\nwhat I will learn with the community and\nhopefully we'll be contributing back to\nthe project the flight project\num keep improving our security posture\nso um yeah that's it excited\num let's go through the links and\nresources real quick we host uh office\nhours every Wednesday at 7 00 a.m uh 1\n30 p.m and 9 pm PT please join them if\nyou need life support Eduardo Katrina\nand I can help you out\num next sliders the next Community sink\nis on Feb 7th sujit the chief Mo and AI\nengineer at Nokia will talk about their\nflight Journey from POC to production\nyou can use the add event link to add\nthe event to your calendar we'll also\npublish this month's newsletter issue\nvery soon yeah that's about it thank you\nso much for your time and participation\ntoday see you all at the next Community\nsink"
    },
    {
        "title": "Flyte-Databricks Integration Demo & Discussion",
        "transcript": "hi I'm Kevin Sue I'm software engineer\nat union.ai in all 16 mainly working on\nthe Fly budget and Evan is a senior\nsoftware engineer from HBO today we are\nso excited to share about the database\nplugin integration\nso today agenda I'm going to give an\noverview about the database plugin and\nthe architecture behind it and I'm going\nto give a quick demo lastly Evan will\nshare some use case and stories in HBO\nso why is fly database plugin basically\nhelp you schedule Monitor and austrial\ndatabase job so there is just just like\nspot job but running on database culture\nand there's help you create a new sport\ncluster when job is running\nand before using database plugin you\nhave to go to database console to\nmanually click the button one by one to\ncreate a new job or you have right bunch\nof screen to help you schedule or\nmonitor your database job so we want to\nmake running spot on devops platform\neasy so we created this plugin it will\nhelp you submit a job to the therapist\nand then monitor your job lifecycle\nquestion\nso this is uh that database jobs API so\nit's pretty similar to sponsor and but\nwe add a new config called database\nconfig and here you can see we Define a\nspa config inside database config for\nyour spark jar and the text name and the\nClusters back uh the address will help\nyou\nuh create right I'll create a new\ncluster for you\nand you can also submit your Sparger to\na existing cluster you just pass the\ncluster ID\nand you can see the tasks\nyou just write a regular Sparkle in the\ntask and then you can get the spot\nsession from the fly contacts and then\nyou use this position to create a USB so\nyou can run this past locally and you\ncan also reach this tags and fly will\nhelp you submit a job to the database\nand then you can run these tags on\nlarge-scale cluster\nI'm going to give a quick demo let me\nshare my screen\nif you guys see my terminal\nyes\nokay awesome so\nthis is my spot workflow so you can see\nthe T1 text is just a dummy task which\nreturns a number of partitions and then\nuh database config Define your service\nback\nhere and then you'll pass the test\nconfig to your spot job\nand here we just calculate the value of\nthe pi and you write your spot regular\nSpark hold in your smart job\nhere\nand push much like it and then\nI'm going to\nrun it locally\nand when you run the job locally instead\nof creating a new cluster\nIce Bar will run your code in jvn Java\nvirtual machine and then it will send\nthe result back to the item process\nand\nyeah this one\nyeah and then you can also just register\nyour power flow I already registered\nhere\nyeah\nyeah\nyou can see my controller\nokay awesome so this is my smart tasks\npower flow\nand then you can\nrelaunch it\nand then\nlet me go back to previous drivers so\nwhen you go to the spa test there's a\nlink you can link to your database\nconsole when the job is complete you can\nsee all the ads\nor the task log\nin this console and then you can see\nyour\ncourses back closer config and then the\ncommand you run\nand you can go to The Spar UI to see the\ndetail of the spa tabs\nyou can see\nthe event timeline on this console and\nthen you can see\nthe spark config you have\nand then you can see\nthe number of Executives you run and how\nmany tasks fail how many task succeed\nyeah\nand then\ngo back to the fly console\nlet's see\nthe spa tax running and then you can\ngo here go to the work floor\njob raw you can see I just created a\ntherapist job and\nthe cluster is being created after you\ncreate after creating uh database will\nsubmit this module to this closure and\nrun your last podcast on this closer\nyeah Bridge mostly and\nyeah uh Tyler could we go back to the\nslide\nyeah so even you if you want to try it\nout you can follow this documentation\nand you can create a local sandbox and\nto run this database job if we want to\nrun this code locally just people\nflacky plucking Spark\nnext slide\nyeah so the architecture behind us so\nuh just like regular flight workflow you\nwrite a python screen for your workflow\nand you build a document for your\nworkflow register this workflow to fly\nadmin and when you run it instead of\nlike creating a comment cluster\npropeller will send a rest request to\nyour database platform and dampering\nplatform what it does is that it helps\nyou spin out a new cluster and then you\ncan also like enable the\nAuto or scaling like terrorists will\nhelp you like scale out or scale down\nyour working laws and in the driver in\nthe driver now like we run the Wi-Fi\nexecute so but python SQL does is that\nit help you download\nthe previous text the in output as uh\ndownload the download the task input\nfrom your asset bucket or you can you\ncan also download a code from your\nbucket if you use password so\nand our entire wolf was pretty stressful\nlike uh next slide please\nso what's the benefit of therapist\ntalking so\nuh it's very easy to integrate with\nother service like you can image like\nyou have a workflow like select a\ntolerance or roads from bigquery and fly\nwill help you convert it to the\nstructure data set and then you can just\npass to the database job so others\nyou're running smart on database you can\neasily to access the data from like\nexternal data system data warehouse like\nbigquery Snowflake and then\nuh after after the job's complete uh fly\nwill help you serialize your spa data\nframe again to the shorter data set and\nthen you can pass you the training drama\ndistribution test Global pilot Etc\nand then so\num you already integrate with like spot\nwe already have like spot Transformer so\nyou can easily to like\num\nand\nanother important feature is like local\ntesting which allows you to more quickly\nto iterate your Spiral Flow locally\nand all all the scaling is one of the\nfeatures in database which help you run\nyour job more of course efficiently\nuh yeah next slide please\nso the future work like we want to\nsupport the database pipeline which\nmeans like we can combine the fly\nworkflow and the database pipeline\ntogether maybe use database pipeline as\nsub workflow so we can run both of them\ntogether in the same time and then\nnowadays we want to support the\nstructure data set encoder and decoder\nfor Delta Lake which allows you to\neasily to assess access the data from\nDelta link\nand yeah contributions are welcome\nand next slide yeah uh having the wrong\nthing all right yeah\num\nso yeah what we're doing is we're just\nmaking uh movie and TV recommendations\nand this involves aggregating streams of\nyou know people watching shows and folks\nclicking on stuff and this turns out to\nbe a ton of data and so most of our\npipelines are\na mix of spark jobs and then just normal\npython tasks where you might have a big\ninstance and train a model so nothing\ntoo exciting but\num there are these two architectures\nthat we really need to have in our\nPipelines\num and what we've seen is like uh when\nyou have these two different\narchitectures\nyou know like who we end up with\ninternal tooling that might be very\noverfit to running spark very they're\nvery limited\num it doesn't have the flexibility that\nflight gives us\num and then on the other end with data\nbricks is what we use for data warehouse\num it's really like UI based which is\ngreat until you want to you know deploy\nproduction ml models and you want to\nfollow more of a code first approach and\nso\nyeah we're in this case where we want to\nuse data bricks we we want to use flight\ntogether so that's\num where we're at all right next slide\num so\ndata bricks and flight uh I think work\nreally well well together\num databricks has like an optimized\nversion of spark which uh allegedly can\nscale for certain tasks it also has a\ndata warehouse so they're they're really\nuh trying to\nyou know handle the ml use case and the\ndata warehouse use case so they have\nlike a query Pane and so we have a lot\nof different teams working out of the\ndata bricks UI when it comes to flight\nwhen I mentioned it has this code first\napproach Uh Kevin touched on all the\nthings that we love like with local\ntesting there's also typing so let's\njust say it really provides this like uh\nsort of like software engineering\nworkflow despite having different\nback-end art architectures\num\nand yeah I guess the final piece is like\nI really don't like having to write code\nto package you know code all this stuff\nthat isn't the actual machine learning\num it isn't the actual ETL all of sort\nof the piping\num and yeah flight really gives us that\num for free which is uh something that I\nI really uh don't take for granted\num when I use flight\num on the right we have an example here\nof like a pre-processing job\num\nthis will run on data bricks\nwhich is\num\nuh with just the different uh like Pi\nflight commands\num you can see here that like I was\nusing an existing cluster and once your\ncluster is spun up it can actually\naccept multiple uh requests it'll Auto\nscale and it's set up so you know my\ntime my tasks even though they were\nsplit up\num ran really fast because it didn't\nhave to spin up some spark clusters\num so yeah that's that's it next slide\num testing just as an example here\nlight makes units testing\nstraightforward basically just\nbootstrapping off of that like that\nspark plug-in\num and so this is just really useful for\nus because we have a few functions that\nwill you know process viewership data or\ncatalog data and now we can unit test\nthem and reuse them something else is\nonce we have the slight tasks if we're\niterating in a notebook we can import\nthe tasks\num from our python module and it's you\nknow then as part of like the data\nscience workflow where we're\ninvestigating and iterating we're able\nto just use these functions like normal\npython on Sample data or if we have\naccess to the databricks cluster locally\nthen you can just pull data so it's a\nreally smooth\nuh Dev workflow\ncool next slide\num this is the final piece that we've\nbeen looking into is\num\na lot of times you know I've seen like\nuh\nthe like the ETL or databricks code\nbeing very separate from the python code\num or there's a lot of limitations where\nit's like okay we're we're gonna set\nthis up so you write a notebook and then\nour tool will then package up that\nnotebook and send it to databricks and\nso there's just like a lot less\nflexibility to have the repo structure\nthat you really want and so we've been\nexperimenting with like what does it\nlook like if you really couple the code\ntogether\num for our projects and you know we're\nable to use relative Imports for\nutilities because fast registration\nautomatically you know figures out what\nyour python module is by doing like a\nnice little search\nand then we can write end-to-end uh\ntests of our workflows which is\nsomething that's very hard to do with\nairflow or other systems for\norchestration and I think it's like\nreally powerful to you have a\nan end-to-end workflow and you pass in a\ndata set and it runs locally like it's\nyou know it's usually going to run in\nthe cloud especially the way the flight\nsets it up so\num I really think it means we have a lot\nless like babysitting to do of pipelines\nand\nwe can just focus more on what matters\nwhich is writing our machine learning\ncode and making our models better\ncool\nawesome the integration looks pretty\ncool uh thanks Kevin and Evan for\npresenting the integration questions of\nfeedback anybody\nyeah I I had a question\num well thank you Evan for for joining\num I think I should introduce myself\nbriefly David special here I joined\nUnion AI last week as to serve this\ncommunity as an open source developer\nadvocate\nso I'm glad to be here yeah um you\nmentioned that one of the things that\nyou liked about flight is has to do with\ntyping because you mean this strong\ntyping on flight tasks right yeah\num what exactly is is what you like the\nmost about that feature\num\nyeah a great question I mean what I like\nis\num you know a lot of the mistakes in\ndefining the pipelines that I've run\ninto uh\nit's\num passing\nthe wrong thing between tasks\num and then the other piece is like\nbeing able to pass like structured data\nsets or data frames\num just because like\nwait if you write it with those tasks\neverything's happening behind the scenes\nto like load and move data around save\nthings and like I really found that a\nlot of the pipelining code I wrote was\nlike taking a string past you know\npassing it into a\nI don't know something that'll let me\nload it up from S3 and so all of those\nlike little um\nall those commands end up they're like\nsurface area for things to go wrong\num and so yeah having that all checked\nit like has reduced the number of bugs\nthat that I see when I write my\nPipelines\nawesome thank you\ncan I uh add something to it I think by\nthe way my name is Caitlin\num I also I work at Union but mostly\nserving the community\num so\nforeign\njust added I just wanted to throw a\nlittle bit more color on that but I\nthink uh the boilerplate is essentially\nwhat the heaven is referring to like\nwhen you're writing code right you're uh\nas a data scientist or a machine\nlearning engineer\nI don't want to think about writing data\nto yesterday I don't want to think about\nconverting data frames from spark to\nPolaris because I find Polaris useful in\nsome cases uh when I'm working locally\nor who don't buy a pandas and that a\nwhen we first started building\nright that was the most urgent thing I\nwanted to get rid of and that's why it's\nbeen encoded through the system and what\nwe as a community now that we've been\nworking with the community for a while\nI've seen that many many teams have seen\nhuge reductions in code basis and what\nthat really means is actually reduction\nin the court surface area as Evan said\nwhich is reduction in bugs and reduction\nin effort and deduplication right across\nthis vast majority of a community so uh\nI would highly encourage the community\nto contribute more towards type\nTransformers and and helping us build\nout like the library of connections and\nand like what Kevin mentioned in his\nstock itself like you know we we want to\nintegrate with Delta lake so so that the\ndata scientists never have to think\nabout how does this area come from Delta\nLake where am I going to put it right\nand and\num I remember in one of my earlier\nfirst presentations when we were\nbuilding flight was\nand the reason we did it this way is\nbecause the data scientists used to\nstore models in a specific part in S3\nand the park or something like name of\nthe data scientist slash date of the Run\nslash time and model link and if you\nrerun it on the same day or something it\noverrides and cause Corruptions and then\nwho was remembering all of this\nsome person in their brain not\nremembering all of this and we just\nwanted to get rid of all and so that's\nhow the Genesis of this Auto\nmanagement of the data happened and I\nthink it's still early in this state and\nwe can do more\num one question I had\num for Kevin and Evan is that\nyou have been using spark locally uh\nwith this new integration\ndo you want do you find that useful and\nhow easy I I don't think you showed that\nKevin but how easy is it to move from\nlocal\nto spark on kubernetes\nto spark on databricks\nnow which is you know\nwhich is typically now a choice which\nwhat I hate about most of these Services\nis essentially there's no choice like I\nI want my most important critical\nworkloads to run in database for example\nbut I have some extra workloads and I\nwant to run it kubernetes so is that\nreally seamless and is that just a\nslight changing config\num any any comments on that Kevin\nI get zippers like so basically if a\nwrong little like Spar on kubernetes or\nSpar on database\nso first you have to set out some\nculture cluster or platform config\nin propeller but you only need to set it\none time like like a access token or\nyour database project ID instant ID\nthat's it and then\nso what you have what you have to do\nlike just like uh specifiers are config\nand then submit use the wrong once we\nrun on terrorists you divide the\nConsular config as well like for your\nlike concept right for your compute\ninstance\nand\nit's very I think it's very\nlike seamlessly to move to the\nproduction construe you just register it\nand then run it why will help you handle\nlike everything how to communicate with\nlike smart operator how to talk to like\ndevs platform is very like easy and\nstrap over and then you don't need to\num from the user perspective like they\ndon't need to know how to use database\nso they just focus on their Sparkle\nright so I think yeah\nso if I had to paraphrase like you're\nsaying that basically no code changes\nyeah the same code runs locally on\nkubernetes spark or on data breaks and\nyou make the choice purely by switching\nconfigs yes you have to enable some\nback-end plug-in is what I get but which\nis yes you have to enable but that's at\nthe platform level so the platform team\ncan make a decision and you can almost\nswitch over to\ndata breaks are from databricks though\nkubernetes overnight\nright yeah yeah\nand that was a big\nuh why this plug-in like uh one one\nthing when you work with data bricks is\nthere some like level of vendor lock-in\nand so us knowing that like you know\nwe're running code on data bricks for\nwhat we wanted to which is its compute\nengine it's infrastructure but\nyou know if we\nfor some reason our company decides to\nstop using data bricks because it's you\nknow for pricing or for other reasons\nlike we have a alternative that's built\ndirectly into flight and so that yeah\nthat uh that's just like a really nice\nuh feature for us\nbecause these things do change like\npeople do switch from Snowflake to data\nbricks and back and forth and it's not\nnecessarily the the developers who have\nuh who who make the choice\nyeah okay so that's great I think even\neven if we are not we are not suggesting\npeople that you data because what we are\ntrying to say is that there is a choice\nand you can make the choice which is\nright for the project for the product or\nthe for the business at that point in\ntime\nclear then\num to another quick question\num yeah thanks for sharing your use\ncases I'm curious how\nyou mentioned we kind of hinted at this\nbut I'm curious how\num\nyour the data scientists you work with\nand maybe yourself like how you've\nchanged\nworking with notebooks and working with\nwith databricks how is how is this\nflight integration\nsort of um change the mindset\nthere\num\nyeah so I think\nwhat is it the team that has just\nstarted using it uh we're not using a\nNotebook based workflow\nthey were using more of like the\nproprietary like sort of like uh tools\nto kick things off and so I for them\nthey're excited to use\nthey're really excited to use flight for\nall the reasons that we've discussed\nbecause they're they're like they're\nEngineers writing pipelines I think our\nnext step\nis um working more at the data\nscientists who use notebooks and setting\nthat up but we haven't we haven't quite\ngotten there yet but I will I will\nreport back\num with with feedback yeah"
    },
    {
        "title": "Rolando Garcia presents \"Operationalizing Machine Learning: An Interview Study\" Part 1/2",
        "transcript": "hello everyone and welcome to today's\nsession about operationalizing machine\nlearning my name is Martin Stein and I'm\nwith a union I'm the head of developer\nrelations and marketing at Union AI\nmy background is actually in machine\nlearning I build production models for\nvarious Industries including real estate\nand put them in production probably like\n2015 2016 we've built large-scale\ndeployments\nI'm super excited that this topic is our\ntopic today\nand I'm really happy to to welcome today\na guest which is Rolando Garcia he and\nhis co-authors wrote a paper about\noperationalizing machine learning an\ninterview study\nthey went out and spoke to various\nyou know participants in the study to\nlearn about\nhow machine learning is being\noperationalized Rolando is going to\nspeak for about 30 minutes 35 minutes\ntoday\nafter his conversation we will invite a\nvery exciting panel today of employees\nfrom various companies including Lyft\nand others and we will have a\nconversation about\nthe pitfalls of operationalizing machine\nlearning\nthe role of software the role of team\nplay\nand most importantly what to do to\nreally get to the right level of\nvelocity and velocity is a is a term\nthat you will hear in a little bit in\nmore detail before I introduce the\nspeaker I would like to go over a few\nhousekeeping notes today\nfirst this is a recorded session\nit will be available on union.ai Via\nYouTube but you of course can also see\nit today on YouTube in a live stream\nwe also produce a transcribed version of\nthis conversation as a blog post in\nunion.ai\nand if you have questions throughout the\npresentation feel free to type them into\nthe chat the team will hand them over\nand we will pick a selection and read\nthrough those questions during this\nsession\nnow I would like to introduce today's\nspeaker Rolando Garcia Rolando is a PhD\ncandidate at UC Berkeley and Orlando\nPolice unmute himself and introduce\nyourself and let us know what you found\nout in your study stage is yours I'm\nRolando Garcia I'm a six-year PhD\ncandidate at UC Berkeley and I'm really\nexcited to share with you our findings\nfrom our most recent interview study\nwhere we spoke to ml Engineers to\nunderstand how they operationalize\nmachine learning\nso a quick outline for the talk this is\nagain something that I will cover in 30\nto 35 minutes so I will be skipping\nthrough a few subsections uh we will\ntalk about software engineering's what's\nmlops you know devops for ML what's the\nbackground here uh discuss some of I I\nwill spend a lot of time discussing the\ndescription or characteristics of the\nproduction ml workflow what it is that\nthe ml Engineers that we spoke to spend\ntheir time on uh and then I will also be\ntalking about some practices and pain\npoints but this will be light and\nsomething that I will defer until the\npanel discussion\num if you have some clarification\nquestions please feel free to ask if you\nwant deeper discussion questions I think\nthat's something that we can defer until\nthe panel\nso uh to motivate this machine learning\nis something that we're finding\nincreasingly in more and more applicant\nplaces so there's personalized medicine\nHealth Care self-driving cars and the\nlist of broad detection the list is\nendless and so what we wanted to\nunderstand was who is a machine learning\nengineer what kind of data worker are\nthey what's their specialty so a machine\nlearning engineer is going to be someone\nwho might be dealing with tensorboard\nplots they might also be setting up RPC\nendpoints where they're deploying a\nmodel for a little NC prediction serving\nthey'll be working with data silos\ncentralized schemas features Excel and a\nbroad set of different tools\nso how much of this work can be done\nwithout formal ML and CS training and\nhow can we build relevant and useful\ntools for these machine learning\nengineers\nso if we look at examples from textbooks\nin machine learning or if we were to\ntake undergraduate classes to learn how\nto do machine learning and how to deploy\nthe things that we would learn is that\nthe first stage is data collection\nthey're sourcing labeling cleaning and\nwrangling then from that collected data\nwe fit a model\nthen we evaluate that model on say a\nheld out data set and provided that\nthere's good accuracy and no overfitting\nwe deploy\nthere's some Focus maybe on the\nevaluation and deployment part\num and so the we when we're training the\nmodel we see that the\nloss decreases over time and then the\nvalidation starts to increase that's\nindication of overfitting we can pick a\npoint in training when the validation is\nclosest to the training loss and we can\nchoose whether to ship\nwe found that this assumption is too\nsimplistic and it's not representative\nof the work that ml Engineers do\nso this new wave of software engineering\nwhich is a subset of uh devops is the\nsubset\nis that is very hard to operationalize\nso what about the naive textbook\nscenario might be incorrect what might\nbe some of the challenges so in reality\nit's never just once and done right it\nmight be for a report maybe you train a\nmodel and produce a plot but in more\nrealistic scenarios like fraud detection\nit might be that people start adjusting\num to your new per to your new\nclassifier and so you need to account\nfor this change in Behavior so the ml\nengineer is going to be continuously\nlooping and then the end user is going\nto be adapting and so there's already\nwe're introducing people\num and the people are adapting to one\nanother\nthe data is going to change very\nfrequently the models are going to\nchange in order to be able to keep up\nwith changes in the distribution of the\ndata\nthe customer needs are going to change\nbusiness goals change as well as teams\nchange so teams changing is like\nemployee turnover employees come in and\nout of the company how do you keep that\ncontext over time\nso the ml engineer here is like acting\nalmost like a ninja keeping uh all of\nthese different changes managing all of\nthis and keeping control of complexity\nso the ml engineer is someone who's\ngoing to be leveraging skills drawn from\ndata science data engineering and\nsoftware engineering\nspecifically they're going to be\nresponsible for data cleaning or data\npreparation feature engineering which in\nour interview study comprises a large\nchunk of what drives innovation in ml\nengineering\nthere's going to be experimentation in\nin with the model as well as with the\ndata\nstaging deployment uh which involves you\nknow incrementally releasing a model\nwatching Its Behavior with real users\nand then uh as the trust increases\ngiving it access to more and more flow\nof the traffic\nmonitoring and debugging are an\nimportant part again just because we\ndon't really have a very analytic\nunderstanding of machine learning so its\nsuccessor failure is defined by its\nbehavior and its interaction with\ncustomers\nso how do people put on keep ml models\nin production we went out into the field\nand we spoke to 18 ml Engineers four of\nwho were ml engineer Managers from\ncompanies that varied in sizes from\nsmall startups\num in Silicon Valley to large\nInternational corporations in sectors as\nvaried as autonomous vehicles retail ads\nso some of the more classical machine\nlearning examples that might do well\nwith like a linear classifier versus the\nmore state-of-the-art examples that that\nrequire deep learning models and more\nsophisticated architecture and\ninfrastructure\nso we're going to step next into the\nproduction ml workflow\nthe first step data collection\na correction that we want to make from\nmaybe the the earlier scenario is an\nemphasis on experimentation at so much\nmodel training and this is because a lot\nof the experimentation is going to be\ncoming in from the data or the models\nand not only specifically the models\nevaluation and deployment is going to\nfocus on change so Dynamic validation\ndata sets are a Delta here specifically\nif you're assuming that the training\ndata is changing over time it's\nreasonable to also assume that the\nevaluation data needs to keep up with\nthat change\nand then finally monitoring and response\nis something that happens continuously\nand at every stage so you're monitoring\nthe experiment through monitoring the\ndata collection you're monitoring the\nevaluation and the deployment and so on\nso the practices that manage these\nworkflow the activities that the ml\nEngineers are performing are as follows\nthe first takeaway and this uh involves\nthe data preparation data collection\nstep it's a lesson that iterating on the\ndata produces a lot of value\nparticipant 11 says they're going to\nstart with a fixed model because it\nmeans faster iterations that means not\nchanging the model\nand often like most of the time\nempirically it's going to be something\nin our data that we can use to kind of\npush the boundary obviously it's not a\ndogmatic we will never touch the model\nbut it shouldn't be our first move\nthe emphasis again here is when you're\niterating in machine learning you can\nget a lot of value from iterating on the\ndata whether that's feature engineering\nor some other data transformation\nparticipant 11 further says that they\nreally think it's important to bridge\nthe gap between what's often a subject\nmatter expert in one room annotating and\nthen handing things over the wire to\ndata scientists a scene where you have\nno communication so this relates to the\nimportance of collaboration in data\nintensive workflows which is something\nthat's well documented in the literature\ng17 also says that they look for\nfeatures from data scientists who have\nideas of the things that are correlated\nwith what I'm trying to predict\nso maybe\num addressing some of the incorrect\nassumptions that we brought into this\nstudy is that the attention should be\nless on iterating merely on models and\nmore on iterating on data\nrather than developing\nmachine learning in silos which is quite\ntypical in academic settings and then\npassing the model or handing it over to\nsomeone else which is actually how\nmistakes happen the tips that we learned\nwas to involve other stakeholders in\ndiscussions and so uh particip if we are\npeople who work with experimentation\nbeing involved in some of the data\ncollection and then also being involved\nin some of the product side discussions\nthat are going to influence the\ndecisions that we make for modeling and\nrather than maximizing the number of\nmodels that make it to production which\nwhich I think in some academic circles\nis a priority we think that the\nmaximization of the iteration speed or\nthe development velocity the number of\nAlternatives that you're able to explore\nin a finite amount of time is going to\nbe key\nso we're visiting the ammo lifecycle uh\nthe emphasis here will be on Dynamic\nvalidation\ndata sets\np8 tells us about the dynamic validation\nsets every field prediction gets into\nthe same queue and three of us sit down\nonce a week and go through the queue\nthen our analysts collect more similar\ndata and so\nit's very difficult for ML Engineers to\nanticipate the kinds of failures that\nthe model will have and often models\nfail to different degrees with different\nsub populations\nso in self-driving cars you might\nimagine that a model that's trained in\nSan Francisco might never have seen snow\nand so in those cases when it's driving\nout in a different setting it might have\ndifferent errors so debugging this\nfinding the error cases and then\nintegrating them into evaluation sets\nfor future ml Engineers to validate will\nbe crucial\nforeign\ntells us about the importance of\nmulti-stage evaluation we spend a long\ntime very slowly ramping up the model to\nvery small percentages of traffic and\nwatching what happens\nwhen there was a failure mode a product\nperson would ping us and say hey this\nwas kind of weird should we create a\nrule around this suggest the text to\nfilter this out so we saw that filters\nrule-based or logical checks comprise an\nimportant part of AI systems so it's\ntrue that there's this stochastic model\nunderneath making predictions but then\nthere's also this layer of human\nreadable and interpretable filters that\nmake sure that some common sense\nbehavior is guaranteed for the user\non the importance of evaluation\num again if you're going maybe to kaggle\nor or Udacity or one of these online\ncourses\nyou see Roc curves F1 scores\num these are all ml metrics right\naccuracy but they don't always translate\ninto value for the customer or some\nmeaningful product metric so p16 says\nthat the first task will be to figure\nout what the customers are actually\ninterested in what the metric that they\ncare about\np17 says that if they can get\nstatistically significant improvements\nof people subscribing to a product then\nthey can fully deploy so it's not just\nfinding a model that has higher accuracy\nit's making sure that that higher\naccuracy in fact leads to more\nsubscribers\nso managing this change\nthe static holdout sets uh that almost\nlike k-fold cross-validation where you\nlike take out some data from the\ntraining and then evaluate that's not\num modern that's not current practice\nthe evaluation data needs to keep up\nwith the training data so as the\ntraining data changes as the\ndistribution shifts you need to make\nsure that the evaluation data is keeping\nup with that\nGlobal metrics for machine learning are\ninsufficient instead it's necessary to\nslice and dice to check the performance\non particular self-populations and then\nalso to collect\num it might be that the reason why the\nself-driving car is making mistakes in\nuh icy roads is because it doesn't have\nthat experience it's not enough to just\nyou know detect that you might also need\nto schedule some jobs to collect that\nadditional data to improve performance\nand ml metrics are insufficient it's\nnecessary to make sure that the metrics\nalign with product metrics and as one\nrespondent said that it's important that\nthey measure success and are held to\naccount by the same standards as\neveryone else in the company which might\nbe through like you know Revenue income\nor some other metric\nthroughout the talk you've heard me talk\nabout filters\num or rule-based heuristics this is kind\nof like human in the loop practices that\nmake sure that everything runs smoothly\nso\num something that was interesting very\ninteresting to my co-author Shreya is\nthat if you're trying to do research in\nin ml engineering I think one really big\nopen problem in Academia is the data\ndistribution shift so we know that you\ncan deploy a model and then maybe in a\nyear there's going to be totally\ndifferent Trends try to think about the\nmodels that were deployed before covid\nand then covet happened so it's like\nhospitals maybe weren't scheduling\nvirtual machines and then all of a\nsudden they need to schedule virtual\nmachines so that's an example of\ndistribution shift and there's an\nabundance of academic papers in this\nspace that try to build models that are\nrobust to this change and so it's very\ncomplicated and we were going to ask the\nparticipants like what how do you manage\nthis this change\nand the answer was that they actually\nhave a us a way of circumventing that\nproblem through frequent retraining of\nmodels so p14 says why did we start\ntraining daily as far as I'm aware we\nwant it to start simple\nwe could just have a single batch job\nthat processes new data and we wouldn't\nneed to worry about separate retraining\nschedules you don't really need to worry\nabout it if your model has gone stale if\nyou're retraining it every day so if you\nhave a model that was trained on Data\nbefore covid that Services people before\n2019 and then a model that gets\nretrained afterward that model will be\ntrained on that new distribution of data\nand so now we're talking about you know\nthere's the mo that continuing this this\nexample\num there's the model before covet\nthere's a model after covet you always\nhave the possibility of rolling\nsomething back I think this was\nsomething that I myself was worried\nabout like if you're putting out models\ninto the world that are making\npredictions and interfacing with users\nwhat if you put out a really bad model\nthat makes a terrible mistake you know\num in my head it's like well how are you\ngoing to keep a model from like uh an\nairplane from like crashing right\num and then in reality the circum\nagainst the MLS found a way to\ncircumvent this problem through a simple\nrollback there was already a model there\nto begin with before you uh deployed the\nnew one and so if there's something that\ngoes critically wrong you're always able\nto roll back\np18 says that it's important to keep\nsome model up and running even if we\nswitch to a less economic model and have\nto just cut the losses so\num this relates to the performance\nlatency trade-off which is um\nagain you might have a smaller model\nthat makes a quicker prediction but it's\nnot as reliable there's going to be\ndifferent applications so the\napplication is going to demand like is\nlatency more important is accuracy more\nimportant MLS have to navigate these\ndecision space\num the validation of inputs and outputs\nis going to be key\num\nthe data gets validated not only as it\ncomes into the model but also the\npredictions that the model performs\nso then clarifying some of the uh myths\nthat or initial assumptions that we\nbrought into the study\nrather than building models robust to\ndata distribution the simpler solution\nis just to retrain frequently so\nperiodic retraining this is something\nthat was very typical\num every one of the mles that we spoke\nto retrains on a regular Cadence\nuh they always serve the latest model\npredictions well no not always this like\na like a One-Shot deployment case that's\nmore of an anti-pattern uh instead it's\nbetter to maintain simple models and\nheuristics for rollback and rather than\nvalidating just the outputs validating\ninputs and outputs\num and having on-call rotations and\nservice level agreements\nto make sure that the model is\nperforming as required\nso the three virtues so this is more for\nuh sparking discussion the the three V's\nof mlaps are velocity\nearly validation and versioning so this\nis these are themes that we saw\num recur throughout the transcripts\num velocity it's about the rate of\niteration High iteration speed is the\nname of the game\num the more Alternatives you can explore\nthe better\nthere is a caveat to that obviously um I\nknow that a colleague of mine Doris Shin\nwith Aditya para miswaran recently\nreleased an interview study on automl\nand they found that its popularity had\nsomewhat declined\num and I think the reason why that is is\num if you're running a bunch of\nexperiments in parallel but you're not\nreally observing the outputs you're not\nreally learning from the previous\nexperiments it might also be a lot of\ncontext to keep in your head instead the\nml Engineers told us that they preferred\nto have a more limited number of\nexperiments that they were continuously\nrunning and then they would have\nhypotheses about well maybe you know I\nneed to change my weight Decay or maybe\nI need to change the learning rate some\nhyper parameter\num and then this kind of like Theory\ndriven guided search or hill climbing uh\nresulted in better\num experiments but there's you know kind\nof a tension the faster you move the\nmore versions you generate so there's\nkind of a software engineering burden\nthere\num and then same with with early\nvalidation\num I think if you're one one way in\nwhich like velocity and early validation\ncreated attention\num ml Engineers told us about the\nimportance of\num\neveryone on the team evaluating similar\nmodels with the same script and so they\nwould have the central repository with\nwith like Jupiter notebook evaluations\nand what they saw was that people would\nFork that notebook run their evaluations\nchange that notebook but then they\nwouldn't merge those changes back and so\nthat's one of the reasons where like the\nSimplicity and direct access can then\nkind of lead to version drift so\num I think we go into some of the the\ntensions and synergies between these\nthree bees and and how they help us\nunderstand the the problems that the ml\nEngineers face but it's something that\nwe can discuss later uh in the panel\nso so for for pain points I guess for\nanti-patterns keeping gpus warm is uh an\nexpression that uh stucca struck us from\nthe interviews which just means like\nrunning experiments for the sake of\nrunning experiments or you know if\nyou're working at a large company they\nmight have hundreds of really high-end\ngpus and people kind of feel like this\nobligation to keep the system saturated\nwith experiments and it's like that's\nnot necessarily productive\num you know it's a caveat on on velocity\num and then people retrofitting\nexplanations you know\num someone said that the reason why\npeople succeed is because they they're\nsmart about the the experiments that\nthey try and they try a lot of\nexperiments quickly\num they might have you know like drop\nout if you read the the academic paper\nfor uh Dropout\num it sounds very much like they\nreasoned it from first principles\num but it's I don't know I'm kind of\nskeptical I feel like it was more like\nyou destroy the image and then you know\nit generalizes and then later afterward\nyou try to fit some explanation right\num undocumented tribal knowledge was an\nissue this relates again to employee\nturnover so there was a case of someone\nsetting minimum and maximum bounce for a\ncolumn and then issuing an alert if the\ncolumn value was outside of those valid\nranges but then that employee left the\ncompany and a new employee came in and\nthen they no one in the company knew how\nto interpret those bounds and so they\nended up getting rid of them\nso uh their prod environment mismatch\nwas something was a problem that that\nthe mles highlighted they also\nencountered problems with data leakage\nJupiter notebook usage and people have\nused Jupiter books might be familiar\nwith some of the issues with\nreproducibility and Version Control and\nthen code reviews\nyou know sometimes people said that\nthere's bugs and ml engineer in in ml\ncode that you're not going to catch\nthrough statistical techniques just\nbecause you're going to have such a\nsmall effect on ML metrics I think in\nthis case the self-driving car they\ncopied the logic for like the left lane\nversus the right lane and well once a\npassing Lane the other one's a driving\nlane and so they have different logic\nand the model was performing relatively\nwell so it's like it's not an error that\nthey could have caught unless they were\ndoing code reviews so it's also\nimportant to read the code even if it's\nnot changing as frequently as the data\nor the metadata\nthe ml Engineers highlighted\num alert fatigue as an issue that came\nup so if you trigger too many alerts\npeople will start ignoring them\num a sign of maturity that we saw with\nML Ops is that we're starting to see\non-call rotation so uh people who train\nmodels and deploy models will then also\nbe responsible for them for like the\ncourse of a week or two weeks and they\nhave to open tickets and write reports\non their performance so\num there's definitely a lot more\nownership with the models as we saw\num taming the long tail of ml box we try\nto see maybe for like tool Builders if\nthey're with some way that we could\nautomatically\num maybe do like automatic bug finding\non on the model or the data like is\nthere some kind of classic failure\nscenario that we could\num fix I think a common one was like\nsetting random seats in distributed\ntraining so if you have multiple workers\num doing gradient descent on the data\nthey should they should scan the data in\ndifferent orders if they're scanning the\ndata in the same order then you're not\nreally doing parallel training\num so initializing random seats was one\nof the few things that several people\nmentioned but other than that it's a\nvery long tail\num it's one of those errors where like\nmaybe they encounter once in their\ncareer and they never see it again but\nthey waste a bunch of time fixing it and\nso really what you have to do is just\nsupport\num this long tail of bugs and provide\ntooling that enables people to probe\nstayed in in the model so someone\nuh early in the interviews told us that\nsomething that might be helpful is\nalmost like being able to probe data on\na worker right before it goes into a\nmodel or right before it gets\ntransformed like that might be useful\nso yeah the silent error\num from a mailbox and then the paranoia\nin in writing ml code and reviewing code\nso this is I think this was the same\nparticipant who told us about the lane\nchanging problem\num\nso for the ml up stack this is a way of\nmanaging complexity\num people talked about well at the\nbottom we understand there's the\ninfrastructure are they running on\nGoogle Cloud vertex AI a sagemaker\nsomething like that components you have\nDocker kubernetes this this\num individual elements in the pipeline\npeople talked a lot about dags so\nthere's data pipelines there's model\ntraining pipelines and\num top level is you can imagine how you\ncan just like you call make on a make\nfile you can run these dags and then\nparameterize them\nand so most of the change was limited to\nthe Run layer and then it was like\neach deeper layer people were changing\nthings less and less frequently\nso in in this diagram uh uh we're\nshowing like the kinds of systems that\npeople used across different layers at\ndifferent stages\num and so we see that for weights and\nbiases and ammo flow like that's waste\nof matching metadata or hyper parameters\nfrom the Run layer very much in\nexperimentation evaluation and\ndeployment and there's other systems\nmaybe like airflow for dag scheduling\num Jupiter notebooks pytorch a hugging\nphase tensorflow we're very common\num individual components for model\ntraining and\num maybe to say like\nsomething that again keeps coming up is\nthat\nthere's someone who writes the pyth\nwhich is the tensorflow Once that's\nwritten then\num\npeople don't have to revisit it and and\nrewrite model training from scratch\nchanges are incremental\num and then again limited to something\nalong the lines of changing the hyper\nparameters rather than changing the\narchitecture of the model\nso we're working on this\num interview study\num we've submitted to Kai it's under\nreview\num there's other work that that covers\nprogram slicing for notebooks uh\nobservability and machine learning\npipelines and then hindsight logging for\nmodel training\nuh and then uh working with with data\nbugs so this other research that's going\non in the lab so\num yeah it looks like we have we're\nabout right on time so I will open up\nthe floor for the panel Martin"
    },
    {
        "title": "\"Operationalizing Machine Learning: An Interview Study\" MLOps Panel Discussion, Part 2/2",
        "transcript": "so Ronaldo that was uh fascinating\nthere's a ton to unpack and I feel like\nwhere do we start and that's why we have\nthe panel here so I'm going to bring in\num the panelists uh we'll start with\nvarsha\num varsha if you could turn on your\ncamera\num so varsha is with woven planet and uh\nvarsha you're a software engineer\num and you heard you know what Rolando\nalso said and it would be interesting to\nsee what you take us on a highest level\nfrom from his statements but before you\ndo this please unmute yourself introduce\nyourself let us know what you do what\nyou do at home planet\nyeah thanks Ronaldo for the interesting\npresentation and uh hey guys I'm varsha\nI'm part of woven planet\num we are basically uh\num startup within Toyota working on\nautonomous car and I'm part of\nperception engineering team working on\nground truth perception and um also some\non the infrastructure side uh\nspecifically the DAC part that Ronaldo\nwas talking about and uh um yeah we use\nflight for a lot of our dags\num yeah Martin do you have a specific\nquestion\num yeah how do you want to do this yeah\nabsolutely so I just have one very\nspecific obviously you're working on the\ninfrastructure part we've seen the\nlayered approach that Ronaldo had\nearlier\num do we do we underestimate\nin machine learning production the role\nof infrastructure what is your take on\nthat\num yes we do uh in some part like let's\nsay we focus a lot on iteration and\nexperimentation but you know for an ml\nengineer to do that a lot of it uh\ndepends on how good their infrastructure\nis does their infrastructure let them\num you know do does it facilitate them\nuh for example uh when we talk about\niteration and experimentation right we\ndon't necessarily have to rerun all\nparts of their uh dag or you know uh\nwhatever pipeline or uh the steps that\nthey have defined let's say in our case\nwe use flight and a few things that\nreally helps us is let's say caching we\ndon't want to rerun everything so does\nyour infrastructure provide caching for\nyou to Simply skip over and only rerun\nsome parts where your config is updated\nor maybe your model is updated another\nthing that at least for us is very\nimportant is versioning not just with\nyour model but also your data set\nversioning and how does that translate\nto how well does your infra help you\nwith that let's say I've trained\nsomething on a particular day with these\ninputs so what what was the original\ndata set that it has and uh versioning\nuh caching and most importantly another\nfeature because we're a startup we don't\nhave the you know keep GPU wants uh warm\nthat Ronaldo was talking about for us\nit's always a fight for GPU resources\nand contention so how well does your\ninfrastructure provide that that\nabstraction where you know when you are\ngiven a GPU how how efficiently could\nyou just run that and train and let make\nsure that you know your other steps and\nprocesses are uh you know outside of\nyour GPU one time so yeah uh\ninfrastructure is definitely uh um you\nknow underestimated\num so yeah both ml Ops and infra kind of\ngo hand in hand\nfantastic so we're going to spend a\nlittle bit more time later when we talk\nabout velocity on versioning one of the\ntwo or two of the two v's that Rolanda\nbrought up and and I think that's going\nto be how do we actually get the balance\nthere thank you so much varsha uh next I\nwould like so please mute yourself and\nstay you can leave the video on because\nwe're gonna go to a panel setting here\nbut I want to bring in DJ Rich DJ uh\nyou're with a lift\num hello so please unmute yourself and\nintroduce yourself and uh tell us what\nyou do sure hi everyone my name is DJ\nI'm a data scientist at Lyft and I work\non their causal forecasting system so\nthat's a system that we've been working\non for the past three years it basically\nallows us to do budgeting and pricing\nover like a three-month time Horizon and\nit's um exceptional in the sense that\nI mean it's of course I'm gonna say that\num but it does forecasting and it needs\nto match all these experiments that's\nwhy we get to call it causal and I guess\nthe other sort of specific challenge\nthat it has is um it's it requires\nmodels from many different teams so from\nthe model development side it's almost\nlike a distributed test we're actually\nnot very compute heavy but we have a lot\nof different people adding their\num\nmodels to a larger system\ninteresting so so you could be actually\nkind of like what we were earlier\ntraditionally that we have a lot of\nmodel throughput but now the way Rolando\nput it is like well now we go through\nvelocity and naturally the number of the\nmodels matters but what comes out of it\nso we're going to spend a little bit\nmore time later thank you so much DJ I\nthink uh super interesting\num next we have uh Brian uh so so Brian\nyou are an ml infra engineer\num tell us a little bit about yourself\nand mute yourself and uh give us an\nunderstanding about uh what's happening\non mlops New World\nyeah thank you Martin yeah so my name is\nBrian and I'm an ml engineer or ml\ninfrastructure engineer at stripe\num specifically we kind of break apart\nanal infrastructure into different parts\nand specifically I focus more on the\nexploration aspect of ML and so what\nthat kind of entails is the services\nthat we own are more around training you\nknow notebooking and basically now we're\nactually trying to move a lot of those\nprocesses onto flight because flight\njust offers a lot of the things that\nwe're looking for primarily velocity\nwhich is the the primary V that I would\nsay that we're focused on\nfantastic yeah I like that term so we're\ngoing to talk a lot about velocity and\nhow to stay sane while increasing\nvelocity I think that's kind of like the\nbalance we're trying to find thanks\nBrian I saw that Michael uh you joined\nus please uh\num uh on YouTube or you know show you\nturn on your camera and use your\nmicrophone introduce yourself and\num tell us what you do at Strife works\nlet's see yeah I'm I'm Michael I'm a\nsoftware engineer at striveworks we have\na ml Ops platform called chariots that\nreally focuses on the life cycle of a\nmodel and so everything from you upload\na raw data set you need to annotate that\ndata set need to train it based on some\ntraining parameters model architecture\ndifferent uh permutations of a data set\nwhatever Transformations you want to do\nthere cataloging that model serving that\nmodel up so you can perform inferences\nand then even detecting whenever you get\nsome model drift things like that and so\nwe focus on that whole thing we use\nflight so\num\nsince we focus on all that stuff we\nstill need to use it use our models in\nworkflows and so we chose to go with\nflight we looked in different\nTechnologies but flight hit our use\ncases and so now we're able to focus on\nthat model development part without\nhaving to really think about like how\nare we going to productionize this we\nhave we have to release our platform all\nin one go\nso we work with a lot of highly\nsensitive data we have we work with\ngovernment government contractors and so\neverything has to be deployed together\nso flight helps us\nhelps us do that and\nwhile we have these things done in a\nJupiter notebook that is not a\nproductionized version of a model and\nwe're also the startup so I was smiling\nwhenever varsha was talking earlier\nabout having to fight for gpus and\nthat's that's part of the scheduling\npart of the model life cycle and that is\na\nyeah that's a that's a fun topic Michael\nthank you so much I also love uh your\nyour role I mean your\num devops engineer that that is on ML\nOps so that sits right between data\nscience to a certain extent and then\nengineer on the other side very often so\nso we're going to have a lot of really\ninteresting uh topics to discuss in a\nsecond so thank you so much I want to\nbring in uh Fabio uh Fabio\num please unmute yourself and introduce\nyourself and tell us what you're doing\nin the context of mlops\nhey everyone sorry my voice I recalled\nthe last days hey everyone\num yeah I'm Fabio I work at a company\ncalled recognize we built a custom\naccelerator chip for self-driving cars\nthat created times faster than than\ncompetitors and I work in the software\nengineering team that provides an ellops\nplatform for for a perception team who\nbuilds the models to run on the chip to\nsteer the cars and\num yeah we leverage tools like like\nkubernetes and flight to just make\nexperimentation faster and to help us\nkeep track of the experiments we're\nrunning being able to reproduce them\nuh before I I started working on this\nrole I built up the MLF team at a\nmachine learning Solutions provider\nand there my team built a kind of an\ninternal developer platform that allows\nml Engineers to\nbring up infrastructure that they need\nto train or develop and productionize\ntheir models in a self-serve fashion\nand for that it's really important that\nyou can abstract the way the complexity\nof the tooling that is typically\ntypically involved and for that things\nlike kubernetes and fight are just great\nback then we did a comparison of all the\ndifferent orchestration tooling out\nthere and with everyone it was kind of\ncumbersome until we then settled on the\nstack that we ended up using everything\njust ticked and we were like okay\nif we like the Heim said it's easy to\nsay that but\nwe would have we should have built it\nthe same way if we had that if we could\nhave done it ourselves\num yeah I originally\nor before working as an envelops\nengineer I work some as an ml engineer\nand then kind of realized that the\nexciting part is happening around the\nmodel\num very often at least\nor at least I felt so\num but also because there was still just\na lot of pain managing the\ninfrastructure and I figured okay this\nhas to be easier right like not\neverybody can become an infrastructure\nexpert most Engineers don't want that\num let's find an easier way for this\nfantastic uh Bobby thank you so much so\nbefore I want to bring in Orlando two so\nRolando please um we you have introduced\nyourself earlier but I want to quote\nthree things you said in your paper and\nthen bring you in and and the entire\npanel now uh those three things struck\nme first thing you said is people spend\na significant amount of time on less\nglamorous aspects of ml like maintaining\nand monitoring the ml pipelines and I\nthink some of the people here will say\nwell it might be less glamorous but it's\nreally you know the essence of of making\nthis whole thing flow that's one thing\nthe second statement you made was\nparanoia caused by ml debugging trauma\nso when I read that I go like wow I mean\nthis must be pretty severe and obviously\npeople go through this and and we need\nyou know varsha said it I think right\nearlier we need versioning we need\nreally to know where we are so we can\ntake a step back at any given time\nMichael as well and last but not least\nhaving high velocity means potentially\ncould mean drowning in a sea of versions\nand and so that we want we don't want\nthat either so what I wanted to to to\nstart with this whole conversation here\nis like you have now a group that\ninvested a little different it's a\ndifferent group than the group you work\nwith in your study right this group had\nmade an investment on an infrastructure\nplay so I just wanted to see if there's\nan area where you are curious want to\nask the group hey is is something more\nor less a problem specifically when it\ncomes to Velocity I think velocity\ninversioning those are kind of like the\nthings that go in two different\ndirections and I feel like uh that's\nwhat everybody tries to solve of course\nvalidation we want to do a little bit\nsooner but I was just wondering if you\nhave a question and if you want to see\nif there's anything different with this\ngroup versus the other group\nsure so uh one quick question I'm trying\nto I'm thinking of the code so we use\ngrounded Theory method and then\nbottom-up coding so\num as I'm reading as I'm listening to\nsome of these questions I'm thinking\nabout the codes that we parseed the\ntranscripts uh something that I remember\nis the phrase ribbon replace uh so\nespecially with small uh companies the\nml Engineers were often you know\nresponsible for the entire workflow of\nml Ops\num and because of that same you know\nemployee turnover something that was\ncommon was people would want someone\nwould develop the infrastructure and\nthen they would leave the company and\nthen they would hire another ml engineer\nand they would rip and replace and then\nokay that other engineer liked Scala\nwell I like spark SQL and so they would\nyou know abandon their components and\nstart from scratch this would be\nsomething that could take up as much as\na year but you know because it would\nlater help help them with incremental\nchanges and deployment and kind of like\nlead the team they kind of thought that\nit was\num you know valid I think with larger\ncompanies they would eventually\num converge to an infrastructure that\nthey trusted and that had been around\nfor a longer time I asked someone from a\nlarge company if this kind of like\norganic growth of infrastructure was\nnormal and benevolent and and they\ncorrected me and they said that they\nwould hesitate to call it ad hoc even\nthough they hadn't you know developed it\nfrom start to finish just because there\nhad been so much\num engineering that had gone into this\nso I know that eventually company says\nthey grow in maturity they do converge\nto towards a trusted infrastructure\ntheir similarities but there's\nsimilarities and Concepts I felt like\ndifferent people had different\npreferences different attitudes like you\nsaid Martin some people might want to\nbreak more in favor of\num you know High Velocity other people\nmight want to keep more simplicity with\nreduced versions and stuff so\num it's gonna matter company culture is\ngoing to make a difference so I guess\nasking the panel here is like what are\nsome of the benefits\num from using from like how would\nsomeone\num Outsource to a third party this this\ninfrastructure service\num\nyeah yeah and how do you incorporate\nthat into your workflow\ngood good question so\num I think if it's clear I'm happy to\num I feel like one of the interesting\nquestions here is maybe on Michael's on\nyour side I think\num obviously the governmental work and\nyou have found a platform\nyou know I feel like really the\nengineering how did you get there I feel\nlike that's one piece and and then\nwhat's the advice for others to to jump\nto to make a similar step that you have\nmade so far what was the Epiphany that\nyou had to get to\nright well our whole\nwhatever\nwe were originally a services company\nthat would uh take on government\ncontracts and send out a software\nengineer and a data scientist together\nso anytime there was\nlike a data scientist wasn't just\nresponsible for the whole model like\nlife cycle\num a software engineer was there to help\nout with that as well and so they would\ntag team on whatever was needed to uh\nto help the mission but also any it\nwasn't just the sole responsibility of\nthe of the um\nof the data scientist and after a while\nwe pivoted towards creating a ml Ops\nplatform and\nso\nwhat is can we can we scope down a bit\nof that question do you want to go down\nOrlando\nhow do people take advantage of that\nservice or\nyeah so the the infrastructure part was\nimportant right so I feel like there was\nalmost like I could break up the time\nthat the ml engineer spent into two\nhalves and the first half was build the\ninfrastructure and the second half was\nthe four stages and so if I can just not\nbuild the infrastructure if there's like\nan out of the box solution for me to do\nml Ops like that would be great you know\nso\num I guess what's your your what was\nyour experience with adopting this\ninfrastructure\num Lessons Learned pain points that kind\nof discussion\nyeah so our our data scientists tried\nbuilding out a a platform that would\nhelp them you know not have to worry\nabout this\nmodel development life cycle and that\nincluded things like versioning and\nscheduling and actually serving up the\nmodel\num but\nuh we found a lot of\nwe needed to really invest in making\nthis like both simpler for some use\ncases and also giving a whole bunch of\nlike\nuh\ncontrol over different different options\nso there's like\nI don't here here's the data I need a\ngood object detection model go I don't\nwant to have to worry about how many\nresources I'm going to take up and then\nyou have the other end of things where\nyou know I'm I am a data scientist I\nwant to be able to play around with\nevery little thing and experiment on\nthese models so we have uh both of these\nuse cases whenever we're out there you\nknow on these on these missions we may\nneed to be a bit more technical on\nthings but once we figure out like no\nthis model works we may need to just\nretrain that model with new data that's\ncoming in so in the beginning it may be\nreally experimental and then later on it\nis just like I need to quickly retrain a\nmodel I don't want to have to go through\nthe whole rigor rigmarole of what I've\ndone before in that experimenting\nexperimenting phase I just want to click\na button that says hey here's my model\nand here's the new data set go retrain\nit\nright now\nmy if I just chime in here right now and\nbring in DJ because I think I want to\nextend that view a little bit from a\ndata scientist point of view and then go\nall the way across to the engineering\npart because that's where worlds called\nlight and I feel like that's what you\nknow we've very often sit at the in\nbetween two chairs and I think DJ you\nknow exactly where I'm coming from so\nwhat was your Discovery Journey what\npain did it take for you as a data\nscientist to to look at an\ninfrastructure solution to say well I'm\ngoing to be a part of that well funny\nenough I actually resisted flight in the\nfirst place because I didn't know\nanything about it we were on airflow and\nso\nuh we had we had working models uh but\nsome we had an evangelist on our side\nbut you gotta use flight uh and then it\nwas just much easier the UI\num was better\nyeah it's\nit's a little bit hard to recall all the\nI'm thinking\num\nI mean\nhmm\nI mean probably the UI was the thing\nthat caught me by surprise because\nairflow we had this terrible\nUI that was just way too complicated and\nunfriendly and a lot of different\nregards and then the flight UI just had\nit task wise which made a lot easier\nalso one thing that was huge was\nit's very python agnostic so we had to\npull this extremely dirty trick to get\nairflow to work which was\num using like a python string like a\ngiant string and then manually managing\nthe image and then that was the first\nthing that we got to tear out\nand so uh\nyeah so once we were into flight it was\njust you could go directly from the\nlocal situation to the uh remote\ncircumstance which was extremely nice\nand the UI was a was a pleasure and yeah\nit was just it was just it was just a\nbig difference\num from airflow uh and now it's like a\nlike a primary feature of a of Grail\num and it's almost like a a product with\nit because you can just send the link\nfor the job and then because of how\nwe've hooked up these links within the\nUI you can actually Point people to data\ndumps so one of the versions that we\nhand over uh to Grails and just say\nhere's your job it's running it'll write\nthis CSV or object to S3 and then you\ncould pull it in it's a little bit\ndifferent uh because it's an internal\ntool so we all have a lot of um data\nscientists using it so it's not this is\nnot like a uh it's not like the lifts\nETA model where the users are the people\nwith their phones this is more like a\nbunch of data scientists and software\nEngineers so that's that's not like the\nthe typical use of flight but it's a way\nthat's been you know a heavy way of uh\ncommunicating the system so it's uh oh\ncertainly a big one interesting and and\nnow looking at Fabio varsha Brian I mean\nyou're coming from the engineering side\nand obviously you you're used to\nengineering principles and you're like\nwell don't give me a notebook here\nbecause I don't want to put this in\nproduction so obviously you know that\nthat sentiment that feeling uh so so for\nyou I mean you didn't arrive there with\na solution that that's based on\nversioning and actually\num you know with what you have right now\nI mean what was the pain for you to get\nthere and that was the first question\nthe second maybe how did you get your\nyour machine learning Engineers that you\ndid a scientists on board\num maybe I can go first so um with with\nThe Arc um you know use cases let's say\na lot of our and especially during\ncovert time so for us a good chunk of\ninputs are actually driving data you\nknow some engineer or some you know\nvehicle specialist would go there drive\nand then get all the data that our\nEngineers need and around the initial\nperiods of covet that was really hard\nfor us so we moved more to a let's say a\ndata-driven approach and what that means\nis for majority of the teams the\nworkflow would look something like they\nwould want to select a scene and that\ncould be either an annotated data\num you know that we would get from our\nvendors or more like um they would they\nwould want to choose a scenario okay are\nwe going to train on a rainy day or is\nit going to be on an icy road so the\nfirst step would always be some sort of\na data selection depending on you know\nif they're from a prediction planning um\nwhichever team that they come from and\nthen the second step for them would be\nmore like a data curation okay uh what\nwe you want to convert this data into\nsomething that our ml model can\nunderstand you know either you know\nrasters or numpy some sort of a curation\nthat would happen and in the actual\nmodel training and once that is done\nthere's always obviously the validation\nyou know depending again on the team do\nthey want to do it on a supervised\nmetrics or you know an open loop closed\nloop simulation some sort of regression\ntesting that they want and the last step\nis analysis so we figured out that these\nare the most commonly happening uh\nworkflow so one of the things where you\nknow to answer Ronaldo's question infra\nhelped us was we abstracted a lot of\nthese uh with flight and some more\nIntegrations that we did on top of fly\nand so that eventually the actual ml\nengineer would simply have to write\ntheir portion of the code uh what let's\nsay they want to train on or how should\nthey how according to them the\nvalidation should look like and\neverything else is abstracted away from\nthem using flights and um\nsorry Martin your question was around\ncode versioning right yeah and so\nexactly yeah okay uh I think flight\nprovides us that feature for default we\njust uh depending on some internal\nnomenclature that we've agreed upon uh\nevery version or every change we\nseparate into Dev and fraud and uh use\nthe flights innate versioning and that\nversioning right from the time the code\nis let's say pushed to production that's\nuh persisted all the way until we move\nto the final analysis should this model\nbe pushed to production yes or no so we\nhave that persisted all along so it also\nhelps us with model artifactory and to\neventually Trace back\num yeah uh I'm again on the sideways you\nknow uh the more we invest in\ninfrastructure the more it's gonna help\nour uh Engineers you know get\nself-driving car out on the roads pretty\nsoon\nforeign\nso I think it was a pretty um similar\nexperience on my M2 I think I share a\nlot of the um the pain that DJ mentioned\num when he was talking about going from\nairflows airflow dags\num unfortunately for us as stripe I\nthink what we did was we were able to\npartner with a lot of our users so my\nteam is responsible for the\ninfrastructure and then obviously you\nknow the data scientists or the ml\nEngineers they're responsible for the\nproduct and generation of the models\num so getting them partnered early on\nand understanding that they want a\nbetter way in being able to train their\nmodels and you know generate data was\ndefinitely the first step and then the\nnext one after that was you know which\nplatform are we going to choose and so\nfrom there we were able to kind of just\nyou know make a make a short list kind\nof try out pocs for each one of those\nand from there it was kind of just like\nflight winning out and features that and\njust the ability to iterate quickly I\nwill take a kind of a spicy take based\non Ronaldo's paper but I do think\nJupiter notebooks was kind of a pivotal\nhere because I think for us at least\nbeing able to speak to users kind of\nwhere they start at which is being able\nto iterate on notebooks was a starting\npoint and then I think because we're\nable to use that code base and and\nimport it into Jupiter notebooks and\nthen extract that out and put it into\nlike a flight dag it let them kind of\ninterrupt between you know production\ntype dags versus like ad hoc type\nnotebook interfaces so that piece really\nworked well for us\nvery interesting Fabio\nyep it's not similar brand what um\nwhat what we went through\num\nback in the day before we had the first\ndag to like that kind of organized\npre-processing and training and\nevaluation it automated that\nthe human was the orchestrator we used\nan alpha project back then\nwhich packages your code in a Docker\nimage and then runs the drop on\nkubernetes with that image and an entry\npoint and then it does the training but\nthe problem is that once your node shuts\ndown the Pod is gone the job is gone\nnobody ever knew that this happened you\nhave a run an ml flow\nbut you there's no like basically the\nout like what you treat as the artifact\nis kind of the model that you train\nright and then we had like I work in the\nmachine Learning consultancy Company and\nthe project lasted for one year that we\nwere on and at one point we realized\nokay actually we're not able to\nreproduce what we ran six months ago\nbecause yeah like we do have the run\nhere we did not lock all hyper\nparameters we did not log exactly how\num\nbasically we weren't able to reproduce\nreproduce runs and we figured okay we\nneed to do this better first of all we\nwe need to be able to kind of chain\ndifferent tasks together because what we\nwere doing at them a bit embarrassed to\nsay that but we started the training\nduring like like before ending the day\ntraining would run for a few hours would\nend in the middle of the night but the\nnext morning the violation wasn't done\nbecause somebody had to manually trigger\nit\nso that was the first like the for us\nthat was like okay now right now we\ncan't continue doing it like this we\nneed to automate that\num then we've wrote down a bunch of\nrequirements and an important one for us\nwas that\nwe need to be able to do different tasks\nwith different resources\nit's kind of a no-brainer right but in\nfact it's pretty difficult to do that\nwith some Frameworks where actually with\nthe better part of the Frameworks\num we needed a good way to do\ndistributed training and not have our\nEngineers create there there was a in\nthe beginning there was a shelf script\nthat spun up compute engine instances\nand then thus kind of set the the\nnetworking so that every worker for\nPietro's distributed training knew where\nthe other workers are running\nbut that's not great right like we\nwanted to we wanted that part to\nintegrate nicely into our into the into\nthe deck\num and also for example\nthere's two flow operators which is a\ngreat piece of software in my opinion it\ndoesn't really integrate well with\ncoupon pipelines as I would expect it\ndoes it really that really doesn't\nso we had this list of requirements of\nwhat we want which would be different\ntasks different resources different\ntasks different images be able to have\ndistributed training with pytorch or\ntensorflow just in the middle of the dag\nwe also needed spark because we did some\npre-processing with spark it would be\ngreat if spark could somehow fit into\nthe frame\nand then we\nwent to like\num what's it called tourist data sense\nand the other blocks a medium and looked\nat top 10 frame like orchestrate\norchestration Frameworks from lops and\nwe tried to build the Prototype with the\ntools that we found\nand we got pretty frustrated and at one\npoint we discovered flight and\neverything was easy\nand um yeah because it does exactly\nthese things it's you can specify the\nresources at the task level and not at\nthe workflow level it doesn't help you\nif it's at the workflow level because\nyou don't want to preserve your GPU for\npre-processing that doesn't make any\nsense at all for machine learning you\nwant to register to the task level you\nwant to have the ability to run\ndifferent parts of your workload with\ndifferent images\nI'm sorry you don't get into situations\nwhere\nyou have to fit all your requirements\ninto one image because you just get to\nrun one image\num\nyeah it has boosted our productivity\ngreatly\num and also these things that we can't\nreproduce runs you cannot not reproduce\nruns because you're forced to version\nwhich is a great thing\nand in the end part we ended up doing is\nthat we kind of had people who lean more\ntowards engineering side configured a\nplatform in a way that we considered as\na best practice and then we made it kind\nof a self-serve thing uh to also be able\nto\nget to enable ml Engineers to use it\nthemselves without putting an MLs\nengineer or software engineer in every\nteam\num\nyeah\nso I think that sounds very similar to\nwhat you Brian described with creating a\nprototype with the different Frameworks\nthat are out there and then just\nfiguring out which one takes most boxes\nthat you have so so I think interesting\nwould be so Ronaldo when you contrast\nthis I think what we hear here is like\nan experience where you have a clear\nbusiness problem but you have an\nengineering approach going about it and\nbringing data scientists obviously in\nand giving them a system you know that\nthey can actually operate within plus\nhaving the advantages of having\nconnectivity to other systems and\nversioning and so on is is that kind of\nlike compared to the 18 people that you\nspoke did they have severe access or in\ngood access to engineering resources to\nbuild the infrastructure or do you feel\nlike they were just like put together\nyou know classical ml Ops Team start use\nthe fragmentation of tools out there and\nit's a wild west and go ahead and just\nfigure out how to sort yourself out\nso I think there were some\num like Simple Solutions I think\num\none of the things that I learned was\nthat folks had like a development\ndesktop under their desk\num and this is like either startups or\nor larger companies\num I think I can maybe speak of my own\npersonal experience here rather than the\ntranscripts I think\num I think I noticed kind of like a\nmovement to the cloud and then back into\nlike a GPU development desktop I think\nin 2017 we were all kind of like oh yeah\njust use AWS for gpus and then it became\ntoo expensive and then we were using\nslurm to schedule jobs but then a few\npeople dominated and they got all the\ngpus and then there was none for me\num and so now I'm seeing like desktops\npop up in graduate students uh tables\nlike everyone just kind of it's like you\nknow what screw it I'm getting my own\nGPU and I'm controlling this GPU but\nagain it's like yeah okay now you have\nvelocity but then maybe your GPU maybe\nyour you know development desktop has a\ndifferent operating system or a\ndifferent version of tensorflow on Cuda\nthan does the distributed training\nenvironment in the Cloud you know so\nthen you have to manage that like\nversion drift in operating systems and\nstuff so\num you know it sounds like it's being\nactively negotiated what the right\nbalance is\nyeah I remember when I went to the cloud\nand then I decided to buy a dgx because\nit's so much better to have this in in\nthe office so I I understand that\nfeeling I think Fabio what you said\nearlier just just was a really\ninteresting point you said the\nseparation between workflows and tasks\nis is really kind of like where you get\nactually the benefit when you focus on\ntasks and you have granularity and you\ncan actually leverage your resources\num it but we also know that notebooks\nare really you know you know not just\nlike a task they're potentially a lot\nmore so how do you get\ndata scientists and machine learning\nEngineers that are completely notebook\ndriven to a task mentality how what is\nkind of like the the impetus to to help\nthem to understand that or not to\nunderstand I I would be wondering about\nyou know Michael DJ what are your\nthoughts about uh what notebooks versus\nreally functional tasks different\napproach to to machine learning\num I mean I personally think that\nthey're necessarily evil\nI don't think that they are uh they're\ngood to rely on in general and no final\nversion of code exists in notebooks but\njust that human interaction and so many\npeople are just accustomed to it so\num but it seems like other people's\nworkflow involves you know migrating\nthat out as well\num but in terms of I think it works\nagainst you in terms of versioning\nbecause notebooks change constantly for\na while they weren't I mean GitHub only\nrecently added that integration that\nallows you to\ninspect them well uh they also are just\nlike not\nuh they're it's it's not very\nopinionated with respect to state\nand debugging things and keeping track\nof Errors\num requires that state is a almost like\na fixed feature to keep track of uh so\nwe've just found that like for example\nsomeone will take a notebook they'll\nhave some results in the notebook\nthey'll then export a chunk of code out\ninto a PR then they'll reference The\nNotebook for why that piece of code\nshould work\nsomeone believes it in the pr it gets\nmerged in later you find out that that\npiece of code was not what you thought\nit was and then like oh actually it was\nbecause you know I ran the cells in a\ndifferent order I mean that is kind of a\nsort of a contrived examples but things\nlike that happen a lot um pulling stale\ndata is one that we've seen so in\ngeneral\nyou know\nI think that they are a source of errors\nbut if you want to get that\nmodel developer from the environment you\nsort of have to use notebooks\nyeah yeah and um we have about six\nminutes left I would love to bring this\ntogether and and make it useful for\npeople who are who are not quite there\nmaybe where you are or still fighting\nwith the elements so to say so if you\neach of you would give one piece of\nadvice right about you know looking back\nand in hindsight everything is 20 20.\nwhat would you want to tell somebody\nwho's in a similar situation as you were\nmaybe two years ago or a year ago in\norder to do differently or than you know\nbetter so they they get to the point of\nmaximizing what Rolando says the\nvelocity do validation earlier and have\nversioning so so what's the secret\nformula do you need to find people do\nyou need to collaborate more do you need\nto look just at systems is a combination\nof all of that so if you want to go\nthrough it would love maybe Brian top\nright uh if you go give Brian from two\nyears ago some advice what would that be\num yeah I think uh for us the the thing\nthat I don't think we looked closely\nenough was definitely that first fee\nwhich is velocity\num I'm also like I can throw another\nquestion out to you to the panel because\nI am also curious but like how do most\nother people measure velocity\num so we take a very naive way right now\nwhich is looking at a number of\nexperiments run and so like looking at\nthat we can tell like having\ninfrastructure that allows you to\niterate faster\num that allows you to add more\nexperiments and one of the things\nallowed us to do that was moving over to\nflight having the versioning having the\nability um to iterate quickly and so\nyeah I think just that first V I think\nis what speaks loudest to me and to\nalways be monitoring that in your system\nfantastic Porsche\nif you would give them if you would give\nyourself some advice from a year ago\nwhat would be the number one most\nimportant piece of advice\nversion everything and make sure that is\npersisted all the way through yeah and I\nthink to add about the previous\ndiscussions you were talking about right\num having your own GPU and having your\nown workstation versus you know when you\nmove to production things are very\ndifferent\num fight has enabled us to you know\nwrite in such a way that we could test\nthem locally uh so all of us have a\ncloud workstation with GPU enabled so we\ncould do that locally and it's the exact\nsame environment we get on you know\nproduction so versioning and uh yeah\nthat that would be one thing that I\nwould tell myself from two years back\nhere oh thank you\nwe're going to take some notes and\nMichael how about yourself\nwe actually started two years ago\nbasically so that's like starting from\nthe very beginning uh on the ml Ops\nplatform bed\num\nI would say for us it would be\nreally coming out with a\nlike a strong test case on being able to\nsay this is like the golden standard for\nthe models that we're trying to train\nand then making sure that we're able to\nmaintain\nlike what is the\nthe fastest time we can get to creating\nthat model with the least amount of pain\nfor a user and so we didn't really keep\nthat in the beginning\nand so\nwe focused a lot on\num\ncreating all these different components\nbut we didn't focus on like the quality\nof the model itself as much at the time\nand I think that could have helped us\nthere's a\nas I had said before we have like really\nstrong data scientists that can create\ntheir own models and we can serve them\nand they they understand like our\nplatform but we're also working on the\nother part where like you're not a data\nscientist but you kind of know how\nmodels work and you know you need to add\npermutations and you know your data set\nand you know like this model\narchitecture should work and then you\ncan test your model once it's done and\nsee that it works we didn't focus on\nthat strongly enough from the beginning\nI think and that's I mean maybe that's\njust the use case that we have I know\nthat at other companies like you have\nstrong data scientists that\num like that's where you're building the\nenvelops platform for but um for us we\nhave also different use cases yeah I\ndon't think you're alone I think a lot\nof companies actually even Outsourcing\nthe data science part and don't have any\nof that so I think that's quite common\nokay very good uh DJ so when we were\nfirst developing uh the system we were\noperating on everyone at that point was\nbringing their own pieces of advice and\none was you know build the simplest\npipeline that does the thing that you're\ntrying to do and then start with the\nmetrics and then start optimizing those\nmetrics in doing that I think we made a\nmistake which is you have to build\nit's easy to create a pipeline that\nactually isn't on the path to building\nthe final pipeline where that first\npipeline you should actually just\neliminate almost entirely so I think\num thinking about the full Pipeline with\nall the components the remote compute\nthe fact that you're using data across a\nlot of different developers like the\nfull infrastructure needs to be\nrepresented in that first version of the\npipeline which does slow it down and\nthen things like model iterations and\ncaring about predictive like all those\nthings those come later but um well what\nthere's also the means is sort of a\nrefinement on that piece of advice for\nthat first iteration is to really\nschematic the whole thing just that\ndesign period is incredibly important\nbecause we actually had to just like\nrewind after a year into the whole thing\nand redo a big portion of it so that\nwould have been quite helpful\nfantastic and uh Fabio I assume you\nagree with the architectural approach\num yeah when you when you ask a question\nI was thinking what I was going to say\nI'm going to say a tip to myself maybe\nthree years ago when I was in a team of\nfive machine learning Engineers could\nhave been an incubator trying to build a\ncompany around machine learning Services\nmy tip would be overcome not invented\nhair syndrome because back then\nwe were kind of considered okay we're\nnot Docker we're not compute engine or\nac2 let's just kind of make that work\ntogether with python and build our own\ncontainer orchestrator my hand would be\ndon't do that there is an industry\nstandard learn kubernetes there's a\ngreat variety and set of awesome amount\nof tools for this platform learn that\nplatform and a lot of thorough will go\naway\nso don't just overcome not amended here\nsyndrome I would say and just use great\ntools that have developed over the last\nyears fantastic Rolando I want to give\nyou a last chance to to uh engage with\nthe with the panel as well is there\nanything you wanted to ask them before\nwe\num close out the session today\nuh yeah thank you for Martin\num so I was thinking about\nlike what's something that I learned\nfrom the interview that I could I wish I\ncould take back in time\num maybe from an academic setting is we\nhave a this habit of evaluating things\nin a vacuum uh it's kind of like What DJ\nsaid of like we have a model training\nPipeline and then we build this model\ntraining Pipeline and then we try to\nfind an application that wraps it and\nthat's a mistake right I think\num product validation is what matters if\nyou start thinking about product metrics\nlike Revenue key performance indicators\num then you that's going to kind of\ndrive you naturally towards\ncollaboration with other stakeholders\num I also think that in machine learning\nonce you reach a certain threshold of of\nperformance then if if you're going to\niterate it makes more sense to iterate\non the application or the software maybe\na heuristic or a filter rather than the\nmachine learning thing per se so that's\njust kind of like\nbefore you start with machine learning\num build out the application\ninfrastructure and then you know see\nwhere it belongs in the global scheme of\nthings and then iterate on the model\nthen so I would say like validation was\nmy takeaway fantastic now I feel like\nthis is I mean there's so many other\ntopics here today\num the product component I completely\nlove the topic about metrics that are\nother than an F1 score right or whatever\nwe do they have imbalance and so on we\nalso want to look at is what we build\nreally valuable or not that's a\ncomponent are we actually discriminating\nwith our models against people it's\nanother dimension that is really huge\nthink about models and and the fair\nhousing Fair Credit act and so on\num that that use data sets that are\npotentially locally drawn from an area\nwhere the demographics is all the same\nright and so you have to think about\nwhat you're doing there so at the at the\nessence what we do is is really kind a a\nmixed field of where you have to have\nrange I think to me what what you're\ndescribing with your three vs is range\nand that ranges entropy and then entropy\nover time will go in different ways and\nwhat you have to have is a really solid\nsolid and robust system to bring it\ntogether and give people a chance to\ncommunicate and collaborate I feel like\nthis allows us to learn and get better\nand I feel like very often people don't\nstart at that level they just get\nthemselves to that level and so I just\nwant to say thanks uh to each and every\none of you this was really really\ninteresting I would love to do a second\npart about business metrics in other\nareas and and bias and models and so on\nand you know how when you have like F1\nscore specifically F2 which is my\nfavorite topic for credit fraud and so\non\num but I just want to say thanks uh if\nthere's anything else anybody wants to\nto say it right now is your last chance\nif not\noh it looks good all right everybody's\ngood then we're going to close this out\nwe're going to have more information on\nthe website and I look forward uh to to\nseeing the panel again I just want to\nsay thank you very much each and every\none this was a real pleasure thank you\ngoodbye"
    },
    {
        "title": "Flyte @ Stash",
        "transcript": "presentations so first uh let's go to\nthe next slide please I just want to\nwelcome Katrina Palin she is a staff\nfrom our platform engineer at stash\num and obviously uh with ML platform\nengineering the interests so NLP and\ndata and feature engineering makes a lot\nof sense but also Finance food and\ngaming\num I think really great topics uh so\nKatrina please unmute yourself and\nintroduce yourself and let us know you\nknow what um what your experience with\nthis flight wasn't what are you doing at\nstash so we're looking forward to your\npresentation today thank you\nyeah sure hello everyone my name is\nKatrina Palin um at stash right now\num should I share my screen it's like\num we can actually uh however you want\nto share your screen or we can work uh\nmove it for you\nyeah\num yeah we can go ahead uh and uh if you\nhave the slides that you can do all\nright here I got it you got it okay cool\nby the way\nall right and everyone can see my screen\nnow yeah\ngreat\noops sorry about that\nall right so\num yeah so today I want to talk a little\nbit about how we're using flight at\nstash and so I'll take a minute right\nnow just to talk about\num\nuh stash\nso what is stash so stash is the\num investing and banking app that\nempowers everyday Americans to invest\nand build wealth\num and so\nwhat we allow our customers to do on our\nplatform we allow them to invest bank\nand learn so we do cover all these\ndifferent aspects on our platform\num you can find us on web as well as on\nmobile\num and you know we provide these tools\nfor everybody's wealth building Journey\num and that's kind of what the customer\nsees on every day but uh for the ml\nplatform team\num sort of on the back end what are we\ndoing we're providing tools for modelers\nand researchers so that we can better\nbetter fulfill our mission and our\nfiduciary duty to our customers so we\nfocus on keeping the platform first and\nforemost safe for and fraudulent\nactivity and preventing Bad actors from\nimpacting the system\nand so I'll talk a little bit about how\nwe came to flight\nso as a ml team\num we were thinking about like okay well\nwhy do we need a new orchestration tool\num and so a lot of what we were working\nwith at the time\num was a little bit of a sort of just\nhomegrown process right with any sort of\nstartup you kind of have uh you do\nwhatever works at the time\nand so you know one of the things that\nwe were trying to look for is if we were\nable to we want to reduce our time to\nMarket by standardizing our uh processes\nacross all of the researchers and\nmachine learning algorithms Developers\num we also want to because we are a\nfintech company we want to provide your\ntransparency and traceability for all of\nour models and so one of the things when\nyou start to get you know a lot of\nmodels into production especially as a\nfinance company you sort of have this\nobligation\num to be able to track a model so that\nwhen decisions happen right when we say\nin a certain activity we identify it\nmaybe as like a fraudulent activity we\nhave a duty to be able to say hey this\nis the model this is the data set that\nproduced this prediction\nand we should be able to do that\nthroughout every single version and\nevery single execution of prediction as\nwell as\num for uh data processing for our data\nsets and then we also want to be able to\ngive the data scientists and researchers\nthese flexibility without bogging them\ndown with devops right so in the very\nbeginning at a startup it's very\ntypical that you know you kind of just\ndo whatever it takes to get things\nshipped out and sometimes that means you\nknow data science are like learning\ninfrastructure but as we're trying to\nscale our team we want to do less and\nless of that and then finally we want to\njust optimize the model execution time\nand as well as our compute cost right\nbecause more and more customers coming\nonto our platform we want lower latency\nand we also want um it to be uh cheaper\nwith every single uh prediction every\nsingle training run\nand so when we were going through this\nprocess\num we wanted to evaluate different\nplatforms so this work started in April\nthis year so this is actually very new\nstill to our organization\num and so you know we went through our\nevaluation process and we started\nimplementing flight just a few months\nlater in June so what are the things\nthat we looked at we're currently on\nairflow for a lot of our feature\nengineering so we were thinking okay\nwell\nwhat if we stay on airflow what if we\nrewrite you know our model training so\nthat it works a little bit better in\nairflow\num what if we use kubeflow what if we\nuse prefect and so these are the the the\nother platforms that we're kind of\nthinking about and\num the things that we sort of considered\nwhile we were evaluating our different\noptions was uh you know first and\nforemost we want um all the\nconfiguration to be in code we want it\nto be observable easy to debug\nespecially as we're working with many\nresearchers\num and different styles of coding you\nknow with notebooks and different levels\nof\nfamiliarity with structuring you know\neven just testable code\nwe want to make the writing a workflow\nfor people's training jobs Pleasant and\nstraightforward right so I think that\nthis is a big deal because if it's not\nsimple and easy people aren't going to\nadopt the technology right so this one\nwas a really big one for us\num and then like I mentioned before we\nwant our workflows to be versioned and\nmaintainable the as we're going through\nthis life cycle and evolution of models\nV1 V2 V 1.5 whatever it be and then\nfinally you know as a you know a finance\ncompany we also want to make sure that\non the system and our data is secure\nright because sometimes we're working\nwith sensitive data it's really\nimportant for us to be able to\num continuously Monitor and ensure the\nsecurity of our system as well as our\ndata\nso we finally decided to move to flight\nright and\nthere were a couple different stages\nhere so we had to kind of decide on how\nwe were going to approach this because\nwe already had existing workflows in\nairflow\num so the first first of course we set\nup flight and one of the unique things\nuh for our setup and I'll talk about\nthis in a more detail later but\num we're on self-managed kubernetes\num and then the second stage uh sorry\nthe second part of the first stage was\nalso configuring uh spark on kubernetes\nbecause we want to move from\nwhat our existing air uh airflow jobs\nwhich were a combination of airflow and\nEMR and now we want to be on flight and\nSpark on kubernetes so\nthat was a big portion of the initial\nsort of setup for flight and our\nmigration so\num sort of what I call stage two we uh\nwrote some spark tasks to just abstract\nspecific spark configurations away from\nmodelers so we want to be able to give\nthem like the flight spark task but then\nput an additional layer of obstruction\nthere just to kind of Hide Away some of\nthe spark settings where that we were\nrequired to set that we don't want them\nto kind of think about\num and then finally there we have to\nrewrite those existing airflow workflows\ninto flight workflows\num and then as a third stage to kind of\nmove forward so we transferred all the\nLegacy models into flight workflows\nwe then created a demo workflow for the\nmodelers to kind of work off of and so\nthat they have an example of all the\ndifferent things we can do with flight\num and then that way we can Implement\nall of their our new batch inference\nprocesses on flight so what's great now\nis that every single new model that\ncomes out\nthat is being done on flight and it's\nreally exciting to be like working with\nthe modelers and having\num them write their own workflows and\ntheir own launch plans they're becoming\nmore and more self-sufficient every\nsingle release\nand so you know we've been on flight now\nfor a couple of months and so we can\ntalk a little bit about what are the\nadvantages like what have we seen\num I would say two things\nflights giving us additional flexibility\nand it's also given us additional\nsavings so\nyou know we\ncan maintain the Legacy workflows that\nwe had on airflow\nthat we're so dependent on EMR and Spark\num and I don't know if anybody else is\nworking a lot with EMR but it's very\npricey so\num we reduce our dependency on um on EMR\na lot here\num and I think that you know that saves\nus a lot in terms of and I'll illustrate\nthat in in a minute but uh some other\nthings that we really really like about\nworking with flight is this project in\ndomain separation so that we can have\nyou know environment-based resource\nallocation and just configuration\nisolation just having that\num in our new environment allows us a\nlot of flexibility in terms of testing\nnew workflows as well as being really\ncontrolled over the types of resources\nthat we're using\num because before\num when we're on airflow and EMR we're\nkind of provisioning just one big smart\ncluster to do all of the work whereas\nnow we can uh allocate resources on a\ntask basis right so being um I\nhighlighted it here as task-based\nresource allocation that's pretty pretty\nbig for us because we may have some\npretty lightweight tasks that don't\nrequire a lot of memory and there may be\none task where okay hey we need to we\nneed a lot more for this one\num we're running those types of\nworkloads very efficiently now\num and then in terms of uh the workflow\nand image versioning it's really helpful\nfor us to be able to identify not just\nwhat workflow ran when but then we can\nalso go back and Trace to which image\num we were running for that particular\nworkflow\num and then with a lot of the testing\nand the iteration that we do caching\nreally saves us time and so we can see a\nlot of\num savings there as well\nand then of course\num the type checking which is something\nthat didn't exist before in the airflow\nworkflows it's helping us a lot now as\nwe're evolving those workflows\num and making sure ensuring\num\nthat uh changes as it as they happen\nwe're not\nwe're not introducing any um we're not\nintroducing any bugs unintentionally so\nit's helping us with a lot of that that\nthere and so when I say okay it's saving\nus time and money\nwe looked at a couple of different\nthings right so our provision time just\nlike getting an EMR cluster for example\ntakes\nquite a bit of time so it's at least two\ntimes faster on average\num when we you know ran these\ncomparisons for\nwhat we would say is a typical batch\ninference model for that we're running\nat least daily\num and then the model execution time\nmuch much faster we had it three times\nfaster and the cost which we can\nactually probably further optimize this\nis with probably minimal configuration\nand optimization\num is at least three times cheaper so\nthis is for a you know job that we run\nregularly and so as you can imagine that\nfive dollars savings\num per run\nit really adds up over time\nso Karina if I might ask you a question\nabout that\n[Music]\num\num so that doesn't factor in right now\nthe caching and doesn't factor in you\nknow or the provisioning of your\nresource on a task level that would come\nin addition to to the savings that you\njust mentioned here right\nthat's correct yes and so over time\nwe're expecting to save even more\nfantastic thanks\nit's really helpful that flight is uh\nhas caching and is aware of the data\nsets that it's moving back and forth\nthat's going to be a big part of our\nnext iteration of optimization so\num one of the things I'll talk a little\nbit more now is some of the challenges\nthat we had in setting up flight and\nimplementing it at stash and so like I\nmentioned before one of the unique\nthings\num that is about about our particular um\ndeployment is that we're running our\nself-managed kubernetes on AWS\nand so we have our own\num we're not we're not basically means\nthat we're not using eks right\num and then we also have our own\num\nproxy and as well as our own deployment\nprocess and so\nin order for us to deploy or update any\nmanifest we have to run the helm\ntemplate generation process ourselves\nand we have a set of scripts that we use\nto apply like our organization specific\noverrides editions some annotations and\nlabels that are required by our\ninfrastructure\num specific to how we've set things up\nyou know especially getting configuring\nIngress and grpc\num that was something that was uh took a\nlittle bit of time for us because we\ndidn't have any other applications that\nwere running\num using grpc so we were actually kind\nof the first in our organization to do\nso and then we also have a tool called\nSpinnaker which helps us with manifest\nversioning and deployment and so it\nhelps us securely deploy as well as uh\nrollback so we can easily sort of test\nupgrades and updates\num and you know because we are running\nour own kubernetes cluster there's a lot\nmore\num\ntweaking that we have to do so being\nable to manage those manifest versions\nand being to roll back to a specific\nversion of the Manifest has been really\nhelpful for us and so you know this is\none of the things that we're trying to\nmaybe look forward in the in the next\ncouple months to kind of\num help write some more documentation\ncontribute more documentation in flight\nfor folks who are um doing these\ninstallations on self-managed clusters\num and then so the next challenge I\nwanted to bring up um just briefly and I\ndon't have a lot more time but uh\nis CI CD and so this is something that\nis just honestly it's it's an evolving\nprocess\num but right now what we've been able to\nconfigure is that the platform team so\nmy team we develop and we publish base\nimages for the modelers so that they\nhave\num something to work off of and then we\nalso provide them uh GitHub auction and\ncustom Runners for those uh to a build\nthose images for their workflows\num and so the way that works is that we\nhave feature branches those get merged\ninto domain-based branches so for every\ndomain that we have we have a branch\nthat is named the same way and in that\nGitHub action we have those environment\nvariables that get set in the image\nbased off of those domains and you can\nsee I threw some pseudo code there on on\nthe right\num and\nwhat one of the ways that we are able to\ntrack all of the different versions of\nthe images and the workflows we tag all\nof the workflows with the commit hash so\nwe can always go back to see when an\nimage was built why Etc and so this is\nsomething that we're is still an\nevolving process we're still working on\nit and I think this is something we look\nto the probably look to the community to\nkind of help build breasts practice best\npractices around so\num definitely\nlet me know if you have any sort of\nsuggestions on ways that you guys have\nbeen using flight\num in your continuous integration\nenvironment and you know we can probably\nuh adapt some of those best practices\nthat you've learned\num and then finally I just want to kind\nof throw and I know that uh\nuh just kind of talk a little bit about\nwhat we're excited but I think I heard\nsomebody had a question\nno I was just about saying so it's like\nit's the right channel to to continue\nthat cicd conversation and I feel like\nuh we just kind of follow up there and\ntake on the conversation on there so\nthis is a really good topic and we'd\nlove to provide feedback from people who\nhave traced to write the same challenges\nand came up with best practices\ngreat yeah thanks\num so yeah so some of the things that\nwe're excited um we are um just starting\nto use the pie torch integration\num we're trying to leverage more\ncontainer tasks as folks are um\nuncovering the flexibility and being\nable to write uh their training jobs in\nany language as well as you know\nleveraging gpus because that's something\nthey didn't have access to before before\nwe were moving\num before moving into uh the kubernetes\nspace and then finally I know we\ntalked a little bit about it earlier you\nknow a little plug for the ml flow\nplug-in I know that's something that we\nare pretty excited about we were looking\nat it um so hopefully we started\nactually implementing mlflow very\nrecently and so what we whatever we\ncontinue to help with that ml uh flow\nintegration that looks like it's in the\nworks we're really excited for that\num and then finally you know because\nthere's a Scala DSL and that's a lot of\nour stashes um back-end engineering\nteams or scholar uh devs uh where you\nknow I think what we would be excited to\ndo is open this platform up to the rest\nof the organization and allow them to\nwrite execute other workflows or trigger\nworkflows from our other back-end\napplications so this is that's some of\nthe things that we're super excited\nabout and that's it\nfantastic awesome thank you so much\nKatrina this is like a wealth of\nknowledge and insights and I feel like\num if there any questions uh here uh\nwith the audience feel free so we have a\nlittle bit of time to ask questions or\nwe can also follow up on slack and see\nif there are any questions right now\nit's not I do hey Katrina have a quick\nquestion\nyeah\num yeah thanks thanks so much for your\npresentation\num I'm just curious what the\nkind of want to I like asking this uh in\ndifferent contexts but\nhow were how was like the strong typing\nopinion of flight\nreceived by you know the workflow\nauthors\nuh people who like data scientists or ml\nyeah what\nI mean I'm just curious how that's a\nreally good question it's been I won't\nsay it's not been challenging so I think\nthat it it was really a little bit\nconfusing I say really but it was a\nlittle confusing at first\num because I was pushing for this\nso strongly as one of the reasons why we\nneed to move to flight um just because\nin my previous organizations and my\nexperience we have accidentally\nintroduced so many bugs due to not\nhaving not having this right passing the\nwrong data type passing the wrong data\nframe\num these are things that are just so\neasy they can go to production so easily\nright\num but there was a there was a bit of a\nlearning curve there and I think we're\nstill learning here because you know the\ntypes of things that we can pass right\nlike you know there's some limitations\nfor example on not being able to pass\nreally large lists for example right and\nso you know having to\nyou know either change the types that\nwere passing or or instructing folks to\nlike use you know flight file instead of\npassing a list right so there are um\nstill learnings there and we're kind of\nkind of come up with a pattern for them\nto use so that you can say oh when I\nwant to do this I should do this and\nit's safe because why you know so\nawesome cool\nyeah I show them your talk on Pandora to\nkind of just sell it so\nnice\num quick question about your guys uh\nspark setup are you using uh client mode\nspark or using cluster mode spark with\nspark operator\nas we've been we're using spark operator\ncool\nand how's your experience been about\nrunning that on like self-managed\ncluster\nit's definitely been a challenge so I\nwon't say that it's not been\num a lot of overhead\num it has been\num\nboth just from\nyou know managing spark on kubernetes in\nand of itself I think is like a whole\nlike it's not necessarily related to\nflight\num\nit is it is a challenge but I think that\nit is one that is worth it\num and we're we're you know we're\nlooking into things that that can help\nus in some services that can can help us\nuh you know a lot of the savings that\nwe're seeing is also from being able to\nrun a lot of these instances on spot\ninstances so\num\nand the tuning is different right\nbecause you have to set the tuning\num in your spark job as well as in your\num in my flight manifest or in your\ntasks right so in your task you're\nrequesting specific uh memory which is a\nlittle bit different when you ask spark\nto ask for memory right so there's been\na bit of a learning curve there\nso Katrina um I really like your uh\napproach to to the migration how did you\ndecide on that approach\num well we tried to uh think about what\nmade sense for our organization and our\nsize as well as the roles and the teams\nthat we had right so a lot of that is\nbased off of how we are organized as as\na company so that you know we have an ml\nalgorithms team we have an ml platform\nteam we also have like a devops team\nthat's responsible for managing the coup\nthe kubernetes Clusters like our shared\nkubernetes clusters so\num a lot of that came from that\nteam structure so that that's sort of\nwhere we started with and sort of kind\nof went through that process to say hey\nwhat makes sense to be as least\ndisruptive to the other teams while also\nproviding value\nI mean it's fantastic and I feel like\nmany other teams are in the same\nsituation some have a different\norganization different structure they\nprobably need a different approach some\nare much more research heavy some are\nmuch more engineering heavy and and so\nit's always who's leading that\ninitiative but it would be fantastic\npotentially too to see if it can get a\nlittle bit of a write-up about your\nrecommendations about how to do\nmigrations you know maybe one or two\nrules I love you know how you structure\nthat that's really fantastic Okay cool\nso I think I just want to say thanks uh\nreally really interesting uh and amazing\npresentation between and uh obviously\nwe're gonna follow up and learn more\nabout what's happening and obviously see\nhow you are going to use mflow with\nintegration and see if you can use tune\nin ml with mlflow and so on so that's\ngoing to be a couple of questions where\nwe want to see what's happening\num so I just want to say thanks"
    },
    {
        "title": "Flyte Community Updates 028 featuring Webinar + Panel Recap, Airflow vs Flyte Cheat Sheet",
        "transcript": "hey Rahul\nhey Martin how are you I'm doing good\nhow are you today doing well thanks\nawesome we're going to start in a few\nseconds so I'm just gonna wait for a few\nmore people in here breaking in slowly\ntowards the end of the year I guess\nwhere are you located Rahul uh I'm\nlocated in Cambridge Massachusetts so\njust outside Boston\nabout it you have a lot of snow there\nah we got a little bit over the weekend\num but you know let's see it's been a\npretty mild winter so far\nawesome hey Katrina how are you\nhello doing well thank you good morning\ngood morning all righty so I'm gonna\nkick it off in a few seconds and Neil\nsays that two fantastic stuff I think we\nhave uh good morning Niels\nand uh\nuh almost everyone so let's give it\nanother minute and another 20 seconds\nand then we're gonna kick it off and I'm\ngonna introduce you later and uh you can\nstay on screen or off screen turn camera\non or off uh for right now I'll leave it\nup to you so cool\nokay let's give another 15 seconds and\nthen I kick it off\nNiels are you also based out in Seattle\nno I'm in uh Atlanta the Atlanta suburbs\nall right\nhello everyone and welcome to today's\nflight Community sink it's December 13th\nstill\n2022 my name is Mark Stein I'm your host\ntoday and we have a really interesting\nagenda so let's take a quick look at the\nagenda slide a couple of community\nupdates today we hear about a talk a\npanel study that we had last week we're\ngoing to talk about this for a few\nminutes about the outcomes this is a\nreally really interesting piece\num I'm gonna go over the release of\nUnion mlv2 we have Neil Sierra or chief\nml engineer who's going to talk about\nthe harmony release\nand after the community updates we're\ngoing to\nview two presentations one by Katrina\nPalin from stash\nand Rahul meta from theoremlp and both\nare talking about flight and how they\nuse flight and their operations daily\noperations but before we move forward a\nfew housekeeping notes number one is\nthis presentation of this talk is being\nrecorded you can look it up and find it\non YouTube later\nalso if you want to join and talk and\npresent about flight and jump on the\ncommunity go to flight.org enough\ncommunity links to find to look at you\ncan show on the slack Channel you can\nfind the ad event to see future\nCommunity settings so you're welcome to\nengage with the community now let's take\na step forward and look into the\ncommunity updates today next slide\nplease\num one of the first\num the webinars that we have done so far\nwas the webinar called\noperationalization of machine learning\nand entity studying panel discussion uh\nwith Rolando Garcia\nI thought this was a very interesting\nconversation also with the panel we had\npeople from varsha from open Planet\nPrime DJ and Mike we can see that\ncompany strive Forks Toyota\nreally you know a great panel there and\nwhat we discussed at that panel was what\nare typical pitfalls when it comes to ml\nOps one of the few\nthere were three statements in that\nstudy that Rolando Garcia put together\nand I've bought I thought I want to pull\nthem out and read them a lot because\nthey struck me and I felt like you know\npeople who actually work in envelopes\nand and do machine learning\norchestration might feel the same way\nnumber one is he wrote people spend a\nsignificant amount of time on less\nglamorous aspects of ml like maintaining\nmonitoring the data and ml pipelines\nthat's the daily life in ml engineering\none quote the second part was running to\nhave talked about the paranoia caused by\nMachine learning debugging trauma so\nthat's you know need to know what bugs\nto focus on and what comes with that\nthat seems to be a really big topic and\nlast but not least\num how to get to a high velocity which\nbasically means potentially drowning in\na sea of versions of model versions and\nso so this was overall kind of like the\nfears that people deal with but then\nthere's also on the other side kind of\nlike what are the strategies you know\nwhat are the important things and you\ncan see all of that in the slides in the\nrecordings which will be posted on the\nURL here so I thought it was very very\ninteresting we'll have a follow-up\nconversation about this and there will\nbe more webinars but thanks to Roman\nGarcia and the co-authors for putting\ntogether the study and thanks to the\npanel for speaking with me last week\nokay next slide\num airflow versus fly cheat sheet I just\nwant to call out this wonderful cheat\nsheet this is for people who come from\nairflow and are used to working with\nairflow and the command syntax and so on\nand uh samita Allah our developer\nadvocate here at Union put together this\nbeautiful cheat sheet I think it's\nfantastic\num it really paves the way for people\nusing both systems more than replacing a\nsystem I feel like this is really when\nyou go into that you already have\nworking knowledge of airflow but then\nyou want to extend this into what does\nthe mean from workflows and you want to\nuse both systems that we want to build\nyou know use the provider between them\nthis cheat sheet will get you started\nreal fast\num thanks anytime okay cool real quick\nlinks and resources that should go very\nvery fast uh next slide please\num it's just a typical reminders we have\noffice hours\num office hours\num we should be known\num to to many of the regular attendees\nthe red eyes with Eduardo and that's\njust mentioned uh Katrina as well and\nsamita so take advantage you get\nactually a really one-on-one time with\nthe core developers here and to the\ndeveloper evangelists so I would say\num it's one of the best you know you\nknow uh settings for for actually\ngetting acknowledge transfer completed\nand then\num that's the last slide is the upcoming\nthing this is that was the last thing\nthis year so we uh the flight\nengineering team developer relations we\nwish the community a happy New Year uh\nand a very successful good 2023 we'll\nsee you in January 10th\num we're all online on slack so please\nkeep engaging with us if you have any\nquestions reach out and we will be there\nwhenever you need us thank you so much\nand we'll see you in the new year\ngoodbye"
    },
    {
        "title": "Scaling Up Model Training Workflows with Flyte & Bazel (Theorem LP)",
        "transcript": "and uh I want to introduce my in my next\nuh presenter Rahul Rahul Mehta Rahul is\nwith a theorem LM and also on the ml\nteam on the infrastructure is the\ninfrastructure lead and uh we'll see uh\nRahul go over uh theorems uh you know\nimplementation and and use of light for\nfor model training workflows also in\ncontext with baseball so that's going to\nbe very interesting so I look forward to\nlearn a little bit more about this we\nhave about 20 minutes so just want to\nmake sure we can tailor towards the last\n20 20 to two minutes Rahul these the\nstage is yours\nthank you very much\num let me change my screen\num okay just a moment\nuh can everyone see my screen okay\nwonderful yeah so uh thank you very much\nMartin uh uh thanks very much for uh\nhaving both of us um so yeah my name is\nRaul metha I lead ml infrastructure at\ntheorem and yeah um like Martin\nmentioned hey I'm going to talk a little\nbit about how we use flight the theorem\nfor model training and back testing and\na number of different workflows that\nsupport our quantitative research team\nuh and in particular uh sort of maybe a\nunique feature about our code base uh\nbasically you know we've been uh using a\nmono repo with bazel for quite some time\nso uh a big part of figuring out how we\nwere going to deploy flight and\nintegrate it with our existing systems\nwas figuring out how to support it in\nthat context\num\nbut first I'll give a brief overview of\nwhat we do at theorem so theorem is a\ntechnology company that basically sits\nat the Nexus of consumer lending and\nmachine learning so uh we as a company\nhave uh permanent Capital across a\ncouple of funds that we raise from you\nknow typical kinds of investors you know\nPension funds endowments uh things like\nthat and we use those to help basically\nsit behind a number of consumer lending\nplatforms and help fund and originate\nloans for them\nso uh since 2014 we've acquired over 8\nbillion on Consumer loans on behalf of\nour platform partners that we work with\nuh and are you know continuing to deepen\nour relationships with them providing\nopportunities for lenders to get into\npotentially new asset classes and uh to\nbasically be able to improve uh on their\nunderwriting with uh the kind of the\nsophistication that our models provide\nso just to add a little bit more color\nto that uh we basically have 3.6 billion\nin assets under management across three\nsort of main funds we see you know about\n3.7 billion of annualized loan volume\nacross all of our origination Partners\nso these would be online lenders that\nare in the personal loan and auto space\num and then also separately a large part\nof our business is also uh issuing\nasset-backed Securities and also helping\nmany of our other partners perform\nsecuritizations and uh sort of like\nsimilar types of private placement deals\nso to date we've issued about just shy\nof one and a half billion of abs and\ntaking a step back here I think one of\nthe really interesting aspects is this\nhas been a sort of maybe a somewhat\nLegacy space for quite some time in the\nlast several years a couple of other\ncompanies have also emerged sort of with\na similar intent of trying to apply\nmachine learning and sort of like\nstatistics principles to basically\nunderwriting the underwriting these\ntypes of loans um obviously you know in\nprobably the more simplified case uh a\nlot of underwriting can happen\neffectively with just a couple data\npoints so think about you know your\ncredit score uh you know the number of\nopen loans that you have your debt to\nincome ratio and similar characteristics\nlike that\nhowever uh ethereum we make use of many\nmore data points we can take in both a\nlot of data points from credit bureaus\nas well as other alternative data\nsources and you know really\nincorporating these into our models and\nimproving the quality of underwriting\nand expanding sort of the set of\nindividuals that we can actually\nconsider to be credit Worthy is one of\nthe fundamental goals of the firm\nso uh now I would love to dive into some\nof the specifics about model training at\ntheorem so across uh sort of the\ndifferent asset classes and lenders that\nwe work with we have uh you know about\nthree sort of core model architectures\nfor predicting various features of uh\nloan performance\num we have six domain specific types of\nmodels to model various facets of loans\nso for example uh you know the cash\nflows that will come in after a loan has\nbeen originated and someone is making a\npayment\num other aspects around that\num we have 30 distinct variants of our\ncore models some of them for specific\nlending partners that we work with and\nothers in you know other domain specific\nuse cases and I'll come back to this\nbecause this is one of probably one of\nthe most interesting points of\ncomplexity uh that we've encountered\nwith flight\num basically where we have a number of\nvery similar model architectures and\ntasks and workflows that we would like\nto use but training them on vastly\ndifferent data scales and that's been a\npretty interesting interesting Challenge\nand just to sort of underscore some of\nthe you know data scale challenges that\nwe also hit up against uh you know we\nhave 40 million loan records in our data\nwarehouse each and you know across all\nof them over 50 billion historical data\npoints on performance and credit\nattributes from borrowers uh that we use\nfrom our model training\nto maybe add a little bit more color\nalso to the sort of scale that we need\nto be able to reckon with and manage as\nan ml infrastructure team uh we have\nover 60 daily you know scheduled model\ntraining and back testing runs uh that\nare sort of like on a recurring schedule\nthat incorporate the latest changes from\nour research team and you know the model\ntraining and deployment process isn't it\ndoesn't stop at theorem with the\nclassifier or the regressor that we're\nbasically uh building out for a\nparticular use case it also includes\nsort of the wrapping of all the business\nlogic around that uh in order to\nactually turn the predictions from a\nmodel into an underwriting decision and\napproval or a decline with a potential\nexplanation of why the decline occurred\nso in order to keep all those changes in\nsync continue uh to ensure that they're\nmost up-to-date in our production\nenvironments and facing our platform\nPartners uh that's sort of what leads to\nthe very high Cadence of model training\nand you know frankly quite a large load\non our model training system we all also\nsupport our quantitative research team\nwho as part of their own experimentation\nwill be running you know over 30 to 40\nsort of full back tests uh on the loan\nlevel with one level history dating back\nto 2008 to validate various model\nchanges uh different experiments uh\ntrying to ensure you know that models\nhave certain characteristics obviously\nthis sort of iteration has become\ncrucial given current market\ndislocations and where the with the\ninterest rate environment where it is\ntoday uh supporting quick iteration for\nour research team in order to try a\nmultitude of different ideas and be able\nto ship them safely into production is\none of the things that we believe is\nextraordinarily important for theorem at\nany time but especially uh in today's\nmacro environment\num and just to add a little bit more\ncolor to that uh from the orchestration\nside uh some of our back tests uh you\nknow that have a fan out going back you\nknow thinking about a quarterly back\ntest from 2008 to 2022 for some of our\noldest data assets these these workflows\nwind up having somewhere between nine\nthousand nine hundred and one thousand\nunique stages AKA unique pods that need\nto run as part of our workflow execution\nso\num scaling this up has been a very\ninteresting Challenge and um on the next\nslide I will briefly go over sort of the\nhow model training at theorem has\nevolved over the years\num especially given as our data asset\nhas increased hopefully this will add\nsome color and uh Katrina I was really\ninterested that you also mentioned that\nyou had included kubeflow in your\nevaluation of different model training\nsystems and uh We've definitely also had\nsome interesting experiences with\nkubeflow and other orchestration tools\nso I would love to go over those briefly\nso originally you know um and I think\nwhat I didn't include on here is prior\nto 2018 we had a sort of very Legacy\nairflow-based model training system but\num the first system that you know uh I\nespecially encountered even some of like\nsome of the remnants of uh when I first\njoined joined theorem was our Legacy\nArgo workflows Based training system\nso we basically would think you know um\npreviously to the like prior to this uh\na researcher would basically request a\nvery large ec2 instance would upload\ntheir model training script run it and\nuh wait for the you know job to finish\nand then save a pickle or some binary of\nthe model to S3 for future use along\nwith some validation metrics\num when we encountered scaling issues\nwith that um the very the first\ncandidate here was basically packaging\nthat model training script in a single\nstage of an Argo workflow and this was\nuh this had some very appealing\ncharacteristics from the side of it\nbeing self-served for our research team\nso I think you know like others have\nalluded to the two things that I think\nyou kind of always hold in tension with\nML orchestration is the scalability and\nyou know stability and security and all\nthe other you know concerns that are\nprimarily held by the ml infrastructure\nand the rest of the engineering team and\nseparately the other aspect that it's\nyou know it's kind of tough to hold\nintention is uh ensuring that that\nsystem is self-serve for research and um\nyou know flight has obviously helped us\na lot with that but in the Legacy auto\ntraining world uh researchers would\nsimply you know they wouldn't have to\ntouch any of the orchestration or the\ninfrastructure pieces you would simply\num basically Implement a particular\ninterface uh Implement a particular\nabstract based class and Implement a\ntrain and predict method and once that\nwas the case so long as your model\nconforms to this particular interface we\ncan package it up in an Argo workflow\nand run it in the cluster\nhowever this was extraordinarily\ninefficient\num and basically it was replicating the\nsame sort of deal that we that basically\nwhen researchers were spinning up their\nindividual ec2 instances the only\ndifference now is researchers weren't\ntied to a particular ec2 instance but\nthe cluster Auto scaler would wind up\nrequesting a similarly you know large\ninstance you know like an R5 16x larger\nkind of what happened\nand one of the issues uh you know that\nwe really began to run into is that in\nour model training pipelines we have\ncertain stages which are extraordinarily\nmemory intensive but then we also have\nother stages like training for example\nlike ubm model that are compute\nintensive and of course there are\ndifferent appropriate instance types\ndifferent kinds of resource profiles you\nwould like to run those different\nworkloads on but the you know sort of a\ntough aspect of the Argo training system\nis we needed to request both a high\nmemory and a icpu node and this will end\nup becoming quite costly and also\nslowing down research iteration\nso next\num the obvious choice then is to\nbasically break up that monolithic\ntraining process into multiple stages\nand be able to orchestrate them as a dag\nand be able to basically schedule the\ndifferent types of workloads on the\nappropriate instances so\num sort of in 2020 2021 we started\nmoving towards kubeflow\num and uh at that time you know it was\nsort of one of the sort of one of the\nbest options that we found it was before\nmy time at theorem but I think you know\nthe main goals basically here sort of\nstill stand uh so using the kubeflow DSL\nuh you know it's possible to express\ndifferent stages of our training\npipeline you know uh in different pods\nand have different resource profiles\nattached to it however\num I think maybe like others on the call\nhave seen with kubeflow we you know\nfound that the kubeflow DSL is\nextraordinarily cumbersome to work with\nuh most importantly it isn't self-server\nresearch to understand especially in\nsort of the 1.0 world of Kubla where you\nstill need to also Define a\ncorresponding yaml specification for\nyour component\nand also we encountered a ton of scaling\nproblems with kubeflow especially in\nthese really large back tests so uh\nearlier this year a couple months ago so\nmaybe about April we started\ninvestigating flight when we hit some of\nour real you know sort of big kind of\nissues uh with kubeflow that were\nblocking kind of mission critical model\ntraining and back testing jobs for\ntheorem and we've started investigating\nflight as an alternative and obviously\nyou know as others have said uh\nworkflows and tasks are simple python\nfunctions far less you know overhead and\nboilerplate I think I've mentioned maybe\nsome of the other other\num Union uh Union folks uh that we were\nable to reduce the amount of boilerplate\nby something along the lines of 85 to 90\nfor many of our workflows uh by simply\nmigrating them to flight\num some of the other pieces that I'll\nmaybe touch on right at the end is that\nmodifying resource requests between\nlaunch plans uh going back to the kind\nof\naspect of our training system that I\ndescribed that we have a couple core\nmodel architectures but we train them on\nvery wildly different data scales uh\nit's very difficult to to do this\nwithout basically relying almost\nexclusively on dynamic workflows in\nFlight kit and which I think uh also\nleaves a little bit to be desired for us\nin terms of uh the type checking sort of\nat build time as opposed to at runtime\nbut we'll Circle back to that later\num so yeah uh I briefly touched on this\nuh also being cognizant of time I might\njump over this one but basically uh in\nmigrating from Coupe voter flight we've\nseen uh you know a ton of benefits I\nthink basically uh all the other aspects\nthat we've already talked about\nincluding componentizing the training\nprocess authoring simpler tasks and\nworkflows but more importantly that\nmulti-tenancy and security are really\nfirst class Concepts in Flight\num I'm also you know certainly not least\nat all\num the fact that flight is actively\nsupported and the team is willing to\naccept Upstream contributions is one of\nthe like biggest aspects for us in\nmoving towards flight\nin our initial deployment here we've\nencountered some issues with the UI or\nuh want to register a particular type of\ntype Transformer and change the way that\nthe compiler actually works and the team\nhas been extraordinarily receptive to\naccepting those Upstream contributions\nand that's something that uh you know we\nreally feel that confident in making\nthis investment long term uh especially\nbecause of how active the community is\nso just to also you know add a little\nbit more to this\num one of the other very big aspects of\nflight for us is something that notably\nis not supported in kubeflow is the\nability to actually do some workflows or\nproper workflow of workflow support so\nuh for our model training and back\ntesting pipelines\num obviously the ability to have code\nreuses is fantastic also it means that\nfor our researchers so long as we have\nuh sort of like a skeleton back testing\npipeline that accepts a training sub\nworkflow with a particular interface\nthat they can Implement\num you know it's far easier to get a\nback test stood up if they just simply\nwrite a couple of different stages in\nthe training pipeline that they need for\nany new model that they're iterating\nwith\nso\num to close off uh that section you know\nthat's our basic overview of our usage\nof flight\num but now wanted to talk through sort\nof a unique aspect of our system at\ntheorem which is you know we're in a\npython mono repo across our entire\norganization and that spans the data\nengineering team ml infrastructure\nresearch as well as our API and model\nserving sort of layer um and all of this\nis in the same mono rebound we use bazel\nto manage this so um a brief overview\nfor folks who haven't seen bazel before\nit's a build tool with similar goals\ntools like make Maven and cradle um it\nallows you to declare targets and build\nfiles um in a high level build language\ncalled uh starlark which is basically a\nderivative of python um but most\nimportantly is it maintains the state of\nthe build dag and enables fast and\ncorrect caching for complex builds it\nalso has support for multiple languages\nand tools so python targets container\nimages kubernetes resources and things\nlike that the the third point is sort of\nthe most important one here given that\nthe fast and correct caching enables you\nknow over the course of development\nwhen a user makes a change or touches a\nparticular file we only will rerun tests\nlinters and rebuild container images\nthings like that for only the affected\ntargets uh sort of in the kind of\ntransitive closure of of that particular\nuh you know entity that you're working\nwith so especially in mono repos rather\nthan needing to rerun your whole world\nevery time and it's slowing down both\nlocal Dev experience and CI we're\nactually able to rely pretty heavily on\nbazel's caching in order to achieve some\nof these goals and improve the developer\nexperience with the size of our existing\nmono repo\num basically so also if you're not\nfamiliar with bazel this is sort of what\na similar but basically what an example\npython Library Target might look like so\nsuppose we have a utility module called\nCash Flow tools\num and uh basically you can specify the\nsources for it any of the dependencies\nyou know other Utilities in the same\npackage or in your same repo or external\nlike pip requirements like pandas numpy\nor scipy similarly you can declare tests\nand the pi test wrapper will\nautomatically run it with pi test\num and also we'll basically\nautomatically set all of your Winters\nand formatters and stuff up for every\nTarget\nso now that we have gone through bazel a\nlittle bit uh coming back to our\nintegration with flight uh a core aspect\nof this is that since our entire system\nis declared in this model repo we needed\nto also provide an integration for\nflight to make it seamless to use uh in\nour existing setup so uh we authored a\nnew set of bazel rules called rules\nflight um so in our build file we're\neffectively able to declare a flight\nlibrary for a particular model\num we we can include a particular\nworkflow and include launch plans across\nmultiple files as well as specify all of\nthe dependencies\nso um importantly you know these are\nreally simple to author for people who\nhave already been in our system\num and also it forces you in the\ndevelopment experience to be very\nthoughtful about the explicit\ndependencies that you're encoding here\num obviously this may seem like this is\na little bit of overhead from the\nperspective of uh researchers but uh\nthat said we have some tooling that can\nhelp automatically infer the\ndependencies and help basically fix up\nthis build file even if you aren't\nmanually encoding all of it\nthe important part here that I want to\nunderscore is that we can use the same\nbuilt-in targets to build our image and\nregister workflows in both\nexperimentation and in CD pipelines so\nthe fact that this is one to one is\nreally good for reproducibility and just\nkind of in general having you know a\nsense of Sanity across both our like\nexperimentation environment as well as\nwhen we're deploying production training\nand back testing workflows\nthe final piece that I wanted to touch\non here is we have also found made some\nreally like interesting improvements to\nhow we actually build the container\nimage for flight uh workflows\num using a feature in bazel basically\ntheir Docker build rules called fine\ngrain layering we're able to basically\nensure that our dependencies uh that we\ncan basically construct explicit layers\nin our container images based on the\ndifferent sections of the flight Library\nhere\num and based on bazel's caching using\nthis fine layering we're able to get the\nincremental build time for workflow and\nlaunch plan changes to basically under a\nminute and what this enables us to do is\nproperly rebuild the image every time\nand push it which in our mind gives us\nimproved safety and reproducibility over\nfast registration and also just means\nthat we have one fewer uh like we have\none less distinct process to support and\nthere isn't a bifurcation between\nresearchers iterating on model changes\nand actually the same way that we build\nand deploy them in production\nso I'm happy to touch on that uh if any\nfolks have questions at the end but uh\nsome of the you know basic things that\nyou know where do we kind of go from\nhere we have some ux we'd love to make\nsome ux improvements like making it\neasier to identify users and tagging in\nthe UI uh also like I alluded to\nbasically uh being able to support\nresource overrides outside Dynamic\nworkflows given those uh particular kind\nof constraints around one or two model\narchitectures but varying very like\nwildly differing data scales for the\nlike different you know lending lending\npartners that we will train models for\num also improving Integrations with my\npi and other build tools uh as well as\nbeing able to actually more deeply\nintegrate flight workflows and launch\nclients with our get Ops infra so\noverall though we are really thrilled\nwith how uh flight has panned out at\ntheorem uh on a go forward basis we are\nimplementing all new model training\npipelines in flight and the migration to\nlike backfill the existing models that\nwe have in kubeflow is ongoing but we're\nhoping to have our full cutover achieved\nby basically q1 next year and uh then\nkind of turn it over to research and\nkind of let them have at it so thank you\nvery much for the time um hopefully we\nhave maybe time for one or two questions\nbut yeah um turn it back to you Martin\nyeah absolutely hey Rahul this was\nfascinating uh amazing love uh you know\nhow he actually accelerated the build\ntime of the images with bazel and so on\nand be super Nimble so let's see if we\nhave one or two questions here in the\naudience maybe uh Neil So or somebody\nelse\nuh yeah uh can you hear me okay yep cool\nyeah just a quick question on the the\noverrides\num story is there I I\nknow there's the with overrides like\ntask method yeah what what would make\nlike what's the ux that is ideal for\nyour team so one of the few features\nthat we actually did like about kubeflow\nis the concept of an OP Transformer that\nyou can register with the kubeflow\ncompiler so the idea is that you know I\nhave a coupon workflow like I can\ncompile it uh compile my workflow\nfunction and then there's a\npost-processing function you can Define\nthat basically will just provide you a\nlist and it'll basically map over all of\nthe tasks there and you can apply an\narbitrary transform to it so the way we\ndid it is we included a particular\nannotation in that when we would see\nthat annotation you would basically swap\nit with a resource profile configuration\nfor a given data set now obviously you\nknow the other approach here which we're\nalso considering is uh making our tasks\nbe able to kind of horizontally scale\nthinking about you know building deeper\nIntegrations with tools like spark or\ndesk for various stages but\num this one feature I think is would be\npretty important for us and something\nI've spoken with other folks like Kate\nthen and Eduardo on the team\num we would love to be able to get\naround to either helping spec out an RFC\nor helping make this a reality but um\nyou know something akin to kubeflow's op\nTransformers would really be sort of\nlike the the high water mark of the ux\nwe'd kind of be looking for here\nawesome yeah let's uh follow up um\noffline about that that'd be really cool\nto support\nokay do we have um another question\nprobably well we can follow up offline\nas well in the slack Channel\num Rahul I think uh EU numbers about the\nboilerplate uh code that you saved from\nmoving from Google to to flight already\nlegendary here uh within the engineers\non our side so I heard about the savings\nalready a long time ago it's fantastic\nso what would be potentially wonderful\nis you know follow your journey and see\nyou know where it takes you and and have\nyou back potentially in q1 Q2 next year\num also what I would love to potentially\nfollow up with you is maybe we can\nactually cover and capture your your\nbasal build system a little bit more\nmaybe a Blog course or something to that\nextent and can you share this with the\ncommunity that might be very interesting\nbut I just want to say on behalf of the\ncommunity thank you so much the team is\nsuper excited also Caitlin eduaro here\non the core fly team are always super\nexcited I want to speak with you and\nthen come back with feedback so we love\nworking with you and we look forward to\nyou know any next steps Journey thank\nyou so much\nthank you for having me"
    },
    {
        "title": "Flyte at Pachama",
        "transcript": "okay\num let's\nmove forward we have a presentation\ntoday I'm going to uh you know introduce\npajama uh they the team from pajama\nwhich actually started with uh Mariah\nwith pajama of a letter introduce\nherself and we also have Bernard\nstadelbauer uh there\num I think the pajama team has a really\ncompelling story to tell about what\npajama is\num the the background what they're doing\nand how they ended up picking a\norchestrator for their tasks and so\nwe're gonna see uh how they have rolled\nit out what they have learned and if\nthey're not inside any recommended\nrecommendations they can give to others\nso with that I hand over to ISU Marie or\ndo you want me to drive the presentation\nuh yeah if you can hand it over that'd\nbe great\nlet me just stop sharing here\nand all right\num great yeah so I guess uh we'll be\nchatting today about a flight at pajama\num and uh overview of what we'll be\ntalking about is\num or I guess uh I should probably\nintroduce myself first uh my name is\nMarie uh I'm a software engineering\nmanager at pachama on the protocol\nplatform team so we work a lot with the\nthe scientists and researchers and\nremote sensing type of Worlds\num and Bernard\nuh hello my name is Bernard\num I'm a data engineer with working with\nMario and we are mostly responsible or\nfeeling responsible for putting records\nonto flight and scaling out a bit\nawesome\num\nso\njumping right into the presentation uh\nwe'll we'll talk a little bit about uh\nwhat is pachama and how it relates to\nForest carbon credits and then also what\nour forest carbon credits because\nthey're a little bit Niche uh and then\nbusiness requirements that have kind of\nshaped our decisions around using flight\num how we use flight why we love it and\nthen finally kind of a development tip\naround registration\num so the the core of pachama's mission\nis to remove carbon from the atmosphere\nand to restore nature\nand that's a big lofty goal so so how do\nwe get there as a business\num and our answer for that is uh Forest\ncarbon credits so the central premise\nbehind carbon credits is that our\nLandscapes are shaped by market forces\nand often those reflect the short-term\nvalue that can be extracted from the\nland so you see on the left kind of the\naerial view but on the right as well a\nforest that was chopped down before\ncattle grazing but if we properly\nquantify the economic value that we got\nbehind that ecological Services uh\nprovided by the existing landscape we'd\nalso see a much larger value and that's\neven ignoring kind of the inherent\nnon-economic value of the the life on\nthe land and depending on the land as\nwell so that's kind of the idea behind\ncarbon credits is find a financial\ninstrument to compensate landowners for\nleaving Forest intact in the most\ntangible economic value we get from is\nis carbon capture and storage especially\nbecause climate change is the biggest\nall-encompassing ecological crisis\nfacing all of us so from this we get the\nidea of the carbon credit\num and uh let's see\nhere are a couple of pictures from from\none of the projects that we work with\num kind of showing this is inside the\nproject land the the mature Forest as it\nis versus right next door where land is\nbeing converted for soy and Cattle\nso the the success of kind of the carbon\ncredit premise depends on the Integrity\nof the measurement and we need to be\nable to really robustly quantify the\ncarbon value in order to sell it as an\nasset because that's how we're going to\nrealize the environment benefit uh for\nthe people purchasing the credits so\nother otherwise it becomes a place where\nBad actors can succeed\num and so when we don't properly\nquantify the impact measured in carbon\nthen it can even have a net negative\nimpact on co2 equivalence in the\natmosphere due to companies relying on\nthe stated accuracy of a carbon credit\npromise\nand so we\ndefinitely want to avoid that as much as\nwe can so uh existing Forest carbon\noffsets do a best effort at this problem\nbut most of the existing approaches were\noriginally formed under the ipcc\nguidelines from the early 2000s so\ntechnology has advanced quite a bit\nsince then we have more powerful tools\nto face this problem including new\nscientific data new remote sensing data\nflight and you know access to massive\namounts of compute as well as machine\nlearning so kind of in a sentence\npachama is working to harness the power\nof remote sensing and machine learning\nto increase transparency quality\nscalability in the carbon Market\num\nso we have kind of these two uh buckets\nfocusing on technology-based assessment\nof the quality of carbon credits and a\ntransparent Online Marketplace So within\nthe first bucket of tech-based\nAssessments we work on things like an\nalgorithmically generated impartial\nBaseline off of which to calculate the\ncrediting numbers\ndeforestation monitoring and scaling\nhand collected biomass measurements to\nbroader areas in the second big bucket\nteams work on easy and secure purchases\nand showcasing quality projects and\ntoday we'll mostly be talking about the\ntech-based assessments side of the\nbusiness since that's the area that\nBernard and I work on and also where we\nuse flight so the two main problems that\nwe work on through our flight workflows\nare biomass modeling and the Baseline\ncalculation which are two key components\nthat go into calculating a carbon credit\nuh so what is a carbon credit uh at\npachama we're trying to simplify the\ncalculation of a carbon credit uh and\nuse algorithms with transparent and\nreasonable defaults and remote sensing\nall the way through\num so at its core a carbon credit should\nbe a measurement of the change in carbon\nfrom some intervention like establishing\na carbon credit project versus the\nchange in carbon that would have\nhappened without the project being\nestablished so if it's a restoration\nproject we estimate how much would have\nnaturally regrown versus how much has\ngrown with maybe active planting or\nthings like that and with a conservation\nproject we try to estimate how much\ncarbon is still intact versus how much\nwould have been taken away under the\nstatus quo so trying to figure out the\nwhat would have happened is the very\ntricky part and the part that often\ncomes under the most scrutiny we're\ntrying to prove a counter factual which\nis very difficult but that also doesn't\nmean that it's impossible because we can\ncan use math to find good answers and a\ngood like example of other fields\num that that do this because scientists\ndon't always get to run a controlled\nexperiment to make good conclusions uh\nso so the other example of where we see\nthis in science is in epidemiological\nstudies because we can't intentionally\ngo out and infect people with a disease\nor expose them to hazards to try to\nlearn about the impact that it has on\nthe population so those are examples of\nof places where we might use a similar\ntype of like logic and math to to come\nto conclusions about what would have\nhappened\num so\nwhen we try to answer the what if bit\num a good control area is very important\num most of our current research is\num for for the Baseline is going into\nchoosing a control area for conservation\nor default avoided deforestation\nscenarios and so what our scientists and\nmodelers are trying to do is to find\nareas of land that are at a similar risk\nof deforestation as the land before the\nproject gets established so the Baseline\nteam Aggregates a number of features\nabout the land that would influence its\nrisk factor for deforestation and then\nruns an algorithm to pick the Project's\ncontrol area so in that way we get\nhere's like a reasonable estimate of\nwhat would have happened and we can run\nthis over many many times to also\ncalculate the the error in that estimate\num the next input is the change in\ncarbon and for this we want to look at\nForest cover gain and loss primarily for\ndeforestation we'll look at Forest cover\nloss and assume that carbon goes to zero\nif the area gets deforested and we can\nsee this with satellite imagery\nincluding changes over time so on the\nleft is a quick video of of again a real\nproject\nthat shows the project area versus the\nsurrounding areas that are getting\ndeforested for cattle soy in this case\nagain and then on the right we see kind\nof\nuh looking for those changes over time\nwith satellite imagery\num and the the last input is uh carbon\nsince we're checking the change in\ncarbon and we're using remote sensing\ninputs and machine learning to create\ncarbon maps for for this bin\num so to do all of this and meet our\ngoals about quality transparency\nscalability there are few high-level\nbusiness requirements when we're looking\nfor our right compute tools and so a\ncouple of things that\nshaped our decisions were accessed to\nlots of compute large-scale data ingest\nand processing and then robust lineage\nand robust lineage is in particularly\nimportant for us because part of what\nwe're trying to bring is transparency\nand robust lineage is already important\nfor just kind of experiment tracking and\nall that but but in addition to that we\nalso need to be conscious about uh when\nwe might get reviewed by Third parties\nhaving all the available data to be able\nto go back in time and say this is how\nwe got this number\num\nanother Factor kind of influencing our\ndecision is our team structure and\ncollaboration so we have a couple of\ndifferent research teams\num at pajama and then one kind of core\nplatform engineering team\num and so a lot of those research teams\nhave similar needs and often share the\nsame data sources and so we wanted to be\nable to build shared components across\nthe team for data data ETL and other\nbits while still allowing for direct\nownership of the workflows by our remote\nsensing scientists and our machine\nlearning modelers\num and Bernard I'll pass it over to you\ncool\nthank you let me see if I can get my\nscreen shared here\num\ncan you see\nmy screen okay thank you very much\num\nI'm doing a\nso hi my name is I think we had that\nbefore\num\nI'm doing a bit more of uh the technical\npart\num and I thought I would also share some\nof the\nthings we will wait Marie could it be\nthat you missed the slide\nuh yeah sorry I think I skipped that one\nso we could go okay okay go for it for\ntime\num I do a bit more of a uh\nin-depth uh Tech bit and also a small\ndemo of something that I think we built\nis that is really cool maybe it's\nhelpful to someone watching this and\nmaybe if they're starting off a fight\num that that's something they could use\nalso if someone has a better suggestion\non how to do registering or whatever I'm\nhappy to discuss this afterwards so this\nis how we do it we quite like it but\nthere might be better options\num and then in the end I do have a small\nsection on what we had doubles with with\nflight what we struggled with end up\nmaybe also as a feedback and\nconversation topic to to talk about uh\nfirst off what we like about flight I\nthink the biggest thing is the community\num thank you so much I've worked with\nflight for uh it's been almost two years\nat least one and a half and we've gotten\na lot back we've gotten so much support\num\nI was supposed to say that the core the\nsuper propeller that scheduling work it\njust works um it is\nI think I've seen one or two failures\nmaybe throughout the one and a half\nyears but there's it's really stable\num and most of the issues that we had\nwere mostly we talk about this about ux\nand small little bits that are not\naffecting the actual compute so so\nthat's that's really amazing and thank\nyou for for everybody who helped us\nthere\num and that like the turnaround cycle on\nthe is also really good on uh getting\nfeedback so just like community and\neverything\num I think probably the second biggest\nthing that\nI would put there is is caching so just\nthe reducing the duration cycle by by\nnot having to really work uh is\nsomething that I found really really\nvaluable it also makes debugging really\neasy because if one task fails you\nusually it's\nalmost a one-to-one mapping to the\num\nto your local workflow and so stepping\ninto a debugger is mostly downloading\nthe data for us it's sometimes quite\nlarge but then yeah you can debug\nlocally which which really helps and the\nthird bit is what Mario already touched\non is we get a lot of reusable workflows\nand it makes it fairly easy to share\ncomplex machine learning and different\ndependencies between teams without\nactually having to put all the\ndependencies into one one container so\nif someone wants to use torch they can\nuse storage if someone sees tensorflow\nwe don't encourage different Frameworks\nbut\num we do at least uh leave us the the\npossibility also we do work a lot with\ndusk so we we're working on a small task\nintegration we do have something\nin-house but we'd also like to share\nthat out\num big because task\ntries to become the the standard in tube\nspatial processing with the NGO project\nand there's a lot of work going on there\nand they take there's there's great work\num that we try to leverage leverage from\nthem\nthe interactive bit is I thought I would\nshare How We Do registering for\nWorkforce\num\nand we rely a lot on cicd so\nI think and I might be wrong there that\nthis is not very clear in the the flight\ntalks\nI'm not quite sure though\num but I thought I'd share it out so\nwhat we do is when this is a gig graph\non each commit to domain we\nautomatically on CI register to\nproduction uh we use some semantic\nversioning for this because that's how\nmost of our projects work\num I think that's that's quite standard\nwe do testing on on Main so we do\ncontinuous deployment and goes to\nproduction on each feature Branch we\nautomatically deploy to development soil\nState iteration cyclists is quite quick\nthere and you have a version that you\ncan refer to if you share it out with\ncolleagues and probably the\nsmall specialized case that we have is\nwe really like Fast registration\nand the biggest issue that we've seen\nthere is picking out the right Docker\nimage because you would like to have a\nDocker image that is as similar as you\ncan to your current code right but you\nyou don't want to build the document to\npush it because that's the benefit and\nso we wrote ourselves a small script\num I guess we could also share that if\nsomeone wants that at some point\num basically what we do is\nsay we are on a feature Branch we've\npushed somewhere and then we added two\nmore commits which are not on not pushed\nyet so there's no image in case the\ndependencies haven't changed or the\ndocument hasn't changed uh what we do is\nwe actually Traverse the git graph and\nthen check against the registry until we\nfind the first image and as soon as we\nfind the first image that's the image\nwe're gonna we're gonna use for\nregistering so we do we do have some\nsmall make file magic that actually\nmakes sure to pick the closest image to\nyou which has proven to be quite quite\nwell valuable\num\nthat's that we do the same um we even go\nback to main and whatever yeah uh yeah\nokay then\nthis is something that I've been wanting\nto do I just didn't know and it's\namazing can you\nplease somehow contribute this as a blog\nor a dog because I think uh fast\nregistration's biggest problem is where\nit breaks certain assumptions in Flight\nuh like flight relies on reproducibility\nand guarantees like you know that the\nenvironment is kind of hermetic but\nfaster decision is a way to like break\nall of those promises I'm like here's\nsome code here's a random image and so\nif you can like walking\nthe registry I don't know if everybody\nmay have access to that but like if you\ncan share how you do that\num it would be fantastic uh maybe there\nis a there is a nicer abstraction\npotentially at the community we can\nbuild\nso that we always always\nyou know prevent any kind of bugs from\nhappening because this this does cause a\nlot of bugs uh it can if you don't know\nwhat you're doing right so no but thank\nyou this is amazing it's amazing you\nkind of like stole the idea from my head\nand put it out there so thank you\nforeign\n[Music]\nbut yeah we manage most of our projects\nthrough make files we also do have a\ncookie cutter and a craft template that\nautomatically we found that we need\nquite a bit of boilerplate or like a bit\nof power play to uh for all of our\nprojects and so we we use cookie cutter\nto do that and getting craft to\nDownstream updates\num so that that's something we've also\nseen that was was quite quite nice\num real quick question I'm curious after\nyou Traverse how do you ensure that like\nthe docker file or no like requirements\nfiles have changed like how do you how\ndo you make sure that the image that\nwould build on this current Branch would\nbe the same as uh we don't so it is not\nwe don't ensure that um we probably\ncould even\nI think we could yeah because I think we\ncould but um it has prevented let's say\n95 of the issues that people were seeing\nalready\num of of that and the rest was\ncommunication so as we tried to\ncommunicate out okay when you change\nyour\num dependencies you want to push at\nleast once uh but that being a quite\nquite the rare event\num\nthat was it for for the the small\npractical bit that we wanted to include\num\nmaybe common issues that we were seeing\nand these are focusing on the developer\nside so this is now on the deployment\nside or anything else so this is mostly\nwhen machine learning Engineers data\nEngineers come to us as a platform team\nuh what they are struggling with and\noverall they're super happy because they\ncan now do for example we do grid\nsearches on on different things uh and\nthat fight makes that very easy\num probably the biggest thing that\npeople struggle with or that I've seen\nin the last four and a half years is\ninput and output serialization we try\nnot to use pickle because that has the\ntypical pickle problems I think you\nmentioned that on on your docs page as\nwell\num but then what the kind of it's very\neasy if you only have simple types and\nif your signature is fairly simple but\nsometimes we would like to do\nmore complex configuration and then\nthat has has caused some confusion when\nto use a data class when to use the name\ntupu when uh what's serializable why is\nthe first output called o0 and then when\nyou're named it there's a few things\nthat that\num we've seen\num we also try to communicate on what's\npossible\num\nthe second bit that comes kind of hand\nin hand with this is when you have\ncomplex types the ux is sometimes pretty\nhard to use\nthere's and this is continuously\nimproving we've recently updated our\nconsole so so it's gotten a lot better\nbut then people wanted to use arbitrary\ntickets and people have crazy ideas\nlet's put it that way and then sometimes\nit's uh we need to find\nwork on ways around it\num\nin general though we are a fan of very\nsimple interfaces and and making them\nserializable so that's definitely\nsomething we are supporting\num one thing we've also seen is\nquite often or sometimes we have the use\ncase where we would like to have a map\ntask but for a sub workflow and what we\nthen usually do is a dynamic task that\nthat starts the sub workflows but when\nthat spans out quite far it gets really\nhard to track in the ux of what has\nhappened\num I think the graph view has improved\ndramatically that that is amazing\num we use that now but but it sometimes\njust says tricky to see how many have\ncompleted how that yeah we've built\ncustom tooling around that but that\nthat's maybe a feedback\num\nyeah and sometimes we we have struggled\na bit with with errors or um things when\nit says please contact your system\nadministrator this is a very small\num this hasn't happened very often but\nevery once in a while uh the the log\nmessage is sort of the errors weren't\nperfect but all in all we're super happy\nwith with what's happening and this\nshould be more constructive than the\nmain thing thank you very much for what\nyou've built I think this is also my\nlast last slide I think so yes\nfantastic that's so interesting okay so\nlet's do a quick round of q a so I do\nhave a few questions but I want to ask\nthe audience uh is there any other\nquestions case then you always have a\nquestion let me go first\nI always have a question so I will let\nother people go first then I will go\nnext\nother people are shy so I'm gonna go\ntake the stage\num so uh\nfirstly those feedback we like so ux is\nsomething that is very hard to do\nwithout users constantly giving feedback\nright like it's like this\nyes we would love to be apple we are not\nwe are not able to make the perfect ux\nfor everybody so please you know if you\nhave ideas like this these are highly\nencouraged\nyou know throw them in an issue or like\njump into the channel and and share draw\na Mark or something that you would like\nand it would be amazing for us to know\nbecause you know and I don't even I\nthink like uh some of the people who\nhave been working on the UI side and\nfeel free to contribute but in my team\npeople who are working in the USA have\nnever even seen the extremely\ncomplicated graphs right so they may not\nknow how to really represent them so as\na feedback to you on your feedback if\nplease help us improve it\num the the second aspect is so you you\nmentioned\num scales how what kind of a scale are\nyou running at how many workflow that\nhelps like just yesterday one new user\nhas been asking like another scale and\nwe've been we kind of say that flight is\nextremely reliable and I almost hear\nthis from everybody nobody's ever\nwritten a blog or anything that says\nlike okay let's just scale it and it\njust works we would love that but in the\nabsence of that we would love to know\nwhat's your scale whatever you can share\nit'd be really helpful\n[Music]\num\nshould I take it married to do you want\nto uh I I can do like one quick message\nor mention about the Baseline and then\nI'll hand it to you yeah\num\nyeah sorry we meant to say this in the\npresentation too but for for one of our\nlike Baseline uh runs because that's\nwhere there's a lot of light usage right\nnow\num as an example for each project that\nwe evaluate\num we might have hundreds of kind of kid\nand pixel matching runs\num and hundreds to to be able to\nestimate uncertainty\num so so that's like over you know kind\nof millions of kilometers squared\num doing the pixel matching\num we will have uh tens of features per\nyear and then tens of years\num and then on top of that too when we\nwant to kind of like tune our parameters\num for for that Baseline we would also\ndo hundreds of those hundreds of those\nyou know millions of hectares or\nkilometers\num and then yeah Bernard\nyeah I think that that sums it up quite\nwell some of these also start Task\nclusters that are 50 nodes I don't know\n50 pods in uh\njust within them and so like things can\ngo crazy really quickly\num but but that all works out really\nwell and I've seen it work at even\ncrazier scales um before so I think\nwe're not at the limit yet that I don't\nknow\num yeah and this is also a single\ncluster sorry go ahead I'm very yeah\nsingle cluster\nforeign\nlimit or anything\num we do know that we want to start work\nuh launch plans instead of sub workflows\nand so so we we have looked into that a\nlittle\num but\nso far flight is not our bottleneck by\nany means yeah I was going to say not\nnot to to hate on other platforms but\nbut I do think dusk consistently uh is\nthe area where we see like scale\nbreaking and it's really refreshing to\njust come over our flight workflows and\nbe like oh you're still working and\nyou're fine\nforeign\nwith regards to Marie you mentioned it\nat the beginning that and I think uh\nBrown too that the team aspect played a\nreally important aspect Reaching Across\ndepartments from the researchers to data\nscientists to the machine learning\nengineers and Engineers so can you walk\nus through the discovery process I mean\nwho was involved how did you make the\ndecision two years ago to to pick flight\nand what were your alternatives\nyeah I I can start with a little bit of\nthe history and then I'll I'll pass it\nover to Bernard who did a lot of the the\nactual kind of evaluation of different\nFrameworks\num but I I would say before we kind of\nsettled and chose on flight there was\num just a lot of ad hoc code and we were\nreally struggling to be able to say okay\nthe researchers have their own kind of\nlike big mono repo and notebooks and\nother ad hoc scripts and we were trying\nto use a little bit of beam in there for\nfor some of the parallel compute but but\nyou know at any time anybody wanted to\ndo something in a similar way they would\nyou know kind of like ask around and\nthen copy paste all that code and so\nyeah we were just having a ton of\ntroubles with that\num and I think there were some ideas of\nokay do we want to try to you know build\nbuild our own thing or not and uh I\nthink we luckily kind of decided not uh\nand to look around for first so that\nthat was kind of the the state of things\nbefore we we went to flight and\num and Bernard if you want to talk a\nlittle bit about kind of how we\nevaluated\num yeah uh I came in to pajama with a\nheavy bias of having used flight before\num even though we still wanted to\nbecause because the problem I've worked\non before was was slightly different uh\nwe still evaluated different Frameworks\num mostly want to see more modern ones\nso we didn't want any yaml I think like\ntypical things you'd find\num when people evaluate stuff\num and we looked a lot at prefect and\nkubeflow\nbut in the end a few of the the key\nfeatures like like memorization\nmaking sure that everything runs with\nthe right python versions and and the\nright Docker image and having that very\ngraceful lineage especially because\nright now lineage Force as long as we\nkeep the input sane then flight is\nextremely good at just keeping the all\nthe the code and all the steps in\nbetween\num nice for us and so I think that's\nwe're kind of the main the main reasons\nand we also I did have some trust in\nthat it that it works so that definitely\nhelped\nthat's that's good to know and I think\nit's always an interesting um you know\ncoming together as a team because now\nyou have to go exactly as Maria as you\nsaid you have to go back to you know\ndata scientists and researchers and tell\nthem yes if you want to run your basic\ntime series The Classical way you've\ndone this here now we we we're actually\ngoing to put this a little bit on a\nplatform but we can all collaborate what\nwas the feedback from your researchers\nwere they okay to say hey look let's\nlet's try this all or did you have to\nconvince them\nyeah I think uh luckily we're a small\nand fairly close team and so I I think\nuh you know a lot of folks from the\nresearch end were also involved in kind\nof the the decision-making and you know\ntrying out prototypes and things like\nthat and um the the reception has been\nreally positive\num especially\num you know the Baseline team that we've\ntalked about a little bit has has really\nlike gone 100 all in\num and you know I think time after time\nthey'll they'll call out how much their\nproductivity has improved how how much\nuh more that they can do how many more\nexperiments that they can run and so\num yeah they've been really fantastic\nwith that and then I think for for other\nresearch teams too where\num who might have had a bit more of\ntheir their own established ways\num it's been like a bit of a slower\ntransition but but I think people are\nstill very excited about it and and kind\nof see the promise of it\num especially as our Central components\nhave matured then it almost becomes a\nno-brainer\nthat's fantastic absolutely great I have\none more question before we are at the\ntop of the hour so\num you know now that you're two years\ninto it you have teams adopting it\nlooking back I mean hindsight everything\nis 20 20 but you know if you see with\nsee somebody else asking for you you for\nthe one or two pieces of advice to to\nget their stories straight and right\nmoving forward into a production system\nlike what do you have what is maybe the\none or two pieces of advice that you\nwould want to give the merrier from two\nyears ago\nuh that's really tricky\num\nuh one clarification Murray it's not\nbeen two years for you guys right yeah\nyeah yeah yeah yeah a little under a\nyear so I think we we started\nevaluating in March of this year\num yeah\num yeah I think uh\nI I think\nwe we maybe avoided some of the the\nrookie mistakes think thanks a lot to\nBernard's experience uh already and uh\nbut but I do think uh you know I I think\nmaybe we we've the rest of our team has\nstarted engaging more with the community\num and you know looking for open source\nhelp and you know seeing if we can make\nPRS here and there and I I would say\num maybe the advice to myself earlier\nwould be to encourage the team like even\nearlier on to to start doing that\num Bernard I don't know if you also have\nkind of advice\num\nokay\nthat's a tricky question\nprobably that that cicd getting that\nright and getting a template right so\nthat\nthe onboarding effort for a new data\nengineer and then you that that it's a\nreally pleasant experience also inside\nthe company\num and there is some tooling that you\nhave to build around depending on how\nyou do cicd whether you how you do\nthings\num I think building that and making it\neasy for internal stakeholders to\nonboard uh is probably the the most\nimportant thing and then also\ngoing in helping them and just fixing\nissues because there's things that will\narise\num and maybe not every data engineer\nknows Docker as well as as they need to\nand yeah so and so forth so just a lot\nof Education I think would be the the\nthing that\n[Music]\num\nI can suggest\nyeah absolutely I think what we hear as\na common you know theme is the community\nand when people come back and secondary\nas you said earlier\num and and foreign\n[Music]\nabout your work and and also with the\npeople here so I think this is something\nthat helps you to get started and get on\nthe track the right way or just you know\ngoing down a rapid hole potentially\nwhere you think there should be\nsomething where there's nothing and so\nso I feel like that's what I hear all\nthe time when I feel like uh I want to\nsay thanks uh to to to you Marie and\nBernard pajama team to your core team uh\nto speak about pajama in your flight\njourney I think this is super\ninteresting uh we'll put it out uh on on\nweb and social media and share it and I\nwould love to follow up with you when we\ngo from the technical assist I'm in the\nassessment part of your business to the\nmarketplace and see what you're going to\ndo in the marketplace Maybe we can have\nyou back next year and talk a little bit\nabout what's happening there but I just\nwant to say thank you very much this was\nsuper exciting\num to to get that feedback about your\ncompany and your use case\nyeah thank you all so much it really has\nbeen fantastic and a smooth experience\nso we really appreciate all the work\nthat you do\ngreat awesome"
    },
    {
        "title": "Flyte Community Updates 027 featuring Upcoming Events and Top Contributors of November",
        "transcript": "all right let's kick it off uh today's\nNovember 29th welcome to the flight\nCommunity sink my name is Martin Stein\nand I'm your host today and uh before we\nget started a few housekeeping notes\nfirst we are recording this community's\nthing and it's going to be shared on\nsocial media YouTube it's our Channel\nagain check out YouTube for flight go\nthere and check it out which brings up\nanother topic if you haven't checked out\nuh slack you can find us actually on\nslack as well go to fly.org and see how\nto attend uh some of the communities\nthat we have on slack or the community\nthing obviously in here and then of\ncourse the blog and to the YouTube\nchannel\nthe agenda for today is packed I feel\nlike there's a lot of really interesting\nstuff in here we have the community\nupdates uh which will be presented uh by\nsamita our developer Advocate at flight\nand I think there's also going to be\nNiels but I should talk about Pi data\ntoday then we do have a very interesting\ndemo about Union ML and Pen to ml it's a\ndemonstration by Neils and after the\ndemonstration we go and have a very\ninteresting conversation with the pajama\nteam with Marie from pajama here and we\nhave Bernard machama here and we're\ngoing to talk about pajamas you know\nwhat is pajama's business\njournalist use of flight and also the\nchallenges they have seen it where they\nare today and what they're organized for\nthe future might look like so with that\nI'm just going to hand it over to samita\nand samita will present and give us the\ncommunity updates\nyeah hey everyone my name is Samuel I'm\nwith the Union AI team\num next slide please\nuh Niels will be hosting a production\ngrade machine learning web flight\nworkshop at biodata Global this year\nthis Workshop will help you understand\nhow flight can overcome the mlops\nchallenges and you can get a hands-on\nexperience of building ml pipelines\nwebsite this will be happening on\nDecember 3rd from 7 to 8 30 a.m PT this\nis a virtual event so you can grab your\ntickets at uh the link that I specified\nhere we'll also be sharing this link on\nslack and if you want to take a look at\nthe content of the workshop then you can\nvisit the pi data site I'm sure this is\ngoing to be an interesting Workshop so\nplease do check check it out and next\nslide please\nuh we have an upcoming webinar on\noperationalizing machine learning and\ninterview study paper followed by a\npanel discussion this talk will discuss\nthe results from a semi-structured\ninterview study of ml Engineers spanning\ndifferent organizations and applications\nto understand their workflow best\npractices and challenges in addition we\ninvited a panel of Industry experts to\ndiscuss mlops pain points challenges and\nchecking on their mlops tooling advice\nRolando Garcia will be presenting the\npaper at this event and as for the\npanelists we have Fabio grads and we\nalso have panelists from stride Works\nLyft and other organizations which we'll\nbe revealing soon and this will be\nhappening on December 6th from 1 to 2 15\npmpt this is a virtual event\num to RSVP you can visit the union.ai\nevents page\num yeah\nplease do uh you know visit the site and\nRSVP and join the event and\num Ryan Nazareth Daniel Shai and Jeremy\nSmith are are contributors of the month\nuh they are our hacktoberfest\ncontributors as well and uh thanks to\nKatrina P for helping the flight\ncommunity on slack and thanks to all the\nother contributors who are helping\nflight grow so yeah thank you\nfantastic thanks samita that's fantastic\nwe'd love to make sure that uh hopefully\nin the next communities think we can\nalso get a contributors in here again\nwould be great to have them talk so\nfantastic thanks for the update I'm just\ngonna close the session out here with\nthe typical uh shout out and uh you know\npointing at the links and resources here\nwe have office hours uh teams usually\nthat's another Point uh Marie and\nBernard you know using the office hours\nsitting down and getting some time with\nhey them or Katrina or Savita on\nspecific questions but also on the slag\nChannel that's really you know the\nengagement tool number one\num we do have a YouTube channel to see\nuh those talks and our open source\nCommunity things we do have newsletters\nthat come out monthly they're also on\nthe web now moving forward and as we\nhead into December quickly we will have\non December 13th our next Community sink\nwith Katrina pillars Dash and Ronald\nmeter from theorem so I'm looking\nforward to that one and I as always be\nsafe people and thank you to all of the\nattendees today thank you bye\nthanks everyone thank you bye"
    },
    {
        "title": "Flyte Community Updates 026 featuring Hacktoberfest 2022 Recap",
        "transcript": "all right\nso uh let's kick it off and uh\nwelcome to the flight Community you\nthink today's November 15th\n2012. it's only 10 days to Black Friday\nthis year in the United States time is\nflying but who am I telling this my name\nis Martin Stein I'm your host with Union\nand before we kick it off we do have a\nfew housekeeping notes number one this\ncommunity sink is being recorded and\nwe're going to share it in social media\nand number two if you have questions\nplease chime in we want to keep it\ninteractive I think there's a way to\ngive you a reaction raise your hand and\nthe team will help you and bring you in\nso we can have a conversation with that\nto the agenda\nthis will be an interesting Community\nthing today we have Community updates\nfrom samita samita is going to talk\nabout the Oktoberfest and it's going to\ngive us a recap it's going to be very\ninteresting we also have Eduardo\npapalinario he's going to give us a DPT\nplugin demo today and I'm super excited\nabout seeing the integration in the DVT\nand learning more about it last but not\nleast we do have a special guest we have\nCalvin leather from Embark Veterinary\nhere today and Calvin is going to give\nus some insights about how Embark is\nusing flight but overall also the\nchallenges with regards to research and\noperational data flows so we're going to\nlearn a whole lot about what's going on\nat Embark today so with that let's dive\ninto our first topic the community\nupdate and to the hacktoberfest so\nsamita if you're there please unmute\nyourself introduce yourself let us know\nwhat happened at the hypo Fest\nyeah sure hey everyone my name is\nsamhita I'm with the Union AI team uh so\nwe have had an amazing hacktoberfest\nthis year uh Sandra next slide please\nyeah so we saw 23 contributors we closed\nabout 42 issues and we received over 50\npull requests so those were some nice\nnumbers and congratulations Ryan\nNazareth on being our top contributor\nand for winning for release 500 annual\nsubscription price plus flight swag\num yeah so Ryan worked on a couple of\nPRS he added a plug-in for supporting\nwex data frame type he worked on type\ntransformer for tensorflow TF record he\nfixed my Pi errors for incompatible\ntypes and added a tutorial on running an\nNLP workflow with flight yeah so thanks\nRyan and kudos to you\num next slide please\nyeah other noteworthy contributions\ninclude tensorflow tensor and model\ntypes by vivekan Tushar get part\nenvironment set up in Union Mo by\nKrishna listing differences on\nconflicting definition when registering\nby Jeremy using pass-through workflow\nstore to perform crd terminations and Dr\nRising call to go generate the flight\nidiomake file by Daniel and adding\nverbose to demo and sandbox and flight\nCTL by Heyman yeah I would also like to\nthank all the other contributors\nwhosoever participated in hacktoberfest\nand special thanks to Kevin Eduardo Dan\nprofel Katrina Nielsen Haytham for\nreviewing the PRS it wouldn't have been\npossible without you folks So yeah thank\nyou\nfantastic and um so\num if you look at the recap and compare\nthis is our second Oktoberfest we did\none last year so how do those hectober\nfasts compare this year versus last year\num I feel the quality of the pull\nrequests is uh you know great this year\nin comparison to the previous year and\nwe've also got more number of PRS I\ndon't really remember the last year's\nnumbers but yeah I think we have got\nmore number of PRS you know we uh we've\ngot PRS related to Integrations uh\ntutorial we've added a tutorial so yeah\nI think it's great that's fantastic I\nlove that great so let's uh see that we\ndon't have to wait until Octoberfest\n2023 let's do uh something else\nhopefully soon uh a hackathon or\nsomething like that and I think you will\nlet us know when when we have the next\nopportunity uh to reach out to the\ncommunity I'm gonna share the links\nright now where to go and what to see\nbefore we go\num into that so thanks again Calvin hope\nto see you soon and uh you know\nfantastic work here at Edinburgh we love\nit\ncool office hours so always same\nprocedure here we're weekly office hours\non Wednesday uh flight community members\nshould probably know this uh by now we\nhave Hayden\num ours at uh early uh West Coast time\nand uh then uh some uh with cathan as\nwell and there's also an US flight\nmaintainers anything uh weekly on\nWednesday so I would say the typical\nthing next slide is how to engage and\nhow to find us is to literally look at\nthe sinks here then go to the calendar\ninvite make sure you add the event so\nyou'll see also Mary herder from pajama\nand two weeks from now to check out the\nYouTube channel check out the flight\nmonthly newsletter and most importantly\nuh don't forget us on GitHub stars and\nGitHub and join the slack community so\nwith that I think this was a very very\ninteresting super excited about this one\nwe're going to put this on online Calvin\nthanks again also thanks to samita\nEduardo and I look forward to seeing you\non November 29th after the holidays over\nhere in the US thank you everyone have a\ngood day goodbye"
    },
    {
        "title": "Flyte dbt Plugin Demo",
        "transcript": "nice so yeah my name is Eduardo I work\non um Union I'm the OSS lead\nand today I'm gonna talk about this this\nintegration that we announced in the\nlast the latest fight release uh the one\none missing piece in the in the data you\nknow scenario DBT so next slide please\nyeah so\num\nfor those who who don't know like DBT is\nthis\nthis is\nessential piece in the modern data stack\nand um that allows you to essentially\nadd this this layer of abstraction over\nyou know SQL\nso you can for example you know if you\ndeal with multiple data warehouses you\ncan use DBT to connect to these\ndifferent data warehouses while you only\nworry about you know sequencing the the\nwriting the SQL necessary to like your\nyour business right\n[Music]\num\nspecifically for this integration we\nwe are announcing this integration with\nDBT core which is the CLI that DBT Labs\num\noffers\nand right now in the flight DBT\nintegration we we have support for the\ntwo most important\num commands to run and test so run as\nthe name suggests it runs the models\nthat you have defined in your DBT\nproject and pass well again as the name\nsuggests like in DBT you you have this\nidea of like having pass for your SQL so\num more commands are being added you\nknow by the community\num we have support for a subcommanding\nsource called freshness that was added\num last week but we haven't really\nreleased it yet but again\nthe framework for supporting everything\nthe DBT\num offers is there we just started out\nwhat is the two most important commands\num can we go to the next slide\ngreat\num I'm going to demo real quick how the\nthe integration actually looks like so\nI'm going to share my screen now so\nlet's see\nyes I want to share\ndesktop one\ngreat Zoom\ncan everyone see my screen it should be\nlike a pale background it's a little\ntiny is it tiny okay let me let me make\nthis a lot bigger\nis it bad enough yeah much better thanks\nadorable great okay so\num\nessentially the integration revolves\naround the idea of having\num DBT of tasks of specific types that\nshow up in um\nconsole and you know in the in the the\nbag that you build and\num as I said before we have support for\ntwo commands in DVT run and test so we\nhave these two tasks called DBT run and\nDBT test\nand in this example really I'm doing\nsomething very super simple like and\nlike I I'm just running\nthe DVT run task pointing to a specific\nproject that I defined\nand\nrunning the test like a DBT task command\ndown here I'm just you know sequencing\nthem so that you know the run runs\nbefore the test and finally I'm I'm\nreturning it\nso\nhow does this look when you run\na workflow that contains these these\ntasks uh you'll have to also increase\nthe font this guy\nI don't know if it allows me though\nyeah sorry uh I'll just run this and\nwalk through the\nhow it looks like in in the in the\nconsole\nlet me know if you can see my screen\nstill\nit should show the console now yeah it's\nit's also super tiny great let me also\nincrease the it's also bad enough oh\nyeah amazing so I just um schedule an\nexecution of that workflow\njust to show that I'm not lying here's\nthe graph how it looks like those two\ntasks that we Define the example run\ntask an example test task\num notice the the little string here\nthat says hey this is a task of type DVD\nDVT run similarly you have a task called\nDVT test and as we all know and love you\nget access to the outputs of each task\njust like any other regular you know\ntask\nand um\nyeah as I said this is like a very Bare\nBones\num example but one could think that you\nknow the one of the utilities of DBT is\nto for example run these on a on a\nschedule\nso you can rely on flight to help you\nclose this\nyou can actually leverage you know\num\nthe some flight constructs to help you\nschedule DBT runs or tasks or any other\ncommands that eventually you'll get\nsupport will be supported\nso in\nlet me also increase the\nphone from this so\nlook at the bottom here I'm using a I'm\ncreating a launch plan calling it my DVT\nschedule launch plan that runs on a\nfixed rate runs once every 42 minutes\nand\nagain Bare Bones is like the building\nblocks you can think of it as like you\nhave you have access now to a subset of\nthe DVT commands exposing the CLI and\nyou can compose your\num your projects and now use DBT to\num increase the\nthe potential for transformations in\nyour in your um in your project\ncan we um go back to the slide please\nI don't know if I have to stop sharing\nare you\nsorry Zoom is really weird\ngreat thank you yeah get into the next\nslide\nso um real quick how to install this\nit's a\num flight kit plugin so you should be\nable to just to pick install fly keep\nplugin TBT\ndon't forget to install the necessary\nadapter you know DVT ships with a bunch\nof adapters in this particular example\nhere we use postgres but you know they\nhave adapter for sqlite.db there's like\na gazillion um connectors including more\num exotic ones like you can connect to\nyou know snowflake\num\nquery\nis let's go to the final slide\nyou notice that we we haven't really\ntalked about\num the the managed solution that DVT\nLabs supports the DBT Cloud\num we we plan to support DVT cloud in\nthe future it won't come in this in this\nMax release but it's something that\nwe're shooting for\num q1 of next year\nagain we only have support for two\ncommands announced\num support for the source freshness\ncommand was added in this PR\nand contributions are welcome like\nadding is you know if you're missing one\nof the commands and you really really\nwant to use it in fight like it should\nbe very simple we already have like\nexamples of three commands adding the\nanother command should be trivial really\nthat's awesome so so in order is there\nany any specific feedback yeah you're\nlooking forward to get from from the uh\nCommunity or on the on the other\ncommands that you want to implement or\nyou hope for contributions is there any\npriority that you say hey we would like\nto have this one first and that one\nsecond is there anything that you would\nprefer\nthat's um a great question and I I'm\nlooking for feedback on\num One Piece like if you see like we one\none\num request that we had from the\ncommunity is that\num DBT produces a lot of logs and we are\nnot exposing them in a very ergonomic\nway yet\num we only log in case you know things\ngo wrong but like if you want to\nhave access to whatever happened for all\nexecutions we should talk\num there are a few options you know and\num in terms of prioritization I would\nlove to have DBT docs as uh as a as a\nback you know\nso that um you can get access to some to\nthe the auto-generated documentation\nthat DBT provides to you as as a flight\ndeck I I think it'll be a great first\ncontribution super low hanging fruit and\nit would add a lot of value to\num the the DBT integration\nawesome hey Edward fantastic work love\nit and let's make sure that uh Community\ncan find you reach out to you I assume\nslack channel is the medium of choice uh\nfor getting a hold of you yeah I would\nsay so but you know don't if you want to\nmessage me on Twitter or I don't know\neven my my personal email like it's fine\nI'm I'm always gay"
    },
    {
        "title": "Flyte @ Embark Veterinary: Research and Operational Dataflows",
        "transcript": "good so now we get to the other\ninteresting part having our special\nguest here let's go to the next slide\nplease we have Calvin leather uh\ncalvinus with Embark so Calvin please\nunmute yourself introduce yourself and\ntell us\nyou know more about Embark and the\nresearch operational data flows stage is\nyours\ngreat thanks Martin\num hey everyone this is Calvin from\nEmbark vet\num I'm a engineer and I wear a whole\nbunch of hats here I do statistics\ncomputer science work\num I love dogs I love statistics and I\nlove hiking with my dogs while thinking\nabout statistics\num so yeah next slide\nI have a bit more uh intro on Embark\num so yeah we're in Mark vet\num we're just to start us off I have a\nquick thank you to my colleagues who\nhelped um do a bunch of the work that\nI'm going to talk about in the slides so\nall my collaborators on the\ninfrastructure engineering team and that\nfirst group and then all my\ncollaborators on the Discovery platform\nteam some who are on this call\num so yeah next slide\num\nso Embark vet briefly we're a direct to\nConsumer Canine dog genetic testing\ncompany so we're sort of similar in\nfunction to what 23andMe or ancestry.com\ndo but we focus just on the veterinary\nMarket with dogs and the the company\nstarted and its goal is to to end\npreventable diseases in dogs by\nimproving dog genetics and access to\ntesting dogs are a really interesting uh\nspecies from a science perspective\nbecause they were bred over over many\nyears there's lots of interesting\ngenetic things going on\num the next slide\nforeign so concretely what what this\nmeans day to day for us is we have lab\ndata coming in raw data coming into an\nS3 bucket you can think of it um from\nfrom a number of of biological assays\nthings coming from fancy machines in a\nlab\num we do that daily because people are\nconstantly sending us their dogs cheek\nswabs to get tested\nwe produce process data from that raw\ndata and that process data feeds into\ncustomer facing products that say Hey\nyour dog's at risk for this disease or\nhey this this we suspect your dog's age\nthat you didn't know is this\num and then we also do a whole bunch of\nresearch to try to develop new products\nnew tests new diseases we can test for\num so I have some animations or next\npieces to this slide if we move it along\nso the customer facing products are\nstatistical or genetic based data\nprocessing\num to identify or predict health\ncondition risk so we can distinguish\nhealthy dogs from sick dogs based on\ntheir DNA with with some simple and some\nmore complex statistical methods and\nthen we do some other fun things like\nlike predicting the dog's age for dogs\nthat were say adopted or looking trying\nto figure out their breed composition is\nit a box or a Labrador\num on the next slide we also do a bunch\nof research work\num and so that's doing a lot of\nstatistical analysis um with some pretty\ntypical methods if you've done any sort\nof prediction modeling\num some that are more genetics oriented\nand a lot of that is you have a whole\nbunch of features for each dog hundreds\nof thousands of them because you have a\nlot of DNA\num and we have a whole bunch of disease\ninformation which we actually model with\nDBT I didn't I didn't put that in slides\nbut we actually probably should think\nabout the DVT plugins\num we have a whole bunch of also\nstructured Health Data that we get\npeople report they do surveys for us we\ncollaborate with academic collaborators\nand we try to find associations new\nproducts that we can offer new diseases\nwe can test for\num so with this comes some interesting\nengineering challenges\num we have a mixture of production data\nflows we have customers constantly\nsending us cheek swabs from their dogs\nand other biosamples\num and every day more and more data is\nflowing through that needs to get\nanalyzed and processed but then we also\nhave these research flows where we want\nto take that data and then use it to do\nDiscovery work to help improve like to\nunderstand new health outcomes for dogs\num so there's there's cost side cost\nimpacts on the operational side there's\nsensitivity errors in bugs this is not a\nresearch workflow where we can send\nincorrect results and then fix it later\nwe have to have a lot of confidence in\nour processing\num so that was one of our interesting\nchallenges that that originally set us\ndown the path to to Landing with flight\nbut there's some other interesting ones\ntoo very wide data very large lots of\nbinary data formats it makes it very\nhard to to go with something that's very\nstructured for like\num I don't know Event Event driven data\nprocessing or some of these other\narchitectures that are designed a lot\naround protobuf or Json messages that\ndon't have S3 backing\num so so unless there's some interesting\npieces around we have a number of\nproducts and we have a number of teams\nEach of which might have different\nscience backgrounds and those all have\nto interact in complex ways that meet\nall of these previous requirements about\nSafety and Security and cost\num so so coming from that those that was\nsort of our quick idea of our\nrequirements we started exploring flight\nand other tooling\num a little over a year ago now\num if we go to the next slide\nuh oh here's a quick image of our once\nwe shifted to flight here is our dag of\none of our actual customer facing\nproducts the final sort of workflow just\nto give you some idea and a number of\nthose sub workflows there's a lot of\nreference workflows there are different\nteams\num so so there's a lot of complexity\nthere about pieces of data being passed\naround\num and so yeah if we go to the next\nslide\num we started on this path of doing a\ntrade analysis because we had some\ninternal built tooling from embark's\nearly days where we're gonna about six\nyears old now\num and we were looking to try to lean on\nthe open source community and use\nstandard tooling for for some of this\nmore complex data processing so we\nsupport a bunch of things\num some from sort of the scientific\nworld like next flow or Cromwell or\nsnake snake and then some more typical\nones if you've worked in data science\ndata engineering\num and when we got through this this big\nexploration\num we ended up picking flight for a few\nreasons the next slide has some quick\nideas of them but the biggest one was if\nyou have a big complex cross team system\nand you want it to be very easily\ntestable and very well understood what\noutputs of one thing are and how they\nfeed into the next component or next\ntask\nthe way that flights type contracts that\nyou can create sort of with the flight\ntypes was was really powerful and and\nwas a lot better than a lot of our\nexperiences working with the more\nscientific schedulers\num but then there's also a whole bunch\nof other things around around\nperformance and support for kubernetes\nand also being able to use batch if we\nwant that we really liked so we got out\nof that trade analysis and and we we\npicked flight we did a whole bunch of\nExploration with different tools\num\nand yeah the last one is probably one of\nthe more interesting ones that that I've\nalways been curious about coming from\nother schedulers is there's a lot of\ninteresting support for large dags\nnatively one of the patterns that I've\noften seen is your scheduler doesn't get\nto understand the the scale of the jobs\nit sort of says hey I'm gonna go ask\nspark to run this huge job and that\nthat's fine for certain things like\nthere are certain things where you're\ndoing a big aggregation job with spark\nand the scheduler wouldn't really have\nmuch to do there but when you have a lot\nof massively parallel jobs\nbioinformatics is a lot of these where\nyou're processing the same dog and doing\ndifferent things on it serially\num letting the scheduler reach in and\nunderstand and have a large dag can be\nreally nice because it can start to do\nsmart things about handling errors and\npassing data around better so that was\none of the sort of more unique things\nthat it wasn't just flight did better it\nkind of just did differently than a lot\nof the schedulers we were used to\num so yeah that was sort of our quick\nsummary of our trade analysis\num let me know if anyone has questions\nthere's the next bit where we talk about\nsome of the things we've been doing with\nflight that we really like\nmaybe maybe just one question\num Calvin they're on a trade analysis I\nmean for a team to to really go through\nwho initiated this I mean not who whose\nname but what kind of role was it you\nand your team was it more data science\ninitiate initiative or from from which\ndepartment did I come actually initially\nwell in front\nyeah they came from the team used to be\ncalled the infrastructure engineering\nteam so it came more from sort of a\nmixture of SRE and and more operational\nscience folks um that was where it sort\nof started From was we we looked at the\ngrowth of our product\num and the number of products we're\noffering and we realized we can't we\ncan't we have to have one tool for\nworkflows just for operational tooling\nwe can't like do some things in a Lambda\nand some things and something else and\nthat's going to get unmaintainable and\nthen once we saw that need we realized\nour science groups need all of this too\nin our more operational science groups\nbecause they also have big compute jobs\nthat they need ec2 instances for and we\ncould give them a lot better of an\nexperience\ngot it got it so I mean the the other\npoint for the community is if if\nsomebody's watching us right now and\nthinking about how do I get started with\nflight who do I involve on my side and\nmy team what do you think uh is the\nright team is do you wanna do you think\nit's a cross-functional team to begin\nwith OR should just somebody just take\nthe lead and try it\nyeah good question I think\num I think it depends on who your teams\nare too and the type of culture you have\nin your science groups especially like\nthere are some companies where the\nscience groups are really want to get\nlike get their engineering hats on and\nand there are other companies where it's\nlike no come to us once like we don't\nwant to have to deal with all of the we\ndon't want to deal with kubernetes we\ndon't want to deal with like IP\nexhaustion right so so I think that\nwould be what I would recommend is think\nabout your company going to like an SRE\nstyle group would be great if you have\nmore of like a a company where science\nwants a more polished platform if you're\na smaller startup and you have more\ninteraction yeah totally get that\ncross-functional team from day one and\nyou'll probably have better outcomes for\nfor like those really small teams\nawesome thank you yeah I think I think\nit always takes like kind of like the\ndata scientist to things more\noperationally and production wise or the\ninfra person who thinks more about how\ncan I get my data scientists my\nresearchers and once you have somebody\nwho can actually wear the two heads at\nthe same time it actually pave the way\nfor solution like flight\ntotally yeah it's hard it's hard\notherwise without that shared without\nthat shared Vision yeah okay fantastic\nthanks please carry on\nyeah so this is gonna be a quick\noverview of some of the things that we\nreally liked that we've been able to do\nwith flight lately\num so one of them a lot of our science\nwork happens in in notebooks um for sort\nof the exploration piece and there's\noften a stage where that exploration\npiece then needs to get really big with\na lot of compute and so I've seen a few\nscenarios I think one scenario that I've\nworked in the past is like that is\nalways handled because by having your\nnotebooks hooked up to a spark cluster\nor a Das cluster or something\num but one of the interesting things\nthat we've been playing with is\num The Notebook can sort of Drive the\nflight cluster using flight kit remote\nand so we've gotten this really cool\nsetup now where a couple of our really\ncommon workflows that people can launch\nvia the flight console we have\nrelatively good docs for how to call\nthem from other places including from\nJupiter notebooks and people have gotten\nused to this a little bit so some of our\ncore science tools that you use\nrepeatedly imputation is one of them for\nfolks who do\nbioinformatics work\num you can just sort of say like hey\ngive me imputation and it'll go launch\n40 spot instances which are cheaper than\nyour on-demand instance that you're\nusing for your notebook and then it will\nreturn the results back in memory which\nis really neat depending on what the\nresults are you people don't even have\nto understand what was going on on the\nflight cluster and how the data got back\nthey'll just get back the outputs object\nand they can do outputs.getwhatever the\nnext slide has an example of this\num but yeah like there this is an\nexample workflow this isn't the\nimputation one\num but this is loading uh loading a\nlarge file and pulling out a piece of\ndata from it and you don't have to pull\nthe whole file\num to your machine because it's a big\none\num but you want to pull out some of\nthese I this one of these columns um and\nit's really nice because they can grab\nthis code block that fetches the task\nand executes it and then just grab the\noutput it comes back into memory and I\nthink there's a lot of low hanging fruit\nthere with I've spoken about this a\nlittle bit with some other community\nmembers\num like some flight types that are\nspecific for certain problems where they\ncould just like the structured data set\npandas parquet one could do some of the\nwork to get the data back into memory\nand then you have a really easy way to\nget that sort of yeah I like that word\nscalable exploration like like create a\nmore closed loop process where there's a\ndance with the flight cluster\num so yeah that was one of our favorite\nfeatures that we're trying to lean into\na lot at the moment\nthat's a usage that actually is is very\ninteresting do your data scientists do\nuse this in the discovery process when\nthey impute something and see what\nimplication works well just need a big\ncompute power and then once they get\nthis right then they actually go into\nthe workflows after that is that the is\nthat how it works\nyeah so we're actually just starting to\ntalk about using this a lot more for\nhyper parameter optimization which is\nwhere this is going to get even more\nlike useful to launch at scale\num but yeah like there is a sort of like\nyeah the way that it works with\nimputation is imputation basically it's\nlike it is in any other data science\nrealm where you're filling in missing\ndata that we don't have\num in our case it's because we only\ncollect from DNA about 250 000 features\nor 260 and in reality the number of DNA\nlocations that change in dogs is in the\nmillions so we can impute those\nlocations and then run more analyzes\nand so yeah that's often the workflow is\nlike hey you think there's something\nthere you want to get more of that\nimputed data which can rely on some\nunlabeled data to do that imputation and\nthen pull that back in and do another\nmodel fit another round of exploration\nso that's the really common one right\nnow but yeah hyper parameter\noptimization is going to be a great\nthing for that where you have a start to\na model and you want to cue up a whole\nbunch of work\nyeah I think I think what you just\ndescribed is actually what it is is\nsomething very interesting because what\nwhat you do is like you make this more\nof a circular motion versus a linear\nmotion right the linear motion is less\nresearch let's build the model let's\ndeploy it where U.S right now say well I\nhave a platform and I can actually you\nknow pick things in areas of interest\nand after I have my model rate done and\nif I want to impute differently by\naverages or whatever else I want to\nimpute by uh then I can do this here and\nactually keep going and and making my\nmodels of whatever I have my workflows\nbetter so I think that's a very\nimportant point that you're making here\nright now\nbut yeah go ahead\ngo ahead oh I was gonna say that that\nthat that that that closed loop that\niteration time is totally what we're\ntrying to get at here and trying to\nbring that down for for many things that\nsome of which are data science some of\nwhich are even more biology oriented\ngreat fantastic I love it\nyeah um Calvin just real quick this this\nis really cool\num just seeing this slide\nbecause\njust the the what I said in the in the\nchat just like scalable exploration has\nalways been a pain point and you\ntypically almost you typically would\nhave to buy into a vendor to say like\nsagemaker notebooks or you know whatever\nbut\num\nyeah I didn't even really anticipate\nthat you could use flight in this way I\nmean obviously supported like you guys\nare using it that way but yeah I would\nlove to like somehow encode this and\ninto some kind of best practice because\nthis is the first time I'm seeing it\nused in this fashion\nyeah I know I know I've been on a ticket\nthere's a ticket somewhere in the the\nflight\nproject to talk about like getting data\nout from executions and notebooks and I\nhadn't really put two and two together\nuntil about a week ago but one of the\nthings you could really probably support\nwithout too much work right is if you\ncode gen these blocks out of the console\nright you can go to any anything in the\nconsole that's already executed and grab\nthe like the thing to get the exit to\nfetch the execution and grab the outputs\nand and that takes care of all the\nTransformations using the existing\nTransformers for the flight types which\nis really neat there's no new code that\nyou have to write to support that so\nyeah I'd love to keep talking on that\nfront that's awesome I mean so and I'm\ntotally with you Niels I think that's\nsomething that's when I saw this right\nnow I felt like this is actually a very\ninteresting way of using the platform\nand the compute that you have on the\nback end and and giving researchers like\nkind of like at the moment what they\nneed and actually overall they have this\ncircular motion of making things\ncontinuously better it's fantastic I\nlove it\nokay please carry on\ncool yeah we're going to the next one I\nhave another another one of my favorite\nthings\num so this is another one that we've\nbeen we've been doing a lot of thinking\nabout and I think this is one of the\nareas of flight I'd love to see more\nlike as a community us defining or\ndescribing how different companies have\ndone this so as I alluded to before for\nespecially for operational data flows\nwhere you need stronger guarantees and\nyou want to you want to write better\ntests having sort of static testing or\npacks between teams that are based on\nthose flight types is really nice so\nwhat I mean by that\num we have a team that does a lot of our\nQC analyzes on data that comes in they\nhave a lot of like biochemistry folks\nthat understand what goes on in the wet\nlab really well\num and then we have another team that's\nreally thinking about predicting the\nhealth conditions of dogs and that QC\nteam needs to pass out like the qc'd\ncleaned up data to that Downstream team\nand there's a there's a handoff there\nthat in my experience is always a really\ngreat place for bugs to creep in when\nyou have those sorts of like differing\nexpertise of the the teams like data\nengineering data science teams if you\nhave those structured that way can also\nhave the same thing where like there's a\nmiscommunication and what was produced\nby one group and what went into another\num and in the web world there's a lot of\ndiscussion of using Pacs and cross-team\ntesting and strongly typed contracts\nand that's something that I feel like is\nis still building up in the if you're\nnot if you're doing work that has much\nbigger data sets and can't just be\nevents that get passed around like like\nwe we briefly explored using completely\nevent driven systems for this and it was\nreally challenging to figure out how to\ndo that well with like we have some data\nsets where it's a gigabyte per sample\nand like are we gonna pass around the S3\npath to that it's gonna get messy\nso one of the things that we're really\ngetting into that we really like is you\ncan use custom data classes and things\nto Define that contract between the\nteams\nso each team Imports this library that\nhas that common definition of here's the\ndata class there's as a bonus you can\neven use Pandera and get even more\ndetailed on describing data frames and\nthings\num and then flight takes care of a lot\nof the sort of like back-end details to\nget that into protobuf and get it so\nthat two different containers running on\nthe Kate's cluster can get the data\nbetween them but when you go and you\nregister you get a nice static check\nright then and there that that for\nexample like the Upstream team didn't\nchange their type definition when the\ndownstream team goes to register and\nthere's this example here when you\nimport that reference workflow you'll\nget an error if you're if you missed\nmiss something that they changed\nso there's some things like that that\nwe're really trying to lead into to try\nto create this like bring over this\nconcept of pax and cross team testing\num and use some of these flight features\nto achieve it\num and there's a even stronger example\non the next slide that we're using for\nsome of our production products and this\nis one of the interesting patterns that\nwe're still not quite sure we like how\nwe did it\num if you have a whole bunch of teams\nthat need to come together into a final\nbigger workflow\num our one of the patterns that we're\nusing for this is we have sort of a lock\nfile that's in one location and one repo\nthat deploys and all it has basically is\nreference workflows that reach out to\nthe other teams workflows and say I pin\nthis version\nand it creates a really nice setup where\neach team can independently bump and\nre-register their versions of things and\ntest them out and backfill old beta Etc\num in their Dev environment and then\nbring that up to their prod environment\nit's only once they update that lock\nfile with all those reference workflows\nthat it actually goes live in production\nbut they can have it on the production\ncluster ahead of time and test it on\ntest case data Etc\nso there's some rough edges there\nespecially around the CI CD and I'll get\ninto that when you have these like more\ncomplex reference workflow setups but we\nhave really liked getting at sort of the\nthe things that we're used to from the\nweb development world for smaller less\ndata heavy flows of of messages between\nservices for example\num yeah so any questions there those\nwere my two favorite things\nI can easily understand that those are\nyour two favorite things that's\nfantastic to see that and and also great\nto see Pandera being used there in order\nto really make sure that the packs\nbetween the teams are actually also\nvalidated via software is fantastic\ncool so we can go on I have a couple\npoints of things that we want to keep\nworking on\num and the first one is just around\nscaling eks it actually has nothing to\ndo with flight I think uh there were\nsome parts with flight thank you so much\nfolks on the flight side like Dan who\nhelped us work through some of these as\nwe were scaling but I mentioned before\nwe we love that flight can support\nreally big dags that also means that we\nhave to support scaling up our Kate's\ncluster pretty big\num and we're running an eks so for\nanyone who else who is thinking about\ngoing for a bigger production product\nwith larger data flows one of the\nbiggest things that was useful for us\nwas to really get our heads around the\neks cni that was one of them that that's\ndetails of how eks does IP allocation\nfor example are a little bit tricky for\nthings that aren't websites with one to\na hundred instances like if you're going\nto scale up to a thousand pods it can\nget a little bit tricky\num I actually didn't put this in there\nbut the other one to think about on that\nfront that's eks specific\num is the uh\num easy rebalancing that one\ncaught us for a loop AWS would take down\nour pods because it was putting too many\nZ and those pods were we're not hosting\na website so it's fine if there's a\ninstability in an AZ that takes down\ntwo-thirds of our pods it's not a big\ndeal\num so that was actually our biggest like\nthe thing that took us the longest to\nwork through\num and we're still scaling up our\nproduction workflows in flight to\nthousands of machines we're still\ncatching little edge cases and things\nthat we're working through\num yeah so the next one is is this is\nthe one I think is more interesting\num and more flight specific\nso we have a lot of multi-container\nworkflows\num especially on the science side where\nenvironinformatics a lot of the\na lot of it is not import whatever\num like whether you're working in python\nor r or Julia\na lot of times there'll be steps where\nyou need to reach out to a c binary and\nlike there's a really common you'll see\nlots of these workflows where it's\neither like Java there's a few Java\nbased tools that have been put out by\nlike Harvard or the broad Institute at\nHarvard and you need to keep calling\nthem from your python code to do some\nprocessing that someone has built really\noptimize Java or C code for\nI mean and that Community hasn't for\nexample picked up cython or something\nyet to build good interfaces to python\num and so you often have different\ncontainers because each of these\nbinaries might be 500 megabytes a gig\num so you don't want to have the Uber\ncontainer with all of them\num the registration gets a little bit\ninteresting where\num let's say you have some data classes\nfor example that Define inputs and\noutput types of some python tasks and\nthen you also have some bash script\ntasks that use those executables when\nyou go to register the Imports wherever\nyou're registering all need to resolve\num unless you hide them in your tasks\nwhich is a interesting pattern that I'm\nnot sure I like but um I know there's a\ncommunity ticket about this one we dug\nit up a while back but that's definitely\none of our one of the things we're\ntrying to figure out the right long-term\npattern for is when you have\nmulti-container workflows especially if\nthey spread across repos because they're\nacross teams\nhow do you think about the CI CD and\nsetting everything up right so that the\nImports will always work nicely and you\ndon't need an Uber container to register\nfrom that sort of has all of the\ndependencies from all of the component\num workflows and I think reference\nworkflows can help with this set it\nusing them right because then you can\nregister all the things that have\ncertain sets of dependencies but it also\ncan get tricky when you have like yeah\nlike within the same workflow you have\nfour tasks Each of which has a different\ncontainer and maybe a couple python\ntasks in between them\num so yeah I think that that's one of\nour that's one of our biggest long-term\npoints that we're pondering on and would\nlove to talk with others about how what\nthey ended up doing\nyeah that's fantastic I think that's a\ngreat topic and I think we can actually\nalso make sure we continue offline with\nthat topic and then actually go to slack\nand then and uh take this forward it\nwould be interesting to see what others\nhave done\ndefinitely I'd appreciate that\nokay and lastly we have the things that\nwe're working on and these are things\nthat we want to keep playing with in\nflight and keep improving\num definitely our\nthe name of the game right now for us is\ntraining really I think um at least\ninternally to Embark\num there's a lot of good tutorials on on\nthe flight side but they also folk like\nthey're written for people who are for\nexample like a data scientist or maybe\nmore of it like someone who has more of\nan engineering background\nso one of the things we're working on is\nhow to improve the understanding for\nfolks that are coming from a high\nperformance Computing a clustered\nComputing environment like a lot of our\nbiology oriented scientists come from\nthere it's often a little different you\nhave a job queue you often submit bash\nscripts to get run on a remote cluster\nthere's a lot of similarities but\nthere's a lot of places where we have to\nget people around this operating concept\nget their heads around it\num bash and Jupiter task supports we're\ncontinuing to work on\num The Bash task I think is the one with\nthe the the most opportunities\num for improving how data gets passed in\nand out in particular like there there\nare inputs there's probably more that we\ncould support to make it easier for\nexample like having better environment\nvariable inputs we've done some\nexploration around that and there's a\nhelper task that my colleague Mike zong\nwrote but I think there's still more to\ndo there that we can improve\num and then the maybes that we're\nthinking about that are a little bit\nfurther is I mentioned this before like\nnicer flight types for some of the\nbioinformatics workflows that help take\ncare of some of the work for you of\nmanipulating disks files on disk we can\nuse flight files for everything today\nand they're good enough\nbut I think the the low-hanging fruit\nthere is is things like um there's a\nlibrary called HTS lib it's a C library\nfor manipulating some of these\nbioinformatics files it has python\nbindings so we could make a flight type\nfor that so that when it comes back to\nthe next Downstream task you already\nhave the the file has been downloaded\nlike a flight file but you already get\nthat handle back that you can use in\nPython to manipulate it and do things\num and then there's some interesting\nstuff around using F1 instances I think\num for for certain things I don't think\nwe've seen anyone do that yet but\nthere's some interesting fpga\naccelerated workflows for bioinformatics\nthat a company called Illumina has put\nout\num and we want to experiment with that\nat some point I don't know if those are\neven supported in any chaos\nso that'll be an interesting one to to\nexplore\num yeah and those those are our main\nthings that we're excited to to think\nabout in the in the coming months that\nreally touch on flight and flight\ndevelopment fantastic so first of all I\njust want to say thanks I mean this is a\ntremendous overview here and I want to\nask here the the audience that we have\nand potentially some of the flight uh\nengineers and developers here if they\nhave any questions and feedback for you\ndeal so maybe Dan just just reaching out\nif any comments questions\nyeah I have a quick one around the\nmulti-container\nstuff I guess I didn't quite understand\nthe the issue that you articulated there\num because I know there's the\nmulti-container like guides right how do\nthose like what's the Gap there between\nwhat you want and what is currently\nsupported\nyeah let me let me pull this up just to\nremember so I'm not misspeaking about\nthat guide because we definitely looked\nat this one\num so so let me give a much more\nconcrete example let's say you have a\npython task that needs scikit-learn and\nanother python task that needs Pi MC3\nand you want to have different\ncontainers for those for whatever reason\nbecause they're big\num\nif you put those in the same\num workflow together without using a\nreference workflow right the Imports\nthat import\nscikit-learn and Pi MC3 those will get\ncalled during registration unless\nthey're put in the function definition\nright\nso yeah\nso you need the the registration\ncontainer needs to have the superset of\nthe dependencies\num that doesn't happen for bash tasks\nlike for bash tasks this is fine right\nbecause during all you need to have is\nthe path to the bash script but we have\na lot of tasks where it's like a dance\nbetween we do as little as we can in the\nbash script and we try to do most of our\ndata processing in python or something\nwhere we can have static type checking\nand unit tests Etc\num so so that's where this comes up is\nthe that interplay between we'll have\nsmaller containers so that we can have\npieces of processing structured but if\nmultiple those are owned by the same\nteam in the same repo people often don't\nwant to use reference workflows among\ntheir own like the tasks that they own\nand compose into a workflow just in that\nsort of the private tasks right like\nprivate methods\num yeah that makes sense\num yeah I mean I have seen other\nFrameworks like orchestration Frameworks\nthat solve this with the Imports inside\nthe\ninside the function so you know I think\nI agree with you that it's not\ndoesn't feel Supernatural but other\npeople have done it that way so it seems\nlike you either do some kind of lazy\nimport thing at the top that's like a\nspecial\nimport function or yeah you import it\ndirectly or like inside the function\nbody of the task\num\nyeah that is that is an interesting\nproblem thanks for clarifying\nall right any any other questions for\nCalvin or comments\nall right so I have one last question\nfor you Calvin how many dogs do you have\nand what are your dog's names\nI have one I'm looking to see if she's\nin her bed she's not she's out in the\nliving room uh I have a Wheaten Terrier\nnamed Trixie\num she's two years old right now that's\nmy my one and only okay awesome okay\ngood Trixie is a good girl that's\nawesome all right hey Calvin that was\nfantastic love it I think this is a lot\nof good uh inspiration a lot of\nInspirations here as well so let's make\nsure we can carry on the conversations\non Slack"
    },
    {
        "title": "Union-Hosted Flyte Sandbox Demo",
        "transcript": "we're going to focus on the flight\nsandbox demo that's a Sandbox that's a\nhosted sandbox I'm just going to hand\nover to Niels Niels please introduce\nyourself and uh let us know we'll see\nyou about your flight sandbox\nthank you Martin uh hi everyone I'm\nNiels bentelin\num yeah I'm super excited to\nshow this little thing that we built\nrecently so this was\num you know credit to Andrew Karina\nEugene a bunch of people that I probably\nmissed like I basically made the kind of\nlike the storefront like content for\nthis but there's so much other work that\nwent in the back end that I'm not\nactually aware of\num so yeah thank you\nto all the folks who made this\nexperience possible I I'm excited\nbecause this is something I've wanted\nfor a while\num like pretty much since I joined the\nteam\nI sort of like\nwas mentioning something like this you\nknow like just like a browser-based\nexperience where you can you know zero\ninstall uh experience with flight\num and obviously we're working on you\nknow we're we're burning both ends of\nthe candle we're making it easy to\nuh install it locally but then you know\non this other end there's just kind of\nlike this\nthis experience to not have to install\nyour cluster and pip install flight kit\nand all that stuff so\nbasically what I'm showing you right\nhere is uh if I just switch over to\num like an incognito browser if you go\nto sandbox.union.ai it'll take you to\nthis page I think you'll have to log in\nfor us actually and\num\nclicking the start sandbox button will\nbasically take you to\nthis vs code\nin browser experience\nand we've created a little walkthrough\nfor you here so you know\num\nmaybe just to take a step back\nthe problem the key problem this we're\ntrying to solve here is for for folks\nwho have not interacted with flight at\nall or don't know much about it\nthat's kind of the primary\num\naudience we're trying to trying to serve\nhere obviously if you've used flight\nbefore and you want to like Noodle\naround with it a little bit this will\nalso still be useful\num but the step-by-step walkthrough\ntakes you through you know this this\nvery uh simple python plain python\nworkflow nothing in Flight about this\num\nin the in kind of like as you step\nthrough it we take you through what it\ntakes to convert that python workflow\ninto a flight one\num by adding these tasks and workflow\ndecorators\num you'll learn how to use Pi flight run\nthis the CLI\num\ncommand line interface for uh interact\nwithin the slides so\num as a as a major caveat you know this\nthis ex like walk through experience is\noptimized for the pipeline run\num\nmode of running flight workflows which\nbasically means that you're working with\nsingle scripts like isolated Scripts\num so just to take you through a tour of\nhow this is you know like an even\nopen this up on a little browser right\nso it's I mean it's not ideal but it's\nkind of cool to just see this IDE that\nwhere you can code and and Python\nscripts you can open it up in a browser\num here I'll just open it up here so you\ncan see you know\nthing is things are working your your\nflights flight workflows are running\num\nwe have an example here of uh\ninteracting with the flight cluster\nin a jupyter notebook so here I'm\ninitializing a flight remote object\nand let me just take the execution name\nfrom here and um\nthis gives you a nice interface for\nyeah so this uh execution isn't\ncompleted yet so let me just grab a\ncompleted execution you can grab model\nartifacts from flight workflow outputs\nyou can grab intermediary data so\num\nhere I'm using flight remote to grab the\nraw and process data that's part of the\npipeline that is currently running you\ncan and it's just python right so I can\njust make visualizations kind of debug\nand see what's going\ngoing on in here\num the workflow also takes you through\nhow to iterate through it so once you're\nhappy with a kind of working flight\nworkflow you can add the additional\nbells and whistles that we all know and\nlove and flight like caching retries\njust simply refactoring your code with\nadditional parameters\nand then finally an example to\nparallelize your workflows with map\ntasks\num so again all of this is running with\npi flight run in uh isolated sandbox\nthat it that Union AI spins up for you\nhandles for you so you don't have to\num\ndo anything yourself really and uh you\nknow\nthere are a lot of things that we can\niterate on here\num I don't want to make any promises but\nfor example uh Docker is not like\nbuilding your own Docker images and all\nthat like um registering workflows with\na larger project is you know currently\nnot supported on this environment but\num you know stay tuned and yeah I'm\nexcited to see what we'll build off of\nthis\nthat's awesome fantastic so obviously\nthat's a Sandbox not a production\nenvironment uh what are the other\nlimitations of the sandbox\nuh I I don't know what the exact\nparameters are but you know it's not GPU\nenabled\num you'll only be able to use you know\nI think two or four CPUs on it so uh\njust just to be aware if you're if\nyou're noodling around in there also you\nknow as a user we can enforce like the\nkind of data you use in there but you\nreally shouldn't be using kind of\nuh proprietary or sensitive data in the\nsandbox\nfantastic great I mean I I love that\nconcept it's so easy to get started now\nso fantastic news thank you so much if\nthere are any more questions about this\nI got to speed up a little bit because\nwe're running towards the uh top of the\nhour if there are more questions we can\ntake them offline as well on the slack\nchannel is there a specific area on\nslack where we can talk about the\nsandbox the new Sandbox or it should be\nthis is more the general section is or\nshould we have a Sandbox section and\nsounds like\noh yeah that's a good question I uh\nyeah I mean I think and I think General\nwould be fine if if there's uh demand\nfor a dedicated Channel I think we can\nspin it up real quick cool let's do that\nawesome Niels thank you so much"
    },
    {
        "title": "Flyte Community Updates 025 featuring KubeCon+CloudNativeConNA2022 recap & talks with contributors",
        "transcript": "okay uh let's kick it off and uh welcome\nto the flight Community sink November\n1st it's already November it's crazy my\nname is Martin Stein I'm with Union Ai\nand I'm your host today and as always a\nfew housekeeping notes we record the\nsession and we're gonna share it online\nlater also flight is a fast and quickly\ngrowing Community if you want to join\nthere are more links later at the end of\nthis presentation on conversation but\nyou can also go to flight.org and see\nthe slack Channel and join us there cool\nlet's take a look at the agenda\ntoday is actually a pretty uh short\nagenda but we have focus on the\ncommunity so we're going to talk about\nuh flight being at kubcon that's I think\na very interesting take there uh Dan is\ngoing to share some Impressions there at\nleast more or two I think then uh then\nwe talk with our contributors of the\nmonth I think that's going to be very\ninteresting it's going to be the main\npart of the conversation today is really\nuh hearing from the community\num we also talk with I think Ryan who is\na participant in the hacktoberfest as a\ncontributor and last but not least we\nhave our Neil spantilan show the flight\nsandbox hosted sandbox not just the\nsandbox that you install on the machine\nbut a host sandbox and what's up with\nthat so that's the problem I do believe\nthat we're going to be done within\nprobably like 20 minutes maybe less\nmaybe more depends on you know how deep\nwe go but\num we can actually so I see in case and\ntexting me here's something\nand uh to go over his\nyes so so then feel free you saw the\nmessage there as well if you want to go\nover that we can do this too so all\nright cool let's kick it off let's uh\nget started with the community updates\nthanks slightly peace\num so then we went uh what we but you\nand a few other team members went to\nkubecon uh last week in Detroit it's a\ncook corn without native North America I\nthink officially it was about six to\nseven thousand people uh who were there\nin person and probably the same amount\nof people who were there virtually we\nhad a booth there flight you can see the\nphoto here Les Mitchell and I feel like\nthat was the first time flight to to be\nrepresented at such a show it's a little\nbit of a stretch because it's a complete\ninfress show and uh not very strong on\nthe data I'm outside or actually and I\nam outside but I you know we felt like\nit was you know probably good good\nlearning experience to go there anyway\nso\num then please uh introduce yourself\nreal quick and tell us about your\nimpressions at coupon\nabsolutely um so I'm my name is Dan for\nthose of you who don't know me I'm a\nback-end engineer with the Union team\num I was one of four team members that\nwent to kubecon this last week from the\nunion side\num just start out by saying it was was\nan absolutely great time we saw a lot of\nexisting users um of flight and it's\nnice how he's put a face to a name you\nknow we work on slack all the time and\nsome have pictures some have don't\num but but it's always great to kind of\nsee people in person get to talk through\nthings and we got to talk to a lot of\nother people to it and kind of discuss\nhow flight may fit into their workflow\nand that was just a great experience\num the small recap two kind of just\noverarching things that we noted\nthroughout the the conference first off\nwe saw a central theme that that there\nis a need for flight\nwe have a chance to attend a number of\ndifferent talks spoke with lots of\ndifferent attendees and kind of saw this\nincreasing importance of cloud native\ndata orchestration specifically like Ai\nand mo workloads at massive scale and I\nthink at the union side and working on\nflight day to day we kind of lose sight\nof a lot of different things one we've\nbeen working on this problem for for a\nvery long time but you go to a\nconference like this and you see that\nthere are a lot of organizations a lot\nof companies where this really is in its\ninfancy and they're kind of just testing\ntools and and there's a lot of\nuncertainty there and tooling and best\npractices\nEtc\num but we feel that there's a lot to\ngain here there's a you know a lot of\nCrossroads that that flight can make\nit's Deployable on a lot of different\nscale and has utility in a lot of\ndifferent environments\num secondly it was great to hear people\nkind of Express their opinions that\nflight does a lot of things right flight\ntends to be opinionated about certain\nthings for example versioning workflows\nand tasks to track data lineages and\nensuring reproducibility of executions\nhaving strongly typed input and output\nvariables\num and we're talking to people they seem\nto like these features they've tried\nother tooling there are significant pain\npoints that they ran into and still kind\nof this feedback Beyond flattering our\negos of course\num it's great to hear you know\nvalidation and reassurance that that\nflight is doing a lot of different\nthings correctly and seems to help a lot\nof people\num so just to summarize kubecon great\nexperience we're thankful for all the\ngreat people we got to talk to and look\nforward to do kind of more of this stuff\nin the future\num I could pivot over and talk very\nbriefly about the performance RFC\num if we'd like to\nyep sounds great\num so uh\nmy work for hopefully the rest of this\nyear is kind of focusing on improving\nthe performance of flight propeller and\nmaking that more observable as this is\ndeployed at larger and larger scales\npeople are increasingly concerned about\nhey you know what is what is flight\ndoing where is this overhead\num how can we reduce that\num and right now I mean flight does a\nfantastic job at amortizing\num a lot of\noverhead of you know parallel execution\nof tasks on cloud native\num things but we really want to be a\nlittle bit more transparent into this\nmake the performance more observable and\nso I wrote up an RFC that that\num as I was told this morning reads a\nlittle bit like a PhD thesis so you know\nnot not for the faint of heart but uh\nbut we'll open that to the community\nhere hopefully later today and um a\ncouple ideas about just how you know we\ncan make performance\num available to end users\num kind of trying to to um\nprovide actionable information so that\nyou know you can look into your workflow\nsay here's this I have a concern with\nhow this is running I know exactly\nwhat's happening\num and and should be should be pretty\nnice\num yeah\nthat's fantastic so so then where can we\nI mean obviously the there's uh we can\nshare this and people can read through\nthis I assume\nyes I was just going to post it on the\nfeature discussions Channel um I think\nthat's where we typically do these\nthings on our slack if there's somewhere\nelse we want to post this as well more\nthan happy to kind of make this\navailable wherever\nokay cool and what's the time frame for\nthat uh\nI've started on a little bit of the\n[Music]\nwork and yeah hopefully get this I'm not\ngonna say it wrapped up it's certainly\ngoing to be a very iterative process\nabout doing this correctly\num but but yeah uh\nmonth and a half or so two months\nfantastic that's cool very good okay so\nif there's any feedback I would say\ncommunity-wise uh you know who to speak\nto uh it's Dan and uh you can actually\nyou know easily uh take a look at the\nflat repo uh and uh go to the RFC\nsection and uh find out what's going on\nfantastic Dan thanks a lot for that for\nthe update I think the coupon report is\nvery very interesting I also look\nforward to the performance RFC\num let's move on and uh say hello to our\ncontributors So today we're gonna\nactually spend a little more time with\ncontributors we're going to do a little\nbit less presentations talk a little bit\nmore about uh who's contributing uh we\nhave Eileen Nicholas and Justin and then\nuh Ryan here at some of the folks are\nlive right now so we're going to speak a\nlittle bit and uh who's uh who's not\nlive uh we're gonna follow up later\nEileen thanks a lot uh for joining us I\nthink last time I already mentioned you\nthis time we had nephew is I'm super\nhappy so please\num if you could unmute yourself and\nintroduce yourself and tell us a little\nbit about what you're doing and what\nyou're doing in pajama\nyeah definitely yeah it's super like\nthanks for having me I know I responded\na little bit late to the invite but I\ndefinitely appreciate it\num\nyeah so um yeah my name is Eileen I'm a\nsoftware engineer at pajama\num and so at pajama we um you know we\nwork on developing a lot of complex\nanalysis\num using like remote sensing data to\nunderstand what's going on in like\nforests around the earth\num like the amount of biomass that's\nthere in the forest um the amount of\nlike impact and progress there are in\ndifferent types of forest projects with\nthe overall goal of providing more\ntransparency and accountability in the\ncarbon Market\num so as you can imagine flight is\nreally really useful to us um because it\nallows us to kind of coordinate all\nthese different intermittent processes\nfrom just like ingesting all of the\nremote sensing data that we need to feed\nour machine learning and other\nDownstream processes\num to providing a platform where we can\ndo like more complex Baseline analysis\nor um you know uncertainty calculations\num and yeah any of those other like\nDownstream post-processing steps um\nflight ended up being like a really\nperfect platform um to host all of those\ndifferent sorts of pieces\num especially because\num you know one tends to kind of like\nfeed the other and there can be like\ndifferent possible paths that you take\ndepending on like the type of project or\nthe type of analysis\num so yeah and then also as you can\nimagine\num a lot of those analysis require a lot\nof like really kind of heavy computes\num so we're kind of interested in\ngetting you know a little bit more\ninsight into what's going on in the\nflight pods that are running those\nexecutions kind of Beyond to like what\nwe're already getting with Prometheus\num on that note the RFC that Dan was\njust talking about sounds like really\nespecially interesting\num but yeah so as we're looking at this\nlike we already use data dog at pajama\nfor like some of our other systems so\nwe're looking at um seeing if we could\nset up datadox so that it could um you\nknow start giving us some of those more\nlike in-depth metrics and APM tracing\nand some of the other nice things that\ndatadog is already giving us\num so the like just most basic thing\nthat we needed in order to do that in\norder to give datadog that level of\ninsight into the pods that we're\nactually doing the executions uh we\npretty much just needed to add AIDS like\nnamed mounted volume to the pods and we\ntried doing that through\num like adding it to the Pod template\nand then we found out when we tried\napplying that that didn't really seem to\nbe supported and it looked like it was\ngetting overwritten somewhere\nso that's when we decided to kind of\nlike dive in and figure out how the pods\nwere being constructed\num and then um we definitely like I\nspent a whole bunch of time talking with\num with Dan about this and kind of\nfigured out a more generalized approach\nso that not only like our mounted\nvolumes supported\num uh through the the Pod template but\nit's kind of now something that you can\nuse more generally to do a little bit\nmore customization into the Pod\num\nif if I uh this was my first\ncontribution so if I remember everything\num of the changes that we made we kind\nof like\num in terms of the plot construction I I\nthink I remember that\num the some of the stuff in the default\ncontainer were kind of being overwritten\nby what was in the primary container and\nthen some of those configurations were\napplied kind of later at the end of the\nPod construction process so we kind of\nmoved all that Upstream so that the\ndefault container could merge into the\nprimary container\num a little bit earlier in the plot\nconstruction process so um any like more\nGeneral configurations could kind of\njust all be accounted for at that point\num so yeah that that was the change\num and yeah otherwise I just wanted to\nsay that like I really only started\nlearning like how to use flight like a\nfew months ago earlier this year um and\nthis is my first contribution so\ndefinitely like as a thanks to Dan for\nlike taking the time to pair with me and\nlike kind of like help me understand\num how all this stuff worked and um yeah\nyou guys are all like a really awesome\nCommunity to work with so yeah thanks so\nmuch it's awesome having this change\nfantastic and I just want to give the\nthe work to Dan because you worked with\nyou so closely and he introduced you\nlast time so then a quick question I\nassume that\num that contribution is valuable for\nother users as well right I mean that\nhas a broad application\nyeah yeah absolutely so uh default pod\ntemplates as far as configuration goes\nyou know we were having configuration\nset on the case plugin individually and\nso default pod templates you can create\na pod template in a namespace and then\nevery single pod that flight starts uses\nthat pod template as the base\nconfiguration for it so it supports\nevery single option is maintainable\nbetween kubernetes versions but\nadditionally Eileen's contribution said\nlet's configure not just the Pod but\nindividual containers in there as well\nand so you can take any any\nconfiguration option for a container and\nsay apply this to every container within\nthat pod or just the primary container\nso so yeah we've already seen a number\nof different community members use it in\ndifferent ways so a very exciting\ncontribution we really appreciate it\nawesome Alina that's fantastic also I\nmean that that you know Bex also a\nfollow-up question because it just got\ninto flights so\num tell us a little bit maybe what's the\nHighlight uh what what helped you to get\nin besides staying obviously that's a\nbig help but you know what went well\nwell for you and and where can we\nimproved just to be a little bit\nself-critical here as well just like\ngenerally with with my our experience\nwith life in your learning process of\nmaybe you know just you know getting up\nto speed with light\nyeah gotcha\num\nwell I I feel like\nhmm\nnothing about it because I I feel like\nit was\num well one thing that was really\nhelpful for us is that somebody else\nwithin the team had a lot of experience\nwith the flight so I was able to connect\nwith him first and that kind of like\nmade it a little bit easier for me to\nlike you know hop in on the the slack\ncommunity and stuff\num but I I really felt like\num it's really nice how responsive\neverybody is like I feel like I can just\npost a question and then right away\nthey'll be like a bunch of different\npeople responding on it like I I feel\nlike with a lot of big slack communities\nsometimes there's the problem where like\nonce it gets very big someone posts a\nquestion and there's kind of the\ndiffusion of responsibility and no one\nresponds and I feel like with the flight\nCommunity there's\num I I usually get like multiple\nresponses\num as as well as just very timely\nresponses\num\nand uh yeah so that's that was\ndefinitely like really helpful and\nreally encouraging for getting\num up to speed I think I guess thinking\nabout it now the um maybe couple of\nthings that I I think I already talked\num with Dan about this like during the\nprocess is like\num getting I think there are a few\nthings like getting flight\num running in like a local cluster so\nthat I could test everything end to end\nand getting the um like the front end so\nthat I could um connect to the flight\nconsole was something that was like not\nreally documented and like Dan kind of\nlike showed me the tricks of the trade\nthere and um how to get everything set\nup so I thought that worked\num so yeah having um maybe an easier way\nto an easier option to like get that\nworking because the console is so useful\nfor local debugging when you're using a\nlocal cluster\num or or just like having it documented\nsomewhere I think like either one would\nbe\num yeah really helpful and really nice\num and then otherwise I feel like yeah\nme coming in as a contributor\num just like figuring out the um like\nthe pr process and like who'd ad and\neverything like that um I wouldn't have\nknown where where to go there if if it\nweren't for Dan like leading the charge\num so yeah I think those are the things\nthat I like wouldn't um really have\nknown but otherwise got like you know\nreally really great support from you\nguys so\nfantastic I love it thank you so much\nand I feel like for us um I don't know\nKeith and has raised his hand for us uh\nreal quick I think there's a lot of\ninspiration there to to understand you\nknow where we have actually our\num you know a hard more difficult areas\nto get people you know in the learning\nprocess to to from the discovery part\ninto the learning part and then into the\nbuild part and basically applying all of\nthose things so documentation is really\nimportant so we put a big focus on that\ncase and you raised in it\nyeah uh Eileen firstly thank you for\nsharing and that that contribution is\njust amazing so thank you uh\nuh so I uh I heard you like the\ncontribution and we actually this is\nmaybe a spoiler for e and others there\nsomething's coming out soon that'll help\nyou really test things faster and that's\nour goal and we would love feedback on\nit like we love that hey this works or\nthis doesn't work um and hopefully that\nencourages you to to another\ncontribution\num so keep keep your eyes open and and\nhopefully help us improve the process\num for giving you already feedback\nuh one one more thing I I read your\nstuff as like software worked slowly you\nknow turning to rust I I completely\nbelieve that's true\nso we are in fact thinking of something\ninteresting with rust uh let me know if\nyou are interested in contributing uh DM\nme please so thank you\nfantastic and for those who don't uh I\njust want to introduce keita too because\nI think you forgot I'm going to tell so\nkathan is obviously co-founder Union and\nTSC chair here with flight and uh is one\nof the founding members obviously and\nthen the core original engineering team\ncreating flight\ncool good let's uh I mean thank you so\nmuch so let's see if we can get uh your\ncontribution do you support maybe on the\nrough side if there's something that\ncase has in mind maybe there's an area\nto collaborate there we'll see uh thanks\nso much let's uh say hello to Nicholas I\nthink Nicholas should be online here\njust gonna check in Nicholas are you\nthere\nI don't see him okay so then\num but I can give you a little bit of I\nwork with Nick uh okay cool let's\ncombine this so let's go to the next\nslide which is You Justin so that that's\nthat's your slide just in your work\nethnic so\num same thing here please obviously I\nmute yourself and introduce yourself and\ntell us a little bit about you know what\nyour contribution is\nyeah so I like I said I work with Nick\nuh lefaso as part of the methane sad uh\nuh you know team and um just a little\nbackground about what methane said is\nit's a it's a subsidiary of\nenvironmental defense fund uh we're\nlaunching a satellite next year in uh\nthat's going to detect methane emissions\nfrom oil and gas agricultural sources\num so we have a lot of uh need to\nprocess raw satellite data and we're\nusing uh flight to do that so\num and we have different users there's\ngoing to be basically the methane sat uh\nemployees that are going to be\nprocessing the data from there's five\nfive different stages of processing the\ndata to make it available for\nconsumption to the public\num so we're going to be processing that\ndata in a kind of a production\nenvironment and we have a lot of\ncontributions from scientists from\nHarvard and other places\num who are refining algorithms and\nwhatnot to to do that processing so we\nhave a we're probably going to run you\nknow a\nnot a vanilla flight deployment and that\nwill will have multiple flight uh\ninterfaces whether you're running the\nthe real production data processing or\nyou're running some Dev and test type of\nstuff\num so that's a little bit about uh what\nwe're using it for\num the the contribution says oh I should\npreface it with we're running on Google\nCloud uh we have an agreement with gcp\nso we're that is our our Cloud uh\ndeployment uh environment\num and you know Google cloud has a\nlittle\nsome nuances I guess idiosyncrasies that\nuh you know the flight authentication\nwith Google uh out of the box is not\nstreamlined I guess is what you would\nsay\num so there's obviously there's two\ndifferent authentication mechanisms in\nflight there's a user or and then\nthere's uh you know to the the UI\nconsole and then there's the\nuh the flight admin and oauth uh\nfor the actual you know flight propeller\nand flight control to communicate with\nflight admin right so\nthe what I was working on was uh a way\nto basically promote the flight packages\nfrom a development environment where we\nhave just a release cycle\num and we promote the flight packages\nthemselves up into a staging or a\nproduction environment\num where we have flight running and I\nwas just looking for a way to\num you know use flight control to\nregister those packages and this as I\njust stumbled upon uh the the fact that\nthe the flight uh Helm chart in values\nand actual go code under the covers was\nusing just a a hashed password\num that was if you don't change it\nyou're exposed uh essentially right so\num anyway I quickly reached out to uh Yi\nand he was extremely responsive\num and just to make sure I wasn't\nmissing something or doing something\nincorrectly uh so yeah and and you know\nhe he led me down the the path to uh\nchanged that uh default password if you\nwill\num and we you know propagated that\nsolution through our flight uh\ninstallations and uh yeah that's pretty\nmuch all there is to it um foreign\nlook I mean it's just like I just want\nto bring Yi in as well I think you hear\nif you if you have a chance to speak up\nmaybe we can hear from your side a\nlittle bit about the discoveries as\nvulnerability and the general uh\nfeedback about them uh the methods at\nleast is\nhey uh yeah I just wanted to uh say that\nthank you again to Justin for bringing\nit up um\nuh this was something that we uh kind of\naccidentally uh glossed over I guess we\nshould have uh expected this to be a\nmore widely\nuh known use case\num so it doesn't affect people who are\nusing third-party\nauthorization servers This only affects\nthe internal admin authorization server\nbut\num yeah once once if you there's a I\nthink we added a little script to\num hash and hash whatever password you\nchoose and then you put that into uh you\nput that into the admin configuration\nyeah I think the our environment is not\nlike as I said the vanilla environment\nright um I think it's not it's off the\nbeaten path most people are not using it\nthe way that we are\num so you know out of the box if you\njust install it with you know off to uh\nflight you know authorization uh flight\nadmin authorization enabled\num out of the box if you don't pay\nenough attention and change that uh\ndefault credential you're exposed that\nwas what uh the problem was for us and\nin Google\num they don't really have a way so we\nhave identity aware proxy on gcp to\nhandle the user authentication but\nbecause of the way flight admin uh is\nimplemented using grpc that does not\nplay well with Google's uh load\nbalancers so\nyou know we still had to have uh the\nflight admin off enabled\num but yeah we it just wasn't obvious to\nus either that the the password uh\nneeded to be changed uh or or what it\nwas right so it's uh basically just a\nlearning uh process and I think most\npeople are using either OCTA or keep\nfolk or AWS\num and we're a non-profit so paying for\na separate pay for OCTA is really not an\noption for us\num so it's one of those things where\nlike flight Plus Google there's a few\nlittle extra hurdles that you have to\njump through\nbut you thank you for helping out though\num and and addressing like the our in\nour situation immediately appreciate it\nthat's awesome thanks e and uh Justin\nthanks a lot for\num you know it's a lot of things very\nimportant\num case and said it earlier here as well\nI feel like security is a big part so we\nwe jump on that immediately and I feel\nlike it's really important for for the\ncommunity to you know\num take a look at this too I feel like\nthat's the most important one of the\nmost important areas specifically\nnovelty with your use case just I feel\nlike uh if I remember correctly what\nNicholas said um uh half a year ago that\nyou also have on the deployment side you\nhave like a very big deployment you use\nlike quite a lot of compute power on gcp\nis that correct have you scaled this up\nat this I mean in the meanwhile over the\nlast half year or what's the current\nstatus for you guys there yeah so up\nuntil the beginning this year uh you\nknow\nwe hadn't been able to like really run\nyou know data pipelines for processing\ndata because we did the satellite hasn't\nit won't launch until next year\num but we do gcp does have a quota uh as\nmost Cloud providers do and you know\nthey're a little bit strapped for the\nthe CPU resources that we need\num and it's region based right so\num you know they allow us uh they give\nus allotment or a quota for certain CPUs\nin certain regions\num for certain project or environments\nso\num\nthe good is is we we just recently this\nmonth have started processing data\num but it's not from the satellite\nitself it's from the same sensors\nmounted to an aircraft so we call it\nmethane air and uh yeah we're scaling up\nto uh 800 900 node clusters\nuh on gke\num in in Google and uh we've been\nrunning data uh ever since honestly it's\nit's this last seven days has been when\nwe first started getting data in right\nit's very complicated in terms of\napplying the algorithms uh from the the\nscientists uh give to us and US\nintegrating it into flight and dealing\nwith uh gcp's quota limitations and um\nyeah so there's quite a lot for us to do\nbut we have we have started making some\nsome great progress and yeah we run like\nI said 800 nodes for a few hours\num and that's just for you know a short\ntime slice window of data when the\nsatellite goes up it's going to be you\nknow uh continuous uh influx of data\nthat we're going to have to uh process\nso we're definitely\nhead in the right direction we'll get a\nlot more work to do yeah yeah and it\nsounds like um with with the latest uh\nunfortunate uh developments in the north\nthe sea with the methane emission of uh\nin the pipeline there I'm pretty sure\nyou probably\num combed over some uh spatial imagery\nthere as well to see what's going on\nyeah I mean we not yet not until the\nsatellites up in the air\num but like right now we're just over\nthe U.S airspace uh with with the\naircraft\num for now but for sure the need for\nmeasuring and and\nlike holding people uh it's actually in\nthe best interest of of uh oil and gas\nCommunity to stop leaks uh as well as\nhis best interest for the the planet at\nlarge uh to stop the leaks so it'll be a\nit'll be a great thing uh when we get\neverything up and running and uh flights\nhelping us to do it and and we have a\nquestion from Eileen Eileen if you I'm\nreading you know data insurance from\nSandler is going to be continuous with\nno gaps between ground contacts question\nmark\num so I don't really I can't speak to\nall of it but we basically have control\nof where we aim the satellite and for\nhow long we aim that uh sensor uh aim\nthe sensors on the satellite and for how\nlong so what we usually get is uh maybe\n30 time slots per day and we have to\npick and choose where we will uh\nuh scan I I guess is the right word\num so yeah it's not going to be a\ncontinuous it'll be a continuously\ningesting data from the the ground uh\nsystems satellites uh and they have I'm\nsorry the ground stations\num so we'll be getting all that data\nfrom them but it's not like a you know\nthere definitely are like Windows so we\nhave a whole another side of you know\nnot the data processing side but the uh\ncommand and control side for the\nsatellite that plans and schedules what\nwhat the flight path is and where we're\ngonna uh scan you know there's a lot of\nthings involved including weather\num it has to be a relatively clear day\nfor us to get uh good readings\num you know so yeah it's a good question\nbut it's\nyou can see it's there's a lot of\ncomplexity in the answer\nyeah no fantastic so so Justin thanks a\nlot for\num you know contributing also uh\nstepping in here for Nicholas a little\nbit more we would love to report about\nand talk to you in the future once you\nget you know potentially your data\nground station data from the satellite\nuh in a way that we can see with how the\nsystem works that will be fantastic so\nwe'll bring you back at that time yeah\nand there's a we're going to have a UI\nall the data is going to be publicly\navailable once we've edit\num you know so it's meant for public\nconsumption\num and if you go up one slide the next\nslide\nhe does have a uh a presentation that he\ngave\num you know that probably is a little\nbit more professional uh than the than\nthe intro that I just gave about what\nwe're doing with it\nI think the intro was pretty good so\nthank you so much yeah so there's\nactually\num a YouTube video about methane sets\ntoo if you haven't seen this check out\nstuff like YouTube channel it's there's\na lot of information uh in there I think\nit's pretty pretty amazing also I think\nwhat pajama does I mean is also super\namazing I love those projects that\nactually have a scale that is like\nbeyond what you usually do and so\nfantastic great thanks a lot Justin so\nlet's move on and say hi to to Ryan and\nI thought he's online yes hi Ryan please\nsubmute yourself and introduce yourself\nand tell us a little bit about uh what\nyou're doing at the Lloyd spanking group\nand uh you're a hacktoberfest flight\ncontributions because they were more\nthan just ones or other stages years\ncool um thanks for inviting me guys can\nyou hear me sorry uh\num so yeah I I'm senior scientists a\nmachine learning engineer at Lloyd's\nbanking group um it's based in the UK\nLondon uh but they've got different uh\ndifferent kind of um headquarters all\nover the UK so we kind of you're very\nthose who don't know not aware of laws\nbanking group they're very big financial\ninstitution we provide services to uh\nabout 30 million customers in the UK\nmeaning the retail and commercial space\nso my role in as a senior scientists is\nbasically to provide um sort of help\nwith like sort of providing\num certain machine learning services to\nrest the bank I'm also doing a lot of\nwork in sort of developing pipelines\nwithin Lloyds and helping other\ndepartments example\num so basically we're not using I'm not\nusing flight specifically in Lloyd's um\nit's just because we've got a lot of\nLegacy systems getting new kind of\ntooling into the bank is quite difficult\num also we also you know kind of hybrid\non-prem and\num it's yeah there's a lot of\nbureaucracy basically so so the idea is\nuh how I came across flight was because\nwe are actually looking at using Pandera\nuh for some of our data validation\nFinance checks within our pipeline\num so I was researching with Pandora and\nI came across this YouTube video by\nNiels\num I think he delivered one on um in\npycon I think and it was like uh one on\nintegrating Panda or flight and\nstreamlip and then I thought oh flight\nlooks quite cool because the stuff\nyou're doing kind of manually building\non a different kind of tasks Etc uh\nflight does all that for free\num you can have a nice UI where you can\nactually visualize uh how a workflow and\ntag is kind of integrated together\num so I kind of started looking at slide\na bit more\num this was like back in I think\nSeptember just before uh hacktoberfest\num I did like a few tutorials on flight\nI quite liked it I normally do\nparticipate in hacktoberfest every year\num and I then suddenly sort of fly\nprivate in this special kind of Swag\nkind of thing so I said okay let me just\nuh\num maybe I can maybe I can contribute to\nit like I've not never used flight\nbefore but let me just see you know let\nme just ping uh the community and see\nwhat what do you think um I was quite\nlike surprised it's quite an open\ncommunity and um I've contributed to\ncover quite a few other kind of projects\nas well like you know pandas and\num Facebook profit they're quite nice as\nwell but I think the whole fact the the\nwhole kind of organization of this with\nyou know the slack group and a specific\nactive refers to the dedicated channel\nis quite nice and enable me to get uh\nstarted quite quickly uh understand like\nwhat issues you know um to work on for\nexample nicely labeled with the hack to\nprofess label Etc\num so basically yeah so if I just\nbriefly go over like my contributions so\num I kind of looked through the number\nof issues that were being like taking\ntaken quite quickly by a number of\npeople\num but I wanted to kind of\num merge together my interests with like\nflights so like I was going just like\ndistributed computing so I thought you\nknow vix was a good kind of ticket to\nlook at maybe I can you know do some\nkind of integration with bakes and then\nI think there's a ticket by Kate and\nyou'd raised on like uh supporting bikes\nas a native type\num and so I try to learn about like you\nknow how to kind of create create a\nplug-in for example and create like a\nstructured data set format so now\nbasically we the any user should not my\nPR's been marked in so any user should\nnot be able to install that plug-in and\nthen create like a Vex type for example\nin a task and then pass it to different\ntasks\num there there's there's a few other\nissues that I've kind of raised as well\nhas a subsequent kind of work on this so\nat the moment it's only serializes to\nparquet but I think we could support HD\nhtf5 as well as Arrow as well for larger\ndata sets\num and also uh this would possibly not\num so at the moment we are basically\nonly supporting like\num a single kind of we're not actually\nsupporting like um passing in a list of\ndata frames so we want to like also\nsupport that as well like multiple\nchunks so there's another issue for that\nas well\num so I think that the next so that that\none will also there was a lot of like\nreviews on that and a strict review\nprocess right thanks to uh samita and um\num I think Kevin as well\num the next one was on a type\nTransformer so this was uh possibly the\ntrickiest of the lot because\num I think I initially started working\nit I think\num\nthe the there's there's a number of ways\nyou could do this basically uh within\nthe tensorflow docs so uh the way you\ncan kind of serialize to a tensorflow\nrecord is\num you can either use a tensor example\ntype or you can use a technical data\ndata set type\num and basically I should have probably\num uh conferred with some of the team\nregarding what their preferred design\nwould be so I initially started off by\nlike just creating a a type transform\nTransformer from tensorflow example\num and then basically created that but\nthen I think\num some E10 and you'll be having\ndiscussions thought that'd be better to\nsupport\num just extending a uh the native kind\nof flight file type to a tensor record\ntype a file\num and then basically letting that\nhandle the serialization and\ndeserialization to tens flow record\num so I'm kind of working on that at the\nmoment I think there's another kind of\nparallel discussion on supporting tensor\ndata the data set separately uh so I\nthink yes I'm currently working on that\nthe next one is just basically like\nfixing a few mypayers there's an issue\non like um basically in the whole code\nbase there's a lot of my Pi errors as I\nspeak just yeah I was working Kevin on\nthat to fix a few areas for like the\nthose are raising like incompatible\ntypes example in the day in the code\nbase and\num I couldn't go through all of them\nbecause they would just do much yeah\num and the last one is basically just uh\nso I think um this this the last bit was\nalso quite useful because I think\num because my my kind of opinions like\nthe tutorial section could possibly\ninclude a lot more things for new users\num I think it's probably a quite bad\nmoment because as a new user was coming\nI was coming in and there's probably a\nfew\num there was not that many tutorials on\nlike\num\ndifferent kind of use cases within\nmachine learning for example there was\none on like um you know like um sort of\nhouse pricing and diabetes for example\nthe NLP one is a good one to have they\ncould be possibly run on like uh how you\ncould integrate like time there and\nstream it as well\num I suppose like um just as a kind of\nuser coming in to seeing how you can\nintegrate different things but yeah I\nwas working on this tutorial\num basically what I did was I kind of\njust uh mimic the Jensen kind of\ndocumentation\num to basically uh create a workflow\nwhich reads in a sample a data set uh\ncreate a word to back uh trainer word to\nback model\num and also a topic model do some manual\npre-processing the DSs so that's a\nseparate task\num and then the the models would be like\nthe three of the models would be two\nseparate classes they're serialized and\nthey're really ready for the next task\nand then we print out like the list of\ntopics and like similar words and they\nall kind of like change get in the\nworkflow\num and then it produced like a beautiful\ndag so uh on the outlets I've kind of\nwritten up a tutorial on the map I think\nit's almost there\num there's been quite a few like um back\nand forth with the reviews to get the um\num descriptions kind of correct but\nthat's kind of almost there so it should\nbe going in quite soon\num yeah\num\nyeah I think yeah that's basically all\nI've uh done and I guess like yeah I\nalso did you enjoy competing the other\nopen source libraries in general and I\ndo my interest also like uh strong in\nAWS as well in mlops\num I'll be paying to also contributed\nmore until like so the AWS kind of\nintegration with light and additional\nplugins that could go in for example um\nmaybe I could discuss how the team in\nthe future\num yeah I think that's pretty much all I\nhave really that's all it was quite\nawesome no it's fantastic thank you so\nmuch I mean this is just uh uh you know\nfantastic to see that uh and absolutely\nyou're absolutely right on the tutorial\nside specifically Now using you know you\nknow an NLP example of work to back or\nyou know other stuff this is fantastic\nto see you know how you can use the\nplatform there so totally appreciated an\nhour so and I think we should probably\nhave a conversation bring you back and\nand take a little bit more of a look\ninto\num because also you come from a data\nscience perspective and I feel like data\nscientists have a very unique\nperspective about you know moving from a\nJupiter workflow to a I mean let's say\nworkflow quotation marks to a real\nworkflow that's more functional I think\nthose are topics that I would love to\ndiscuss you so whenever you're ready we\ntalk more about data science now yeah\nokay let's do that awesome thank you so\nmuch Ryan that's fantastic so for slides\nlinks and resources uh let's go to the\nnext one\num obviously we have our office hours\njust want to remind everyone one with\nEthan what time with Dave with Katrina\nand uh with samita the late night and\nEmma loves uh hours so take advantage of\nthem I think they're super helpful\nincredibly helpful and then Outlook\ntowards the next open sourcing it's\nNovember 15th with Kelvin leather from\nEmbark Veterinary so if you haven't\nbooked it take the calendar invite if\nyou want to look at the resource\nspecifically the methane set video that\nI mentioned earlier go to the YouTube\nchannel and also if you want to stay up\nto date just subscribe to the newsletter\nwith that I just want to say thanks to\neach and everyone here I think this was\nsuper interesting looking forward to uh\nto seeing you all on November 15th take\ncare bye"
    },
    {
        "title": "Flyte @ Lacuna.ai - Leveraging Flyte for Transportation Data Science",
        "transcript": "now we're going to the next part to the\npresentation to the fireside chat so\nthis is next slide this is about\num Laguna AI I had the pleasure to to\nget to know when she who's gonna\nactually give us a little bit of an\nintro about what Laguna AI is and what\nthey have done and then we're going to\nhave a little bit of a conversation\nabout what led them to to use flight\noppositely using flight what were the\nproblems that they saw and they needed\nto have a solution with and how did they\nand their team react\num you know going from potentially you\nknow different approaches to it to an\nintegrated platform like flight but\nplease uh welcome maybe you want to\nunmute yourself introduce yourself and\ntell us a little bit about what what\nLaguna does and what view role that like\nuna is\num yeah well thanks Martin\num you know thanks for having me really\num it's a pleasure to be able to speak\nin front of the community\nso really quick uh about myself\num you know I'm a machine learning\nengineer at Lacuna and uh we try to\napply machine learning where it works\nwell you know I think there's a narrow\nscope where you can apply it you know\nefficiently in the transportation space\num and uh you know we mainly work with\ntransportation and general policy\num so you know as also as part of you\nknow um aside from being an engineer at\nLaguna I do a fair amount of data\nscience engineering and then to a\ncertain extent uh you know data\nengineering\nso I have a lot of I also have a lot of\ninterest in in doing devops uh you know\nuh\nworked on that you know onto it\num\nto a limited capacity in the past but\nit's something that I've always been\nvery interested in\num next slide please\num yes so what is Lacuna um you know\nbasically we are a transportation\ntechnology company uh working to\nmodernize\num essentially uh public right away and\nhow that's managed\num next cloud\num yeah basically our product uh provide\nuh our products provide customers with\nuh digital policy tools uh to help them\nto operationalize stereo infrastructure\nand dynamically manage streets for\nessentially different modes of\ntransportation\nuh and also will help them to manage\ntheir Kirk spaces\num so we try to you know help our\ncustomers to drive real economic\nenvironmental and you know Equitable\nchange uh through Transportation I think\nwith our transportation systems becoming\nincreasingly intelligent we envisioned\nthat uh digital policy will be our\nfuture\nso so wouldn't you who are you typical\ncustomers is it cities obviously and uh\nbut who would you consider your typical\ncustomers uh yeah um so you know I think\nwe we kind of sit in in the middle uh\nbetween uh essentially you know\noperators uh those are the\ntransportation providers and the cities\nand and yeah uh could we go to the next\nslide\num I think there's a little nice\nvisualization there so we\num so on one hand you know we we have\nthe operators and those are you know\ne-bike providers they can be taxi\nproviders even scooter providers your\nlast mile as long as your long-haul you\nknow Solutions and the future uh you\nknow drones and robots and then on the\nother hand you know we work with cities\nwe mainly work with cities in the US and\nthose are City level and county level\nDepartment of Transportation\nso these two parties you know they have\ndifferent incentives and different\ninterests and I'll go is is really to\nalign those incentives to achieve you\nknow to basically achieve shared a\npositive outcome\ninteresting and so so just to to get\nmaybe a use case or an example of of\nwhat what Lacuna has achieved can you\ngive us a kind of like one example about\nmaybe one recent uh example that you can\ntalk about what you have accomplished\nyeah sure um so you know I think in\nmajor and metropolitan areas\num we have uh well uh for a couple years\nnow and we have you know these large\nscooter providers\num you know who are essentially using\num uh your your curb spaces your\nsidewalks\num you know for\nparking uh the other Scooters or\nwhatever end up happening is that you\nhave you know all these scooters lying\naround and no one being able to manage\nthem so we help cities uh to integrate\nyou know technology uh sensor-based like\nSolutions through an open standard\nso very similar to UI Union and um uh\nyou know essentially we you know we we\nhelp the cities to be able to manage\nthese providers right and to be able to\ntell them you know where where where are\nthey able to you know Park their devices\num you know what how are they able to\ndeploy their devices where to deploy\nthem and and you know the capacity at\nwhich\num you know they should be providing um\nservice\num you know in certain areas uh you know\nmaybe a little bit more than other areas\num yeah so we very much help\num uh you know cities to manage these\nprivate providers interesting and so\nit's just that we get a little bit of an\nidea of the scale of of information and\ndata and potentially resources can you\ntell us a little bit about\num it's not just like the the two cars\nor something or a small Fleet from\nsomebody this is really a large scale\ntell us a little bit about this game you\nyeah I mean I think a major metropolitan\nareas you know I think you're dealing\nwith uh you know 10 to 20 uh providers\nfor just one uh mode of transportation\nright and these providers they are\nthey're essentially fleets so each\nprovider can have you know thousands of\ndevices and these thousands of devices\nare are you know essentially\num\nproviding information so this is very\nmuch like a big data Internet of things\nyou know type of problem they they're\nproviding a lot of sensor information uh\nyou know back to 12 service and and so\nyou know there is a scale is definitely\na challenge\num you know and and there are\num for the most part\num\nuh real-time information I think that's\nalso you know part of the challenge that\nyou know we had to sort of ingest these\nuh events this sensor data set real time\ninteresting and is it I mean obviously\nthere's a huge amount of information\nflowing through the system has to be\naggregated and processed is is is that\nwas the data Challenge and that\naggregation challenge in the real-time\nchallenge uh or their creation\nchallenges to begin with the the problem\nthat that you saw and then what led you\nto look for a solution like flight or\nwhat was it uh that you were using\nbefore and then looking at a solution\nlike flight what triggered that search\num yeah so I you know I think uh unlike\nmost of the um you know I think uh and\nnow use cases that leverage flight very\nvery well um you know I think we use it\nuh mainly for general purpose data\nscience\nand uh and like you mentioned yeah\nabsolutely I think uh the scale of the\nmassive amounts of data that we need to\ndeal with you know is um there's a huge\nchallenge\num I think the way you know I kind of\nidentified\num all pain points and then sort of\narrived you know to centering our\nefforts around like a workflow platform\nis really\num sort of an intersection of kind of\nmany challenges\num so I I joined Lacuna at early stage\nof kind of uh tackling the data problem\nyou know the data inside of the data\nintelligence level\nand I'm part of a team that's you know\nvery research oriented uh that focuses\non sort of the depth of the problem\nbut at the same time you know when I\njoined there was no tooling essentially\nto be able to access the data\num\nso you know I think one of those needs\nuh you know aside from tackling the data\nright was also that we needed to get\norganized and basically allow us to\ncollaborate you know a lot more uh\neffectively and and avoid situations\nwhere you know we're kind of you know\nrewriting each other's uh code or or\nre-implementing you know methodologies\nthat were no\num and uh as as part of my experience I\nalso you know uh wanted to avoid having\nto kind of hand roll any custom\nsolutions that addresses sort of the\ndevops and infrastructure overhead\num so you know uh to be able to explore\nsort of this massive amounts of you know\nuh iot data right at scale\num we\num you know\nwe've essentially needed to wield our\ndata very well so that we can uh you\nknow tackle the problem both uh you know\nat the depth as well as you know sort of\nthe breadth of\nthis problem space and if you think\nabout it you know I think the the\nproblem that we deal with is you know\nboth deep and complex\num it's a huge solution space that we\nhad to explore\num so if you think about Transportation\nyou know there are the different\ngeographies which you can zoom in and\nzoom out and at the microscopic and\nmicroscope or Copic level essentially\ndifferent problems and then there's a\ntemporal scale you know you had to think\nabout historical data versus you know\nreal-time data and the time scale that\ngoes with that you know daily hourly or\nmonthly\nyeah and then finally I think digital\npolicy is a very nice and field yeah I'm\nsorry\nexcuse me um yeah so digital policy is a\nvery nascent field um I think it's a\nit's a big domain\num you know I think uh it can be\nanything ranging from managing\ncongestion to managing you know your\ncurve spaces\num so you know the I think the solution\nspace is is really huge right and to be\nable to kind of explore that\num in an organized fashion we wanted to\ngo from applying methodology to be able\nto you know apply those methodologies\nover data and what I what I like to call\nis to what I like to call is basically\nmoving into higher order of\nexperimentation to go from you know just\nsmall data sets applying uh no\nmethodologies to be able to experiment\nyou know at the data level\nand and you know I think it's kind of a\ncombination of these challenges that I\nalready identified that kind of led me\nto\num think about you know kind of\ncentering our efforts around our\nworkflow platform interesting and as as\nthe solution space is so gigantic what's\nwhat's your first impetus to to offer\nthe companies first uh go\num just to to look into research areas\nand identify you know potentially you\nknow valuable business uh opportunities\nor was that already clear about what\nsolution could be provided in that vast\nsolution space\nyeah you know I think um uh it's a\nlittle bit both and really there is a\nlot of ambiguity around uh you know\naround data and around methodology and\nand also around\num sort of the use cases that we want to\nbe able to unlock so it's certainly like\na very challenging problem and it can be\nyou know the different levels of\nuncertainty can be you know a little bit\noverwhelming and and really I mean I\nthink\num to kind of untangle that you know we\nreally want to just minimize uh you know\ncomplexity as much as possible\num you know I I think uh sometimes\num sometimes when the use cases are are\nyou know a little bit more clear I think\nyou know we can kind of uh think about\nuh sort of uh\nwhere the what the sort of um\nin that specific space right put that\nuse case like what the advances have\nbeen you know what what's the what sort\nof the capture and knowledge through\njournals and things like that and then\nthink about you know kind of applying\nthat over data\num but there are use cases where you\nknow I think uh it very much come out of\nresearch where hey you know this is\npossible Right like this use case is\npossible does it you know fulfill any\nbusiness needs\num so yeah it's a little bit\num\nthere's a lot of uncertainty and uh you\nknow there's a lot of ambiguity and and\nyou know certainly I think uh uh I think\nI mean I think as data scientists right\nI think the earmark of like a good data\nscientist to be able to manage ambiguity\nessentially I mean you have to have an\ninquisitive mind uh to deal with that I\nthink it's a trademark with all of the\ndata scientists now now let's take a\nlittle bit of a you know time machine\nstep back and when you looked at the\nsolution space and Technologies and\nplatforms\num what was it what were the criteria\nobviously we know some of the criteria\nnow but what kind of the solutions you\nwere looking at and and\nwhat was the most important part here\nfor for building flight\nyeah um you know I think\nso we were at very early stages of uh\ntry to adapt you know or something uh\nbased essentially an opinion a framework\nlike flight\num\nand uh you know I I think uh\nI mean one of the things that um you\nknow I I you know I really wanted us to\ninvest in is is basically I saw the\ncompounding value you know over time of\nyou know investing on a framework and\ninvesting adoption early right I think\nuh\num you know me as you know as a data\nscience engineer you know I want to push\nfor operational efficiency uh you know I\nwant to minimize situations where like\nwe're tuning infrastructure you know\njust to get something working or you\nknow how to provision you know special\npermissions or Worse right having our\nyou know data scientists to have to deal\nwith sort of nuances of uh of\ninfrastructure\num\nI mean I think as far as um you know\nwhat what kind of uh arrived uh what\nkind of led me to uh to flight and and\nto Union to a certain extent is uh you\nknow I did\num you know so I was aware that you know\nwe want to invest in a workflow\nframework and very early on I did a\nthorough evaluation you know with other\npopular Frameworks Luigi Argo I think\nmetaflow\num and uh you know I I was during the\nevaluation process you know I I was I\nwas looking for certain things that I\nreally had to work for us\num and and you know and some of those\nthings are just uh a good trade-off\nbetween sort of the sophistication uh of\nthe tool versus you know the the\nSimplicity\num well the surface the sophistication\nof the overhead of the tool versus you\nknow um how simple would be to to adapt\nit to adopt it\num and I think ultimately what convinced\nme is that uh you know I think fly has a\nvery clean abstractions uh I think uh\nyou know it had very uh oral\ndocumentation so it was very easy to\nkind of you know just pick it up and\nthen finally I think the the community\nwas very reachable\num was very responsive\num so you know with that I was able to\ngo from kind of nothing to like a\nworkflow to kind of a converting one of\nour existing\num\nmethodologies that we were applying over\ndata very easily into a workflow and and\nI think that you know the initial like\nPOC\nwas something that tangible that I could\nkind of bring forth to the team and kind\nof show them you know\num that really along the way I think\nthat they the way uh you know one of my\nteam members has been thinking about\napplying this methodology can be\nconverted into like a workflow in a\npipeline fashion like very easily very\nnaturally really\ninteresting and um I I think obviously\nthe data science ahead very often is is\nnot the functional you know programming\ntask for Focus programming data science\nscience much more Jupiter you know\nnotebooks and so on how was the\nexperience uh among or with your data\nscientists adopting more of a workflow\nin the ocean for data science and\ncreating that\nreproducibility and so on and so on did\nthey understand this right away or was\nthe hunger there so tell us a little bit\nabout how what happens with the team\nyeah I I think uh well at the very\nbeginning you know I I think uh when we\nreally didn't have anything and then we\nhad basically everything in in you know\nsort of disco organized in different\nJupiter notebooks\num it was uh it was a difficult case to\nmake for me uh you know that I had to\nmake to the team because you know these\nthings I'm talking about operation\nefficiency or lower infrastructure\noverhead or having good abstraction\nright they are all very abstract\nConcepts\num that's that's you know for somebody\nwho comes from\nacademic or from you know pure data\nscience domain rights very hard to kind\nof grasp\nI want to say you know I very much had\nto play the role of what kind of\nadvocacy you know trying to evangelize\nit the end of the day it's it's not\nsomething that's you know that's\ntangible so I think uh being able to you\nknow uh sort of start the process of uh\nconverting some of our work into like uh\ninto something tangible into something\nthat you can visualize right I think\nthat's another advantage of why that I\nreally like that it has like a really\nreally a clean UI\num I mean I think that's the you know\nthat I think that's a light bulb moment\nthat we're you know I think yeah the\nscientists really like really saw oh you\nknow I think the way we've been thinking\nabout like uh uh sort of applying this\nmethodology in a sequential or in like\nuh in a functionally dependent like you\nknow kind of uh fashion is can is really\nlike like a workflow with you know\nstitching like tasks into workflows\nit's a different thinking I feel like\nyou're exactly right when you go into\nflight console and you see you know your\nworkflow and you understand what's\nhappening in in certain aspects those\nare experience that data scientists\nclassic people who are not in production\nand we're actually not putting building\ndata on my products we're on the\nresearch side usually don't see that\noften they they go to the Jupiter\nnotebook but then transition this work\nthat is really very valuable into\nsomething that actually scales the\nentire engine path and you can rely on\nand your team can do a lot more that you\nknow gives your team you know using the\nterms wings so they can do a little bit\nmore so is that the sentiment that they\nstill have\nuh yeah you know I think uh you know\num Jupiter notebooks you know definitely\nI I use notebooks a lot it's but it's\ndefinitely the place to prototype and we\ncontinue to use that but I think it's as\nit's you know kind of uh uh real in\nplace right like I think uh uh when you\nstart having to apply to Big Data or or\nwhen you want to you know reuse\nmethodologies and be able to\num you know like I said like experiment\nover like different data sets right and\nyou want to kind of do that um you know\nin a efficient manner you know highly\nparalyzable you want to kind of get the\nresults you know as as soon as possible\nthen I think you know that's when we\num\nstart thinking about you know maybe\nmade this process a little bit more\nformal right like maybe like taking out\nyou know some of the uh more used\nco-pass and convert that into like a\nworkflow and and part yeah part of the\nreason is that and also I think uh the\ninfrastructure abstraction and sort of\nthe Titan security uh you know measures\nthat you can have with with flight I\nthink that's also\num that's also very relevant to us you\nknow I think we have very uh you know\nsort of high security restrictions where\nyou know you can only operate your data\nright in a certain environment and and\nyeah I think that's where you know have\nsomething that's has that already solved\nalready extracted for us uh is very very\nuseful\nthat's fantastic so I mean in hindsight\nthings are always 20 20. um now that you\nhave gone through that experience uh\nwhat would you tell other people similar\nto on a civil situation as yours maybe\nthey don't have the devops head that\nhelped you to understand really making\nthe right decisions from a data science\nmachine learning point of view but what\nwould you tell them how would they find\nuh how should they make a decision\nfinding their right tools what should\nthey look at and what should they focus\non\num yeah I mean I think for people who\nare who are thinking about why or\nthinking about you know workflow\nframework\num in general\num I think uh the most important thing\nis that you know just be really\nrealistic about what they're looking for\nand and I would say in the short term uh\nand then you know in the medium term\nsort of in that priority right I think\nthese workflow Frameworks need to work\nfor your needs\num\nuh you know so you want to be really\nrealistic about you know what works for\nyou what doesn't work for you\num another suggestion I have is is to\nreally start small and and you know I\nthink don't be afraid to kind of\num start\num you know\ngoing through some of the tutorials and\nthen you know working through you know\nuh some of the working through the\ntutorials and basically thinking about\nhow that would apply you know if if the\ntool has has all the all the capability\nto kind of provision you with what you\nneed for\none of the things that you're doing\nright whether that's like machine\nlearning or just uh just data conversion\nmaybe\num and I think it's important that the\ntool can abstract and nuances of\nunderlying infrastructure I think most\nof them do but you know in in different\nsort of opinionated manners right and I\nthink uh\num ha you know taking all the complexity\nof the infrastructure and be able to\naccess\num both code and data and you know and\njust through like libraries of sdks and\nthis highly abstracted fashion uh I\nthink that's very important\num and then another another thing that I\nwould encourage new adopters to do is is\nreally reach out to you know I think the\nflight Community here\num I think the community is is really\ngreat\num very extremely knowledgeable\num very supportive and you know in my\nexperience has been you know very very\nresponsive\nfantastic I want to open it up uh to to\nthe rest of the community here and I'm\nsure Kason and the team have uh\nquestions as well so please uh feel free\ncase them\num so when you I think your journey\nstarted about six eight months ago right\nor if I'm not mistaken I know it was\nyour very regulated environments are\ngetting things like you know can take a\nlittle bit of challenge but uh how many\npeople are using pride in your and\nwhat's your goal or a longer term\num and and how do you see\nuh like just a pure scaling aspects of\nit in terms of you know organizational\nas your team that get added how easy is\nit for them to get ramped up and how is\nit for them to discover prior art we use\nit Etc\nyeah\num yeah great great set of questions\num I think um I I want to start with you\nknow having uh all new you know data\nscientists and researchers to you know\nsort of the ramp up experience which\nwhich surprising to me has been you know\nvery very quick uh you know very low\nbarrier I think uh\num you know I don't remember concretely\nthat I had to you know kind of sit down\nwith them and kind of walk through them\nwith you know really anything I think\nthey will they're both yeah I mean my\nteam members are great and and you know\nthey were able to pick up the framework\nyou know with minimum support I really\nyou know just looking at examples and\nlooking at documentation so I think that\nhas been like a very positive experience\num\nin terms of all size uh you know I think\nwe're a pretty small team uh you know\nthere are three of us kind of using\nflight actively\num and we have a fairly I would say we\nhave you know almost a minimum cluster\nuh for nodes\num I think uh a lot of a lot of what\nwe've done is is really\num\njust centering around abstraction of\nFrameworks or also our workflows and\ntasks right so we have I think well over\nlike 20 you know different workflows uh\nfor\num you know a lot of the research areas\nand you know in the congestion domain or\ngeneral data evaluation uh you know also\npatterns for processing you know massive\namounts of data and I think um\nso over time\num you know uh we definitely want to\ninvest more in building workflows and\nidentifying areas where you know we want\nto kind of add complexity to sort of uh\nmodify the existing workflows\num\nyeah so continuation adoption and I\nthink this is specially critical as we\nget into you know bigger data sets where\num you know basically YouTube notebooks\nwill all live it's uh its capability\num and uh you know I think uh for me\num I think I'm I'm excited to also to\nstart experimenting with additional\ncapabilities such as you know spark and\nyou know the stage maker plugins\nand and and finally you know as machine\nlearning engineer you know I I\ndefinitely want to apply it to you know\nml use cases\num you know bring all some of the stuff\nthat I have and notebooks and and start\nbuilding like all core and all pipeline\num I think\nuh ml Ops as many of us here know can be\nvery very challenging you know I think\nwhen you deal with real data concept\ndrift and data drifts those are all you\nknow extremely difficult problems\num so\num yeah those are the things that we\nwant to start tackling like early\nessentially\nfantastic so when shoot I was checking\ntime here so that is super interesting\njust wondering if there's one more\nquestion uh within the team for for you\nif just looking here if anybody has a\nquestion\nI of course have a question but I will\nlet other stuff I know Casey you have\nlike a lot more most inquisite\nuh if not then I hand it to you Keith\nokay\num this is actually more of a general\nquestion for the community so\nbesides the community purely from\nactually the technology point of view\none thing\nthat we do well and we should continue\nimproving further I think that you think\nreally matter to you and to your users\nand improve your daily lives and how we\ncan further improve it or what we should\nyou know you may not have the exact\nsolution but that's okay saying that do\nthis more or through this and one thing\nthat we don't do well we should work on\nand improve it\num\nso those two things might really help us\neven you know decide areas to focus on\num\nwell\num\nyeah I I mean I think\ndocumentation has been fantastic\num you know I think uh the examples uh\nreally you know\nhas good coverage of mostly use cases\num\nI think uh\nis it yeah I mean I think there are also\nopportunities within the documentation\nto kind of uh add you know maybe a\nlittle a couple more examples or add you\nknow a little level of depth\num\nI think my my personal uh feedback\num and and I kind of raised that to ye\nin the past is um I think one area that\nyou know maybe the project can improve\nis just general debugability of\num you know being able to kind of uh\nyou know go into like the darker\ncontainer or go into the Pod and be able\nto see like you know whoa whoa you know\nwhat happened or I think uh um\nuh during compilation steps you know\nwhen compilation fails like you know\nmaybe you add a little bit more insight\nnow into\num you know why I failed yeah things\nlike that\nso that's fantastic feedback\nright windshield I just want to say\nthank you very very much for being our\nguest today I think this is super\ninsightful and hopefully also very\ninspirational for all the leaders who\nhave to make or should make a decision\nabout finding the right platform for\nthem to really bring the entire data\ndata science machine learning ml Ops you\nknow process under control and not have\nthose fragmented Solutions but really\nhave one thing that actually works\nacross and actually it's open source as\nwell so really grateful for you to join\nus today"
    },
    {
        "title": "Flyte Community Updates 024 featuring Developer Relations",
        "transcript": "all right uh today for my home office I\nteach you some traffic uh so it's a\nlittle different environment welcome to\nthe flight Community sink my name is\nMartin Stein I'm with Union AI I almost\nlook at our name and I'm your host today\na few housekeeping notes first as always\nthis is a zoom call it's recorded uh\nwe're going to share it in social media\nso you can see it watch it on YouTube or\nany other places as well later\nand the Flight OS open source Community\nis a vibrant and growing community and\nwe would like to engage people in our\ncommunity and join the community so if\nyou are a member of The Fly Community\nplease feel free to join uh and reach\nout and actually bring more people in\njoin us via the slack Channel and also\ncheck out the flight.org with that let's\ntake a look at the agenda\ntoday we have uh three great topics so\ninteresting topics one is of course the\ncommunity update next week we have cook\non North America it's going to be very\ninteresting going to spend a little bit\nof time talking about that\nsecond topic is about the devrel\ndeveloper relations uh the flight to\ndevrel team we're going to introduce the\nteam as a small team but it's a growing\nteam and we would like to get your\nsupport to help growing that team and\nthen we talked with uh speak with wenchu\nfrom Lacuna AI it's going to be a little\nbit of a mix between a presentation and\na fireside chat as I'm looking forward\nhaving a little bit of time learning\nabout uh what the team and when should\nhave done at Lacuna and what the problem\nis that they solve the business problem\nthey solved and what the technology is\nand what the tools that they have picked\nlet's dive into the community updates uh\nof course you know my name let's go to\nthe next one\nuh talk about kubecon next week at\ncoopercon we have a flight Booth Groupon\nis uh October 24th October 28th\nphysically in Detroit North America so\nthe booth that we are at is uh the\nstartup Su startup Booth 91 uh swing by\nsee us we will have some swag there for\nyou to to take away also there's some\ndemos there's some office hours so I\nthink it's going to be very very\ninteresting there's also a cncf slack\nChannel you can join that and look for\nnumber six cook on Union Ai and do you\nfind more information throughout the day\nthat's the physical part of cooking we\nalso have a virtual part the virtual\nBooth uh is actually open Sunday 23rd I\nthink it's actually Monday when it's\nofficially open there's a URL that you\ncan swing by that URL check out some of\nthe materials that we have uploaded\nthere's a video there's a little bit of\nother material there there's also office\nhours and demos listed here Monday we\nstart Tuesday Wednesday Thursday Friday\nso it's going to be highly interactive\non the virtual side if you can make it\nto Detroit that's your next best uh\noption to to Really participate at\ncoupon this year\ngood\nCommunity updates I would love for\nsamita to tell us a little bit about\nthis upcoming webinar Summit I'm just\ngoing to hand it over to you uh maybe\nyou can tell us what you found\ninteresting and what the talk is going\nto be about\nyeah so uh this will be a talk on\noperationalizing machine learning and\ninterviews study paper uh if you're\nfollowing Shreya Shankar on Twitter you\nmight have already come across it so to\ngive you all a brief overview of the\npaper uh a team at UC Berkeley has\nconducted interviews with about 18 ml\npractitioners and compiled a list of\ntheir insights observations and\nrecommendations regarding deploying\nmodels to production so the stock will\nbasically be about the common practices\nfor successful ml experimentation\ndeployment and sustaining production\nperformance and mlops pain points and\nanti-patterns uh so this will be a\nwebinar with a presentation followed by\na panel discussion Shreya Shankar and\nRolando Garcia who are currently\npursuing their phds at UC Berkeley will\nbe presenting the paper and it's\nscheduled on November 8 2022 at 3 pm PT\nuh we believe this will be back to the\ngirls with interest testing inside so if\nyou are in the mlops space we recommend\nyou to join us and stay tuned for the\nwebinar details yeah fantastic and I\nthink there's also a call to action for\nthe community if people would be\ninterested joining the panel I think\nobviously people with background\nexperiences on I think we open for\nhaving a conversation about panelists to\njoin is that correct\nyeah okay fantastic so uh please reach\nout to slack Channel we'll talk about in\na second about a new uh channel in the\nslacks channel in our slides you know\nhow to engage with that bro fantastic so\nI'm looking forward to the webinar on\nNovember 8th next is a write-up from the\nml Ops Community session ask me anything\nwith Caitlin so Cason you'll remember\nthat uh fantastic session that we had\nthere that was in August a little bit\nago Centra took all of the Fantastic\nquestions that came in for flight\nincluding topics about pipelines and\nmodels and orchestration tools and you\nname them top challenges and\nmaintainability how do we monitor\nquality and put it into a blog post I\nthink this format is is great for not\nasking anything so many questions so\nmany answers check out the union AI blog\npost uh and I think there's always\nsomething that you might have missed and\nand uh ask me anything but not reading\nabout it goes like Oh interesting and we\nwould engage you uh and want you to to\nengage with us here uh join the slack\nChannel and if you have questions if you\nwant to continue the conversation uh our\nslide channel on the flight slack\nchannel is the right form for that\nokay\ngood now something uh on behalf of\nthe developer relations uh this time\nwe're we're spending a little bit five\nminutes here in the community saying to\nto cast the light on the deferral team\nthat we're building and we think it's a\nyou know just a very nascent young team\nbut I want to answer a few questions\nhere and I want to you know project a\nlittle bit where we're going what we're\ndoing what we're looking for\num and give a little bit of overview and\nthen have the team introduce themselves\nwhoever's online today\num I get often the question what is\ndevrel and why do we have devrel and\nmany people actually know it or at least\nthink they know it but I felt like one\nof the best definitions is to me that I\npersonally use is growing you know the\nsuccessful usage and implementation of\nflight this is literally what devrel\ndoes and the deferral framework that\nwe're using here at flight uh is it's\nvery close to what Caroline leftover\nwrote In Here fantastic book uh to\ndeveloper relations and it really comes\ndown to one thing it's it's creating\noutstanding develop experiences and\nwe're just at the very beginning of that\nand our goal is to improve this the\nthree areas that Caroline separates and\ndistinguished and I believe they're all\nright on there's developer success which\nis you know how do we provide support to\ndevelopers by assisting them and helping\nthem with with their challenges our tool\nfor developer success is of course the\nslack Channel and uh that's how you can\nengage with us but there are also other\nuh you know GitHub obviously this is\nalso to sort of extend the form here uh\nbut there are also people behind that\nwe're going to talk about those people\nwho are here to help and to support but\ndeveloper success is something that we\nput forward uh as top of our in our\npriorities and we're going to invest and\ndo a little you know not just a little\nbut a lot more on the developed success\nside because flight obviously is such a\nfantastic Tool uh and we see a lot of\nadoption there so there's a lot of\nsupport that actually is needed when it\ncomes to questions is like where do I\nfind this other that many people have\nsolved this and and we just got to make\nsure before and put forward that the\nright set of content learning resources\nare also available which brings me to\nthe developer education part the team\nright now is working on on uh part of\nthe content and helping to make content\neasier to to navigate and I think we're\nvery interested in learning more about\nwhat works in the content site what does\nnot work what learning resources does\nthe community want to have and what\nlearning resources uh should we improve\nthat's the second pillar that we're\nfocusing on and the last pillar uh in\ndevrelis is you know classically we\ndon't really like the word marketing but\nthe tools that are used in marketing we\ndo like them which are the Outreach\ntools and that's kind you know clearly\nsetting up um you know what we heard\nearlier samita mentioned a webinar\num you know that we talked about\nimplementation of MLS and so on and so\non those are topics that think clearly\nuh fall into that but\nwell I'm going to introduce the team in\na second actually they're going to\nintroduce themselves but really here's\nthe call to action for our community we\nwant to hear from you we will be adding\na new defro uh channel in in the slack\nChannel and defrost section area where\nyou can actually provide suggestions and\nfeedback about developer relations for\nflight specifically you know about what\ndo you expect from develop success and\nfrom education and from our Outreach\nfunction if there are areas where we can\nengage if the opportunities we should be\naware of there's going to be a central\narea to share this with us I'm looking\nforward to getting this rolled out\nquickly this week and last but not least\nuh you know we're looking to to grow the\ndefro team uh join this like Channel\nfind me reach out to me and uh you know\nwe're happy to uh interview and then to\nfind candidates there's also on union.ai\nuh chop postings there you can take a\nlook at the unionized job postings just\nengage with us but with that I want to\nhand over to the team and to have the\nteam introduce himself real quick so\neverybody knows who's actually running\nthe developer relations on flag peace\nyeah okay then you want to talk about\nsomething\num yeah I just wanted to quickly chime\nin one thing I think\nwe at Union really you know and samita\nand tiger and others that you'll meet\ncare about the community deeply and we\nreally want to help Foster it but this\nis our side of effort from unions uh\nside that doesn't mean the Deborah is\ncompletely closed right you can join in\nand you can help us build the community\nand grow the community and Foster the\ncommunity on your own side join the\nchannel uh that doesn't mean you have to\njoin Union it's a slide which is an open\nproduct be part of the community and be\npart of the developer relations uh team\nand help us with it uh and yes of course\nwe are hiring at Union so if you know\nyou want to do this full-time we would\nlove to have you at YouTube\ncool okay\num hello again\num I'm a developer Advocate at Union Ai\nand uh it has been a year and a half\nsince I joined Union and yeah my tasks\nuh include sharing insights and\ninformation via blog posts managing the\ncommunity on slack building Integrations\nwith other platforms and yeah the\nholistic goal is to make users and\ndevelopers happy and in my free time I\nlike building web and ml applications\nand yeah that's about it\nthat's fantastic so I saw actually you\nin your free time you just posted\nsomething last weekend uh what was the\nthing that you put there zoeta\nyeah it was an ml application that\npredicts the location of an M8 slash\nvideo which is part of fsdl 2022 course\nso yeah fantastic awesome yeah so so\nglad to have you here so let's uh\nintroduce if the rest of the team is\nonline should I uh I hope you sorted out\nthe internet connection issues that you\nhad earlier so if you if you have please\nintroduce yourself\nI don't think he has so Shiva has joined\nthe team as a community success engineer\num that goes back into that function\nthat I mentioned earlier to really make\nsure that we have developer success uh\nDevo X and and really\num the experience has to be at a certain\nlevel therefore we have Community\nsuccess Engineers who really are here to\nsupport our developers to make sure that\nthey get the most out of uh you know\nflight obviously as a framework as\ntechnology but also the resources that\nare around and available plus the\ninformation about uh as being an\ninformation Source where to find more\ninformation so this is the role of Shiva\nShiva joined us just recently uh you can\nfind him obviously in the slack Channel\nand uh he is always there to to help you\nwhen you have any questions on on slack\non the with regards to flight also we\nhired Tyler Tyler is our new community\nnext slide Tyler's our new community\norganizer so Tyler please if you if if I\nthink he might be in a traffic gym we\nhad a massive traffic jam this morning\nso uh I don't think you're online are\nyou\nno he's not so so Tyler join us actually\njust uh recently as our community and\nevents coordinator this is you know very\nindicative for for what Cason said\nearlier we want to do more we also want\nto support uh uh community members here\nif you have an idea about uh putting\nforward a a meet up about flight and in\na location let's say Europe or in the US\nor South America Asia and Africa we're\nhere to help and this is not just like\nuh saying well let's put it on our\nwebsite let's put in fly.org but also\nsupporting you with Logistics there for\nflight uh meetups so Tyler is a\ncommunity events coordinator he has done\nthis actually before and uh if you have\nor want to set up a flight meet up uh in\nyour location reach out to Tyler uh\nTyler can be found on the slack Channel\nas well in the slides live Channel and\nwe're here to make sure that you can get\nyour event organized so with that here's\nthe team uh obviously samita and shivay\nand Tyler and I see as I said earlier\nwe're looking forward to add a few more\npeople but also as what Caitlin said is\nvalid too you can also help uh without\njoining us here organizationally but\njust being in a flight Community member\nuh we would be happy to have you there\nwe also have something that we call\noffice hours uh Center if you could put\nthe slides on real quick uh when we talk\nabout the office hours and there are\nusually three sets of office hours we\nhave one set called the red eye with hay\nthem\num that is you know always Wednesdays\nKatrina also Wednesdays and also samita\nWednesdays so that's exactly what when\nshe you know when he says Community\ndocumentation\nits institutions like the office hours\nthat actually help us and we want to do\na better job here too so just what we\nheard from you about how we can improve\nhere join us and tell us what we can do\nbetter here if we need different hours\ndifferent times or more time zones let\nus know we're happy to do this the next\nthing is uh November 1st uh with check\nSheridan from Walt I'm super excited\nabout uh that uh upcoming sync uh like\nthe conversation with uh wenshu we're\ngonna dive deep and learn more about uh\nwhat drove people what motivated people\nto adopt flight uh what are kind of like\nthe outcomes that they were able to\nprovide I think it was very impressive\nwhen shooting you what what your\noutcomes are so join us here I just want\nto say thanks again windshield this was\na great pleasure to hear you today have\nyou here today and we would love to have\nyou back at the community meeting\num saying thanks to everyone else and uh\nI'll see you on November 1st uh at the\nnext Community sink yeah thank you for\nhaving me thanks everyone thank you"
    },
    {
        "title": "Flyte Ecosystem UnionML v0.1.4 Release",
        "transcript": "the release slide yeah\nhi everyone I'm Niels Ben tln\num working on Union ml right now mostly\nand a little bit of Pandera but I wanted\nto show off a little bit of the upcoming\nrelease the zero one four release and I\nwanted to demo\ntwo new features in Union ml\num\nso yeah the demo Gods aren't smiling\nsmiling on me today but let's see if I\ncan do some kind of demo today here so\nthe first feature\num which should be familiar to all\nflight users is where we basically added\na way to\ndo what we're calling patch deployments\num\nso I don't know if this is the right\nmove or not but we kind of like veered\naway from flight specific terminology so\nthis this is basically just fast\nregistration right so you built your\nDocker image\num\nyou have all your dependencies in there\nyour codes baked in there but then if\nyou iterate on that code you want to be\nable to just push those changes and\nwithout\num bypassing the whole Docker build\nprocess all over again\num so\nyou can do this with Union ml deploy\num and then the reference to the the app\nmodule and the union ml model so that's\nthe app colon model and if you supply\nthe patch flag\nit'll basically do fast registration for\nyou\num so this hasn't been available so far\nin unml it's going to come out in zero\none four so that's exciting that'll\nreally\nhelp with the iteration process uh same\nas flights fast registration\nthe second feature I wanted to show off\nuh hopefully it'll work is\num Union ml ships with these app\ntemplates\num so when you say Union ml init my app\nwhich is the name of your app\num\nit'll create a project directory for you\nthat is basically has all the artifacts\nand source code and configuration you\nneed for your union ml app\num there is a template flag\nwhere you can pass in certain\num\npre-configured templates so in this case\nif you say basic AWS Lambda S3\nit'll create a project for you that is\nnot only just uh\nall the the training and prediction code\nbut it also ships with the ability to\ndeploy a trained model\nto an AWS Lambda serverless function\nwhich also reacts to S3 events so if you\nsay dump an S3 file\na file into S3 and assuming that it is\nsome kind of uh compatible file format\nthat udineml is able to load up into\nmemory and pass into your prediction\nfunction\num\nyou'll be able to basically react to\nyeah uploading files from uh just say a\ndifferent application so if you have a\ndifferent application that uploads these\nfiles Union ml will handle generating\npredictions from them assuming that they\nare features\nso with no further Ado let me see if I\ncan share my screen and\nget something to work\nso\nhere just a thumbs up to just make sure\nthat you can see my terminal and my\neditor\nokay I'm assuming yes\num so I'm going to start off with just\nshowing you this init command so\nI'm creating a project called my S3\nevent app\nand um if I look into this app you can\nsee a couple of artifacts in here like a\nDocker file the docker file for the AWS\nLambda function\nsome sample features\nuh the requirements and then the app\nthis app.pi file contains the union\namount app\nso actually I already have a project\nhere that I'm going to work off of this\ntest S3 event project it's the same\nexact template\nso\nthe first thing I'll show you is the\npatch registration so I have a flight\nCTL sandbox here\nand um\nI've already registered the the app\nbasically the training workflows and\nprediction workflows and I've already\nrun\num one of the training workflows here\nbut\num say I want to iterate on this code so\nan update I'll make to this the code\nbase is just a pretty arbitrary one so\nI'm just going to print\nbidding estimator now\nso now I've made a change to my code\nbase I don't want to rebuild my Docker\nimage\num\nso I'm just going to say Union deploy\napp\nand just as a reminder model here refers\nto the model variable that contains all\nof our app logic\nand then I'm going to pass into hash lag\nand this should go fairly quickly\num great so now I've deployed\nuh app version and just to be clear\nin Union ml land and app version is\nbasically just the set of workflows that\nare logically associated with your\nmachine learning application\num so now if we go back to the console\nrefresh the page\nwe see now there's uh\na version of flight workflows and tasks\nEtc and that has this kind of like patch\nand a random string here to\num to indicate the the patch so now I\ncan just\num\nrun the command line Union ml train with\npassing in a few of the hyper parameters\nhere\nand this will automatically just use the\nlatest uh app version that I've\nregistered so if I go the console you\ncan see that we're running the patch\nversion here I'm not going to go dig\ninto the logs to show you that it's\nactually printing out the the change but\nyou can see that everything here is is\nworking out as expected\num so this the impact of this will be to\nincrease the iteration speed\num so once you've got your Docker image\num pushed out to some you know remote\nlocation you can just use that same one\nand flights fast registration protocol\nwill work here just you know happily\num\nthe the last thing I'll show you\nactually it takes some time to build\nthese images so I'm not going to run\nthrough the entire process but you can\nsee here\nwhat I did was\num\njust to give you some background\nactually\nthe the way you you as a user\nonce you've set up your template app and\neverything the thing that you need to be\ncognizant of is this Lambda Handler so\nthis is a very lightweight integration\nwe didn't do too much by way of like\nthat uh syntax sugar to just to make the\nactual Lambda Handler uh implementation\neasier but it's fairly straightforward\nright so this is just whatever um aws's\nuh lambda's conventions are so the\nHandler gets an event in a context\nobject\nhere I'm I'm just loading the the model\ntrained model object from my environment\num environment variable here and\num I'm going iterating through the\nrecords and these records contain\nmetadata about where the S3 file was\ndumped what the key was what the Muppet\nwas Etc\num so all I'm doing here is I'm fetching\nthe data from S3\num passing the features the resulting\nfeatures uh into my model.predict\nfunction so it generates predictions\nand then I upload those predictions to\nS3\num\nand how this all connects to\num\nAWS Lambda is I have this template.aml\nfile I'm not going to go through all of\nthis but basically it's a configuration\nfile that specifies\nsome configuration for your AWS\nserverless function\nI wanted to react to a file upload\nevents so I basically have this app\nbucket that where I'm expected to dump\nmy features\nand I wanted to react to any file that's\ndumped into the features prefix with a\nDOT Json file extension\num so that's basically how Union ml\nconnects with S3 and AWS Lambda and so\nonce I\nsay sam build it'll build my Docker\nimage I'll blow this up a little bigger\nand then I say sam deploy it takes me\nthrough this guided experience of\npassing in values to a bunch of\nconfigurations uh the one to pay\nattention to here is the bucket name\nparameter so I can I can say what bucket\nI want to\num react to\nand it's pretty magical\num said the Sam CLI will spin up a bunch\nof all the services you need so all this\noutput is spinning up the function\nuh the bucket et cetera Etc\nand\num\nlet's this is kind of the Moment of\nTruth\num\nbasically what I'm gonna do\nis I'm going to copy\nthe sample features that I have locally\nup until this bucket I actually need to\nchange it because uh\nI use a different bucket name\nso\nokay\nthere it is\nokay so I up successfully uploaded that\num\nI'm gonna just double check to see that\nthe features file is actually in there\noh no this is the wrong\nbucket name\nokay so I just uploaded these features\nI'm gonna invoke this um\nSam logs\ncommand and it's supposed to print out\n[Music]\num\nthe logs of my\nmy application here I forgot the name of\nmy application already so\nit's gonna see here stack name test S3\ndemo\nawesome\nOkay so\nmy request was started and ended here so\num\nit's working\nand the last thing I'm going to do\nis check that the predictions were okay\nso the predictions aren't there yet I\nthink it's still running but you get the\nidea\num that basically I can upload files\ninto some predefined prefix\num pattern\num\nand if you've set everything up\ncorrectly your union ml app prediction\nfunction is actually running on this\num\nAWS Lambda server that will kick off as\na result of dumping these files and then\nproduce predictions for you\nso\nI'll just end the demo right there and\nI'm curious if there are any questions\nor reactions thanks yeah I think that's\nfantastic I love the convenience that\nyou have there for I mean with all the\ncloud formation actions going on in the\nbackground there I should make it super\neasy so any specific feedback that you\nwant to get from from people watching\nthis I mean what kind of feedback are\nyou interested in getting\nyeah so therefore this specific feature\nwe are curious to learn about all the\ndifferent prediction modalities that\npeople are interested in uh Union ml\nobviously supports batch prediction with\num with flight\num we do have fast API Integrations and\nusing AWS servers serverless functions\nas functions or as web apis so that's\nsupported but this kind of not this\nisn't exactly streaming but we're\neventually going to go into the\nstreaming streaming land\num but\nreacting to events things like that I\npersonally don't have experience you\nknow in production using those kinds of\nuh use workflows or using those that\nkind of prediction modality so we would\nbe curious to to learn about you know\nhow people are\ndoing inference serving outside of the\nbatch or like online like API based uh\ninteractions\nyeah yeah no I think that's fantastic so\nlet's see if we can get some feedback\nthere also oh that could work in a\nfunction in a combination with you know\nmodel Ops and observational stuff see\nhow your model performs that you update\nwith uh more data and so on that could\nbe a very cool you know use case with Y\nlocks potentially and see how that works\nout together so I think it's fantastic\nthanks a lot for sharing this Niels\num if we have more feedback I would say\nengage through the usual channels\num and we had to turn around I had one\nquestion actually\num my name is Ethan yeah so uh means\nbasically this is I think one thing that\nthe demo probably did but did not talk\nabout was that you were actually\nautomatically using the model that you\ntrained offline independently uh you\ndidn't really have to create your model\ndeployment\nspecifically like taking the model\nstaging it to the right place right so I\nthink maybe you want to highlight that a\nlittle bit\nyeah yeah so um the the step I didn't\nshow is you can uh it's a single it's a\nsingle extra command but currently with\nSam and AWS Lambda stuff you\nyou have to kind of download the model\nartifact\nin the runtime where you're deploying it\nso in this case it was local\num\nso sorry I got distracted by that for a\nsecond\num yeah\nso it's a union ml like fetch model and\nit'll grab the model from the remote\nflight cluster and then\nit'll be in your your local environment\nand from there you basically build the\nthe Sam Docker image\nyeah and so if you update the model\nwould it keep on updating the model as\nwell like they rebuild a new model the\nversion no that's that's a type of\nuser Journey we haven't I mean it would\nbe awesome or would be interesting\num but we don't support that right now\nso you kind of need to redeploy the\nentire stack or you need to update the\nstack the cloud formation stack\num with the new image\nyeah and this is something where we\nwould love to get inputs right because I\nthink with within flight we do support\nbuilding\nuh this newer versions and Union ml does\nsupport fetching them it is that we were\nnot sure should we automatically fetch\nnot automatically Fetch and this is\nsomewhere where the community can chip\nin have ideas try this out it's it's\nreally simple and the best part is this\nis AWS there are free and AWS Lambda\ninvocations that people can use this can\nrun on the free invocations and it\nscales beautifully with Lambda there's\nno external server Services nothing to\nbe run just use AWS Lambda so fantastic\nyeah and the last thing I'll say is is\nthere's\num there's a lot of optimizations from\nhere right like we're just using you\nknow\nwhatever like if you're using pi torch\nit's just doing the prediction in pi\ntorch but you know uh with the Onyx\nplugin and there's like I think there's\na new thing AI template that Meadow put\nout recently\num so there's a lot of optimizations to\nmake the predictions even faster\nvery cool so so last question deals\nwhere can people learn more about Union\nml\nyeah so uh on the union.ai union ml URL\nyou can get a pi level sense of what's\ngoing on\num and uh you can go to\nunionml.readocs.io\num if you want to dive deeper but those\nare the best places\nawesome great thank you so much news"
    },
    {
        "title": "Flyte v1.2.0 Release Notes & Roadmap for v1.3.0",
        "transcript": "then go to the slide roadmap with\nEduardo and also we basically had a last\nupdate about the flight road maps that\nwas like about two months ago so today\nwe want to catch up and see you know\nwhat's next uh you you two months ago\ntalked a little bit about a forthcoming\nrelease which was 1.2 and I think right\nnow I'm going to dive into this a little\nbit and give us a little bit more detail\nwhat's in one or two and potentially\nwhat's next station series\namazing uh can we go to the next slide\nforeign\ntasks but there's a lot of cool stuff\nalso there we already briefly touched on\nthe DBT plugin but there's another\nplugin that I'm very excited to see you\nknow the news is hugging face\nand I'm calling out like two features\nthat you know I'm pretty sure be also\nuseful one is the um now you can name\nyour executions with longer names so you\ncan have like the more descriptive names\nto you know your workflow executions\nright\nsuper amazing and this third bullet\npoint here the\num\nthe flight workflow crd being offloaded\nthis is basically a massive Improvement\nin performance for a very large\nworkflows that flight users will get for\nfree they will not have to do they it\nwill not do anything they don't have to\nenable any any flag any feature anything\nso please if you see anything weird let\nus know but um we're also very excited\nabout this you know like\num especially for\nflight users have a very large workflows\nthey will see a\ntwo maybe three ax Improvement in um\nexception times\nMartin already talked about this but uh\nwe were slating the the next release the\n1.3 release to the end of the year and\nI'm here I'm calling out like two\nfeatures that will be out with this\nrelease one is uh human in the loop\ntasks\nit's a feature that you know have been\nrequested for years at this point and I\nthink we finally reached a point where\num the the ergonomics of actually you\nknow exposing this to the SDK\nlooks reasonable so we're gonna go out\nwith with that and also um out of core\nplugins\num another feature that people have been\ntasking on and off\nthis will simplify you know the the\ncontributing to the black backend\nplugins and yeah we'll see we'll see how\nthat goes but 1.3\num these are not the only features that\nwe're gonna go out with with 1.3 but I'm\ncalling those two out because I\nI do think that um this would be another\nyou know just step jump forward in in\nterms of\num the flight ecosystem and like the\npower of\nexpressing workflows and pipelines are\nfine\nthat's fantastic so I was just wondering\num two questions that I have for flight\n102\nfor the upgrade process and for people\nmoving is there anything that people\nshould be worried or concerned about\nwhen they go from an older version or is\nit just a smooth sailing\nit should be smooth sailing for the\nmajority of the users but the ones that\nhave\num\noff\nturned on but they use the the built-in\num oauth server that we ship with flight\nso\nbut even in that case\num we have\ninstructions on\n[Music]\nI think that's the most important\nstatement you have instructions\nso we just got out uh first I can I I\ncan comment on that so essentially\num there was a default config that we\nhad within the authorization server that\nnatively shipped within flight\nand we noticed that some users were\nusing the default config which is not\nthe best way to secure your system\nso we are now mandating that people\nessentially override the config and this\nis for your security\n[Music]\nwe within the flight Community we deeply\ncare about security and and going\nforward that would be a requirement if\nyou're using or2 especially with the IDP\nlike Google which does not ship with the\nnative authorization server awesome okay\ngreat fantastic that's important uh of\ninformation good to know and then for\none of three I think this is cool also\nhaving the quality planning meeting and\nthe first flight modeling planning\nmeeting uh is obviously it says first so\nit implies that more uh so can you tell\nus a little bit about who should join\nthere and and you know what the nature\nof this quarterly planning meeting is\nfor people who you think should be in\nthere\nEduardo are you able to chip in\nif not okay I I I'll fill it fill in for\nhim\num so essentially we we did one planning\nmeeting yesterday but we have decided to\ndo open Planning meetings for flight uh\nso if you are already contributing to\nflight using flight and I know there are\na lot of them out there today\nuh this is definitely a chance to come\nin and influence the core team uh\nwhich is working on flight including\nfolks from Union\num and and make your uh\nrequests be you know top of the line for\nus uh and the other thing is like we\nwill be sharing about what we have\nachieved where we could do better and\nhow we can uh and what are the next\nthings that we're going to ship\num we are from a point of view of the\ncore team we are actually entering into\na very\nfor the next month it's more of a\nPolish cycle essentially polishing the\ndocumentation and cleaning up the\nexperiences and getting started and and\nsoon follow up on that is going to be a\nmega\nuh execution cycle and the reason is\nbecause we've been working a lot on\ngetting a lot of products in the same\nrow uh metaphorically to to execute much\nmuch faster so stay tuned I think it's\ngoing to be an amazing uh\nend of the year as well as q1 next year\nand if you want your things to be added\njoin in if you want to pitch you want to\nhelp\nand help organize the\nthe planning meeting please join in that\nwas really help us so how can people\njoin in at this point after you had the\nfirst meeting what's the next how do\npeople join you in the moving forward\nright now is there a slack Channel or\nhow do they come\nI think definitely uh there is no slack\nChannel maybe we should add one Sunita\nif you think that's right uh but if even\nif there's no stack Channel definitely\nask in the community but there is a\nmeeting link shared somewhere and this\nis where Eduardo would have been better\nthan me\nuh but there is a meeting link uh as a\ncalendar invite ad event rather that you\ncan add to your event it should remind\nyou every two months or month\num to join in okay"
    },
    {
        "title": "Flyte Community Updates 023 featuring Orchestration by Flyte",
        "transcript": "alrighty let's get started\num\nCool October 4th flight Community is\nsaying uh welcome to the flight\nCommunity sink uh my name is Mark Stein\nI'm with Union Ai and I'm your host\ntoday and let's take a look at the\nagenda I think we have three four uh\ngood topics uh Community updates uh we\ntalk about the flight ecosystem projects\ntoday we're gonna do an update and a\nlittle bit of a demo I think as well\nabout Union ml by Niels and uh we will\ndive into the flight road map and\nthere's also one topic that I think is\nnot on the agenda but we will spend some\ntime talking about orchestration and we\nhave our samita here that wrote a\nfantastic blog post about orchestration\nwe're gonna have samita share of the\nsecrets with us a little bit and I'm\nsuper excited about that but let's kick\nit off with the community update\nuh my name is Mark Stein you can find me\non\num obviously on Twitter if you want to\nengage direct message and share some\nsome updates or some requests for what\nwe can change here and what we can add\nI'm open for feedback okay\nnext slide\num community of contributors today we\nhave zeph RAF of course uh Eduardo and\nEileen we're going to focus on RAF and\nEduardo let's talk about their\ncontribution first because I think\nthat's a fantastic contribution\nthat we should shed a little bit light\non in order are you online are you\navailable today\ncan you tell us a little bit about uh\nadding this\nand it's okay if kids in the background\nit's actually very cool so tell us yeah\nit's uh yeah so um\nArif from gojack you know um submitted\nthis PR some time ago and\num we worked together to get the\nan initial integration with DBT like it\ndoesn't do like DVD Cloud for example\nthis is a great like extension point\nbut um you can you can actually\num you know use we tested it like a\nbunch of the the uh the the adapters\nthat DVD comes comes with like you know\npostgresql sqlite and\num yeah\nI I hope you know people in the\ncommunity can now leverage this this\ngreat tool as well in in the flight\necosystem\nsuper excited to have it you know out\nnow awesome and I'm sure you're looking\nfor some feedback as well from from\npeople\num leveraging it I feel like they can\nfind you typically uh in the usual spots\nwith just uh our slack channel uh any\nother area where you would like to be\nengaged yeah no I I would say that um in\nthe probably the next OSS sync we're\ngonna have a demo of like using one of\nthe adapters to actually you know\nlet teach people how to use the the\nplugin if they cannot figure out by\nthemselves so yeah reach us you know on\nthe USS slack uh ping me directly or you\nknow in the public forum or\nwait for a demo they'll be the article\non like this how to use this\nit's fantastic and I think Caitlyn uh\nyou you had a view about that too if you\nwant to share it\nyeah the DBT Community actually had uh\nreached out to us some time ago about\nsuch an integration and this is great uh\nI I will definitely reach out to some of\nthe folks from the DBT team and see if\nwe can talk about this much more openly\nawesome fantastic okay cool let's take a\nlook at uh Eileen's uh updates on Eileen\nwas actually adding a\num\na a container level configuration on the\ndefault pod template which is actually\nuh to allow uh container name\nimplementation uh to the Pod builder\nthen I think I don't know if Arlene is\nin the meeting today but I know Dan you\nare and you actually uh approved the\npull request can you tell us a little\nbit about the significance of this if\nyou're around\nyeah absolutely\num so the default pod template is kind\nof what we're changing configuration for\nfor flight pods too\num instead of just kind of one-off\nconfiguration that it was previously so\nthis allows you know every configuration\noption that you could specify in a pod\ntemplate to be to be used\num and is very maintainable across\nkubernetes versions as things get\nupdated\num so this specific contribution was\nimplementing container level\nconfiguration so previously it only\nsupported pod levels but now every\ncontainer that's launched by flight you\ncan use any of the configuration options\ninternally there and they're all applied\nso so very very cool that's awesome\nthat's great yeah I looked at the the\npull request from Eileen and saw uh what\nyou added the finalized a really really\ngreat uh way to get to uh container\nlevel configuration which is really\nimportant for certain workflows so I\nthink that's fantastic great great job\num got to make sure that uh you know\nthat Eileen hears about this too next\ntime we have to make sure that we do\nsomething in a time zone where you can\nactually get time Eileen potentially in\na second update separate update we can\ntalk to her a little bit okay cool let's\nmove on let's talk about uh the big\nevents for for this month\num so there's kubecon cloud nativecon\nand Detroit uh October 26th to 28th in\nthe virtual Booth starts a little bit\nearly on the 24th already so here's the\nbig part\num flight is going to be represented\nthere uh Union has about a startup booth\nand the startup Booth numbers as you\nobviously start up 91 so if you're there\nuh swing by the folks from from Union\nfrom the flight team there you can chat\nwith them and engage with them I think\nthey will be super excited seeing you\nalso I want to mention that\num we have 20 discount on on the on the\npasses for Coupe console if anybody\nwants to go there from the from the\ncommunity and needs a 20 discount reach\nout to us contact Sandra on the slack\nchannel on the flight slack Channel and\nmake sure she's aware of this we can\nhelp you with the 20 discount that's one\nthing if you want to join uh the virtual\nuh uh booth and not not just the booth\nbut also virtually kubecon we also have\nattendee passes for that so they're\nactually pretty sure they're free I'll\nhave to double check on that but if\nyou're interested in going virtually to\nclipcon let us know they're limited\nthey're very limited I think we have\nonly five to ten left for those virtual\nuh passes so if you're interested uh\ngetting one of those same story reach\nout to Sandra on the flight slack\nChannel and she will make sure that the\nfirst 10 will get their virtual pass\nthere cool so with that I think um are\nyou going to see a little bit more we're\ngonna talk about\nas we move forward as a content content\nfor groupcon but before we go into the\ncoupon content\num I think there's a big content update\nthat we've just published uh I think\nlast week I would love to have uh samita\nwho wrote this together with Sarah\nFlores uh help us to dive into that\ncontent a little bit about\norchestrations I'll hand it over some\nItalian my question for you are you know\nwho is that content for what are you\ntelling us there what what are the\nthings that we could learn reading from\nthis and then obviously a request for\nthe community to weigh in common and\nshare this kindness of stations\nyeah hey everyone my name is Sam hetha\nI'm with the Union AIT\num I just wanted to give a quick\noverview of the two recently published\nblog posts of ours so the first one is\nuh orchestration for data ML and\ninfrastructure this blog post uh is a\ndeep dive into three categories of\norchestration really data ML and\ninfrastructure and we have also included\na couple of tools that you could use to\nimplement the orchestration strategy and\nin the end we have included a section to\nhelp you choose the right orchestrator\nfor your use case so we recommend you to\nthink about and answer uh questions like\nhow scalable should your pipelines be\ncan you tolerate sporadic scalability do\nyou need infrastructure automation do\nyou need custom environments how skill\nis your theme and what's your preferred\nwould using an ml orchestrator be\nOverkill I couple these questions with\nsome discussion around them so I\nrecommend you to check out this blog\npost in case you are interested and let\nus know what you think of it\num next slide please\nso this is one of the blog posts that we\nworked on uh this is a dive into the\nnewly added ml features a flight so\nflight decks is to visualize your data\non the flight UI Auto GPU to CPU\nconversion and pytorch this enables you\nto convert your\ntorch.tenser torch.nn dot module or\npytotch checkpoint which is a newly\nadded flight type by the way from GPU to\nCPU automatically which means uni not\nreally think about the two CPU construct\nanymore if for the GPA to CPU conversion\nand Onyx type Transformers so you can\nuse the scikit-learn pytorch and\ntensorflow Onyx type Transformers to\ngenerate the Onyx or the Deployable\nversions of your models uh and Spark\npipelines so this is a new flight type\nyou can now seamlessly pass Park\npipelines among your flight tasks and uh\ny logs integration we have had a\npresentation on this in one of our\nearlier sync up so this helps log your\ndata and ml models from within flight so\nyeah do uh read this blog posts and let\nus know uh what other ml features you're\npaying flight needs to have yeah thank\nyou I I really like when I look at the\nflight deck visuals on the right side I\nthink they're really cool that you have\naccess to them in the middle of\nexecuting your your pipelines your\nworkflows and getting results out of it\nthis is something we always feel like\nthe the kind of like opaque nature of\nsome pipelines gets completely removed\nand you actually get a clear\nunderstanding about what the data looks\nlike at certain levels of processing uh\nyou know compute or modeling and so on\nand so on so I feel like it's a really\ncool feature I love this a lot and then\nlast but not least the auto GPU the CPU\nconversion is is really so it's I don't\nknow it doesn't sound like much but I\nthink it's really really important to\nhave that\num so that makes the premium convenience\nthere's an annoying con it is an\nannoying constructive uh right in pie\ntorch code so yeah awesome very good I\nthink this is fantastic so everyone\nplease check out that uh blog post and\nblogflight.org and then engage with uh\nthe flight uh obviously ecosystem here\nwe are looking forward to your back from\nyou okay"
    },
    {
        "title": "Flyte Quarterly Planning Meeting - v1.3.0",
        "transcript": "what's up\nhello\nthis is uh\nthe union team to talk about the the max\nflight release the 1.3 release that's\ngoing to go out at the end of the year\n2022.\num our applied today should just go over\nthe the road map for 1.3 and also talk\nabout some of the the big features that\nwe are slating for this next three days\nlet me\num share my screen\nwait how do I do this on Zoom\nshare screen it's green it crates\ndo you guys see my screen\nand you are not exaggerating Zoom is\nreally weird now it went to my other\nmonitor but anyways\ncan you see my screen yes\nis the zoom okay\num I'll just increase it a little bit\none more yeah\nmaybe now that's too much Blue Mountain\nperfect good\namazing so\num I'm gonna go through this list real\nquick\nwe we're planning to make\num\nsmall change to simplify and synchronize\nthe releases of all components\num a mother changing that vein is the\nmove to the monorepo so\nwe're gonna you know go through the\nusual process of proposing RFC and why\nare doing this but we feel that we have\nso many\nso many reposing the flight\num or\nand every single time that someone has\nto make a change for example like in\nFlight IDL they have to touch like\ngazillion repos wait for you know all\nchanges to propagate so we thought that\nthe dive experience will be simpler if\nwe invest in actually getting the money\nrepo and just for our data point at this\npoint in time today is the October 3rd\nthe\num the cloud team inside Union had\nsuccessfully moved to immuno repo so we\ndon't expect that it's going to take a\nlong time\nit's just to you know broadcast that we\nare we're actually doing this for open\nsource as well\nit's a little bit unusual but we feel\nthat overall the cost will be without\nweight uh or the benefits will outweigh\nthe cost\nof actually having you know a simple way\nto test these\nthese changes that cross repos which are\nnot so many equipment\num\nbesides that we have a few bugs and\ntickets that we already you know decided\nto work on on the 1.3 release including\num\nchanges like\num\nthis book that that's uh highlighted\nhere or\nwe don't have a great way to\nrefresh the client's secrets in case\npropellers is running\num\nI'm not saying then and here I'm looking\nat you I'm not saying we're gonna do\nthis I just want to you know invest some\ntime in understanding like how\ncomplicated this would be and maybe\nassign this ticket and someone can work\nyou know with with you to get this done\num\nwe also feel that like the also in the\nsame theme of you know improving the dev\nexperience like we want to be able to\num diagnose failures in any point of of\nour offering experience including you\nknow when people interact with\nwith the um the sandbox deployments so\nsmall feature just to help people you\nknow from one single command and get a\nsense of what's broken with their\nsandbox deployments because you know we\nwe get questions every week about\nwhy my sandbox is not starting with some\nscrewed up state so\nagain we just want to make the the lives\nof users a little bit better\num\nkeep going flight kit\num\nthen made this change in the back end to\nlike unify how you define\num\npods and containers but this change did\nnot propagate yet the flight kit and we\nshould should invest some time in making\nthat a reality it'll be way simpler to\nsome you know would stop with this this\num arbitrary\num definition\nagain people come to us every week and\nthey are a little bit confused about hey\nwhen they should go at a with a with a a\npod or a container\ntask in I mean flight kit this will\num simplify also the the dev experience\num\nbe this bug\nis part of a project that he is leading\nit's essentially item 250 wait where is\nit no 251\num we want to expose\nan API to let people\num specify like global settings for\nproject right e\nH I'm I'm not actually entirely sure how\n250 is related to 251.\num but yes I think it is at the very\nleast dependent on 251 so\nthat we can talk about like how they are\nrelated but yeah it's just in terms of\nlike actually calling the API to get\nthese um project level like settings\nyeah anyways yeah\num\n252 is\nsmall tactical bug like my flight run\nshould offer a more consistent\num experience when you're not liking the\nin the right directory\n253 is one of the the big features\ncoming in the 1.3 release where\num we're gonna have a human the loop\ntasks in\num in the system including like\nhow you define them and visualize them\nin Flight console and how you define\nthem in the actual like\nauthoring experience like in-flight kit\nyeah expect more details on this\num\nit's a feature that a lot of people ask\nfor but\num we've had some trouble in defining\nlike the actual like like surface\nfor for it but um\nI think we found a reasonable compromise\nthat we have you know we have a version\nfor 1.3\n254\num\nas you probably know we have the spark\nplugin but it doesn't expose all of the\num or a lot of the first part API we\njust want to\num\nmake the lives of people who want to\nconfigure\num tasks that use spark a little bit\nbetter\nto 55\num\nwe're not entirely sure if it's really\ncoming in 1.3 but I\nI left it there then\njust in case you know we we need to\nsync up see how much work it is\nprioritize you know given the gazillion\nthings that you you and everyone else\nwants to do in this in this release\num\n256's work happening in this I won't go\ninto too much detail\num it looks like a small bug in the AWS\ndeployment\nwe hope to get it out in 1.3\nthis ticket's not super well defined but\nwhat is this feature do you remember\nthis is for like\nthe dog Hub stuff right this is yeah\nignore that that is yeah\nlike almost like a placeholder uh ticket\nI think it points to a more detailed\nticket but yeah the thing that Kevin is\nworking on yes cool it should be done\nshortly\namazing yeah so for for 1.3 also in the\nsame vein of you know improving the our\nour users lives we are working on um\nimproving not only the deployment guides\nbut the actual like\ninstructions on how to how to\ntroubleshoot and you know deploy stuff\nin AWS and gcp\nthese three tickets are kind of like in\nin the same vein\num\noops\nsupport for desk\num\nwe had\na an external contributor who wanted to\nto make this this desk plug-in\na reality\num\nwe need to work a little bit more with\nhim\nthen and um I don't know who else\nit's a I think it's a great addition to\nthe flight ecosystem I don't know if\num there was any more movement on this\nbut again this falls into the category\nof you know having\num\nan idea of how much work this would be\nto get support for desk in 1.3\num\n261 is\nit's not a greatly named ticket but\nessentially it's just a a feature to\nmake testing\na bit more secure so that we can\npublicize our tests\nmore widely\num\nI want to talk about 263 which is uh\nyou know in this This 1.2 release we\nwe had an external contribution that\num created conduct packages for\num\nour main repos including flight kit\na bunch of like the flight kit plugins\nunit ml you know Pandera and um but\nthere's a lot more work there\num\nI would say less than one third of the\ncurrently implemented plug it in Flight\nkit are have the corresponding conduct\npackages so you know some some\nhousekeeping here too\nactually have the remaining flight key\nplugins\num\ncreated in as a condo recipe and\nalso you know this this work also\ninvolves like\num documenting how to do it it's not a\nlot of work but like we just want to get\nit done this this next release\npeople like I would love to have you\nknow more OSS people here but um this is\none of the features that like 264 is one\nof the features that people in OSS asked\nfor is essentially lets us\num\nspecify\nin the definition of a task extra\ndependencies\num\nwe have more to say about this\nin the next few months\num\n265 small bug we're not really\ncaching the result of flight decks\nshould not be a whole lot of trouble\n266 uh we\nthis is an idea you know again based on\num\nOSS feedback but\npeople want examples of how to run CI CD\nworkflows\nand um we we just want to provide you\nknow template\nlike how to do this\n267 uh\nit's a bug\nnot a whole lot to talk about this\num 268\nI'll be honest and I think this isn't\nisn't that the feature that Nick was\nworking on well in the Spotify\nsee\noh they just oh my God sorry\nI feel like we talked about this then do\nyou remember much like what's the state\nof 268 this feature of like people want\nto\nremove all the executions and\nwhat else from from the database\nyeah you just want to like I don't\nrecall we'll probably have to revisit\nthis one I'm not sure exactly where it\nis like that I mean the idea is is if\nyou start deleting old executions it\ngets very dicey with you know linking\nthings back and yeah and if you try and\naccess them so um doing it cleanly is a\ndifficult difficult problem but I\ncertainly see the appeal of it\num we should certainly revisit it I will\nleave this open we need to probably\nreach out this guy put out like a PR\nalready for this\nsome time ago like I'm about to go okay\num\n270 looks like a small bug\num\n271 then you you can probably\nsay a few words about you know the fast\ncash and like what exactly is coming 1.3\nyeah yeah so uh from a very high level\nlike flight propeller in the background\nwhen it detects or executes a cache task\nit requires a number of different phase\ntransitions at the task level and uh\nconsequently at the no level as well\nand so what should be an extremely fast\noperation or what users expect to be an\nextremely fast operation uh can be\nslowed down in implementation because of\nthings like updating you know the crd\nand NCD if there's phase changes or\nthree phase changes that has to happen\nto ensure correctness in the system we\nupdate that crd three different times\nwith the node status updates\num this idea of fast caching is\nthis operation we can do just in one\nstep and we can evaluate Downstream\nnodes on the same step while still\nensuring correctness of the operation\num so uh we should be able to evaluate\ncash tasks extremely fast and this will\nhave significant impact in workflows\nwhere you have like you know eight ten\nyou know multiple cash tasks that are\nchained as dependencies of one another\num these should just require 10 lookups\nto see if they're cashed or not if it's\na cash hit\num should be you know measurable in\nmilliseconds for for the execution of\nthis so we're very excited about the\nperformance implications of this\nit should be it should be a cool feature\nnice\ncan I add one more thing to this though\nI think we've seen this multiple times\nand I you know Dan and I have also had a\nchat about this\nis uh caching at the sub workflow in the\nworkflow level itself uh I think lots of\npeople have been talking about this\nuh I'm not saying we need to wrap it\ninto the same task but probably in a\nsmall RFC and that might be\nwhat we would like yeah so so now that\nyou brought that into it you opened up a\nwhole can of worms um I I can mention a\nfew quick things about this\num currently caching's uh handled at the\ntask level in in Flight propeller\num and so there are implications on that\nand so as part of the change for this\nfast cash work\num we're proposing moving caching to be\nhandled at the node level\num which in turn doesn't mean a lot to\nmost people when they're talking about\nthis but internally in Flight propeller\nexecution it means that handling caching\nof sub workflows and launch plans and\nall this other stuff turns into a very\neasy task\nonce this changes in so yeah once we get\nfast caching working\num\nI know sub workflows launch plans kind\nof caching anything in the workflow type\nlevel\num something that a lot of people have\nbeen very interested in for a very long\ntime\nso it should be should be very possible\nvery quickly following this work I think\nthe problems over there are how do you\ncorrectly identify the uniqueness of a\nworkflow I think that's really in my\nopinion a challenge uh but you know that\nanalytic that deserves another tree\nyep\nnice\num\nin uh just in terms of feature features\n274\num is this related to what\nstripe was asking about then like so\nthis is this is exactly what strike was\nasking about actually okay um and I have\na fix implemented there's what's in NPR\nright now I should be changing this\nupside to myself and change to interview\num but yeah so so if there's you know a\nresource quoted that's set to two CPUs\nor something like that and the user has\na task that they try to execute\num that requires three CPUs\num right now there's there's a a weird\nback off and flight propeller that\nallows this to just run and continue to\ntry and uh schedule that task where it\nwill never be possible to schedule it so\nnow uh with this change we detect that\nwe fail that task right away saying\nwe're never gonna be able to schedule it\nuh once this goes up but but that's not\ntrue even dude you could chain the\nresource quota\nyou could but in its current state it\nwill never get scheduled so you win\nright if you update that oh you mean in\nthe current state of the research quota\nin the current state of the resource\nquota it's impossible\noh so but you either have a tasks that\nsit around and just wait until like the\nnote execution deadline in Flight\npropeller which defaults to 48 hours\njust Waits and weights and weights and\nwaits forever or we say\nit can't be scheduled here\nunless user updates a resource quota and\nwe return that in an error message uh to\nthe UI\num\nokay and how do you determine that you\nlook at the instantaneous value of the\nresource quota you look at the\ninstantaneous value of the resource\nquota so that's how it works right now\ninternally on the plug-in manager back\noff controller to determine the back\noffice as it looks at what uh what's\ncurrently scheduled and what capacity we\nhave to uh yeah but isn't that like\nspring parsing or something\nwe can talk about it more offline if\nyou'd like but yeah yeah yeah I would\nlike to like understand I think okay uh\nI I see why you're doing this it's just\nthat the same thing will apply to let's\nsay a GPU\nsay you say I want a GPU but the GPU\nnodes are just not available\n[Music]\nthe kubernetes cluster it returns an\nerror and says you can't schedule this\nno really yes that's that's and that's\nwhat's happening in this case is we try\nand create a pod that has resource\nrequests that are larger than the\nresource quotas available\neven larger or is it just beyond the\ncurrent use like let's say if I have a\n10 resource code of 10 and I'm using\neight already and now I request three\nso yeah and so we we\num\nthese two yeah the two use cases are\ndifferent so it doesn't take into\naccount the amount used currently what\nwhat we're saying is if the if the quota\nis 10 and you request 12. yeah if you\nrequest three and three is just not\navailable\nyes okay okay good yeah it's interesting\nI I actually feel this should be an\nopt-in Behavior but yeah but luckily\nprobably default could be obtained but\nuh or maybe knocked out Behavior\ncool awesome sure\nnice\nyeah\num finally\nthe last you know the 275 last ticket\nit goes in the same vein as the like\nimproving the dev experience like we\nfeel that sometimes people want to make\nlike to experiment you know the the demo\nin the sandbox\num deployments in\nadding this this pre-check flag\nwe'll just allow for a faster iteration\ncycle so we're also investing that it's\nactually a\nyeah that's already in\nreview\num\nyeah uh\nthere is one feature that's not\ncapturing the ticket yet but we're\nwe decided to commit to this release we\nwant to briefly touched on it already\nwhich is um\nout of core plugins we feel that\num\nthis will help\nlike simplify the the whole like\ncontribution story you know if you like\nright now\num\nall backend plugins are shipped\nalongside with the with the the\npropeller binary and that\nprevents a faster iteration and we can\nalso you know have a a simpler model for\nlike plugins that not necessarily need\nto be implemented in in go so\num yeah keep your eyes out\nfor an RFC\nand eventually\num some of this work\nbeing done in 1.3\nand with that I will stop sharing\ncool let me\num yeah I guess I'll see you again in\nthree months"
    },
    {
        "title": "How Infinome is Using Flyte - Krishna Yerramsetty",
        "transcript": "next\nkrishna who has sorted out all the audio\nissues let me introduce krishna real\nquick uh krishna is with the infinum\nbiosciences and he has been with us in i\nthink at least one other\nfireside chat about uh bioscience\nbiotech and uh flight and i think this\nwas very interesting july end of july a\nlittle bit ago and we said well let's\nlet's listen to how infinom is using\nflight and get krishna back here so\nkrishna please uh mute yourself\nintroduce yourself and tell us a little\nbit about your background then we're\ngoing to kick up the presentation in a\nsecond\nawesome can you hear me yeah\nawesome all right\nyeah so thanks for the introduction i\nrealized that picture\nis several years old now\nand i've grown a beard since then um\nyeah so i am a data scientist in\ninfineon uh so my form of background is\nin chemical engineering\ni did some machine learning as part of\nmy dissertation actually neural networks\nbefore deep learning was a thing\nand then i worked on several biotech\ncompanies since then\nmostly on data science statistics not\nnecessarily in the data engineering side\nof things\nso only in the past maybe one year maybe\none and a half year i've been actually\ntrying to\nwork a little bit or know a little bit\nmore about workflow management tools\nand in my previous job i was exposed a\nlittle bit to prefect\num and when i started at infinome i\nwanted to\nkind of stand up some of her analysis\npipelines\nthat's where i need to fly it and the\ncommunity and everything has been super\nsuper helpful so far and we actually\nhave a couple of pipelines that are\nrunning on flight right now and people\nhave been using themselves\nfantastic love it uh so\nto your presentation what are you going\nto talk about and do we hand it over to\nyou or do we have these slides uh on our\nsite uh i don't know\noh i i didn't send you the slides i can\nshare my screen and go online and then i\ncan share my slides after the\npresentation fantastic\nawesome\ncool\nokay cool so\num krishna tell us a little bit about uh\ninfinome i don't know if many people\nknow about the company but how big is it\nand what is infinum doing\nyeah so infinum right now is 18 people\nuh so we're a synthetic biology company\nwe were founded in 2020 right in the\nmiddle of the pandemic\num\nso what we're trying to do is what we\ncall lean bioengineering\nso synthetic biology for people who are\nnot familiar is is using microbes\nessentially to make products that are\ntypically made using other\nchemical processes typically\nmaybe let's say petroleum-based raw\nmaterials\nso what we want to do is you know start\nmaking synthetic biology\nuh profitable in some ways and faster\nso part of that is what we are calling\nlean by engineering where instead of\nputting i mean let's say\nmonths to actually make one improvement\nor one cycle of improvement\nwe want to reduce that time two weeks\nand also instead of putting tons of\npeople on it\ncan we actually have just two or three\npeople\nper project and still\nachieve\nimprovements within weeks so that's what\nwe're calling lean by engineering\nand that typically has been the\nchallenge with\ntraditional synthetic biology companies\nis is getting those products off the\nground\neven though i think there's a lot of\ndemand right now\nfrom the market it's harder for biology\nto work\ncompared to chemistry so\ninteresting so so when when and you\njoined the company obviously shortly\nafter they got funded so so i assume\nthat\nyou're the only data scientist on the\nteam or a small group of data scientists\nand how did that go data scientist joins\na team of i guess engineers\nyeah so we yeah so we they're biologists\nessentially so um\nso the ceo\nis a data scientist\nuh so he\nbut he's also\na biologist in the sense that he's you\nknow he's\npublished a very similar paper actually\ncalled prosar which is using machine\nlearning for protein engineering\nso he kind of\nstraddles i guess both worlds\nso he's he's the first data scientist i\nthink in the company and then he hired\nme\nuh so right now we're trying to add one\nmore person to do that so the\ninteractions with biology i think i\nthink one of the things that we learned\ni think i'll highlight this in the slide\nas well is\nwe want to put biology first\nand make sure that the bench scientists\nlife is easy\nwe don't want them to jump through hoops\nto submit let's say a job on ngs job\nthey don't have to like worry about what\ncolumns they need to you know exact\nformats of files and all those things so\nwe want to make their life as easy as\npossible and that is why we wanted to\nchoose something like flight so that\nthey can actually use these self-serving\napps to submit jobs\nlook at their results\nand make any tweaks to the parameters if\nneeded instead of\ndata science being the middleman because\nthey have the best context for data\nrather than someone like me\nespecially when there's multiple\nproducts going on at the same time\nawesome so yeah so if you go to the next\nslide sandra so the agenda like i said\nis you know i'll give you a brief\ninteraction to infinom which probably we\nalready did\nand then i'll talk about some things\nthat led me to choosing flight at\ninfinao\nand then we'll talk about a couple of\npipelines that we stood up in the past\nfew weeks\nand then the lessons learned at the end\nand take any questions if you have any\nso i think we have 40 more minutes so we\ncan go slow please stop me and if i\nthrow acronyms at you please stop me uh\nyeah so let's give this a discussion\nyeah\nso like i said my professional\nexperience i'm not a data engineer by\nany stretch only in the past one year\ni've been dabbling into this stuff so\ni'm a chemical engineer mostly mission\nlearning statistics\nworking in biotech\nand i'm part of a two-man data science\nteam along with our ceo we're looking to\nhire one more member soon so if you're\ninterested in working in synthetic\nbiology\nslash data science reach out to me\nso the next\nif you go to the next slide\nperfect perfect\nso infinium like i said we were founded\nin 2020 we have two locations\none is in boulder colorado where we do a\nlot of our microbiology and that's where\nour microbiology labs are\nand the other location is in burlingame\nclose to the airport in san francisco or\nsouth san francisco where we have a lot\nof our analytics machines so when i say\nanalytics it's\ngca\ngas chromatography\nmass spec it's not the analysis that we\ntypically do with memory crunching\nthat's something that\nit took me a while to understand when\npeople were talking about analytics is\nsomething different from analysis\nyeah so that's where we're located um\nprobably half and half personal between\nthose two locations\nand the leadership team is actually has\na lot of experience in biotechnology\nthey come from um\ncodexes\nother previous successful companies like\ncodex's androgen\ninterexon nova science and so on\nand if you go to the next slide\num\nso the problems like we discussed before\nwith the traditional way of doing things\nis they're slow\nuh and they need a lot of people working\nwithin for each of these projects so\nwhat we wanted to do is combine a couple\nof new technologies that came out in the\nrecent\ndecade or so maybe less than that uh one\nis called crispr if you're aware of it\nit's it's it's a technology that lets\nyou\nmake very precise changes in a genome\nlike if it's a plant genome it's a\nbacteria genome water\nand for that these two ladies jennifer\ndoudna and emmanuel carpenter won the\nnobel prize in 2020.\nso we want to combine that with what we\ncall directed evolution for which france\nis on all she won the nobel prize in\n2018.\nso combining these two tools or\ntechniques\nwe are able to speed up the cycles from\nlike i said months to weeks\nand also get away with having only two\nor three people working for projects of\nhaving tons of people working for\nproject while driving the same\nimprovements over time\nyeah that's what we're calling a lean\nbioengineer\nif you go to the next slide\nuh this there's a lot to take take in um\nbut\nbut what we what i wanted to highlight\nis this is an iterative process so\ntypically we start with a single cell\nyou know there's nothing\nthat's it's a common let's say e coli\ncell\nand we make single precise changes using\ncrispr\nand then we test these\ncells\nusing different techniques like\nsequencing or mass spec or grass\nchromatography\nand these machines generate a lot of\ndata and then we analyze these data and\nsay hey this particular cell with this\nparticular mutation seems to be\nperforming better than the other cells\nso once we know what cells are doing\nbetter than others\nwe can combine the mutations within\nthose cells in the next round so instead\nof having one mutations per cell we can\nhave two mutations per cell and this\nonce we do this for four or five cycles\nhopefully at the end we have a cell that\nhas four or five different mutations and\nis much much much better than what we\nstarted out with initially\nwhether that's for producing uh\nyou know let's say insulin for example\nor for any any other product\nso that's the general cycle of of how we\ndo things\nand this uh\nquick question here so\ni mean looking this is pretty complex\nhere and there obviously it's a small\ncompany but this you have to have a lot\nof automation now from the from from\nyour point of view obviously you're\nsitting probably in the top right of the\nslide right at the analysis\nright and this is that just just\nunderstand more classical you know\nuh pair t test or you know comparison of\nindependent groups and see you know what\nthat means how they differ or can you\ntell us a little bit about what you're\ndoing there\nyeah good question so there there are\nparts of uh workflows there are\ndifferent workflows that do kind of a\nvery complicated details or pairwise\ncomparisons different different\nconditions um a popular tool if you know\nis something called dc2 it's been there\nforever\nin rna sequel where you compare\ndifferent populations and say this\npopulation has something different\ncompared to other properties and that's\npretty much what we do with one workflow\nand the other workflow is\nis more statistics than anything else\ntrying to understand\nif you put these two things together\ndoes the data tell us that they are\nsynergistic\nright and then we say this combination\nseems to be better than\neither of those by themselves for\nexample\nso there's a little bit of statistics\nthere um and\nyeah so it's mainly being traditional\nonce you get to that once you get that\nraw data process\nit's statistics mostly\nit doesn't even come into like what we\ncall mission learning right now\nbut having said that we have done a few\nthings in the past where\nyou know with the recent advances in nlp\nfor proteins for example you know um\nwe're trying to\ni guess\nreduce our world of things that we can\ntest using these priors that we already\nhave\nbased on\nlet's say an nlp model that says if you\nput this amino acid next to this amino\nacid maybe you'll get a protein that's\nnot going to do as well\nso if we have that information we can\nreduce the space of things that we need\nto test that we have to test\nso that's something that we are also\nexploring which is more\nyou know\ngreat i guess\ncontemporary machine learning\nyeah and i think the interesting\ntakeaway at least for me as looking at\nthis with no\nprior knowledge about this is that you\ncan do classical statistical modeling\nand and t-tests and\nhypothesis testing and and use workflows\nthat are built on flight to really\nactually make sure you have reporters\nabilities not always that you have to go\nall the way to building complex machine\nlearning models is that correct\nyes yes so uh so right now what we're\nusing flight for is\nto take the data coming of these\nmachines and it make it in a way or\nprocess it in a way that we can actually\ndo these t-tests\nso we're we're not even using flight for\nt tests yet that is something that we\nwant to do going forward is make that\nstandardized as well but for now the\nthings that are easy to standardize and\ngive us the most benefit are there\npre-processing steps\nbetween the machine and the t-test if\nthat makes sense\nall right thanks\nyeah yeah\nyeah but the value obviously\ncomes from those final\ntests or analysis where you actually get\nsomething actionable out of them\nyeah so if you can go to the next slide\nany more questions by the way so far\ni know i read a lot of biology and\nacronyms at you so\nall right so\nthis is where the robber meets the road\ni guess for for this presentation so\nwhat are the challenge we are facing um\nso uh for us i think there's there's a\ngood difference that i kind of started\nappreciating the past couple of years\nwith typical companies that use workflow\nmanagement tools um or even you know\nsoftware pipelines versus\nmost biology companies is the analysis\npipelines are evolving really fast so\nthere's no pipeline that you can build\ntoday and expect to use for one year\nwithout making major changes that\ndoesn't happen in biology things change\nso fast because we're still in the\nresearch phase\nright so we need to keep up with those\nuh evolving pipelines and we don't want\nto spend too much time developing\npipelines and standing them up and\ntrying to do you know really make sure\nthat everything is failed safe failures\nare okay because you know it's we can we\ncan deal with them if it's one in ten\nthat's fine\nso that's the major challenge and then\nwe have a hierarchy of analysis\nso like i said the pre-processing step\ntaking the data from the machines making\nit a little bit more human readable\nis is the maximum value i think for now\nuh that's what we are building the\npipelines for flight using flight for\nright now\nthe secondary is what uh martin was\ntalking about you know t-tests\nenrichment depletion\nuh those are secondary analysis we want\nto get there hopefully by the end of\nthis year\nand the traditional analysis is a little\nbit more complicated uh\nbut i don't know if there are any common\nthreads that we can use flight for we\nstill have to ruminate on that\ninternally\nbut the key thing i think for us is both\nfor biology and the data workforce we\nwant to identify common patterns across\nmultiple projects\nbecause we're dealing with more than six\nprojects right now\nand i can't keep up with you know\nall of them what's happening with all of\nthem so we want to make sure that most\nof these things are automated as much as\npossible\nso as we go along this journey because\nwe're a small company we're still\nlearning with these different projects\nright now we want to identify what\nworked for biology what work for\nanalysis\nand automate each of those separately\nand probably flight is the way to go for\nautomation automating some of these\nsources\nand the last thing i want to say is\nbiology is always the bottleneck it's\nhard uh so we want to make life easier\nfor being scientists\nand even if that means\nsometimes\ncustom pipelines need to be built or or\nwe need to spend a lot of time trying to\nmake a particular file format easily\nreadable by biologists that's definitely\nwhat we want to do we want to don't want\nto make take the shortcuts for data\nscience and make life easy for biology\nand that's where i think flight really\nreally shines for us\ngo to the next slide\nso why flight so like i said uh\nwe want to give the power back to\nbiologists so we want to stand up\nsomething that they can actually use\nplay around with different parameters\nfor their models because not every time\nnot every parameter is fixed so with\nsome of these workflows there's nuances\nto each data set\nso we want to make sure that we are\ngiving them the power to uh\nto run these analysis\nand not having data science be an\nintermediate\nfor a reason like because it it results\nin slow tone of analysis and i've seen\nthat multiple times in my career so\nthat's where we wanted to use something\nlike a workflow manager like flight\ncan you still hear me by the way\nyeah\ncool\nand the other thing is\nis\nis most of these bioinformatics\npipelines are kind of like data flow\nprogramming\nmodels like you push data through\nseveral of these small\ntools\num so and again having tasks and like\nyou know how to flight treats tasks is\nperfect for what we want to do maybe\nthere's a better way of doing this but\nfor me i've always thought about you\nknow tasks as as the key\nuh for most of our bioinformatics\npipelines\nand some of these types are trivial and\nfast while others are slow and also\ndynamic workflows are really important\nand we just don't want to use all static\npurpose all the time because that\nbecomes really hard to program\nand be able to work with python r and\nhopefully julia too in the future and\nhaving container tests at least on paper\nseems to be the way to go even though\ni haven't really personally tested uh so\nfar we've only been using python uh\npipelines\nbut hopefully in the future we'll start\nusing r a little bit as well\nyeah so so one one one thing that comes\nto mind and um probably see that\nnow the data passes through lots of\nprocessing steps what does that look\nlike when you don't use an orchestrator\nlike flight i mean i mean what's that\nworld tell us a little bit about that\nexperience yeah it's\neven better so i use next flow uh we\nwill talk about them a little bit\nbriefly but even before that lots of\npeople use batch scripts right so they\njust put these things together i've done\nthat in the past so that's and it's not\ntractable you don't know\nwhere things are failing and you don't\nhave any reproducibility for some of\nthese things so yeah so that's\nbioinformatics is still a while\ni'll rest in in that regard so\nright right okay fantastic yeah i i i'm\naware of\nsome of those processes and then uh\nbeing able to work with python r i mean\nit can work with any language r as well\nin a raw format that's supported today\nin flight uh but obviously there's\nthere's a missing sdk but we maybe\nthat's that's something for\nfor the um quarterly planning unless we\ncan talk about that topic for flight\nthat might be a good topic okay\nfantastic let's keep going\ngood question any more questions so far\nagain apologies if i don't use the\ncorrect language because i'm not a data\nengineer but please uh feel free to\nask any questions or comments\nhey krishna this is\na problem union i have a question\nfor you and this is probably orthogonal\nto in the current part of your talk but\nlike you said you have currently using\npython sdk how many use cases do you\nthink are in our\nparticular and because and the reason\nwhy i'm asking this is because across\nmultiple decent talks you've heard are a\nfew times from a community point of view\ni'm just\ntrying to get\nwith interest as well as\nyou need to have an rsdk a native\nsupport for our influence\nyeah i think our\ni mean\nover the years i think python has\nactually\nbeen\nmore widely used than r over time so the\ntrend is i think increasing towards\npython over or over time that i've seen\neven within biotech\nbut r still has a market\ndefinitely in statistics so you can be\nthe are with some of the libraries that\nacademic sessions\nacademics\nthey push out a lot of our libraries\nconsistently and and python statistics\nlibraries are still a little bit lagging\nbehind our\nso that is the i think\nthe definite\nuse case for r is some of these\nlibraries um and i can send you a few\nthat are\nused over and over in bioinformatics i\nmean if you you can even search single\ncell uh pipelines in r\nthere's so many of them some of them are\nstarting to be developed in python as\nwell but are i think it's still the main\nngs\nworkflow to\nour library\nthis is what i wanted to know about like\nyou know it's basically within so it's\ncorrect to say that it's within the\ndomain of bio tech and\narea rs dominant for\nmany use cases still\nand\na native of our immigration might really\naccelerate\nuh your\nportfolio or your uh the way you develop\nit\nright yeah make your life easier\nessentially\nexactly yeah i think so uh\nyeah it'll if if that is\npart of you know uh flight\ni think biotech is almost\ni don't wanna i don't wanna say that's\nthat's pretty much 80 percent to think\nof\nof what people in biotech use\nphytoplasts are should cover most of it\num\nand if i just add to that obviously\ni mean on the statistical modeling side\num it it makes a lot of sense uh most\npeople are you know using\num are there and also not only bio but\nin life sciences uh when you have things\nlike fda submissions and so on uh as\nwell\nthat's a whole other area where there\nwas another tools was sas that was\ndominant that has changed right\nthere's a change from sas to r on on the\non the science side that is i think very\nnoticeable and i feel like on anything\nthat is\nml related it's not really the domain of\nr so you can do it but people know that\nthere are you know better tools and\nthat's all in python so i feel like data\nscientists today have to do at least two\nlanguages and in many cases actually\njulia is really an important part it\ncannot just be you know just saying with\none language i'm going to be good i\nthink those those things are over i\ndon't know do you agree with that\nstatement\nyeah i think i think the sas to r i\nthink i've seen that in the past i\nhaven't kept up with it\nuh recently but that's definitely lots\nof people have transitioned over to our\nfirms as\nthe all those traditional i think you're\nright the fda old school statistical\nanalysis companies did transition to r\nyeah so there's that market yeah\nokay\nall right thanks\nyeah\nso if you go to the next slide\nso uh yeah so i guess\njust four different next generation\nworkflows are probably common within uh\nwithin infino\nso edit detection\nthis is the main workflow that that\nwanted me to go there out of kubernetes\nand flight because it was at that time\nit was really slow it took five days\nsix days\nto run for 100 samples um so uh we\nwanted something that scales really well\nwith samples\nand it's super fast so that was the\nperfect use i was exploring kubernetes\nand then i ran into flight and i thought\nthis is perfect marriage so that's how\nuh\nwe implemented it is and now it's\ncurrently i think less than a few hours\ndepending on the number of samples\nwe can get the results back\nthe second one that we stood up recently\nis plasmid qc which uh which has some\nunique challenges in the sense that it\nuses a lot of different tools and also\nthere's some scaling issues depending on\nthe number of samples in green depth\nthe third one which we haven't\nimplemented yet which is a mix of r and\npython to do the t test comparison stuff\nthat martin was talking about\nuh and the last one is more traditional\nor contemporary ml workflows that we\nhave done in the past but we want to\nresurrect an infinion uh hopefully we in\nthe past we use sagemaker and i've seen\nsome\ncool things that you guys are doing with\nflight and sagemaker so\ni'm excited at some point to test it out\ncool and if you go to the next slide\ni'll show you a couple of\nso what we've done is we kind of\ncombined flight with streamlight so\nwe've used streamlight in the past and\nlike i said because our analysis\nworkflows change so often and even our\ndata injection changes so often we don't\nwant to go down the route of trying to\nlike dash or something because\ni mean dash has some advantages but\nstreamli because it's python based and\nit's so much easier to\njust have a\nslider using one line of python code i\nlove streamed it so i combined flight\nwith streamlit so using streamlit\nuh biologists can submit their jobs they\ncan choose the parameters for example\nusing a drop-down menu and say hey these\nare my parameters for this job this is\nwhere you can find the data on s3 just\nrun run this pipeline for me\nand then they can see the results in in\nstreamlit again um and then yeah so like\ni said it takes\na few hours right now compared to a few\ndays in the past\nuh one thing with this particular\npipeline is it's uh\nthere's a lot of small map tasks for\nwhich i think\nthe ray integration is probably the\nright thing to do i don't know anything\nabout ray but it seems like it's based\non some google searches that there might\nbe some efficiency that we can get out\nof it by using\nthat instead of simple map tasks\nuh so this was our first pipeline works\nbeautifully uh so far\npeople have been using it\nwith some minor issues here and there\nbut nothing to do with that\nitself\nand if you go to the next slide\nthis was the most recent one again the\nsame philosophy streamlined as the\nfront-end flight as the back end to run\nthese jobs\nthis is a complex pipeline with multiple\ntools so there's a whole bunch of tools\nand the key thing is i think to keep\ntrack of the versioning of these tools\nand the problems of how the analysis\nresults were generated using what tool\nand so on so for that again flight is\nperfectly suited so we have a list of\nhow how how are these results generated\nwas it using minimap ii what version of\nmini map 2 was used and so on\nand we get a nice uh set of results at\nthe end\nwhich\nwhich is much easier to digest for\nbiologists\nif you go to the next slide i think is\nthe\nyeah so any questions so far on\non what we were trying to do i know some\nof that is very superficial if you have\nany questions on how exactly i've\nimplemented some of these things please\nfeel free to reach out\nwe can have a chat about that\ni do have a question for you krishna so\ndashboards is i think a very very good\ntool to share results is there another\nneed\nto to create i mean usually the more\nscientific you know documentation and\nreports with uh how do you do this or is\nthere no such need as dashboard exactly\nwhat you have to do and then when you\nput together a paper that's all a\ndifferent manual process how do you\ncreate paper science papers oh yeah so\nthat's all a different process so this\nis uh\nyeah we just we don't generate reports\nyet\nbut in the future probably yeah once we\ngo down that route of you know if we\nwanted to\none of the things that we were talking\nabout recently is if we wanted to make a\nreport of all the successful mutations\nfor example you know how do we pull\nthose things and is there a format for\nit where you say when someone goes in\nthere and says hey give me all the\nsuccessful mutations for this particular\ngene\nyeah\nthat we tested can we can we provide a\nreport of all the things that we tested\nwhen was that tested in the lab and how\ndid the results look like\nand what results were generated for that\ni think that would be super helpful\nyeah what i have seen\nis very often the usage of parametrized\nmarkdowns\num and then you can run things and then\nobviously the experimental of the job\nchanges variables change but the\nmarkdown actually picks all up and then\nit produces the same scientific output\noh interesting okay\nyeah that's something we can chat about\nand see how this\nis yeah i've never done that before so\ncool\ninteresting\nany other questions or comments\ncool\nso yeah so a few things that i loved so\nfar super helpful team i can't say this\nenough and everyone's been super\nresponsible and slack\nexamples on the website and deployment\nwalkthroughs especially when i first\nstarted i were also super helpful\nkubernetes\nlike i said was what we were looking for\nintegration with aws\nit was very nice\nshell paths was important for a lot of\nbioinformatics tools um\ncontainer tests obviously for working\nwith different kind of languages like r\nand julia\nmap tests were super nice\ni still don't know i'm using them\ncorrectly um i need to\ndive into them a little bit more\nthe one thing that i really like\ncompared to i think my previous\nexperience with some of these tools was\nthe local dev experience\nwith pi flight and the sandbox\nare super super nice to to reduce that\nfriction between production and dev\nenvironments\nso that was really nice for me to see it\ni mean it's not completely zero but it's\nmuch lower than what i thought it would\nbe when i first started\nflight console is nice uh ability to\ncache results is good task level\nresources was super helpful because some\nof these tools are very very memory\nintensive so you being able to say give\nthem a lot of memory compared to other\nother\nother tasks was also super helpful\ngo to the next slide sticky things\nmore than anything else i think because\nit was my first time um\nwith kubernetes half in battles were\ntrying to understand cool movies more\nthan anything else\num\nfirefly there were some minor\ndifferences with the prod versus what i\nwas doing locally\nshell tax and map\nuh\ni don't know what the recent status is\nbut\nwhen when i was developing i don't think\nshe'll task work with math\nbut shell tests are super super uh\ncrucial for bioinformatics and sometimes\nthese are so small\num that you can like spin up\nusing map multiple of these things that\nwould be super uh useful for venezuela's\npythons\nslack channel yep sorry this is\nit again uh yeah are you like do you\nhave any pro for the shared tasks with\nmap because i think i should just work\nout of the box and if it does then\nwe can take a look into it\nbut\nwe would love to learn your use cases\nbecause yeah i can see a lot of updates\nin map tasks recently in the last two\nyears so\nperfect yeah i can send you a couple of\nuh a couple of tasks um that that are\ni think used\nin almost half the biomedics pipelines\nthat might be a good setting\nyeah so yeah so a few sticky things um\nagain nothing nothing major so far my\nexperience has been i think\nsuper positive compared to my\nexpectations when i first started out so\nthank you thank you thank you for\nputting this tool together saved a lot\nof those times\nfantastic so thanks krishna for for\nyou talk so i would love to still see if\nthere are a few more comments or\nquestions from\num from the fly team here as well\nso\ni think we wanted to learn a little bit\nmore about the use cases that was one\nthing um\nanything else where you know create\nfaith in eduard you want to chime in and\nand talk a little bit about the future\nwith flight and\nyou know potentially have\nfeedback for\nkrishna\nfor mine i think i would love the map\ntasks uh solve your problems as they\nshould\nyeah definitely you know if i don't\ndon't think twice about fighting issues\nit's okay even if\nthere's a misunderstanding we're\ndefinitely\nall going over that issue\nyeah yeah it's all that documentation\nright that's a great part yeah\nyeah like i said part of it is probably\nme not keeping up with things uh in the\npast few\nweeks yeah\nbut\nthis is this was great like it really\nhelps us understand the landscape\nyou know we've had\na few folks share their journeys up you\nknow in biotech recently and a few more\nlined up i think uh\nthat's it's just amazing to hear\nthat\nthere was this tool exists that can\nsolve you know problems within the world\nthat it was not designed in the\nbeginning to solve purposes\nwe are working we had no idea about this\nbut it's great and we want to make it\nbetter now for you guys yeah please let\nus know\nyep yep it's definitely um\nlike i said one of the best tools\ni've used so far and\nand the proof\ni guess is is how much time it's saving\nthe biologists\nright now and they all love it so\ni wish i could\nkeep them uh give you their testimonies\nmaybe in the future\nright thank you thanks\nthank you so much i we really appreciate\nit let's go back to the slide deck and\nuh to the next part so really appreciate\nthe insights krishna we will have you\nback and once you get to the other parts\nand uh big progress and uh learn about\nhow your workflow evolves we would like\nto do that and talk about that so thank\nyou so much\nthank you"
    },
    {
        "title": "Flyte at Hacktoberfest 2022",
        "transcript": "now we're going to talk about something\nsuper exciting which is the hectographer\nso well I mean you can hear my accent\nI'm German the real Octoberfest kicked\noff last Saturday uh and that's actually\nstarts in September not in October so\nmaybe next year we have to do our\nhectoberfest or we have to\nconvince the hectoberfest organizers to\ndo a little sooner but Oktoberfest is\nhappening uh we are going to participate\nand I will hand over to samita to talk\nabout the hectoberfest and please take\nit from yourself\nyeah thanks Martin hey everyone my name\nis samhita I'm a developer Advocate at\nUnion AI so we're back again this year\nwith a month-long virtual festival\ncalled hacktoberfest that celebrates and\nsupports open source uh this year Meet\nflight Union ml uh at hacktoberfest\nWe're not gonna have Pandora but yeah\nflight and Union uh the contributors\nagenda is quite simple uh back in the\nmonth of October meaning create pull\nrequests and when SWAT and of course uh\nshow open source some love uh regarding\nthe point of contacts uh I'll be\nhandling flight Eduardo and NEOS\ntogether will be handling the union ml\nrepository next slide please\nyeah now comes the interesting part\nwe'll be giving away a grand prize to\none top contributor uh for two merged\npull requests you will be receiving a\nflight Mark for three merged pull\nrequests or tutorial video or an article\non or an article you'll be receiving a\nflight t-shirt for a plugin you will be\nreceiving a voucher or the North Face\njacket and for three plus merged pull\nrequests you'll be receiving uh flight\nso price box\nnow how do you contribute to\nhacktoberfest you can register anytime\nbetween September 26 and October 31st on\nthe hacktoberfest site pick any\nhacktoberfest label issue in the flight\nand or Union ml repository put a command\ncomment on the issue if you're willing\nto work on it after the issue is\nassigned to you you know you can start\nworking on it and create a full request\nonce it's accepted or merged uh you'll\nbe missing your swag more details about\nthis process are going to be enclosed in\na blog post that's coming soon so yeah\nhere's kicking off hacktoberfest 2022 uh\ntogether let's make it a success thank\nyou\nfantastic thanks Amita so samita quick\nquestion for you we had uh the hectober\nfirst last year right so what was the\nmost memorable thing that that you took\naway from last year what was the one\nthing that he said this was really\nfantastic\nI think the contributions you know some\nwere like some were very uh uh you know\nthey were of a reasonable size you know\nthey were not just like a simple\ndocumentation changes we have got uh two\nor three that were uh you know that\ncreated an impact\nso yeah that's that's cool\nthat's great that's fantastic so let's\nsee\num that we can do this uh have the same\nimpact this year we'll make sure that we\nget the hacktoberfest\num made public in the community so we\nwould like uh also New community members\nhopefully to hear about it so spread the\nword about Oktoberfest and flights\nparticipation we'd love to have a lot\nmore participants this year so thank you\na lot uh samita for organizing that\ngreat"
    },
    {
        "title": "Flyte Community Updates 022 - Flyte Cheat Sheet - Sept 20",
        "transcript": "okay thanks a lot all right so welcome\nto the flight uh Community sink almost\nfeels like a tradition two things like\nfantastic my name is Morgan Stein I'm\nwith Union Ai and I'm your host today\nso we have a few housekeeping notes as\nalways number one we're going to record\nthis whole session and we're going to\nshare in social media so don't forget\nabout that number two\nif you want to chime in\nplease raise your hand uh with zoom and\num I'm mute yourself state your name so\neverybody knows who you are and give\nyourself your background and then chime\nin so we want this to be a conversation\na dialogue that's just like a monologue\nsign I welcome people to chime in so\nplease do that\num last but not least\nflight is a very fast growing Community\nwe're super excited about that but we\nwant to grow further and faster and we\nwould like you to go out and spread the\nword so please send\nwelcome and new users to fly.org have\nthem sign up on slack have them go to\nthe GitHub repo ideally you like it and\nstore it we would be very happy about\nthat so with that\nwe're going to talk about the agenda we\nhave three important items today\nCommunity updates lots of stuff is\nhappening super exciting so we have a\nfew events even tomorrow uh and then\nhack topafest it's almost becoming a\ntraditional second time I'm gonna talk\nabout this and last but not least uh if\nuh and Wayne Krishna has sorted out\naudio issues I think thumbs up Krishna\nright you can hear us good thumbs up\nawesome we're going to hear from him or\nhear from him about if you know how\nthey're using flight it's going to be\nvery very interesting okay let's go to\nthe next one\nCommunity updates uh if you want to get\nin contact with me you found my I find\nmy Twitter handle there but the most\nimportant piece is the content here\nCommunity updates is something that\nactually involves a lot of people not\njust like presenting a few things I want\nto make this very clear\num samita from Union uh sat down and\ncreated a flight cheat sheet which we\nthink is an amazing tool to help data\nanimal Engineers to really understand\nthe context of code Snippets crucial\nslide Concepts so really on one page\nversus you know going through the whole\ndocumentation finding pieces sometimes\nyou just want to look up you know some\nfundamental fly features and tie to the\npython code and this is what this cheat\nsheet is for it's versioned this is our\nfirst one we would like to have a lot\nmore I'm just going to be very very\nhonest so we can do a lot more cheat\nsheets about different contexts and\ndifferent um tasks I don't know areas\nlike flight test you know and so on so\nthere's the areas where we want to to\nReally involve the community on flight\ncheat sheets it's at least feel free to\nget started on a cheat sheet if you\ndon't know how to contact uh the\ncommunity join the slack Channel and ask\nhey how do I get started on cheat sheet\nI have a great idea I would like to\ncapture and we can help you fantastic so\nthanks to samita who put this forward\nyou can find this cheat sheet on\nblog.fly.org so go there and uh look at\nit you can download as a PDF document\nand share its three teams okay next one\nnext up is a great event that we will\nhave tomorrow Tuesday September 21st in\nBellevue it's an in-person event\nwith two companies that have been\nworking closely together any scale and\nUnion on open source projects so this is\nan open source event for the flight\ncommunity and for the ray Community what\nwe go into here is any scales Richard\nLeon who's talking about Ray air a\nscalable and unified toolkit for ML\napplication shirt and Union AIS in the\nworld of glenario and he's still going\nto talk about flight most important\nintegration that we have with raid is\ngoing to Showcase that and I feel like\nthis is going to be a very valuable uh\nworthwhile Time Event it's in the Hyatt\nRegency in Bellevue if you're in the\narea if you're in the Northwest if\nyou're in Seattle Bellevue you can go to\nmeetup.com the URL is down there sign up\nthere I think 11 seats left we have a\ntotal of 50 seats we know some people\nmight not show up so 11 seats are left\nso please go there today and register\nand come tomorrow so you're gonna like\nthis one next one\nis the global Dev slam so this is you\nknow I didn't even know that existed\nthat's my ignorance so apologies for\nthat but when I saw this like wow that's\nvery interesting so we have Nielsen\nHazen from Union going there\num obviously they're talking about\nflight and the union ecosystem uh I\ndon't know Neil's Haytham is there\nanything else that you want to highlight\nI'm just going to call you out here on\nthe spot if not it's all cool but\num I would assume um potentially to hear\nfrom that\num update to get an update later from\nyou guys I don't know if there's a live\nstream or not there's no URL there's the\nglobal Dev slam URLs but I hope there's\na live stream as well can you confirm\nthat\nnot sure if you see him anyway we're\ngoing to hear this later we're going to\nfigure that out if there's a live stream\nwe're going to communicate this through\nthis like uh Channel as well but\nworthwhile as well next one\nlight quarterly planning meeting so this\nis something we uh have not talked about\nthat often but this this is a community\nevent uh we want to make sure that we\nhave Community chime in on the quarterly\nplanning into flight uh so obviously we\nhave the release 1.2 which will be out\nend of September and we will have to\ndiscuss we want to discuss would like to\ndiscuss the version 1.3 so this is your\nopportunity to join and share ideas and\nrecommend features not just like for\npeople who contribute in code but also\npeople who use flight and say hey look I\nhave been missing this integration or\nthat feature or if we could do this then\nit would be much better for us and save\nus a lot of time so we would like to\nhear from you so there's an ad event you\ncan see this add event uh and just click\non that go on the slides Channel we're\ngoing to put it in a slack Channel as\nwell so you can go and add event there\nand then Monday October 3rd 11 30 12. it\nsounds like a short amount of time 30\nminutes it might run a little longer if\npeople show up with great ideas I hope\nso and then we have a first at least to\nmy knowledge quarterly Community\nplanning meeting and we will make this\nobviously on a quarterly Cadence more\npublic and also we'll report about what\ncame out of this and I'm sure Eduardo\nwill talk about this after the meeting\nin the next community"
    },
    {
        "title": "Flyte Community update 021 featuring Dr. Fabio Grätz, on building ML systems using open-source tools",
        "transcript": "all right welcome to the flight\ncommunity sink my name is martin stein\ni'm your host now with union ai as\nalways a few housekeeping notes before\nwe get started uh this zoom call is a\nrecorded zoom call we'll share it later\non social media\nso if you have any comments about\nrecording sharing reach out to me\npersonally the flight community is\ngrowing rapidly so this is also a call\nto action for people who see this on\nyoutube if you want to join us go to\nflight.org\nyou can join the slack channel and you\ncan join the flight community we would\nlove to have you there\nso with that um\nlet's take a look at the agenda for\ntoday i feel today is going to be a 30\nto 40 minute conversation depending on\nhow much information we get from our\npresenters including uh niels today so\ni'm looking forward to learning after\nthe community update a little bit more\nabout\na flight ecosystem project called union\nml it's an open source project we have\nspoken about union email\nbut we'll give niels abandon land today\nthe stage to give us a little more uh\ndetails more insights here about uh\nunion ml\nand the second part will be about the\nflight road roadmap eduardo\nis here today ago auto drives the open\nsource\ndevelopment work at union for flight\nand we'll have adora to give us a\npretty detailed overview about what\nwe're working on um wrote that provides\nbut also a few other topics that he will\ntalk about later so with that let's get\nstarted uh and dig into into the\ncommunity updates my name is martin\nstein\nuh and uh here's the first community\nupdate so that's a recorded chat i'm not\ngoing to spend too much time about\ntalking about this but he can go to the\nrecording it's on youtube it's uh\nkatherine omar who's\nthe chair of flight.org as well and you\ncan see him talk to sage elliott about\nstrategizing for data and male ops\nobviously very relevant topic i feel\nlike it's a great uh\nconversation with sage elliott on r\nsquared you'll find it on youtube the\nlink is on the slide and we'll share it\nlater\num upcoming and next\nslides upcoming there is a machine\nlearning hangout\nwhich i'm super excited about\nthis is about flight and rape and we\nhave two people\nwho are experts one is richard leon he\nis with any scale and is company who\nopen sourced rape and richard is going\nto talk about\nrey heir\nand edward is going to talk about the\nflight integration into array that we\nhave\nand this is going to be an event on\nseptember 21st in person in bellevue\nwashington so if you're in bellevue\nwashington seattle or if you plan to to\nstop by uh we would love to have you um\nrsvp pretty early it's limited seating\nfirst come first serve\non this two hour event starting at 5pm\nso all the information is there you can\ngo to the\nurl shown\non the slide here or you can just google\nmachine learning hanging out flight and\nrain you'll find that information so i\nlook forward i will host the event i\nlook forward to seeing you there on\nwednesday september 21st at the hyatt\nhotel in bellevue okay\nfabio uh great to have you here uh\nhi to where are you in berlin already in\nuni\nno i'm in\ni think you've heard it so fabio is uh\nwith morantik's tcml offsleet\num and i'm not going to read through all\nof his experiences so much to talk about\nand what he has done but there's one\nthing that i actually i feel like is\nreally really interesting\nfor you and that's on the next slide\nit's a talk that you're going to give um\nat the\nuh somewhere if you could go to the next\nlike this it's a talk that you're going\nto give at the open source summit in\neurope uh first of all maybe we can\nintroduce yourself real quick to the\ncommunity and tell us a little bit about\nthe talk and i'll have a few questions\nfor you so don't go into all details\nbecause i might cover some of those\nquestions too\noh i'm muted okay first of all thanks\nfor the intro martin um\nyeah i'm fabio i live in berlin germany\ni\nlead the machine learning operations\nteam at the company called mirantix\nmomentum we are a solutions provider we\nbuild machine learning systems for for\nindustry clients in germany\nand my team basically builds the\ninfrastructure\nand the\num\nlet's let's say like this we we we built\nan internal developer platform that\nallows our ml engineering teams to\nself-serve infrastructure they need to\ntrain models and to deploy them to\nmonitor them that's basically what my\nteam does\ni was a machine learning engineer before\nin nlp and computer vision\nand i'm a physicist originally so that's\nwhere my heart lies\nmy original heart lines let's say i\nnow i love ml and kubernetes and\nlike it's perfect when they're combined\nso that's why i like flight so much\num\nyeah so much about me maybe um if we can\ngo to the next slide and i'll talk about\nwhat i'll be talking about so there is a\nconference in dublin from the linux\nfoundation the open source summit\nand they have one format called the ai\nand data forum\nwhere they talk about\nopen source tooling\nin ml big data analytics etc\nand\ni submitted an abstract half a year ago\nwhere i or for a talk in which i want to\ndescribe how we we have momentum\nuse open source tools to build\nmachine learning systems that um that\nactually make it into production because\nor stay there because many projects fail\nthere\nand kind of the main\nargument in the talk that i'm going to\ngive is that\nlack of automation and orchestration is\nthe reason why so many projects fail\nbecause\nteams treat ml systems like scientific\nexperiments where lots of one-off manual\nwork is involved and then\nwhen the model is in the real world and\nthe real world changes because\nwhere for example the economic situation\nchanges or\nanything changes system breaks because i\ncan't handle that because the data is\nthe data distribution the real world's\nvery different and if then\nyou don't have any automated release\nprocess and need to do everything\nmanually\nmaybe it's hard to bring a new model to\nproduction and\nso i argue that uh\nin when you want to build more robust ml\nsystems or email up systems you need to\nautomate and orchestrate and i'll also\ntalk about flight in that talk because\nwe use flight introduction very\nsuccessfully and\nif you could you go to the next slide\nplease\nso\nthis is one\nlike a diagram that comes from an\narticle from google cloud architecture\ncenter\nwhere they discuss different different\nlevels of maturity that the l systems\ncan take so\nin the least mature stage they say\nthere's everything manual\nand then as you become more and more\nmature there's more and more automation\nso the\nthe training of the model the\npre-processing of data\ntraining of the model evaluation of the\nmodel etc and then deployment and\nmonitoring if the model is automated\nand also the release of such a new\nversion or a new version of such a\ntraining system is also automated\nand\nin the throughout the talk i explain how\nbasically we build such systems using\nopen source software\nand which software i would recommend at\nthe different stages and as you can also\nsee here basically for the workflow\norchestration i'm\narguing why people should take a look at\nflight and what we think it's a good\nidea to do that with slide that part\num now there are lots of boxes on this\ndiagram here um i think it would\nprobably be too much to explain all of\nthem now that i do that in the talk so\nif you're interested please take a look\nat the talk\nuh maybe just one maybe just one area\nobviously i see uh with the light that\nyou have pandering for for data\nvalidation dimensions and niels is\nprobably gonna\nsuper happy seeing this as well\nthis is a good part so so when you look\nat this diagram how long i mean\nobviously people don't start when they\nstart out with that and you say well\nthey put models in production i always\nwould say and then the work is to keep\nit there\nthat's just putting it there so how long\ndoes it take until people arrive at that\nkind of like holistic view how much is\nthat a process of a year or two years uh\nyou know that takes a while i would just\nsoon\nyeah yeah right we did try different\nthings and then also kicked out a bunch\nof things um so in the beginning we\ntried for example to do the\norchestration\nand the execution and retraining with\nthem alpha projects but that didn't work\nwell for us and we also tried to deploy\nmodels with ml flow for example which\nalso didn't work so great\nwe had other orchestrators before that\ndidn't work so great so it took us like\ni would say two and a half years to come\nto the point that we're at today and i\ndon't think it's done like we'll\ncontinue to like improve it\num generally speaking i would say though\nthat um\nwe put all of the in this infra or all\nof the setup that we show here or that\nthis diagram shows here with the\ndifferent icons that i put next to it we\nput all of these into\nsomething like the cookie cutter\ntemplate so for our engineers\nuh it takes like 45 minutes i would say\nto spin all of this up from scratch for\na new project right now\nso um\nwe have opinionated services for all\nthese things where the operations team\nmy team\nsays\nah you want to monitor a model i think\nthis is how you should do it please\ndon't mess with the details this is how\nyou do it\num so yeah it took a long time to get to\nbasically being able to close the entire\nfeedback loop of continuously training\nand deploying models and then monitoring\nthem and being then being informed what\nyou need to change and then going for\nanother iteration\nbut um\nfor our ml team that's actually very\nquickly we're very quick to roll\nsuch a thing out of course like the\ndetails they need to fill themselves\nright like coming up with data\npreparation model training etc that can\ntake a long time but at least the infra\npart um\ndoesn't take long for them to roll out\nokay\nand and so now as a piece of advice i\nmean if you give somebody advice i mean\nthey don't have to spend two and a half\nyears\non their experimentation you have done\nthat and i feel like one important\nquestion is like\narchitecturally how would you start what\nteam would you start with first i mean\nflight is is not a strange layer that is\nso is fabric so fundamental would you\nstart with the orchestration layer or\nwould you look at building an\norchestration layer later on once you're\nclear with you know how to build models\nand so on in the first time\ni would say it really depends on your\nteam um when you start with two three\ndata scientists that have maybe\nnot a lot of experience in\nmanaging infrastructure themselves then\nmaybe\nhosting all this stuff on kubernetes\nlike we do ourselves maybe not the best\nidea so maybe get try to get the service\nmanaged in a way right\num\nif you do have more people and you also\nhave people that can run and maintain\nthe infrastructure yourself i would\nstart with an empty kubernetes cluster\ninstall flight in there install an\nalpha tracking server in there install\nelasticsearch permit that grafana seldom\nin there install the service mesh\nand start building it i would say but it\nreally depends on kind of the team that\nyou have\nthe less if you're the fewer people you\nhave with infrastructure knowledge the\nmore i would argue to like getting this\nstuff managed\ndoes that answer the question or was it\nyeah i think it's very important because\npeople when i look at this they might\nfeel overwhelmed just looking and say\nlike where do i start what comes first\nand then i think you can your answer is\nvery important money depends on on you\nknow how big and what what your team is\nwhere the competences are and the second\npart is when you when you have a team\nthat's small i mean all just don't have\nall the confidence\nhow do you ensure that you develop this\nkind of like um devops and mlabs culture\nand organization you have done this in\nyour organization maybe there are a\ncouple of\nyou know\nyou know\nideas that you can share with us what\ndoes it take to develop that depth of\nstem loves culture\nyeah i'm happy to i mean maybe one i\nwant to add one thing to the question\nbefore so when you\nso i did basically this article that i\ntook this diagram from they\nexplained like the the least mature way\nto run on the metal system is by having\na bunch of jupiter notebooks and the\nhuman connects them so the human is the\nworkflow orchestrator in a sense\nand they argue that the first thing so\nif you're there\nwhich makes a lot of sense to build\nprototypes right that's how prototypes\nstart then i would say the first thing\nyou should do is you like you should\nmoderate your code make a testable put\nit into modules because that is the\nprerequisite for putting it into into\ninto docker images which is the\nprerequisite to running it in an\norchestrated way that's what i would\nstart and take like the the parts of\nyour code that you want to repeat reuse\ntake them out of a jupyter notebook put\nthem into python modules unit test them\nso that's where i would start because\nthat goes a long way once you want to to\norchestrate these different steps in a\ncontainerized environment so maybe that\nin addition to my previous question\nwhere do you start\num\nyeah so\nokay for the second question now um kind\nof team ellos might be the devops mls\nculture first of all i don't think we\nhave the perfect answer like it's very\ndifficult to do actually\ni think what we discovered is in the\nfirst iteration\nwe basically said here are cookie cutter\ntemplates for the infrastructure use\nthem\nand we now find that this that doesn't\nwork very well\nbecause if i give an ml engineer\nthousand five hundred lines of yammer\nmaybe it works if they apply them but\nit's very difficult for them to\nunderstand what's happening if you do\nthat\nand i think you need to abstract\nthese thousand five hundred lines of\nyaml and put them behind a simpler api\nwhere the ml engineers can specify the\nfew variables they actually need to\nspecify and then understand what's\nhappening\nso i think what what we're trying to do\nright like have been trying for the past\nyear\nand i think it's really for us it has\nbeen the right way to go with\nnot giving\nthe sort ops team that tries to build\nthe tooling for the for the company to\nbe able to self-serve infrastructure\nthey configured infrastructure in an\nopinionated way but then they extracted\nthe way\nbehind a simpler kind of a simpler\ninterface so that people understand ah\nyes i want to install flight now into my\ncluster and the things that i need to\nspecify now is which projects do i want\nand what are the kind of the resource\nquotas for those projects\nthat i want\nbut the rest like there's a giant\nyarmulke file of a hand values file when\nyou want to run flight right\num most of that is something that i\nwouldn't want the ml teams to change\nanyways because it will probably break\nso\num\nwe we basically create\nour own hound chart that uses the flight\nchart as a subchart\nand\ncontains an opinionated way of how we\nconfigure this for our cloud environment\nand i think\nfor us this has really been key to\nreduce like reduce the reduce the\ncomplexity by\nbasically extracting the infrastructure\naway and only\nlet the user specify what we actually\nneed them to specify but not\nhundreds of other lines of\ncode interesting so so basically what i\nneed to say here is um there's just\ndeveloping that culture is an ongoing\nprocess it's not a process that it stops\nat some point i mean\nand you have to abstract and simplify\nyou know some levels of complexity for\nteam members who shouldn't be exposed to\nthat complexity i think that's what it\nsounds like very much if you have a very\ncapable infrastructure if each and every\none and the teams ml team state design\nseems just what they need but then\nactually let them do what they're best\nat i think that's what it sounds like\nexactly so that's the mistake we first\nmade in the first iteration so the ops\nteam created in my opinion super\npowerful infrastructure and they said\nyeah yeah look we have these servers\nthat's not perfect they solved your\nproblem and people were like yeah but i\ndon't understand i don't understand\nwhat's happening because it's just so\nmuch code\num and then that's exactly what we did\nknow what what you what you what you\nsaid and we think we went back and said\nokay what do we what do we\nwhat does the user actually have to\nspecify for it for to use this\nlet's just hide away the rest so that\nit's an opinionated way of configuring\nthis and we make it simple so that\npeople can use it because i mean of\ncourse you can't expect every ml\nengineer to to\nto understand all diffinities of\nkubernetes\nright right and i would assume as to\nmove along that that kind of like\nlearning culture about um obviously\nmlaps and devops\nat some point you are able to see\nobviously the impact by reducing the\nball or blade code that you write\nbecause now you have a lot more\nautomation a lot less boilerplate code a\nlot more handled by the systems i think\nthat's one of the benefits having a lot\nless\nerrors to deal with and you're much\nfaster when it comes to deploying things\nso the benefits uh should be vertically\non that side\ndefinitely so um\nby introducing automation wherever\npossible\nuh it allows us to bring a new when we\nneed to like change one small thing\nmaybe\nin the in the training code\nyou can just click a button and then the\nnew model version is deployed and then\nthere's like a decision gauge somewhere\nwhere we actually the human clicks yes i\nwant to deploy that\nbut by autumn by basically automating\nthe release process of an ml model this\nmuch for example using flight\num you can get the burden to to or the\nbarrier to releasing new model versions\nvery low you can do it very often\nand by doing that you can\nkeep your model sharp let's say like\nthis because you can always train\ntraining on your data\nand when the real world changes not a\ncatastrophe because also changing the\nmodel is kind of a minor thing then\nso yeah that's where and that's why i\nalso advertised this talk now here\nbecause fly does help us a great deal\ndoing that\num\nkind of keeping our models up to date\ngreat fantastic i think you have one\nmore slide uh let's take a look at that\ni think\nand this is more for the community and\nfrom purely from the tech side\nuh but we can do it after this slide i\ndidn't know oh please if we can go back\nto the previous slide i i can't it's not\nabout the previous slide it's a meta\nquestion it's like\nif\none thing that you really have\nliked\nwhen working with flight and that we\nshould continue to invest in as a\ncommunity\nuh would be great to know and\nand maybe our one future looking thing\nthat we should as a community look at\nokay so you're asking what maybe if i\nhave from all the features that site has\nwhich one is one that i really like yeah\nand if there is\nthe ability to move forward really fast\nright what is that one thing uh and then\nwhat is the one other thing that we\nshould add\nyeah okay\nokay so picking a feature that that the\none feature that is to me the most\nimportant feature for that actually very\ndifficult but i think i'm going to go\nwith versioning uh\nit's very hard like it would be\nimpossible for you to not version your\nthoughts and workflows in flight and\nit's great that fight pushes you to do\nthat\nbecause you need to do that\nand\nslide forces you to do that and also i\nreally like that flight comes with the\ndifferent domains\nand we map them to like feature branches\nstaging branch and production branch\nwhich is kind of\nobvious actually we matched them to\nfeature branches main and tags but\nit's not the same thing but it's really\nnice\nbecause it forces you to think in that\nmindset okay like now we try something\nso this is just a\nit's just a development version of the\ntask but still halfway later we can\nreproduce what the development person\ndid\nthen bring it to staging then bring it\nbring it to production\nbut even half year later being able to\nrun kind of the like some feature branch\nworking again so that's a really good\nfeature um so that's we really enjoy\nthat and maybe if you allow me to add\none more feature that i think is just\njust as important\nfor us being able to run different tasks\nwith different resources sounds like a\nno-brainer but actually that\nit's an important thing that not every\norchestrator can do so what we do very\noften is have like data preparation\ntasks with like fm error spark clusters\nand then training just uh just this\npython task or also like python\ntensorflow tasks and being able to\norchestrate that without writing a lot\nof boilerplate code that's just\nthat's what was that that's what\noriginally tipped us to flight and i\nthink that's just a really cool feature\nthat others should\nask themselves why they don't have it\nmaybe\num\nfeature that that i'm missing\num\n[Music]\nso flight has lineage right so when you\nreturn a data set from one task put it\ninto the other task\ntrain the model and then return the\nmodel from the train task to the deploy\ntask slide knows where\nflight knows which model version\nkind of was the output to the task that\nhad the certain data set versions in\ninput so this lineage information is\nthere you cannot see it in the ui right\num so there are tools like marquez or\nother lineage tooling\ni think it would be great if these could\nbe plugged together\nso that i could just have a dashboard\nrunning in my cluster and i can look at\nthe lineage information kind of in the\nindividual way that would be a great\nfeature i would say\nokay yeah actually there is some work i\ndon't know if those folks are here but\nthere is some work with data help i\nthink that's ongoing and and i think\nthat people from market is also reached\nout so\nfantastic thank you for you that really\ni'm sure the community will\ntake that and move it forward\nokay yeah so this is another slide that\ni took from this article that i find\nreally interesting that i mentioned\nbefore that i also bring up in the talk\num kind of the key to\nuh or\ni heard the saying before i'm not sure\nwhere i heard it\nbut\nlike classical software gets better over\ntime because people iteratively improve\nit and fix boxes and fix bugs in\nkind of an iterative manner can this\nfeedback loop oh something breaks let's\nmake a pr and if make a feature branch\nfix it the r merge another bug feature\nbranch\nimplement feature add a new test pr\nmerge right this iterative manner\nimproves logical software but the mail\nsystem they just die typically right so\nto\nrun them successfully over time and then\nprove them you also need kind of this\nfeedback loop\nand in order to have this feedback loop\nyou need a lot more infrastructure and\nstuff than for maybe classical software\num which\nwas on the previous slide but ultimately\nyou also want to pre like such a such a\nfeedback loop you want to have some\npipeline training code in the beginning\nthat needs to be packaged and through ci\ncd needs to be registered in this case\nfor example with the flight server and\nthere it runs on the front drop\nevery day trains the new model on your\ndata deploys it\nand then\nevery once in a while you realize oh we\nmissed something we need to go back to\nthe initial code and maybe filter out\npart of the data for some reason so you\ngo back to the to the source code change\nthe pipeline code a bit through the icd\nregister and release a new model version\nand then tomorrow this working runs in\nproduction\nand then either you go back to the\nbeginning because you realize something\nelse is wrong or you maybe automatically\ntrigger new model training with the same\npipeline version but ultimately you aim\nfor this kind of this feedback loop\nthing\nalso for ml systems that we all like we\nhave now in non analysis in the\nclassical software systems right but\nthat's kind of the goal and in the talk\ni argue that this is what you need in\norder to not lose your models due to\nmodel shift because they just die\num so this is what i want to show with\nthe slide but i go into more details and\num\nin the talk zone if you're interested\nthen\neither reach out or or watch that talk\nso the talk just a quick reminder before\nwe move on is september 12th to 15th so\nwhat is the date specifically is that\nthe 12th or 13th of the talk i think\nit's the 14th but i would have to also\nclick on the the event link there\nokay the event link is here\nso i would highly recommend um to to\nfollow you and you can watch it live i\nassume right\nyeah um yeah yeah it will be live\nand um maybe also if you're i mean\nyou're here because you're interested in\nflight right i think maybe what is\ninteresting\nflight related in that talk is how we do\nthe connection from you're in your git\nrepository now you change the learning\nrate\nhow do you bring a model\nlike a new training system in flight\nit's a production that runs trains and\nmodel the next morning like so how do\nyou how do you do it with a connection\nfrom your source code to having\nsomething run\nin production so i go a bit in detail on\nthat because we spent some time\nfiguring out how we want to do that\ni think we should capture that that\ninformation on the slide or page\non the events and material that you have\nthat goes with that as well so people\nwho are watching this know where to go\nso talk with samita to get this awesome\nfabio thank you so much for your time\nthis is always super interesting talking\nto you so i look forward to your uh\ntalking at the open source summit europe\n2020 december next week a week after and\nuh let's move on"
    },
    {
        "title": "Flyte Roadmap - Deployment Journey",
        "transcript": "so there you go eduardo uh machine\nlearning engineer at union he's focused\non the open source aspect\nof\nflight so he's really uh one of the\nforces behind making flight what it is\nnow where we go with flights life's\ntopic of this presentation that\nobviously releases upcoming releases\nwe'll have\neduardo give us an overview about what's\nnext and also a little bit into topics\nlike reintegration and the award which i\nthink very cool deployment story to talk\nabout so stage series eduardo\namazing so you know as mark was saying\num my name is eduardo i lead you oss\ndevelopment at union\num let's keep going we're gonna talk\nabout the a little bit about the eddy\n1.2 release and what's what's\nwhat happened there what's happening\nthere so sandra can you go to the next\nslide\nso uh i'm not going to go into a lot of\ndetail and you know some of the perf\nimprovements that we made during this\nthis latest release but i want to\nemphasize two features that were um\nadded in this release i expect that\nwe're gonna\ntalk more\nabout them in this um this sync up in\nthe coming weeks but to see again just\nwet your appetites like we\nat union you know in the flight project\nwe we value performance a whole lot\nand um\nespecially when you when you start\nthinking about you know having flight\noperating at scale like performance\nbecomes like really the defining feature\nof the orchestrator\nso um one of the features that you know\npeople or babies from from spotify\nworked on alongside with our very own\ndan\ndoing this this last cycle was\nthe ability to offload\nthe crd static workflow spec to a blob\nstore like that speeds up you know um\nthe scheduling of workflows because that\ncd now can um\nbreathe more\nbetter\num another feature that we added\nit's still an ongoing work but it's it's\nslated for the one to release\nis um a cache that we're adding in\npropeller so that\nyou know if you have workflows that\ncontain multiple\ncache tasks you're gonna see a huge\nimprovement in those\ni'm just also calling out like a couple\nof bug fixes that we made\num that would also impact on perf\nif you're running on g7 gcp um we had a\nvery\nyou know a tiny bug that was causing us\nto ride to gcs\nmore than more frequently than we should\nthis is fixed so you're gonna see you\nknow for free you don't have to anybody\nanything just like improve your\nworkflows but the performance of your\nworkflows um\nwe also um fixed the spec where\nsome workflow inputs were being written\nmore than once\nwhich was great um\nthen work on this this idea of having a\nsub cueing side of propeller so that you\nknow we can\num\nget updates to coming from pods\nin a faster way which means\nin they are scheduling the downstream\num\ncontainer spots will be a bit faster\nbecause now we wouldn't have to reach\nout to get the state of the pot so\nonly now um the 1.2 release was\nhad\num a lot of\nfocus on performance and again\nif you operate flight at scale you're\ngonna see a massive improvement coming\nfrom you know the the first and second\nitems in this in this list\nso center let's go to the next slide\nyeah thank you um\na feature that is coming this this\nrelease is an integration with rey um we\nalready talked about this during this\nmeeting but just to reiterate\nhow easy it is to integrate ray into\nyour flight workflows\num\nat the very top like the that box there\nis the\nthe ray decorator that you use to signal\nthat you want to run that compute in in\narray cluster\num the middle box is some glue code that\nwe use in flight to define\nthe configuration\nof um\nthe\nyou know the head node and the worker\nnodes in in in array cluster\nand the bottom is our\nloved\nones the at task net workflow um flight\ndecorators\nnotice how\nthe integration between you know the\nrace specific code and flight is like\nvery seamless like you basically don't\ndon't think about like how to\nhow to ship the code to ray it's all you\nknow done\nusing like this three-step process where\nyou write your record you write the the\nconfiguration job for a and in your um\nflight workflow\nlet's go to the next slide let's dive a\nlittle dip\ninto like how\nthis actually works so let's see on the\nleft hand side\nyou have your\noh people are\nthe chat is super active today this is\nawesome um\nso saying on the left hand side you have\nyou know your your flight python code\nwith you know the task network flow\nand um you want to run\na task\non a um an array cluster\num you go through the usual process you\nknow like generating an image and\nregistering it but then it comes a point\nwhere you want to\nto run your workflow you go\nthrough the usual process of you know\nkicking it off using either flight\nconsole\nflight ttl or flight remote\nthat talks to the flight control plane\nwhich you know\nin itself um talks to the flight engine\nwe lean heavily on the kubrick operator\nfor this integration between fighting\nand ray\num the detail here is that the\nthe passcode runs in the rays um head\nnode\nwhich is like the the top um\nportion of the the\nray integration on the right hand side\nbut um all the the ray um annotated or\ncode you know with the\nwith the ray.rebolt decorator it's\nactually shipped to the working nodes in\nin um in rey if you go back can you go\nback one slide sandra sorry i just want\nto emphasize one thing so\nin the middle box the one where we we\nconfigured the array job um if you\nnotice there is a field there for\nconfiguring the worker node config\nin there you see a\nproperty called replicas that defines\nthe number of nodes that of worker nodes\nthat you want\nin your ephemeral ray cluster\nso yeah let's uh let's keep going go to\nthe next slide center please um\n[Music]\nyeah overall\nkubray is an essential portion of of\nthis integration and as we've shown like\ndub the the idea of like integrating\ncode that you know already runs in in a\nray cluster with some code that is\nflight specific\nit feels like pretty natural you only\nhave to write really the code between um\nthat that\nspins up this this ephemeral array\ncluster or connects to to an existing\nray cluster\nlet's go to the next slide\nreal quick um we're playing some future\nwork around this area right now\nthe\nthe integration only works for a single\ntask\nand um we have a spec around like how to\nshare this ephemeral cluster across\nmultiple tasks even like mood for\nworkflows and sub workflows you have\nsome ideas there i think the most\nexciting thing is that um we want to\nextend this\nthis idea of um\n[Music]\nofficially calling these ephemeral\nclusters to other compute platforms\nbecause today we already have um a\nsimilar integration with spark it works\nthe same way we spin up a spark cluster\nrun your spark task and then um\ntear it down\nnow we're doing this with ray and we\nhave plans to do this what are you know\ncompute platforms like like desk\num please go take a look at the\nat the blog post that we published\nearlier or last month\nfor you know more details on\nhow this\nthis works and\na few examples\nanother you know\ncall it a feature of uh\nthis this upcoming release is what we're\ncalling the the getting started tour\nwe feel that um the getting started\nguide like the current getting started\nguide is\nis useful but it could be way more\nhelpful to like beginners\nso we are\num we want to make you know the\nthe natural questions that people have\nwhen they read the getting started more\num more easily accessible\nexamples you know how to go from like\nhey\nyou have your local executions how do\nyou go to the cloud how to go to\nproduction what is what is the actual\nprocess of you know getting um flight\nworkflows running in the cloud\ni also want to emphasize that we we\nvalue our docs very much we hear from\nthe community that you know people like\nour docs but we feel that um\nthose could be those could be even\nmore um\nhelpful and this is a call to action to\nyou know\nlook for\nthe\num\nthere are there's a a bunch of github\nissues on our um\nmain flight repo\nthat would be\ngreat\nfirst issues related to docs\nagain it's not that we don't we don't\nvalue your coding contributions\nit's it's great but we also think that\num\num\ngetting the community to contribute to\ndocs is like absolutely valuable\nso yeah let's keep going sandra\nthis is just one example of like how how\nwe're thinking you know as i said the\ngetting started guide has um\nit has\nsome holes and we want to make people\nlike take\ntake the next steps\nwhen they decide to like give flight a\ntry\nwe want to make those those steps like\nmore easily accessible so here's like a\nlist\nof um a few of the the\nthe steps that we want to make it feel\nvery natural to people like hey you\nalready when you're getting started guy\nthat helps you get a sense of flight can\naccomplish\nbut now you wanna for example\nrun tasks you know with different images\njust like a feature that fabio\nmentioned uh during his uh\nhis talk where like hey this is a great\nfeature that you know a lot of\norchestrators don't do and\nit is described in our docs it's just\nnot easily accessible and we want to\nmake that very apparent to people hey\nthis is a a good feature that um\nflight has and beginners should know\nshow should know about them\num\nin a more easy\neasily accessible manner you know\nanother example like custom peep\ndependencies you know like we we\num\nagain as have you mentioned like\neverything in flight is is version and\nthat includes the the base images that\nyou use but we don't we didn't teach you\nknow the beginners\nhow to set up their workflows in such a\nway that hey now i need this this extra\ndependency\nthe the funny thing is like this is this\nis all described in the docs it's just\nnot easily accessible and that's the\nproblem that we want to solve here\nyou know\nso um\nanother two examples the tasks beyond\npython like we have this like flight is\na\nmulti-language system\nsince its inception you know like if you\nif you really you want to run your\nworkflows like in in other languages\nlike we not only we have you know\nofficial support for\num for python and in java\nscala but like there's contributions\nfrom the community coming from you know\num\nfor like a javascript sdk or\nwe even like support for you know\nvery um\nlow level like tasks like hey you want\nto run your code in in r or i don't know\nrust\nit's um\nthis is all supported it's just that we\ndon't talk about it\nat least from from like a beginner's\nstandpoint that's like it it's hard to\nfind and once you know that that thing\nexists\nit's very easy to execute on flight it's\njust you know it's\nkind of like the unknown like the\nmentality of the beginner\nagain\nthe idea here is to like\nhold the hand of the people who want to\ngive flight a try\nso that they can understand more easily\nwhat flight can achieve\nso so\nreal quick before we go to the\ndeployment so just a call to action just\nwant to make this loud and clear right\nnow so\nyou're looking forward to hear back from\nthe community about you know how to get\nuh the started uh portion better\norganized the the segments in there that\nactually you're looking for feedback as\nwell so\nwhat's the way to communicate with is\nthe slack channel or or how do you want\npeople to to reach out to you\nyeah it's a great great question martin\nthank you um i did not make that super\nclear with the call to action uh\nyou know reaching out privately on slack\nor on the public slack\num\njust\nif you\nsee a github issue that you think it's\num it's not prioritized now and you want\nto work on just let me know us we we can\nyou know assign that to you and\nyou can work on it it'll be great or\neven better like if you find a hole in\nthe docks and you you feel passionate\nabout you know teaching the community\nabout that\namazing go create the issue assign it to\nyourself let's let's work you know\nthe the flight talks are are an amazing\nresource and the heat can\nalways be improved that's the idea\nbehind this\nokay great so basically just dive in get\nactive get busy yeah\ncool i like this a lot all right so now\nyou have another section uh segment you\ntalk about the deployment journey what\nis that yeah so\num\nwe here you know\nwe hear from the community that like\nflight is um this is\nthis powerful system that it's not\nnecessarily the easiest to work just to\nstart working on\nand even when you have um\na system working in in you know you\nalready\nyou're passing the the point of of\nhaving a um\na poc like how do you how do you take\nthe next step you know we want to teach\npeople and make it very easy\nso that they know what to to look for\nwhen they are\nconsidering taking flight you know from\num\na local you know development environment\nall the way to production even like\noperating flight at scale so let's\nlet's talk a little bit about this\nto send you the next slide\nso um\nfirst you know we have um we\nit didn't come in this this we came it\ncame in the the one release but we have\nthis this thing that we call the the\ndemo cluster it is the easiest way to to\nget started on flight like you do just\nto\nflight ctl then we start and you are\ngood to go\nthis is a\na great feature to\nget started on the platform to like hey\nif you're uranium you know locally on\nyour own box or in a small cluster like\njust\nuse the demo cluster to iterate on small\nworkflows get a get your your feet wet\nwith how the flight programming model\nlooks like\num\nthe demo cluster is a self-contained\nimage that runs in a docker com\ncontainer\nand\ninside of it um\nwe run um the flight binary containing\nyou know both control playing the data\nplane\nand the ui elements like flight console\nand what not\nand in that image we also run a\num k3s cluster that runs the the like\nthe\nthe secondary elements that you need in\norder to run a flight deployment like a\npostgres database you have like an\nobject storage\nyou use many of it\nagain the demo cluster is like the great\nentry point for like trying out flights\nyou know it's easy to download it starts\nout super quick and it it gives you the\nthe entire um experience of like\niterating with flights\nbut once you're you're there like how do\nyou take the next step like how do you\nthink nowadays how to go to how to go to\nthe cloud so santa can we go to the next\nslide\num\nhere's like a list of things that now\nnow that you\nyou you want to take the next step like\nthe it's a small subset of\nof um requirements that you need to\nthink about now\nfirst you\nyou have to decide like where you're\ngonna run it like um we have docs on\nlike running this um running flight\ninstalling flight on on eks\ngke um\nas i mentioned before like you need a\ndatabase\nyou know in\naws you have\npostgres compatible the aurora database\nbut like you need a database you need a\nin object storage\nyou need to start thinking about\ndiametral service accounts cluster roles\nbinds like all the kubernetes specific\nconstructs\nsome of those are required to actually\ninstall flight on on the cloud\nbut um it's at that point that you know\nyou you can take advantage of um what\nflight can do\nonce you are in ikea's kx cluster right\num it's at that point that you can you\ncan start\nrunning some of the um the already\nexisting plugins like hey now i want to\nexperiment with my my ray in\nrake code you know that i already run\nseparately now you want to orchestrate\nthat with with flight you have some\nspark code you want to connect this to\npresto\num\ntypically when you are at this stage\nyou are still running both you know the\ncontrol plane and the data plane single\nin the same cluster\num\n[Music]\nso you know a natural question at this\npoint is like hey how do i how do i take\num\nthis notion that hey\nnow i can i can separate again i can\nmake sure that you know not everyone has\naccess to the control plane i can run\nthe data plane in separate clusters like\nit's a\nagainst this idea that there is a\nnatural progression of like how how you\nthink about flight installations flight\ndeployments right it's good next slide\netc\nnow you got the production now you have\nyou know your team of data scientists\ncml engineers they're all you know they\nfell in love with flight they want to\ntry it they want to integrate it in\nin their\nproduction scenarios\num\nmost like\nthese these top two items here like how\nyou configure ingress\ntls certificates dns and authentication\nare\nthe\nmost support heavy um\n[Music]\nitems in our list like we get questions\nabout ingress and authentication\nfrom the community because well\nespecially when you talk about\nauthentication like there's um\nit is a thorny subject\nand um\neveryone does authentication a little\ndifferently\nbut um we\nwe work with the community to make sure\nthat you know you are able\nto for example\nuse your your own corporate idp\nwhen uh thing to flight um security\naccess you know to the control plane um\n[Music]\nelements\nschedule a propeller like uh\nthis is all supported in flight\nwe just\nwant to make this um\neven simpler for people who want to get\nto this stage you know it is right here\nwhen you also have to think about hey um\nwhat is happening with the cluster you\nknow you have to set up like i was able\nto promise and all that\num flight produces\nuh i'll call it um a choke full of um\ninformation in terms of like what's\nactually happening at the flight layer\nin terms of you know updates to\nworkflows executions and all that so you\ncan set up your um infinite streaming\naround that\nand it is right here when you have to\nthink about you know integrating um ci\ncicd in your\nworkflow\nto help you know people register flight\nentities\nschedule executions get the catalogs get\nthe\nthe usual ci cd like pipeline that\npeople think about\nand finally you know when\nall that is all said and done like it's\ntime to to go live to really\nthink about like how flight can help you\nscale your workflows senator can you\ngive the next slide it's\nlike\nagain from the get-go performance was\nwas a big feature\nin in the flight design\nlike we want to make sure that people\nwho need the scale\nthey can get it from the system you know\nand one way that um we've seen people do\nthis is they they very consciously they\nseparate the the control plane from the\nactual\num\ndata plane like the\nclusters that are gonna run the compute\nso um you can run you know\nflight console in one smaller cluster\ndoesn't have like heavy heavy machines\nbut um\nyou can start thinking about having you\nknow um compute separated or segregated\nby by machine type\nor um\nthere's all sorts of knobs that you can\nthink about how to separate you know\ncompute from the management of your\nworkflows and fight allows for that you\nknow it's not like you don't you have to\nworry about like writing that blue code\nit's right there for you to grab\nagain\nperformance performance performance like\nit's a big feature from from the system\nwe\nlike in this release we worked a lot on\nit but um it's been a\nconsistent effort from the team indeed\ncommunity\nand um i advise you to once you get to\nthis stage and you you need the kind of\num\nperformance that flight can give you\ngo visit docs like there's\nthere's a lot to ex experiment there\num\nin terms of the community you know um\nyou should\nthink about giving back a little bit\nfrom what you you get from from flight\nso\njoining meetings like this one are great\num are a great way of of\nyou know giving back to the community\ndon't forget to\njoin the\nslack\num channel\nworkspace\nit's um just go to https\nselect.flight.org\nwe we try to\ndevelop flight in the open as much as\npossible so\nyou go to the\nthe\nthe flight\nrepo you're gonna see like a list of of\nissues you know tagged by milestone with\nyou know um we even\nwe try to be very cognizant about like\nissues that are good first issues so\nif you're feeling you\nyou got the bug the contribution back\njust\nvisit that and\nassign an issue to yourself\ntalk to us\nmake flight better\nstart the repo tell your friends use\nthat in in in production like flight can\num\nas you know we got from from fabio\nearlier today like there's\nwe are\nseeing a lot of evidence that flight can\ncan help you orchestrate your um your\nworkflows\nand just a very very minor update like\nwe we already talked about this but the\nwe change the the release cadence of\nlight\nwe want to make the system more reliable\nyou know more stable so\nlook out for releases coming every\nquarter and there's one coming at the\nend of this month like september\nand\nyeah\nlet's make flight better\npotassi eduardo i i love the material\nwe're almost at the top of the hour so\nwe've got to rush a little bit i would\nlove to see this information also on the\nfly.org website because it's really\nimportant just like what fabio shared\nwith us it's a journey\nand and once you know the steps a little\nbit better and what the dependencies are\nthings get a lot easier so this is\nfantastic thank you so much for sharing\nthis information so send us to a quick\nuh\nlinks and resources overview so uh\noffice hours i want to mention uh there\nare office hours with taithem and\nkatrina and caithin uh so you see uh\nthere are throughout the day uh take\nadvantage of those office hours i feel\nlike a lot of really good questions\ncould come up here and you have access\nto to the community it's amazing so\nnote the the times and\nget on to\nget on to ad event and make sure you're\nthere um upcoming synthetic september\n20th we will have um\nrepresentatives from infinium and laguna\nthere laguna is also very interesting we\nhaven't i haven't spoken with them so\nfar and always i think but we'll do it\non the 20th it's going to be very\ninteresting and then what uh edward said\ni just reiterate you know um get there\njoin us and go on uh and join the slack\nchannel star\nflight please do that pay a little bit\nback we appreciate this very much with\nthat i want to say thank you to all of\nthe speakers fabio niels eduardo i\nreally appreciate your contribution\ntoday and the community for spending\nyour time and i will see you in two\nweeks have a great day"
    },
    {
        "title": "How the Schibsted Media Group Are Planning to Onboard Their ML Workflows to Flyte",
        "transcript": "good next is something special so panel\ndiscussion i'm really excited about\nuh talking to the team the shipstat team\nhere\nand um\nyou know shipstat is a we're going to\nask them later what ships it is but you\nknow i did a little my googling homework\nand uh found that of course ships that\nis a media company\nuh and it's a media company that has\nbeen around for a little bit since uh\n1839 so it's quite\nquite a media company i'm not sure if\nthey started as a media company back\nthen but i'm sure\nthe team will tell us a little bit\nbut i would love to introduce the team\nright now and let's kick it off with uh\npaul um\nbest photo paul is a technical architect\nat shifstat and paul if he could could\nyou please and would you unmute yourself\nand introduce yourself and give the\npeople here a brief background about\nyour work and your role\nyeah thank you very much for having us\ntoday\nit's it's really nice to be a part of\nthis community\nso this is it's fun to be able to\ncontribute back\nyeah so i'm paul besco technical\narchitect in ships that\nmy background i've been working with\ndistributed systems in media for a long\ntime uh\ni have a phd in networks and distributed\nsystems where we're focused on sort of\nexecution engines something a la spark\nengines just a little bit sort of\nearlier\non so playing with those concepts\nbut i worked with video conferencing in\ncisco previously and found myself in\nshipstead eventually\nwhich is a great company to work at\nwe do a lot of interesting things which\nwe'll tell you more about\nso yeah and\nin shipstate i work in something called\ndata foundations and we're one of the\nsort of central teams so shipstead is\nsort of composed of a lot of different\nbrands we are something like 40\ndifferent companies\nin total\nand we deliver these central services so\nwe do like data collection behavioral\ndata and these kind of things and then\nwe build up sort of the products based\non that for personalization etc\nyeah and i work in data foundations uh\nsort of across uh seven teams so\nfantastic and so for how long have you\nbeen with shipstead\nso um in total it's six years i think\nand then i was i was out for about so i\ntook a break uh joining another company\nso i'm called a boomerang so i'm back\nfor my second stint uh actually\nwell it just speaks for ships that\npeople come back that's that's great\nnews fantastic so next we have uh i hope\ni said correctly much ahead um\nmuchad is a senior data engineer and\nwhich ahead if you uh could please\nunmute yourself uh introduce yourself\nand give us a big background about your\nrelationship stat\nsure thanks\nyeah my name is mujahidz it was actually\na great pronunciation um i work in\nshipstats almost for three years\nand i work as i worked mostly as a data\nengineer but recently i switched to\nmostly platform data slash platform\nengineer\nin shipstead i worked\nmostly on some internal segmentation\ntool and that consists of ml and also\nbatch data processing patch and\nstreaming and then i switched to\nplatform team which is actually\nmaintaining\naws accounts permissions cicd\nmonitoring\nlogging\nkubernetes also recently started working\non\nunifying or having a standardized batch\npipeline processing\nand we actually wanted to\ngo with flights so i'm also helping on\nthis uh migration\nfantastic that sounds like more than a\nclassical and typical data engineering a\nhead that you're wearing here with all\nthe stuff you have done right\nyeah\nthat was a bit earlier now it's not much\nof data engineering but mostly\nplatformer operation\nfantastic okay great thank you next we\nhave uh bjorn scheffler bjorn is a staff\ndata scientist\nand bjorn if you could please unmute\nyourself and introduce yourself and give\nthe people here a brief background about\nyour role and responsibility at shipstat\nabsolutely yeah thank you thank you for\nhaving us um yeah so as you mentioned my\nname is bjorn i'm a star data scientist\nat chipset\nhave been with shipset for about one and\na half years now um and i'm i'm both\nkind of\nworking\nin a team and then also across the the\nwider organization a little bit so\num in the team that\nis this team is called\nthe contextual team so what we are doing\nis focusing on\nmostly on contextual advertisement so\nplacing basically\num adds next to articles that are you\nknow semantically similar in in some way\num\nwith the idea that then we don't need to\ntrack um track user behavior we don't\nneed need to place place cookies on on\npeople's\npcs we can we can basically um just use\nthe content that we have to place uh\nrelevant ads next to it\num so that is that is part of my role\nand then another part is um initiatives\nacross the ship set organization that is\nfor example\num ai upscaling courses um that that we\nare giving\num\nyeah to help help people basically to\ndevelop in in\nai and machine learning fantastic so\njust just for clarification uh is the\nplacement is that an nlp based task or\nis that more a uh you have metadata and\nuse more the classical relating method\nor both i mean it could be many ways so\nyeah it's i i think there is there's a\nlot of nlp i mean it's it's partly you\nknow just\nthe old pipelines are basically\nkeyword-based um but then we are also\nexperimenting with with semantic search\napproaches\num so and then then we'll need uh you\nknow more\num kind of newer nlp techniques to to\nsolve that\nand maybe somebody even stein distances\nand so on so yeah good good love it\nthanks for bjorn next we have um yini\nyini is a\nml engineer and i hope i don't uh\nbutcher the name so\nthe interesting piece uh information\nhere is we have a full ml workflow team\nhere that we talked about data\nengineering and platform data science\nand now ml engineering so please you\nneed if you could and mute yourself and\nintroduce yourself and tell us a little\nbit about your background your role at\nshipstat\nhey thank you for having me my name is\nini and i'm machine learning from the\nsame team as beyond and\nregarding the machine learning we do in\nthe team i think\num so we start from\nthe for example the data analyst and\nthen the data modeling and then\nsome\nsoftware engineering to put the ml\nsolution into production\nuh this is basically the journey that\nthey did in terms of machine learning\nand\nyes that's it\nfantastic sounds easy that's it but we\nknow it's like\nthere's a lot that comes with it to make\nthat happen so we'll speak a little bit\nlater thank you so much uh last but not\nleast we have olek and and i don't try\nthe last name again\nlike uh yet to shock i think is what i\nwhat i remember but all is the\nengineering manager you know like if you\ncould have mute yourself introduce\nyourself and tell us a little bit about\nyour background\nyes thank you martin\nmy name is oleg i am engineering manager\nin one of the teams that is in the same\ndepartment as everyone\nwho spoke before me um we are mostly\nfocused on building user profiles for\npersonalization\nadvertising user insights and we are\nused by a bunch of teams within\nshipstead\num\nyeah my background is\nbasically software engineering and then\na little bit of\nml\nand then data engineering and now\ni worked for three years in chipset two\nof them was a data engineer and switched\na hat a little bit for a year and now\ni'm engineering manager wow fantastic so\nuser profile sounds very much like cdp's\nor customer data platform is that what\nwe should see there is that related to\nit or so\nyeah pretty much you can put it this way\nfantastic now cdp is fantastic and\nthere's also actually quite friendly\nwhole lot of\ndata science in there understanding you\nknow how your profiles together if you\nhave some you know footprints uh from\nhere and from there and then actually\nmaking sure you have a very meaningful\nuh you know\nprofile and then obviously what you want\nto do there i assume you do a lot of\nattribution maybe probabilistic\nattribution as well\nyeah we have also as as bjorn talked\nbefore we were also thinking about\nmaybe the combination of somewhat\ncontextual\ndata and and what we have and um\nbasically providing some mixture of what\nwe have and they have\nyeah\nadvertising platform of future or\npersonalization uh in in a better way\num so\nyeah we'll we'll see where it gets us\nfantastic great so let me start with a\nvery you know simple open a question\nabout ships that obviously the company\nand so this is i think uh whoever wants\nto answer it\nthe company has been founded 1839 so now\nit's a big international media group so\ntell the audience a little bit about\nshipstat who wants to do this paul or\nlike who wants to take the lead here\ni can do it\num so go for\nshipstead is uh\nyeah it's a family of brands we're about\naround 50\nand\nsort of the majority of\nthose are newspapers and the\nmarketplaces that i think these are two\nmain\num sort of called verticals if you want\nto put it this way\num\nand and there is a bunch of ventures and\nand new companies that are more\nfocused on helping customers making\nbetter choices and overall the goal of\nships that is to make\n[Music]\nlives of humans better by you know\nhelping them to make better decisions\num\nso newspapers obviously do a lot of uh\nnews personalization and and\ntrying to\nkind of serve better content uh\nmarketplaces have their needs with um\nshowing relevant information to the\nusers so they basically can find what\nthey're looking for and\nthe rest is a bit of a mix so it's hard\nto put it in in one box\num so there is a i cannot name a very\nspecific\nniche unfortunately\nbut yeah that's that's us and we're\nspread across uh\nfour nordic countries uh which is uh\nnorway sweden\nuh denmark and finland\ngot it got it i mean it sounds like much\nof that business was classically you\nknow classifieds and so on that has\ndisappeared to some extent and now it\nhas shifted into you know obviously what\nyou're saying right now so it's it's\ntransformation is happening they're\nstill happening there uh i think it's\nvery very interesting to see that use\ncase\num so what i would love to to ask the\nteam is is is another very\nsounds like a simple question but it's\ni guess it's very complex\nhow do you as a team define\nyou know data and ml workflow\nso there's a lot of implication here but\nmaybe i can ask the data engineer which\ni had\nbecause you're sitting at the core of\nthe data here so what what is a data\nworkflow to you\nyeah so\nwe should start from the source of the\ndata uh where it's coming from and why\nwhat is where is it going to be used\num so we can we have other teams that\nare actually responsible from collecting\nthe data and bringing it into data\nplatform\nand from there it all depends on\nthe confidence of the product that you\nwant to use it for\ndo we want to use the data in every hour\nor do you want to just\num daily updates or hourly updates or do\nwe want to actually be very responsive\non updates like every\n10 seconds for example\nso it all depends on the raw data and\nproduct and we have different variety of\nproducts so it's kind of\ndepends on the use case i can say\nokay great and now as obviously we have\nheard various use cases from from\noleg about profiling building profiles\nto\nbjorn's\nplacement of contextually relevant uh\nads next to people i think there's\nmultiple ml use cases down the road so\nhow does how does the data workflow\nserve all those use cases what's what\nshifts that's recipe for that\nmaybe we should ask\nthe data scientist ml engineer here if\nyou're\nhow do you how do you connect those\nthings\num yeah i mean um\ni mean that's basically at the heart of\nwhat we're doing right we we need um\nthe data flowing in some way being\npre-processed being um\n[Music]\ntransformed in us in a certain direction\nso that we are able in the end and to\nto build a model and to to make\ninferences from that model um\nand yeah we have a variety of different\nmodels that that um\nare in production for us right now that\nranges from you know classical word\nembedding word embedding models to to\ntopic models to\num you know more modern uh\ntransfer learning models um\nso\nthere is a variety of different\npipelines that um you know need to\nfunction for us so that we can\nthen use it for for the\neventual purpose of of placing these\ncontextual advertisements\ngot it so i think paul\nwhat we have here is a whole lot of use\ncase and i think for uss the architect\nhere so you have to listen to all of\nthose use cases i assume and then i\nthink you have to come up and figure out\nyou know what's the right architecture\nso tell us a little bit about uh you\nknow how did you come up what's the\ncurrent solution\nwhat are what is the tech stack that you\nhave today and what was the process the\njourney to get to this tech stack\nyeah and we're like feel free to help me\nout here if i if i butcher something but\nyeah so\num\nin terms of sort of when we came to to\nto this point where we wanted to\nreevaluate okay how are we processing\nour data and mainly from the batch\nperspective\nwe we sat down and we sort of looked at\nwhat are our requirements uh basically\nso\ni think we came up with something like\n26 dimensions or something um\nand i it was kind of interesting to to\nhear i was looking and listening to the\nsit recruiter talking and it was\nwe could draw a lot of parallels from\nour journey and what uh what i'm sorry i\ncan't remember his name but what he was\ntalking about in that\nvideo as well so\nuh basically we have all these\nrequirements right we have some that are\norganizational so we want to be able to\nmove more quickly with less people we\nwant to have\na clearer separation between\nplatform data engineering data science\nand not because we don't want them to\ntalk together but because we want them\nto actually be able to focus on on the\nskill sets that they\nlove to work with right\ni can't remember it was an article where\nit was what was it\ndata scientists spend something between\n60 and 80 of their time working with\neverything else except actually data\nscience and\ni mean if we can push that number down i\nmean we're pretty bad at that as well i\nthink that's also sort of one of the\ngoals in terms of separation actually\nmaybe focus is a better way of putting\nit so\nthat's sort of on the organizational\nside\nand then we also have all of these\nfunctional requirements which is where\nyou tick a box right so\nthe\norganizational requirements can be a bit\nmore sort of tricky to say yeah we\nmanage this or not i mean i i think we\nhave a good\nfeeling about flight that it will push a\nlot of these needs in the right\ndirection\nand then from the functional\nrequirements i think we had some yeah 26\nwas that what i said where we are\nlooking at things like okay is it data\nor task oriented right where there's\nsort of this trend in the market\nshifting towards more of the data side\num\nother ones are like\nokay is it\ndo we have multilingual for instance\ndoes it support a lot of different\nlanguages\nwhich is\nmore or less important maybe depending\nuh the modes of execution that we call\nit where fast register is a huge thing\nright being able to iterate quickly is a\nbig thing so so that you don't get this\ntime sink\nreproducibility interpretability\nrecoverability etc etc so\ncomparing this with what we have today\nthen so we have these 26 requirements we\nwent out into the data orchestration\nlandscape which is growing it's pretty\nbig right i think we looked at 10\ndifferent solutions\nand we were kind of depressed for a long\ntime because there was no sort of\nsignificant leap as far as we were\nconcerned right so\nwe were using luigi today if we go to\nsomething like airflow it feels more\nlike a lateral move\nso\none of the things we want to be able to\ndo is cover for instance\nboth the engineering data engineering\nperspective with spark etc which is the\nmore classical etl type stuff and then\nwe also want to be able to run and\norchestrate our machine learning and\ndata science models but there wasn't\nreally anything that gave us a lot of\nbenefits so i mean doing this kind of\nmigration it requires a lot of time and\nresources so you have to be able to\ndefend that in some kind of way\nbut with flight we we were really\noptimistic once we sort of found it\nyeah where was it 2020 2021 i think you\nopen sourced it so it's relatively new\nbut uh yeah since then it's been really\nfun to sort of take uh\ncomparison comparing these two solutions\nthen should we invest in luigi or should\nwe move on with uh something like flight\nfor instance\nnow i've been talking for a long time\nhere so i'm not sure if i'm answering\nyour question properly yes so please\nabsolutely so oh like i'm going to bring\nyou in here real quick and so obviously\nyou have uh had paul think about this\nand make architectural comparisons\nbetween luigi and flight\nuh\nwhat was your role in that context tell\nus a little bit and your your thoughts\nabout making a decision\num\nyeah so for me as a\nmember of one of the teams\nwe have um\nwe have a history with spark so uh we\nhave been running sparks since uh\nor the old team in london was running\nspark since 2015 in the old nazis\ncluster and around 2019 when uh i joined\nthe ships that we\ndecided to move to kubernetes and um\nback then um we still decided to keep\nluigi but we had to watch it a little\nbit create a internal library that would\nallow us to do the sports submit to the\nkubernetes and it kind of worked we were\nhappy you know\nthere was nothing nothing else\nso for two three years we were\nhappy with what we had\nbut\nui was not perfect there were some\nhiccups here and there\num and then i think in 2020 we decided\nto\ntry this new cool thing called deep\nlearning which was\nnot so new anymore but we thought that\nwe can\nsqueeze in a little bit more performance\nfrom one of our pipelines and try to to\nget something more out of it um\nand that is where i realized that luigi\nis probably not the best tool to\napproach this kind of tasks because\nit turns out that you need them to\nmonitor the luigi pods in kubernetes and\nyou need to schedule them in the right\nplace etc etc and there is all these\ndetails that suddenly data\nit's not like they don't want to do it\nbut i think that it's another skill set\nso you need to bring someone who knows\nit for a couple of days to solve a very\nspecific problem and then and then shift\nit back and\nthis unpredictability of what kind of\nproblems are you facing\nsort of\ncaused a lot of stress for the team\num so my idea from\nfrom there was to try to find something\na little bit better that would make\nyou can call it a developer productivity\ni don't know developer experience for\ndata scientists that will help them to\niterate faster in in\nin the fashion that they used to\num\nwe thought about data bricks i'm not\nsure if i'm allowed to pronounce the\nname of the competitor but\nstill the the idea is that\nwhether you go with notebooks and how do\nyou switch to the existing code base\nbecause that's also very kind of tricky\nthing to do\nso my idea was always i wanted to have\na project that i can\nload into the kubernetes cluster submit\nthe task try it out it doesn't work\nrevert do something else and kind of\nhave this back and forth back and forth\nas a\ni don't know developer machine right\nthat became a thing some time ago and i\nthink one of the\ndatabricks blogs actually mentioned that\ntheir machine is becoming a thing and\nthey use it quite a lot so i have been\nthinking around something like that for\ndata scientists to be able to experiment\nfaster and deploy basically you have a\nmodel that can be ready for\ntesting or for for evaluation basically\nand comparing to the existing model that\nis running in production\num\nso that was um that's how i see the the\nflight can help us in this\nin this way\nand and before we hand it over to to the\ndata science and ml engineering team i\nwould love to bring in which i had so so\nyou mentioned earlier you had like\nobviously your platform background and\nthe kubernetes background obviously i\nassume for you kubernetes was one of the\nrequirements for the new platform\nuh yeah so kubernetes is something uh\nour\ncentral uh uh\ninfo team actually pushing for because\nit's highly maintained\nby them and we have lots of things in\nplace like central logging integration\nin kubernetes\nsecurity patches\nthe upgrade of the clusters\nmonitoring tools etc so we have\neverything in place and if we are\nactually\nusing this standardized tool we don't\nhave to maintain multiple different\nmultiple different platforms for\nactually doing the same thing for\nexample\nsome teams might be using emr some\nthings they're using luigi and\nall the upgrades of\nfor security patches and\nlogging agents monitoring agents etc are\nactually diverging into different\ndirections and people are specializing\nin different\nwith different things\nand that actually also adds some\nfriction between\nnot friction but it makes it hard for\npeople to work\nwithin other teams and now it's actually\neasier if we have a\nsimple\nor single platform\npeople can actually interact with each\nother easily so that's\nkubernetes is something we actually want\nto have more it's plus from the platform\nperspective it was like that but plus\nthe developer productivity perspective i\nthink it's very good because you can\neasily\ncreate a local image and easily deploy\nit and\nyou have the actually production\nor some environment that's very close to\nproduction and you can easily test and\niterate on\nyour cloud platform other than running\nlocally testing locally and then\ndeploying the cloud and the whole\ndeployment\ntakes some time and then you you notice\na mistake so you do repeat everything\nkind of kills the whole iteration\nprocess for developers\nuh so from platform uh we it's we love\nthat we have a\nwe have a single platform that\nwe don't have to maintain\nthree different platforms\nthat's great so so typically the\nquestion is i mean\nyou i think oleg and and paul and i\nthink you with your background i think\nfor you it's probably very easy to say\nwe need this as an infrastructure but\nthen you go to data science and machine\nlearning and then potentially you send\nsome people shifters down the spine when\nyou say we need kubernetes and we need\nto run your workflow in here\nso that's that's ini let's bring you in\nand see obviously\nwith those infrastructure requirements\nuh how did you feel and how did you get\ninvolved in making that decision of\nusing flight\nuh\nso basically\nuh\nuh i'd like to start from the like the\nthe history from our team like so in our\nteam they they used to\nuh\ntrain or manage or machine learning\nmodel in a very manual and offline\nfashion\nso we start from a jupiter notebook and\nthen they\nand the for exploration\nand the demo things and after we decide\ngo with this machine learning model into\nproduction then they\ndo some\nthen they write it into some like\nproduction ready\nscript\nthen they normally manually train that\ntrain with the script and manually\nstoring that\nmodel into some specific\nbucket that's our journey for machine\nlearning for very long time because we\nonly have very limited number of machine\nlearning model and they don't train it\nvery regularly so at the beginning it's\nvery manageable\nbut as the number of machine learning\nmodel in our team grows it becomes very\nlike clumsy to do it manually it's a lot\nof manual labor because we need to\nrun the script and the storing the model\nand also sometimes we need to to do this\neyeballing to evaluate the model\nso\nfrom that from this uh journey we start\nto think about maybe we need to find\nsome like standard platform to put our\npipeline in so that we can manage them\ntogether and we can use it to\nautomate our training\netc and also\nprovide us better platform for\nversioning and\nobservability\nso i think that's the kind of the reason\nthat they decide to go with some\nuh machine learning operation friendly\nplatform\nyes that's basic my\nyeah\nso so i think what you just said is it's\nlike i feel like sounds so rings so true\nand sounds so\nyou know the experience they had before\nso typical for so many\norganizations right now\nthat at some point they write you know\nnotebook scripts and then jupiter\nnotebooks and\nand then they put the stuff gets put in\nproduction i mean for you it's probably\nmaybe even a one-dimensional step from\npython writing and putting production\nother teams write r that gets translated\nin machine learning in python and then\ngets put in production and then features\nare very different from what they used\nto be before and so that's all very\ndifficult stuff to do and then obviously\nthe more you have the more you have to\nmonitor so\nat some point this becomes it's no\nlonger peace of mind and it's very\ndifficult\nso was it for you clear what you wanted\nto begin with solution-wise or did you\nhave to go through discovery journey as\nwell and figure out which one is the\nsolution the platform that could help\nyou there\nuh personally i have tried out to\nmanage the training pipeline with the\naws resource i have tried i have tried\nit out during a two-day live day to set\nup\nlet me think about uh the\ni think it's the step function as the\norchestrator and use the lambda as the\nworker of to run the training script and\nuse the event bridge as the scheduler to\nautomate it i'll try to do that\nand i mean aws resource is very\nversatile and\npowerful but it do need a lot of\nengineering experience on that and you\nneed to manually prevention the resource\nand do all the stuff yourself to set up\nthis work stack\nso\nthat's a totally different experience\ncompared to using flight\nyeah i think i think it's interesting\nbecause i went through the same journey\ni actually had my feature uh builders\nbrought up as lambda as lambdas uh saw\non the fly for batch processing and i\nfelt like yeah it's good but then once\nyou put your inference\nthere and you do modeling as well\nyou have like all the restrictions that\nall the sudden kick in\nand uh so and you don't get out of those\nyou know experiences kind of like\nhow do you control your model drift i\nmean there's no answer for that it's\njust like putting up an endpoint and\nthen building the features for the\nendpoint right so you get a partial\nsolution then you look for other partial\npoint solutions you don't have a system\nthat works together was that a similar\nexperience for you\nyeah exactly just like you said it's\njust uh where we can manage or\norchestrate this training but in terms\nof monitoring and the rest\nuh there is no\nlike uh\noff-the-shelf solution to to\nto make it competitive compatible with\nand also i forgot to mention there is a\nlimitation on lambda function so\nbasically you can only prevention\nmaximum 10 gigabytes for the lambda\nfunction and also in our case uh depends\non like how many training data you are\nusing\nthere is a high possibility to run out\nof memory during training it's also a\nvery\nlike a limitation when you use this kind\nof\nuh materials in aws\nyeah exactly so i think and then bjorn i\nmean obviously you're on the data\nscience side and and and you you you and\nyour team come up with\nkind of like you know what is the the\nnext best model that you can throw at\nsomething to solve something so how did\nyou work together now with enid because\ni think now you cover kind of like you\nknow just one the beginning part of the\nworkflow and then the production side\nwith uni and that's so how did you\nactually work together and how does how\nis that now using a platform like flight\nyeah i think i think it's an ongoing i\nmean ongoing discussion you know within\nthe team um\nhow exactly we want to set up things and\ni think now it's you know partially it's\nit's it's new models that we want to\nwhere we want to build pipelines in\nflight and it's in its old models where\nwe need to port\nthe older pipelines that exist in into\nflight and sometimes it's easier\nsometimes that's a bit more difficult um\nand i think sometimes it's you know\nsometimes it's even easier to start from\nscratch and to\nonce you once you know kind of this this\nkind of dark like\nworkflow\nhow it should look like in the end i\nthink then conceptually you can you can\nmore easily come up with kind of the\nsteps that that should be implemented in\nthe pipeline\num and then i mean i i really enjoy that\nyou know\nthe idea that we are all converging\ntowards kind of a\ncommon solution across the department\nenables us to\nyou know offload some of the\num some of the knowledge you know to\nmaintain\nkubernetes and and other things like\nthat um to the platform team and and\nfocus on kind of the data science\naspects\num and coming up with with new models\nand i think um i can only echo what what\nenia said about um you know this this\nkind of traditional way of working um\nstarting off with jupiter notebooks\nexploring the data and then\nyou know testing various models and then\nyou find something that is good and\nseems to work well so what do you do\nthen right how do you take kind of the\nnext step\nbring that into production and and\nactually let it work for you um and i\nthink that's something you know i've\nseen this in in consulting and data\nscience consulting before before i\njoined chipset um and i'm seeing it now\nas well you know when we have these ai\nupscaling courses where\num the participants in the end are\nyou know um\nmaking projects in in the various areas\nof of ml\num\nit's\nyou know you can you can make nice\nprototypes but then what is the next\nstep um and i think a framework like\nflight um\nthat basically you know where you then\nhave some some cookie cutter\ntemplates um\nthat enables i think people to more\neasily take this stuff into into\nproduction\nyou know especially if it's if it's data\nscientists that are not heavy on the and\non the engineering side\ngreat yeah i know i think it's fantastic\nso we're almost at uh the at the end of\nthe\nthe sync but i do have two more\nquestions for for the team\none is if you i mean hindsight is always\n20 20 so if you look back you know all\nthe good and also not so good decisions\nright but the other teams they're just\nlike experiencing what you need what you\njust already described right now that\njust doing it exactly that way so\nlooking back and uh what is it\nthat you would recommend a team similar\nto yours where you were like two to\nthree years ago\nmaybe maybe the one\nrecommendation from each of you that\nthat you have for for your counterpart\nlike from a data engineering point of\nview from a data science point of view\nfrom an architectural point of view\nincluding things you should take the\nlead i mean so what were the learnings\nand maybe you can share one\nthat would be fantastic to know so i'm\ngoing to start with paul what's the one\ntop learning that you would\nyou know want to share\nyeah no\ni i i have a lot i could share but i\nthink i want to go with actually\ncommunity because\none of the things that really struck me\nis the applied community in itself um\ndon't underestimate the power of a good\ncommunity right you you're very\nwelcoming\nuh you created a support channel for us\nwithout asking you just did it and and\nwe have good chats going there and\nso many people contributing to flight\netc i mean yeah you can have all your\nrequirements and everything and\ndifferent needs but i mean finding a\nproject that gets a lot of love and has\nan open and warm and welcoming community\ni think can push things over that right\nso maybe one has a softer quality but\ndefinitely something i would keep in\nmind\nfantastic uh ini what's what's the\nhindsight uh learning that you would\nwant to share with other machine\nlearning engineers\num\n[Music]\nthat's a good question\ni\nlet me think about it\ni don't know\nlet me ask you that question uh so is it\nis it i think one that that often comes\nto mind\nshould you engage with your\ninfrastructure team sooner or later or\nshould you go down the road and figure\nout things in your own i always think\nlike on the machine learning side data\nscience side the question is when do you\nengage with with you know your platform\ninfrastructure team was did you do it\nearly enough or was it always a you know\na thing that happened there or was it\njust like coming in late\nuh\nin terms of engaging with like platform\nthing uh i think i have great experience\non that because i was i was working on\nreally\nmigrate training pipeline into flights\nduring last month\nso\ni think in our scenario the platform\nteam which is muke hit is mainly\nworking on managing all the\nback back end of the flight\nand he managed all the like the\nenvironment and\nthe probation and the under the\nresourcing thing for us\nso\nin my experience i i really kind of like\ncollaborate with him a lot of the time\nbecause it is i'm the first user of the\nplatform that he set up for or\ndepartment and\nthere is a lot of things that we need to\nwork together to make it clear and\nand\nproblems only appear when you really try\nit out right\nyeah so in my case i engage a lot with\nthe\nthe platform within the organization and\nin terms of the engagement with like\nflight community i think i use for\nexample i use the dog daily and when i\nstuck at some problem i always go to the\nslag channel and try to search the\nsimilar problem\nand i found that normally i can find a\nlot of similar things and also the the\nresponse of that\nproblem or or a thread is always very\nhelpful\nso\nthat's a good i think that's a big plus\non the experience of this flight\ngreat fantastic so let's go to much\nahead then to oleg and then to bjorn so\nobviously i mean you seem to be at the\ncenter of everything here right now is\nis that the secret to share that you\nneed somebody uh to drive this and to\nbring everyone together\nyeah so we i helped with the deployment\nof the flight and i'm glad that he had a\ngive it a try and we noticed lots of\nmissing pieces lots of things networking\netc so\nwe have fixed them on the way\nfor the flights with kubernetes\nintegration i really like that\nwe can actually\nwe can manage the cluster resources and\nthat's\nquite specific because we might want to\ncreate some secrets and\nthe namespaces that flight creates then\nwe can actually easily configure this\nand the sound templates\nand\nwe can\nactually bring all the customizations\nthat we have in our own\nkubernetes setup\nand actually use it\nwith lights\nso that's um was actually a good plus\nfor us\ngreat fantastic so oh like hindsight uh\neverything's 20 20 what's what's your\nsecret that you want to share\nuh yeah start thinking about how are you\ngoing to put this model into production\nfrom the very beginning i think that\nwould be the most important thing for me\nabsolutely short and sweet no i love\nthis i think this is really people just\nstart it's the same thing when you have\na project everybody wants to run to\nencode\nbut you gotta have a strategy uh and i i\nassume prior sitting on the data science\nside we often feel i mean i have a data\nscience background we often feel like\nwe're left out a little bit and uh and\nthen all of a sudden you get thrown into\nthe mixed version and you're like oh wow\nso what was your experience and how\nwhat's your recommendation specific for\ndata scientists\nto get started on orchestration platform\num i mean i think like if i if i think\nof myself a couple of years back then we\nwould like custom custom make some some\nsome solutions and i think that's um\nyou know nowadays i think\nuh converging to what's a common\nsolution is um makes a lot of sense to\nme um\nthat would be my\num my guess\nlet me ask you one more question do data\nscience scientists have to understand\nand know kubernetes\nthere is this article by um i i don't\nknow if i pronounce the name correctly\nto peon\nthat wrote a great article about this\nand and i think um\nit's not strictly necessary i think data\nscientists have a lot of other things\nthat they can that they can focus on\ni agree and chip's latest book is\nactually really interesting with that\nregard to that regard to you okay\nfantastic so i just want to say thank\nyou shifts that team this has been uh\nreally very insightful we would love to\nhave you back\ntalk about your next experience\nobviously you're you're doing a lot more\nwith flight that's fantastic so i just\nwant to say thank you and uh we're going\nto put this uh video up onto social\nmedia as well please feel free to share\nit and comment on it so i just you know\nwant to say fantastic i i rarely have\npeople with such different backgrounds\nbut they all work together and\ndelivering one product and i think this\nis really amazing so kudos to you and uh\nhave a wonderful evening over there also\nthank you very much thank you i just\nwant to also say\nsuper thank you to to everyone and\nworking with flight uh you're doing an\namazing job\nthanks thanks paul we teamwork\nappreciates it and uh you know where to\nfind them on the slack channel\nall right take care bye"
    },
    {
        "title": "KubeRay x Flyte Integration",
        "transcript": "and uh hand it over to yi and kevin sue\nso generally and the stage is yours\nplease tell us uh\na little bit more about flight and ray\nuh yeah thank you mark uh my name is yi\ni am\na part of union ai and uh here to give a\nquick demo on\nthe upcoming ray task and\ncube ray integration uh this work is\nthanks to kevin sue who this meeting is\na little late for so i will be giving\nthe demo in instead\num but yeah with that\num\nuh yeah okay so i was actually you know\nwhat let me just let me know if you\ndon't mind i'm gonna share my screen\ngo ahead\nstop one share cause obviously yeah okay\ncool um\nall right that\nsweet\nuh\nokay yeah so this is uh\nupcoming\nuh ray task uh look for it in the up i\nmean 1.2 release which i think we're\nsliding for\nuh hopefully around the end of well\nmaybe like mid-september maybe a little\nbit later\nthis if you\nthis is the high level diagram it looks\npretty similar to\nthe spark tasks that some of you may be\naware of\nso\na new task configuration\nuh registration everything happens as\nnormal\nwhen the flight engine hits it um\nit will launch a\na cubery crd and from there the operator\nkind of takes over\nso a lot of people have been asking for\nuh\nray integration so uh glad we finally\ngot around to it\nthis is uh hopefully going to solve a\nlot of problems\num i want to point out the we left some\ninstallation instructions in the slide\ndeck\nand the reason we did that is because\nthis currently uses the cube ray job crd\nthis crd was\nnot in 0.2 so you're going to have to\nupgrade to 0.3\n0.3 of ray was released\nlast week i believe\nso\npull those changes we had some\nminor hiccups with the helm chart so we\nwe found that just applying the manifest\ndirectly worked a little bit better\nfrom there\nenabled the ray plug-in as you normally\nwould\njust a\ngonna give a\nbrief overview of\nhow it would look\npretty standard this is actually in\nmostly in the flight snacks cookbook\nrepo i pulled it out into a separate\nthing just to isolate it\nwe can upload this as well make it\npublic if people want a\nstandard normal docker file nothing you\nhaven't seen in any of the other\nexamples and\num\nhere we the way the integration works is\njust the\nbit that gathers the\nthe features is the\ngoing to be the body of your array task\num and\nthis makes it so that we can preserve\nlike the vast functionality of\nof ray so\num\nlike flight will not enter\nuh intervened with\nthe actual execution of the remote\nfunction here\num\nit's a little short for a demo so i\nthe execution\nyou can see\ni mean\nthe we just have a normal python task\nand a great task after it\nagain you can always i don't think this\nexample does that but you can always use\na different image for the ray for the\npython task than for the ray task\nthere's examples in the documentation\nthat\nshows how to construct multi-image\nworkflows\nand we wanted to make sure that the\nlocal story worked as well so if you\nif you just have ray code\nor if you just have a normal task that\nyou want to run summary code through\nif you use init\nhere and then you point it at an\nexisting ray cluster\nit will also work we\ni don't have a ray cluster up and\nrunning so we actually just changed the\num\nwe made the the other the first example\nuh not destroy the cluster on\num on exit so you can still you can\nstill make use of it and connect to it\nand port forward it but\nconsider this to be a\nlike a corporate uh persistent ray\ncluster that you have access to\nuh that's just to\nshow that we were kind of\nuh\nlike\npreserving the normal ray execution and\nthe workflow as much as possible um\nyeah that is it for the quick demo\nafter no other questions oh yeah i think\nactually let me go back to this slide\num\nthis currently uses the\nthe cube rate job crd i think we're also\nplaying around with um\nflight managing the head node instead\nand then we're also trying to see if\nthere's a use case for ray cluster reuse\nuh assuming we get this part of it you\nknow down um\nhopefully that's something that we can\nwork on later this year so if you guys\nhave any feedback or questions um\nopinions on the ux please let me kevin\narena while anyone else in the channel\nknow so thank you"
    },
    {
        "title": "Flyte Community Update 020 - Aug 23 2022",
        "transcript": "okay welcome to the flight community\nsink my name is morgenstein\ni'm with union ai and i'm your host uh\ntoday\nfirst as always a few housekeeping notes\nbefore we kick it off\nthis is a zoom video call we are going\nto record it and we're going to share in\nsocial media\nuh that's uh you can go to youtube and\nyou find the recording is usually there\nsometimes it takes a week but you'll\nfind all the recordings there also\nthe flight open sourcing\nis a\nwonderful forum to engage with\nflight developers\nso people who see this in social media\nshould join go to flight.org\nand\ngo to the ad event and you can join\nthe open source and community sync there\nlet's go to the agenda\nso today we talk about\nuh two very exciting topics\nwe have rey and flight and uh earlier i\nhave to admit i googled for rain flight\nand there's a person whose name was ray\nflight so so you can find that first\nreally interesting but we're going to\ntalk a lot about ray flight and talk\nabout ray and flight so we're going to\ndo this today and we have uh the machine\nlearning team\nand data team from shipstead here going\nto talk about\nthat journey so far using orchestration\ntools flight and what's next for them so\nthat's going to be a very very\ninteresting\nvery exciting conversation i really look\nforward to that\nso uh first a quick community update\ni'm not sure if associated as somewhere\nelse on the call i actually doubt it but\nassociated with here um raise your hand\nnope um that's okay so associate is our\ncontributor\nfor\nthis\n[Music]\nopen source sync so i just want to call\nout some of the contributions\num help value changes for on-prem um\ninstallation so sujit actually uh has\ndone a really really nice job there\nso he was picked by i also want to say\nthis by sumita allah who is our\ndeveloper advocate\nfor flight and sumita is behind many of\nthose people that get shown here so if\nyou want to get shown here so make sure\nthat you talk to samita she's always\ndoing a fantastic job\nthanks amita\nso um with\nwith that let's move on to the next part\nthe community updates\nwe had kaithin\nthe tse chair fly.org take a part in a\nask me anything with the mlaps community\nthat was a one hour session and it was\nreally interesting about how many\nquestions you can get in slack in one\nhour\nand caitlyn did a fantastic job\nanswering all of those questions you can\ngo back to the mlaps community on slack\nin the ask me anything channel and see\nall the questions that have been asked\nand also the answers that uh katherine\nhas given we're also putting forward a\ntranscript that should show up uh soon\non the website okay\nyeah a big community update is the race\nsummit today so we're going to spend\nsome time today talking about the race\nsummit\nand also what we have for the race\nsummit so the\nthe union team who is actually\nshowcasing flight there has a booth and\nso it's all about flight\nso they have you see in the photo on the\nright hand side that photo is actually\nless than 20 minutes old so we just put\nit into this presentation here right now\nto see what kaithin is showcasing there\nit's all about flight so it's a great\naudience so we're super excited being\nthere and we also have\nye tong\nflight key flight contributor and\nfounding member union here talking about\nthe integration\nso let's move on and\ngo to\none more post that is in the making\nright now again by samita allah and so\nshe's going to post this rain flight\nblog post probably i think\nearly next week we're going to have this\nout on the 30th so this is going to be\nvery interesting gives you more details\nmore information and coincides nicely\nwith what's out there on the\ndocumentation side\ncool all right"
    },
    {
        "title": "ZipRecruiter's Flyte Path",
        "transcript": "seth is\nuh has a very interesting story it's the\nzip recruiter's flight path\num so we're going to take a little bit\nof time and listen carefully and then\nlearn and i feel like this is very\ninsightful specifically for\norganizations and teams that are\nactually kind of like this point where\nthey discover well we might need\nsomething that is different than what we\nhave because for scalability represent\nreproducibility and so on\nand seth and his team have gone down\nthat path and discovered obviously\nflight\nand we would love to hear a little bit\nfrom you seth today about what your\njourney was so far what the pain points\nwere that you encountered and what\nbasically got you onto the path to\nflight where you are today and uh maybe\nin hindsight at some of the learnings\nsome of the 20 20 pieces that you want\nto share with other users who are under\nsame situation stage is yours\nthanks martin\num like i said uh i am seth and uh i\nwork at zip recruiter um\nsniper critter is a\nonline recruiting service that uh\nprovides a marketplace for job seekers\nand employers uh to connect with each\nother\num\nsupercreator has served over 100 million\njob seekers and over 2.8 million\nbusinesses\num and i've been working as a creator\nfor uh over five years now\nuh i've been working\nprimarily in\ninfrastructure\nalso doing devops\nuh and then about uh almost two years\nago now uh transition to an ml ops team\num\nso i will share my screen so let me ask\nyou that right away so transferring to\nthe mlaps team so was that a new team or\ndid you guys uh create that team from\nscratch uh how did that happen\nyeah we we created it for from scratch\num\nuh like we saw that there was you know a\nneed for you know mlaps\num because like our our devops and our\nlike our csc system was pretty mature\num but it didn't really uh tackle all\nthe issues that are uh specific to ml\num because not everything can be done\nin this sort of like um git op style way\nthat uh that we had been doing i needed\nsomething a little more dynamic\nawesome\nsee so um\nshare my screen\nall right can you see\nall right um yeah so like\nzip creator does a lot of machine\nlearning\nlike we have a lot of different use\ncases that are critical to the business\num like we do nlp like resume parsing\njob description parsing\num and this is actually like\npretty\ninteresting area\nwe're still doing development in this\narea\nbecause the language used in like\nresumes for example is actually a lot\ndifferent than normal english\num so parsing it is a bit different\num\nand then you know we have class tire\nclassifiers to like uh try to\ncanonicalize uh job titles\num since there can be so much variation\nbetween\ndifferent job titles uh\nand also try to figure out from things\nlike job descriptions whether the job is\nlike remote job uh so that can help with\nfiltering uh of course at the core of\nour business is like matching uh jobs to\njob seekers\num\nso um\nand then there's like ranking like\nuh the search results of jobs and\nalso ranking job seekers when we present\nthem to employers\num since you can get a lot of\napplications and you want to filter the\nthe best\nuh job seekers to the top\num and we also have like broad detection\nuh because we take you know uh\njob seekers\num safety\nlike very seriously every company um and\nuh we wanna protect them as much as\npossible so uh you know protect them\nfrom like scammers who are posting fake\njobs and and trying to rip them off\nand we of course have like many other\ntypes of models um you know for other\nthings like predicting uh clicks\nclick-through rate and things like that\nso like\nuh as i was saying before\num we for\ntwo years ago we didn't really have\nmuch ml ops standards\nbut we have like all these\ncross-functional teams\num who work\npretty independently from each other and\nas a result they have various\nlevels of maturity\nuh like we have one team for example\nthat uh is like way more mature than the\nrest uh and has like a lot of\nuh technologies that they use and and\nit's pretty advanced and then uh we have\na lot of teams that are like\nuh have pretty bare bones and\nuh\nyou know don't really have great ml ops\npractices\num\nso our team is like formed uh\ntwo years ago uh to sort of address this\nuh and we've just sort of one by one\nbeen tackling uh each problem within the\nmachine learning life cycle uh like we\nstarted with uh\nexperimentation\num and like\nand then uh we started moving to uh\nmodel training sort of like\nuh in a logical flow\num but then we sort of took a break from\ntraining and went to model deployment\nfor like compliance reasons uh we needed\nbetter auditing of like what models were\nactually getting deployed\nto production\num and then we moved our way back to\ntraining\num\nand uh like i said we've we have uh\npretty sophisticated like cacd\ninfrastructure and you know some of this\nincludes like uh airflow\num\nfor like data pipelines\nand\nuh we actually had to like uh\nbasically build our own kubernetes\nplug-in because we were using airflow on\nkubernetes before uh there was like an\nofficial plug-in\nuh so like our\nway of using airflow is very custom\num and it's completely declarative uh\nlike we defined dags and yaml\nand we just saw that this wasn't really\ngoing to work for uh ml in general uh it\nwas like\nsuper hard to like you know express\nthings and just uh yaml\nand um in like the super advanced team\nthey had like even created an internal\nframework uh on top of airflow uh our on\nour version of airflow to um\ntry to address uh some of these concerns\num and some of these are like still\ninherent to airflow\num\nso we started looking for\num some new solution\nand we thought about well maybe we'll\njust you know move everyone to airflow\nwe already have a bunch of\ninfrastructure ready for it um you know\nwe can do that\nuh but then we started like looking at\nyou know the actual\num use cases of like all these ml\ntraining pipelines and and saw that like\nairflow wasn't really\nfitting the bill um it\ndoesn't have like uh the kind of\nversioning that we need you know\nmachine learning training can be\nhighly experimental so you need to have\nall kinds of different versions that\ncould be active at the same time\num and that concept just doesn't really\nexist in airflow\num you know you only have like basically\nthe latest version at any given time\num\nand then another issue is that like you\ncan't really pass data around very well\nuh in airflow like between tasks\num it's completely up to the task to\ntake care of it\nuh airflow does have like the xcoms\num\nfeature which is kind of like this weird\ninterface of passing references to data\nuh but you can't really pass like\nlarge amounts of data through that uh\nbasically only references to s3\nor whatever object storage you use um so\nyeah we have\nlike a lot of code that's just taking\ncare of\npersisting data to s3 and then passing\naround this reference and then\ndownloading it from s3 and and\ndeserializing it and there's just a ton\nof code like that everywhere\num\nand and then you know you can't really\nsee what the lineage\nis uh between the tasks of the data\num\nand it's like really hard to tell\nlike what the actual data inputs are to\nthe task and\nin which uh\noutputs it has\nand you couldn't really figure this out\nunless you actually started looking at\nthe code\num which is not a great thing\n[Applause]\nand then\nyou know ml\ntraining can be really expensive in\nterms of compute resources\num and time uh and also money\nand um\nso\nif you're\ntrying to like do you know these all\nthese expensive tasks and then you have\nlike some\nuh\nyou don't want to like repeat that work\nall the time if you don't have to\num\nand so\nyou know airflow doesn't really have\nanything like that\num the internal framework uh that we had\ndid to try to do some amount of caching\num but it was really\nuh a really primitive mechanism of just\nlooking for like a success file in s3 um\nit didn't really have like a content\nbased hashing or anything like that\num\nand then\nyou know like our our\nit's hard to do like remote development\nuh with our like airflow stack um\nlike uh because\nuh\nlike the way that like dags are deployed\nand we want to keep things\nseparate um from what's deployed through\nour cscd system\num\nand so it would take a lot of work for\nus to be able to sort of recreate a like\nairflow in kubernetes development\nenvironment\nand\nso we started looking\nat other solutions to see like is there\nanything that just sort of has these\nthings out of the box\num and we evaluated uh\nmultiple uh solutions and\none of them uh is flight\nand of course all those things that were\nred x's are check marks for flight uh\nthese are all just like\nbatteries included features which was\nreally nice um\nuh\nand then it also has all these other\nnice things like the uh intuitive python\nsdk\nlike writing a workflow and flight feels\njust like writing a script in python\nit's like\nvery intuitive\nand then\nwe really like this like auto automatic\ni o management where like things are\nsort of just like uh\npersisted to this like intermediate\nstorage layer uh on your behalf like\nit just feels like you're returning uh\nvalues from a function call but\nso it's completely\nhappening in the background you don't\neven need to think about it which is\nreally nice for like uh our data\nscientists um\nbecause like uh\nour like one of our major goals is to be\nable to have\nuh data scientists uh work independently\nand not have to rely on a data engineer\nor an ml engineer to\nto uh turn their models and\ntraining pipelines into like a pipeline\num\nso like we want um\nthem to be able to just like author\ntheir pipeline uh in it to be low effort\nuh from moving from like something like\na notebook environment uh to a pipeline\num and then\nthe interface is like very clearly\ndefined um with flight which we like\nand\nlike we can keep the orchestration\nlayer separate from the actual model\ncode\num we did see examples of\nin like our own models where uh people\nwere like\nuh dealing with like s3 paths for things\ninside their actual like model classes\nwhich was\nuh sad to see but\num\n[Music]\nbut yeah with like with flight it's\nreally easy to just sort of keep this\nlike orchestration\nstuff completely separate from your\nmodel code so you can potentially\nuse a different orchestration tool\nin the future\nand then you know\nflight is already kubernetes native uh\nwhich is great for us uh everything we\ndo is in kubernetes um we even do\ndevelopment in kubernetes like we have\ntooling that\nuh can\nbasically create uh something sort of\nlike a virtual cluster within a cluster\nwhere it'll spawn\nnamespaces that are basically copies of\nof our apps and uh and we do that uh\ndevelopment in those namespaces as if uh\nwe were the actual app\num\nand\nso like uh to be able to do something\nsimilar like that would be would be\nideal uh for\nuh pipelines as well\nuh and then it has a spark integration\nalready uh which is nice because we use\na lot of spark um\nwe we already like have our own sort of\nimplementation of like spark on\nkubernetes uh that was separate from the\nspark operator\nbut like anything that would be like\nintegrated\nwith the workflow would be nice\num so\nbasically from there we went to\nuh we sort of narrowed down our choices\nand we started um\nuh pocs for the different uh solutions\nthat we were looking at\num\nand so like for flight uh the flight psp\nspecifically\nwe started with like the sandbox uh the\nflight sandbox\num just to sort of test out the examples\num\nand you know verify that you know it\nworks the way that we think it\nworks based on the documentation um\nand we really liked what we saw there um\nso then\nwe moved on to sort of like the next\nphase of let's actually deploy it on our\ndev infrastructure and\ni think some companies might find that\nlike kind of a weird thing um\nthat we would have to like actually have\na full-blown deployment for a poc but\nuh for our company\nit's um\nsort of like the first real like\nalmost like a litmus test\nuh to see like will this thing actually\nwork for us\nbecause our\nkubernetes environment is very locked\ndown\num and is we have like\nvery opinionated usage of it and\nuh we take um\nwe have like a lot of security\num\nconstraints that that we deal with like\nyou know we have a gatekeeper running in\nour cluster that\nyou know\nprevents\ncertain types of resources from being\ncreated if they don't have the right\nsettings\num so like when we were setting up\nflight you know we had to\nfight with gatekeeper a little bit\nof\nthis happens pretty much any time we\ndeploy uh third-party software\non our cluster\num\nand and and usually when that happens\nlike\nyou know sometimes we'll find out that\nthat third-party software required\nsomething that we're not comfortable uh\nsupporting\num like uh\nlike they could have you know some kind\nof cis cuddles that they need that uh\nthat would give access to the host or\nsomething\nand then would\nbasically give them like root on that\nkubernetes node\num\nthat's happened before so and we had to\nlike uh\nbasically say okay we're not gonna go\nwith with that product\num\nbut um yeah like so it took a a pretty\nlong time uh we got a lot of uh help\nfrom keith and\nand hatham uh and the rest of the union\nand\num\nand uh so like\ni just want to have a special thanks to\nthem uh\nthey helped us tremendously\num and you know like we had to like\npare down the r back\num quite a bit\nuh just to be like the most minimal set\nof permissions uh to be able to run\nworkflows in separate namespaces\num\nlike we we don't we have our own uh\ntooling for standing up clusters and uh\nand managing them so we don't use flight\nuh to manage clusters\nuh and we have a\nseparate cluster for each environment so\nlike our development our staging and our\nproduction environments are all separate\nclusters\nuh and separate vpcs and and everything\nuh\nthey're completely network isolated\nuh so we're not using domains the way\num\nlike would use them out of the box\nwe do have like one use case for\nuh domains right now uh which is then in\nthe development environment we have like\nuh a domain where it's like\nworkflows that were deployed through our\nci cd system\nuh and then we have a domain that for\nlike workflows that were fast registered\nuh\nfor development purposes we just\npreferred to keep those domains separate\nso that like things that were uh\ndeployed to dev as like a reference\nserver for other people to work against\naren't affected by um\nor like\nmessed up by uh things that\nwere deployed from\nsomeone's feature branch that is not\nit's not even on the main branch yet\num but yeah so then\nuh we once we got\nor not once we got played up but um in\nparallel to\nthat effort of just standing up the\nflight infrastructure\num i was working on\nrefactoring a real\nmodel training pipeline\nto be able to use flight\nso we took\nan airflow uh pipeline an existing one\num\nand\nuh well\nwe took the the model from it and\nbasically just ported it to flight uh so\nit's like you know pretty standard\nuh simple uh pipeline it's it's shown\nhere\num\nas a data preparation step processing\nstep and then finally like a learning\nstep for uh for the training and\num\nlike uh\nbut this was from\nusing the internal framework\num that was on top of airflow before\nuh and so it was quite a bit of work\njust to sort of like decouple it from\nthat framework first\num so that we could then\nuh apply flight to it uh sort of\nnatively\nand\num\nand then like\nwhat we got was\nuh over 66 reduction in orchestration\ncode when we moved to flight\nuh so that was like a huge win\num\nand\nwe're like really excited about that and\num it worked really well like um\nuh we we really like like uh you know\nhow\num we can split up our spark tasks uh\nand have separate spark configurations\nfor them so if like the downstream test\ndoesn't need as much resources\nthat's really great\num\nlet's see\nuh yeah so then now i'll talk about the\nlike uh custom tooling that we've sort\nof like been built because\nor i guess before i get there um\nobviously we decided\nuh to go with flight um\nand so then we're uh\nstarting to like productionize it uh\ndeploying it uh itself was sort of like\na productionizing step for us uh because\nof how strict our infrastructure is\num\nbut we needed some additional tooling to\nmake the development experience a bit\nbetter\num\nat the time when we started developing\ntooling\nuh things like pi flight run didn't\nexist\nso it was like really painful for us to\num\nlike we had a script that would do the\nserialization and the registration\nuh of like our\ntasks and workflows um but it's kind of\nlike a ugly script and\nand then um\nit's kind of like uh hard to generalize\nuh for like all of our different apps um\nbecause like within our infrastructure\nlike uh we like map uh apps to name\nspaces so every single\napp has its own namespace and and has um\nuh\nlike its own i am roles and things like\nthat and secrets associated with that\nnamespace\num\nso like we created uh basically an app\ngenerator um that generates a flight app\nwith like you know its own\nuh docker files and and kubernetes\nmanifests\num and and one of the manifest is like a\ncsd job that\nuh you know handles serialization and\nregistration\nuh so like whenever someone merges to\nmaine for example\nuh their\nflight code is automatically serialized\nand registered with the flight admin in\nin the dev cluster\num\nand then we also created a\nwrapper for the flight remote class from\nuh farm flight kid\nexcuse me and uh this\nis sort of similar to pi flight run um\nit does like uh serialization\nregistration and like remote execution\num but this like lives inside uh the\nsame like\nuh\nworkflow code that you have\nso i can actually do like a demo of this\npart um\nso here is like uh you know just a hello\nworld uh workflow\nuh has like um you know just\nsimple greeting\num\nand then i have like you know this\ncustom like wrapper for the flight\nremote object\nuh complete launcher\num and then i could just pass it a\nlaunch plan\nuh and then optionally pass it inputs\num\nand then this is you know inside the\nmain so\nuh it won't run normally uh during like\nthe cicd flow where uh where this would\nbe registered uh it's just for like\nlocal base development so\num so i can go to my command line and\nthen i can just run this workflow like a\nscript and uh\ni can\ndo that now\nand so it's popped up uh\nin my browser that to the execution page\nfor what i just\nlaunched on the command line uh so\nnow this task should be\nrunning\nuh remotely\non the cluster\num it may take a while for things to\nspin up up there it succeeded\nand then you can see the input and you\nknow the output hello\num\nbut yeah like this is like pretty uh\nflexible like this is like really\nintegrated with our\nuh\nci cd infrastructure too because like um\nlike it automatically figures out like\nversioning based on how our csv system\nworks and\num\nlike in\nautomatically figures out like what the\ndocker image uh tag would be and things\nlike that uh and also like intelligently\nfigures out\nwhat the actual like uh\ncode dependencies are so that when it's\nlike\nsyncing the code into the container\nit's not\nsyncing our entire repo it's it's only\nsyncing the parts that are relevant to\nthis particular app uh yeah i guess i\nforgot to mention before that uh our\ncompany works out of a mono repo um so\nlike\nthat's like a reason why we have a lot\nof custom tooling is that a lot of tools\ndon't necessarily work well with model\nrepo so we usually have to make wrappers\nthat\nuh sort of narrow the scope of the repo\nand that's sort of what i did here with\nthis flight launcher\num but yeah this is like pretty flexible\nyou know you can\nuse arc parse or\nwhatever library you want to make this\nmore fancy\num\nbut yeah like it hides a lot of like\nsort of the details of serialization and\nregistration uh which with like\nthe sort of built-in uh\nflight tools can be kind of\nkind of hairy on the command line\nwith all the different uh options\num\nlet's see\ni have been signed out perfect\noh there we go\nyeah um so then sort of\nthat's what we have now and it's working\npretty well for us um\nbut we think we can actually do better\nuh we want to make this like even more\nintegrated with notebooks um\nand so like i came up with this idea of\nlike uh\nanother like sort of wrapper for flight\nremote uh it's\nlike i i do have uh like this example\nthat's shown implemented\num\nand basically the idea is that like uh\nyou it's like\nit's a wrapper not only for flight\nremote but also the uh task decorator\nand the workflow decorator um and it\nbasically just allows you to optionally\npass this additional session object to\nthe workflow uh or task function\num and if if that session object is\npresent then it will use it to auto\nserialize register\nand remotely execute\nthe task or workflow\nand then it will like wait on the result\nand unpack it and return it so it's\nexactly like running it uh in a python\nscript\num and if that session object is not\npresent then it just runs locally as\nnormally um\nso yeah this is like um\nsomething that we're going to continue\nworking on and extend for like the\nuh\nfor workflow and launch plans\nso that\nwe can sort of seamlessly integrate uh\ntasks that are defined in libraries with\nthose that are like defined uh in a\nnotebook\num\nso yeah i'd be like curious to hear what\npeople's thoughts are on on this sort of\nthing\nand then uh future work that\nwe're interested in doing is like\nlike i said we want to you know fully\nintegrate with notebooks\num since like notebooks are basically\nthe ide of all of our data scientists\num so they always want to start there\nand we think if we can\nlet them basically\nauthor their workflows from a notebook\nthen that will like\nmake their lives\neven easier\num\nand then this there's this other sort of\nidea that's brewing\nthat i'm calling username spaces\ninstead um\nsort of like what i touched on earlier\nthat um\nare in our infrastructure like we\ncreate sort of like uh replicas of like\napp name spaces\nuh for each user that's developing on\nthat app\num\nand uh\npretty s i'm not sure how like what the\ntimeline is because it's another team\nthat's involved in making this change\nbut\nat some point um\nthese sort of like user name space uh\nwill have like uh\nit'll have the same like secret\ninterface uh where like the secrets will\nhave the same name and the i am roswell\nor the service counsel will have the\nsame\nuh name but like the the actual like\nsecret values will be different uh for\nthe username spaces uh this is like to\nkeep um this is like you know for\nsecurity reasons uh so that's like\num\npeople can't for example access\nuh you know\na database with like uh same credentials\nand privileges as the actual app\num\nand can't do anything destructive there\num\nright now it's just like basically a\ncopy of the app secret\num and so like how this like ties into\nflight is that um\ncurrently you know like with uh\ndevelopment like a fast registration\nsort of runs\nthey run in the actual app name space\num so this could potentially be\nproblematic for us in the future if if\nthis sort of uh enforcement for like\ncompliance reasons and things uh\nrequires us to like run in a actual like\nuser name space sort of like our other\ntooling requires\num so we're sort of trying to brainstorm\nahead of time like\na potentially ways we could maybe\nrun workflows from\na project uh which is you know a\nmap to an app for us\num\nrun workflows for like a project but in\na different name space but like\notherwise uh the same like configuration\num\nso yeah that's um something\nwe'd be interested to know if like other\nuh companies or other\nmembers of the community\nuh have like\nsimilar um requirements\num\nbut yeah that's it\nthank you awesome\nseth this was a wealth of uh\ninformation super insightful i i think\nwe have a few questions here and we i\njust want to see keith and you you had\nearlier comment here\num about ye that uh when you saw\nsomething that seth is working on\nobviously we want to contribute in some\narea but uh\nwhat other feedback do you have said\nfirst we said thank you there was a\ncomplete like start to end picture of\nhow you you know journey was and it's\ngood for us to know at least as a\ncommunity\nwhere we can improve and\nlike we've been slowly working and\nchipping at the problem and we think we\nhave a\nhuge roadmap over the next year to\nfurther chip\nbut we love to\ncollaborate like please communicate and\nwould love to collaborate like for\nexample\nuh\ni think if i don't know if you guys use\nbazel for monorepo\nbut\nuh toyota and\nthat's one other team that uses uh\nmoderate\nthey build bazel build plugins for\nflight\nuh and so and i think strike is using it\nas well i'm i'm not 100 sure but mix\nhere uh\nyeah for us it's like\nit's very language dependent so like we\nhave like\nseveral languages that uh all have like\nguilds and communities within super\ncritter and uh so we all have like\nseparate tooling\num for like building things um\nso like i think there may maybe bazel\nhas used a little bit for some c plus\nplus stuff but\nuh nothing outside of that\nyeah no if you are i think it's open\nsource too it's under the flight we need\nto give it some love we haven't given uh\nthe\nbill plug in a lot of love or even\nadvertise it well enough i think it\nneeds to be\npromoted a little better because i know\nthere are companies who use people\nother thing with\nthe session stuff that he showed\nreally interested you it almost as if\nyou took\nuh you and i we were discussing that\nsome days ago like probably a couple\nmonths ago and he's constantly saying i\nwant the session thing on the session\nuh\nwe've not really uh\nand we've been getting we've been\nworking on how to get it from jupiter\nnotebooks into the code and yet keep it\nseamless and clean and like all the\nproperties that we want to keep which is\nyeah you know uh repeatable and\nreproducible so we are very interested\nin that if you\nwould be open to sharing let's actually\ndeep dive into what you have we could\nfast track it and help you from our side\nwe love we love this this line of\nthinking so\nsure yeah\nbe happy to like at the moment it's like\nvery specific to like our own\ninfrastructure but like the concept of\nlike the flight session is like very\nit's like a very thin wrapper it's\num\nso yeah i could see how it could be like\nuh\nincorporated into the\noss\nyeah we could make it like having\nactually a user with specific use cases\nthat they want to have for their\ninfrastructure actually makes it even\nbetter to build it in the\nuh in the open source because what\nhappens is you build it in the open\nsource with an open source point of view\nbut add hooks so that your use case just\ncontinues to work and and then i'm sure\nwhen we add documentation other people\nwho want to do the same\ncan get it done in like a few minutes\nlike that and\nwe are like this is extremely exciting\nand i think neil's also had a comment on\nthis evan had a comment i'm sure other\npeople are thinking about this all the\ntime\nbut uh that was\nwe are interested and the last user for\nuser thing\ni think there was some other team that\nalso wanted that so i will try to\nremember and find something i talk to\nthings that i forget um but i have it in\nmy notes so i will connect you with that\nuh with them and yeah\nit sounds interesting also like because\nsecurity is one of the\nwe want to operate under zero trust\nessentially as a system too it's very\nhard to build open source systems which\nyou can trust you probably realize\nbecause people\nyou know zero trust causes a lot of\nfriction on getting on board\nand and most people don't see the\nlong-term benefits they only see the\nshort-term benefits of onboarding\nproblems\nor short-term problems rather and so you\nknow it's a big uh\ntrade-off that we always have to make as\na community but we'd love to hear\nhow and where we can actually add it as\na as a nice\nfeature that keeps on getting you know\nadded in the future\nuh but niels or anybody else that's it\nfor mine but thank you for the\namazing so so fantastic so we had time i\ni wanted to sneak in one more question\nfor you seth um this is kind of like a\nhindsight question if you would meet uh\nyour old me from two years ago what\nadvice would you give yourself now\nknowing all the stuff that you know or\npeople in a similar situation obviously\num yeah so like i think\num\nlike\n[Applause]\nlet's see\nyou prepping me for this and i forgot\nwhat my answer was\num i don't know like like for us it was\nlike\nit was kind of challenging too because\nit wasn't all like a continuous timeline\nof like uh doing all this work uh\nbecause we like started the project\nuh and then we just like\num took a break from it and went to like\na different project and then came back\num\nand and like uh so that like sort of\nlike broke our momentum a bit\num so i would say like maybe if like um\nif we hadn't done that like it would\nhave\nbeen smoother\num but then again like uh\ni think\nwe were like sort of adopting flight\nprior to 1.0 so like\nuh things were changing really rapidly\num so if we might not even had like had\nthe same\nexperience if we hadn't like sort of\ntaken a break and come back\nbecause so many things did change\num\nbut yeah like uh\num\ni don't know if that's actually helpful\nwell actually it's actually a good point\ni mean obviously people experiment and\nlook at different solutions and i feel\nlike what you're saying is like once you\ndeep dive take it seriously right and\ndon't jump around because until you have\nreally explored and then an early\nsolution i think the second takeaway is\nflight has matured just like you know we\nhave\nmaybe one way beyond one o one one is\nout there and working on one two\nand that's coming out and and so i feel\nlike uh the other question is for people\nwho are listening here who are in a\nsimilar situation as you are right now\num what kind of like data science team\nor workload or if at any is there a\nthreshold that you feel like you have to\ncross to reap any rewards in the future\nuh what comes to mind from your point of\nview\num\ni'm not sure i understand the question\nlike um\nyeah so let me rephrase it from from\nbasically you know using a system like\nflight\num\nwhat were the you know\nrequirements for you internally to say\nwell this is so important for our\nworkflow because we have x data\nscientists and putting them all you know\ntheir work at scale\nuh with any other system that we don't\nhave right now we have to come up with\nan innovation so what was the threshold\nand obviously when you have like one or\ntwo data scientists that might not have\ncrossed your mind but i'm just wondering\nwas there a threshold in terms of you\nknow volume that you produce models that\nyou produce the things that you put in\nproduction or was there not such a thing\nat all to to consider using flight\noh yeah like um for sure like we we\ndefinitely are trying to like accelerate\nuh how quickly we can um\niterate on model development like\nbecause for a lot of um\nour teams like you know since they are\nstill\nso many of them are just doing things in\njupiter notebooks like\num which you know obviously has problems\nwith like reproducibility but\num\nyou know it would take them a while to\nget that\nuh working and then it would take uh\nyou know additional time to hand that\noff to some other person like an ml\nengineer\nand then have them like\nyou know refactor all the code into\nsomething that's like a pipeline\num so you know it can be like a you know\nmulti-month long process just to get\nlike a new model uh created and\nuh so we were looking for something that\nwould be able to make you know this sort\nof like development cycle much shorter\nand simpler and and not require uh as\nmany handoffs\num\nand so like yeah we were just looking\nfor\nyou know something where you we could uh\niterate on like a pipeline and a model\nuh sort of like at the same time\nin in\nin flight uh seemed like it could do\nthat for us uh and then when we had like\nthe\nthe huge like reduction in like uh\namount of orchestration code that was\nrequired to do it um it was just like\nalmost like a no-brainer like yeah this\nis obviously better so like\nuh it can't be\nuh necessarily a bad decision to move\nforward\nright\nokay fantastic so seth thank you so much\nfor your time this was super insightful\ntoday"
    },
    {
        "title": "Model Monitoring with WhyLogs and Flyte",
        "transcript": "uh so the the y labs team has been\nworking\non a uh integration of wi-lock\nintegration and with flight uh and so\nwith that um if you could unmute\nyourself and introduce yourself and\nthe stage is yours\nnice i'm gonna try to keep this under\ni don't know five or ten minutes but i'm\nmorillo i'm a ml engineer in y labs i've\nbeen there since april and i'm based in\nbrazil this is why my name is funny but\nuh if you cannot pronounce the argo with\nthe regular murillo it works as well\nand um i i just wanted to talk a little\nbit about why logs first and then why\ndid we integrate flights and why does\nthis matter for users right\nso white locks is a the open standard\nfor data logging as we call it and\nit basically takes snapshots of your\ndata\nand\nit allows you to\njust keep track of what's happening with\nthe\nrelevant or statistical information\nacross like different timelines or\nhowever you want to do this right there\nare three main components or three main\ncharacteristics in my logs that to me\nmakes it stand out from\nthe other existing tools\nthat is it's very efficient so we can\ncreate these snapshots this information\nthis metadata very quickly and also very\naccurately because we look at every data\npoint\ninstead of just sampling and losing out\nsome information with outliers and so on\nit's customizable so it allows you to\nkeep track of different things not only\nstatistical info but also\nsome more complex stuff that comes in\nsay when you're logging\nimages or text and so on and so forth so\nyou can customize that you can\nbring along also performance metrics\nwhich is not\nrelated to a table but it's related to\nhow your model is performing and you can\njust bring that along as well and the\nlast thing is it's mergeability so\nyou can\ncreate a snapshot today with your entire\ndata set but then tomorrow when you just\ningest the new or you append the new\ndata set there to your pipeline\nyou can basically merge that to your\nexisting profiles and this is done with\nsketching this is the approach that we\nused right and this opens way for you to\njust use distributed systems such as\nspark or ray or whatever you can create\nprofiles in the workers and then append\neverything back into the\nuh into the driver or you can even\nattach this to your streaming pipelines\nand just create micro batches there and\nthen just append those together and keep\nuh\nanalyzing and monitoring your data uh in\nthe end of the day right\nto create these snapshots can i share my\nscreen by the way martin\nyes absolutely\nokay um\nwhen you can see let me know\nyep\nyeah okay so to log your data sets uh we\ntry to do this as quickly as possible\nwith uh with our newest python api so\nbasically you have to\npass in an appendis data frame or other\nstructures here you know basically\ncalled y.log\nand\nthis will create the snapshot that i\nmentioned with predefined metrics it\nbrings in a bunch of\na bunch of information on distribution\nor frequent items and so on support\ndepending on your data types\nand it was a good idea for us to be able\nto just ingest and\njust pass along in flight tests so this\nwas the first integration point that we\nhad uh with lights i can actually show\nthe code here that does that\nbut basically we can create a profile\nview doing exactly what i showed you in\nour docs page let me just\nyeah i'm not sure if this is bigger or\nsmaller but\nuh it's good i'm wishing you guys can\nsee it and but basically you you just\ncan\ningest the data frame and then return a\ndata set profile view which is the\nobject that we we call and this ability\nis the first integration point so you\ncan just profile once and do whatever we\ncan do with while logs profiles\nafterwards\nsuch as building a constraints report so\nyou can run a bunch of different\nverifications to\nis your column greater than this number\nor is your mean between a certain range\nor do you have a certain uh new count\nor a percentage of nodes there anyways\num and then we have like\nbuilt here a renderer that allows you to\njust\nappend to your existing task a deck that\nis going to neatly represent if your\nconstraints passed or not so this is\nvery useful for users to just\nsee if it validated or not and then pass\nalong for example a boolean or whatever\nthey like here but also having this task\na presentation act sort of that can be\nanalyzed and visually inspected uh to\nfurther\nsee if the constraints passed or not and\nlastly there's also uh one thing that we\nbuilt which is the summer drift report\nuh renderer\nuh that can take in two data frames\nbasically one is the reference data\nframe that you say this is the ground\ntruth and this is the new data that i'm\ningesting so i want to compare both of\nthose those guys and you can also just\nappend this to an existing flight tag\nand this will\njust give you some information of how\nyour data is behaving like just uh just\nlike that right\nand here i just created a a workflow\nthat tries to mimic what would be a\ntypical workflow and i already ran this\nbecause we didn't want to uh spend so\nmuch time waiting on on\nuh on everything to be built and\nran here but basically what i did was\nget the target data create the profile\nview pass this the constraints report\nand then if the constraints passed we\ncan make the predictions or if it fails\nwe can just look at into the summary\ndrift report and this is basically it\nright we we can take different actions\nbased on\nuh different use cases but\njust to demo this what the the\nconstraints report look like we can\nclick here and watch our flight tag and\nwe see that everything's passed and if\nwe want to filter down to the failed\nconstraints\nor just just to fail we see that there's\nnothing here right\nuh but\nin this other example where i\nintentionally just created something\nthat is not passing we can also inspect\nhere okay so s3 mean uh between 1.3 and\n1.5 is failed so we know what we failed\nin the different checks that we uh we\nconfigured earlier earlier and also as i\nshowed you here in the graph when we\njust failed this report we're just going\nto build this summary drift report so we\ncan\ninspect also\nwhat does it look like what what does\nthe distribution look like is it because\nof uh drift and how our data is behaving\nand so on and so forth and we can just\nvisually inspect this there's also some\nother uh other metrics here that if i am\nsuccessful enough i can inspect yes\nyeah okay so this is not exactly\nrendering as a as i wanted because\nthere's a newer version that fixes this\nbug but anyways you get a picture this\nis just a very useful and powerful way\nof\nvisually inspecting what happens\nwith your data sets or your your ml\npredictions and take actions with that\nand this is very powerful this is going\nto enable users to\njust\nact more responsibly in their data\npipelines and this is why we integrated\nwhite logs with flights\nfantastic if there are any questions i'm\nhappy to answer as well great that's\nreally good so so congrats awesome\nintegration so what's the current status\nuh can user reckon users actually start\ntesting and grabbing uh the pieces so\ntell us a little bit about availability\nin the next steps what do you what else\nis\nwhat's what's it's already merged into\nthe main range of flight kits i'm not\nsure if it's released yet in the newest\nversion of flights but it's already\nmerged so users can just go ahead and\ngrab the\nlatest version of the repo and just\nstart building it you know\nit's under the plugins section there\nit's documented i hope it's well\ndocumented if it's not let me know\nwe can just iterate on that\ngood\nthey're fantastic i like the summary\ndrift report i feel like i assume you\ncan actually also configure your own\nmetrics in there and see the drift and\nsome of the metrics that you can figure\nout that's really super helpful because\nground truth is one thing but that's\nreally kind of like you know the\nsnapshot and then doing this iteratively\ndown the road when you get new data and\nsee what what has happened that i think\nis really a perfect place to show fly\ntext and the presentation layer and the\nrender that you have in flight x so i\nthink this is a fantastic example i love\nit\nyeah yeah thank you\nso much also can you just put the call\nto action\nin the chat just like it's pip\ninstall uh if i click plugins dash y\nlogs right there of course of course uh\nlet me just grab the our documentation\nbut also i can just um i i can just post\nhere in the chat the actual command for\nus to\nto use that\nyeah\ni think though i wanted to say that the\nwhite locks team is doing amazing stuff\njust uh really really cool things um and\nthank you for the integration um murillo\nis a rock star so thank you\nthank you"
    },
    {
        "title": "Moving to ARM on Flyte",
        "transcript": "i'm going to introduce robert everson so\nrobert is a reliable reliability\nsmartphone software engineer\nuh at lyft and his parents farm in\nnorthern uh california i think have\nawesome animals so i like to see the\nsodas robert so that's pretty pretty\ncool stuff so i wanted you to unmute\nyourself introduce yourself and uh you\nknow and tell us about\nwhen can we start using arm on flight\nand how far are you and give us a little\nupdate about the current status\nyeah all right uh robert everson uh i\nlike work at lyft i've worked there\ntwo and a half years now um working on\nairflow flight\nhive um\ntrino um druid and a lot of other data\ntools um trying to keep them reliable\nand running on kubernetes\num\nunfortunately i don't have a demo but um\nlately we've been working on uh moving\nall of our our workflows and our tasks\nto to run on arm instances to uh\nto save a cost and um to kind of move on\nto kind of the next generation uh\ncompute um\nand so we've added um the architecture\nflag into the flight into flight kit and\nall and flight plug-ins um to allow us\nto switch back and forth between\nvarious architecture types\nand this is helping us move kind of one\ntask at a time so we don't have to do\njust a giant big big bang upgrade make\nsure every every workflow can run on arm\nand then add arm nodes to the mix and\njust you know the luck of the draw\ndepends on which which node you get on\nso we can we can certainly target\nspecific workloads um to run on arm um\nyeah so it's been it's been going great\nuh we haven't open source it's the the\nwork yet uh it is it is up in the in the\nopen source repos um just on some some\nvarious lift specific branches um\nbut uh we will uh start working on that\nprobably um you know this month to to\nstart open sourcing that and getting it\nup so that others can use the\narchitecture changes and\nsave money and um you know use use the\nnew\nnew architecture flags\nfantastic any any questions from\nthe people sitting here for robert\ni think everybody's probably like myself\nexcited to to see when we can use arm\num katherine\ni can add one thing i think uh this is\njust amazing work robert and i know\ni i don't know if everybody started like\nthe cost benefit ratio of using alarm\nmachine versus the intel which is almost\nalmost one half\nor even lower\nin terms of savings and i'm talking\nabout just on demand and preserved and\nspot instances it's just amazing um\nthere are many many workloads that are\ngetting\nbecause of m1 and because of uh all of\nthe work with that amazon has been\npushing there's a lot of libraries\nwithin the data science ecosystem even\nin python that i get imported to arm\nand other languages actually directly\nsupport arms so i'm really excited to\nsee this because\nfrom our experience we know\nthe flight eventually ends up being one\nof the most expensive services within\nthe company that's because it just\nscales to that level and volume\nthis can really reduce the cost for many\nmany operations and you can choose it\nwith the work that robert's done you can\nchoose it depending on your workload you\ncan say like hey this is lower priority\nor you know i don't care about\nthe time but i care about the cost you\ncan just turn it on so fantastic thank\nyou robert"
    },
    {
        "title": "Flyte Community Update 019 - Aug 9 2022",
        "transcript": "we're good to go welcome\nto the flight community sink august 9\n2022 my name is martin stein\ni'm with union ai and i'm your host\ntoday a few housekeeping notes as always\nwe're going to record the session and\nwe're going to share it on social media\nlater\nand uh\nif you haven't joined\nobviously when you see this turtle video\nlater if you want to join i want to be\nin a live session go to flight.org\nand join the slack channel there's also\nat events so you can see the full\nprogram\nwe're going to talk about this at the\nend of the session one more time but\nnext to our agenda\nthanks\nso today we have a i would say a full\nsession\neasily one hour gonna have some\ncommunity updates i hope that's not\ngonna be longer than 10 minutes uh it\ncould be a little bit longer\nand then we go into demos and\npresentations\nmoving to arm on flight by robert\nhe's going to talk about that topic it\nshouldn't be too much too long of a\npresentation or talk i think it's\nprobably five seven eight minutes robert\nsomething like this is what i would\nexpect and then we go into model\nmonitoring with y-locks and flight\nmorillo so you're going to talk about\nthe latest integration super is excited\nto see this what ylaps has done and then\nwe're going to take sip recruiters\nflight path\nwith seth that's going to be very\ninteresting piece because we're going to\nlearn\nuh what it takes what decisions it takes\nand what pain it takes to get from one\ndecision to really discover about that\nwell we don't have a solution we need a\nsolution and flight could be one of\nthose solutions okay next\ncommunity updates let's kick it off uh\ntoday with our contributors let's go to\nthe next slide\nand i have two contributors that i want\nuh today to\nintroduce themselves uh first we start\nwith evan settler so evan if you could\nand meet yourself if you could introduce\nyourself and give the people a little\nbit of brief background and why\nyou know what your work is with regards\nto flight you contribution go ahead evan\nyeah hi everyone uh my name is evan i am\na data scientist at hbo max where i\nbuild you know predictive models\nand i'm helping modernize the ml\ninfrastructure um\nwe use uh spark a lot and one of the uh\none things we use are um flight or sorry\nwe use spark ml pipelines\nand so the contribution i made was to\nadd a custom flight type for spark\npipelines why is this useful it means\nthat when you run your spark job it\ncould be a pre-processing job or you\ncould be training a model then you can\nreturn it\nfrom the flight task\nand then pipe it into a prediction task\nor whatever you want so it really um\nmakes the uh yeah using spark ml really\na smooth process\nthat's fantastic love it i i'm a big fan\nof spaghetti mountains buggy male\npipelines\nso i look forward to testing this out\nfantastic\nand um\ncalvin you have um\nyou're with embark so same thing i mute\nyourself and introduce yourself\nand give people a little bit of a\nbackground of what you have been working\non i think it has been the paperwork\nplug-in but i will let you talk about it\nsure hey everyone this is calvin from\nembark we're a direct consumer dog dna\ntesting company that also does a lot of\nresearch on veterinary medicine um\nwe use flight for our we're shifting to\nflight for our production data\nprocessing pipelines that take that\nincoming\ndata from direct to consumer dna tests\nas well as for our research pipelines um\ni've been doing some work on the jupiter\nnotebook infrastructure\ni started with adding a flight deck to\nthe paper mill plugin so that when you\nhave a paper mill task that runs a\nnotebook you can see the notebook in the\ngui which we were kind of doing\nin a hacky way outside of that anyway\nwith a separate task for a deck but it\nmakes it really easy for scientists to\nlike paralyze their notebook with a\ndynamic and then come back and look at\nthe results in the gui without having to\nlike go to s3 and\nmuck about um yeah and from here we're\ngoing to hopefully be doing some more\nstuff with jupiter notebooks and science\nwork to kind of make the lifecycle\neasier\noh fantastic levitt\nuh so that's that's a pretty cool thing\ntoo and uh building the flight decks i\nwant to test this out\nsoon as well so with that i feel like\nthe call to action here is for for each\nand every contributor out there um join\nus and if you have a good contribution\nuh you know reach out to us or we will\nreach out to you and see it and i'll\nhave you at one of the next open source\nsix okay let's take a look at the next\nslides that's all about the talks um\nthere were quite a lot of talks in the\nlast four weeks i was a little bit\nsurprised that we had so many but i'm\nsuper happy about this so first of all\nwe had um kethen uh talk about uh with\nthe ml ops community\nwhy airflow is great but it's you need\nan extension on the aml side you need\nsomething that's built for ml\nand uh caitlyn broke down in this in\nthis conversation the key drivers for\nwhat it takes to have ml specific\norchestration tool why we need that\nso there's a link there you can take a\nlook at this very interesting\num\nour niels bentilan\nspoke with uh\nalexi from the data talks club i think\nthis was a fantastic recording as well\nit's about union ml\nand why\nyou need a lightweight api an\nabstraction layer on top to really help\ndata scientists machine learning\nengineers with their tasks about\nbuilding deploying and scaling their web\nnative email apps so far i thought this\nwas a really great\ncontribution so the link is here on\nyoutube\ncheck it out\nnext\nwe had neil's as well\nniels i just watched this twice this\nvideo i think it's really good\nfrom scipy\nand literally that part was a 30-minute\ntalk about you know you know production\nworkloads and machine learning very very\nimportant topic to bring this together\nvery often we just talk about one or the\nother but really the combined talk and\nthe criteria reliability being\nreproducible and recoverable and audible\nauditable it was a really important uh\ncriteria so i would highly recommend\nwatching this track 30 minutes really\nreally good track and last but not least\nsamita allah has\ndone a presentation for the kcd\nin gennai\nthat's also on youtube and i felt like\nthis was a really really good overview\nfrom you samita to\nbasically see\nwhat it takes to productionize models in\nthe kubernetes native environment so\nreally fantastic so those are really\nfour great resources highly recommend\nnext\nwe had one more thing that i'm super\nproud of that we did this this was a\ntwitter space conversation with kelsey\nhightower\nmlm production we had various flight\nusers\non the panel\nif you haven't seen it i would highly\nrecommend watching it uh and i think\neven kelsey had an epiphany about\ninfrastructure as one of the elements\nthat has to be managed in an ml\nproduction environment together with\ndata and mls not just data and mls also\nthe infrastructure and the right factor\nhere\nbut it's really great orchestrating tool\nwhich flight is cool so let's look\nforward uh upcoming talks\nkeith katherine is\ndoing a ask me anything um with the\nmlaps community on august 18th there's a\nlink on on the website if you want to\nchime in there's a live event so you can\nask casey any questions\nso\ngood luck catherine and the next\nupcoming thing is the race summit\nuh so\nwe will be we will represent flight uh\nat the race summit um\nin august uh 20 on august 23rd 24th in\nsan francisco uh so there's kiosk number\nseven uh you can uh see us there and\nthere's also going to be a little bit of\na you know obviously we wouldn't be\nthere without having a reintegration uh\nthere's been more to come about this\nwe're going to talk more about the ray\nintegration uh but the ray summit is\nbasically where we basically bring\nthings together and hopefully\nget some uptake on the ray integration\nso\ndistributed computing and orchestration\ntogether one power package super\nexciting"
    },
    {
        "title": "Flyte for Biotech",
        "transcript": "get to the flight for biotech specials\nso i want to introduce your\nniels bentilan niels is an ml engineer\nat union and i asked niels to give us a\nyou know eight minute ten minute\noverview about flight for biotech just\nframe the topic and after that uh intro\nwe'll move on and have the panel\nconversation so neil's the stage is\nyours\nwell thanks martin um hi everyone i'm\nniels ventiland\num yeah so\ni guess just to i just wanted to set the\nstage here um i used to be in biology i\ni was in an immunology lab as a research\ntech for\nfour or five years and before that i did\ndevelopmental biology\nbasically taking care of south american\nopossums\nand then mice after that\nso i have a little bit of experience\nin a lab doing wet lab research\n[Music]\nso uh the the key\nthing i i wanted to\njust lay out here um today and i think a\nlot of the panelists later will have a\nlot more to say about this um than\nmyself\ni guess before this talk i just did a\nlittle catching up in the field see\nwhat's the latest\num\nbut the main question i want to ask here\nis you know what are the main challenges\nin\nin computational biology and also\nbiotech more broadly in life sciences\nthat the next generation of tools need\nto address to make progress in this\nfield\nand um you know the tool like science\nand engineering have always been\ngone hand in hand right so we needed the\nmicroscope in order to make a lot of uh\ndiscoveries about cells and cell theory\nyou know things like that need a\ntelescope just to see out into space\nso and then in biology like fluorescence\nmicroscopy\nflow cytometry all these things to\nget at what the cell is doing um at a\nmore granular level\nso\nyou know now we have computers now we\nhave a lot of analytical tools so what\ndoes that look like and what what are\nwhat are the things that these tools\nneed to do for us to make progress\nso this is just a cartoon of\nbasically the scientific method but you\ncan look at this and kind of replace\ndifferent boxes for\nthings that are specific to biology and\nbiotech\nso you know you formulate a hypothesis\nyou do some experiment you collect data\nin vivo and vitro and silico\nand in my experience you know most labs\nwill\nhave a bunch of multimodal data like\nimages texts like unstructured\nsemi-structured data it's all sitting in\na computer somewhere and you know the\nresearchers analyze it and you know\nthere's\nit's all kind of local there's not much\nway of uh i mean i guess maybe platforms\nnow exist where you can do this but\nyou analyze the data and you falsify\nyour hypothesis you refine it and then\nthat turns eventually into theory and\napplication\num\noh wow there's an animation\ni didn't do that okay\nthat's just the flight logo for some\nreason\nso the main challenge is\nfrom my perspective and i think maybe\nthe panelists can revise this add stuff\nto this but\na lot of it has to do with you know\nscience\nneeds repeatability and experiments how\ndo we\novercome the reproducibility crisis\nthat's something that's been\nwell documented so far and\nyou know\nthe higher level you go into like\npsychology and social sciences this\nbecomes even worse\num\nyou know reusable components like how\nmuch effort is being duplicated across\nlabs and research teams and groups\num scalable analysis wet lab in silico\nexperiments generate a ton of data you\nknow where do you store it how do you\nprocess it\ngoing back into the reusability piece\nyou know collaboration between labs\nacross teams across organizations is a\nchallenge\ncomplexity management how do you connect\nand compose all the different pieces\ninto some\nscientific workflow\nthen data governance who owns the data\nwho's responsible for maintaining it\nmaintaining quality and how's it\ntransported across organizations\num so in the next slide i just hint at\nyou know various things that we know\nflight does that might help in certain\nof these elements i'm not going to read\nthrough all of these but you know some\nof them including containerization\nversioning of\ncode and data artifacts\nthe fact that you can collaborate across\nteams with these you know isolated\nmodular packages\nobviously flight kit itself is an sdk\nfor\nexpressing complex workflows\nand then data governance which is i\nthink\nthat's a piece that i'm very much\ninterested in but you know flight has\ngranular data access permissions and\ncertain victim\ndata quality and documentation features\nbut um that's that's where i'll end\num and hopefully we'll have a broader\nconversation around this\nawesome fantastic thank you niels so you\nknow i was thinking um the topic is\npretty much about data and analysis\nworkflow is reading through you\nand listening to what you have said\nabout you know reusability and\nreproducibility scalable workflows data\ngovernance i think we have a whole range\nof topics today to discuss with that we\nhave quite a few speakers today as well\nso we have of course uh you know the\nteam from union caithan uh\nwho is the co-founder and ceo of union\nniels who just\nheard meals about\nhis background in biotech i also would\nlike to welcome chief freedom kenny from\nlatchbio calvin from embark krishna\ninfini infinome i always stumble over\nthis krishna and uh jason from cymotion\nand again congrats uh jason to the\nrecent corporate events there so with\nthat um i would like to introduce the\nfirst speaker krishna krishna if you\ncould unmute yourself and introduce\nyourself and give us a give people brief\nbackground why are you here today\nyeah absolutely thanks again for uh\nhosting this meeting um so my name is\nkrishna\nyaram city i am a data scientist at a\nvery small startup called infinum bio\nwe're located in boulder\ncolorado and burlingame california so\nwhat we do is we're trying to speed up\nthe traditional synthetic biology\nprocess\nif you will so\npeople if you're familiar with the\ndbtl cycle design will test learn cycle\nso we're trying to speed that up be more\nagile in some ways\nand actually unlock some of the\npotential problems that we have right\nnow in terms of you know different\nproducts that could be made using let's\nsay e coli or east\nso that's the general motor of the\ncompany so myself i'm i'm a data\nscientist i'm not a software engineer i\nwent to school for chemical engineering\nfor\nof all of all things and then i worked\nin biotech for close to 10 years now as\na data scientist\nso it's been so i've been working with\nflight a little bit for the past four to\nmaybe eight weeks\ni don't remember exactly and it's been\nsuper super helpful for us because we\nare just two person team if you consider\nthe cto also\nas uh as a big scientist software\nengineer so yeah so flight and\nkubernetes have been super helpful for\nus to get our ngs workflows uh up and\nrunning on the cloud on aws\npredominantly\nso yes that that's been my\nexperience is that is there more you\nwanna yeah absolutely i think it's\nfantastic i mean also i think it's\nreally amazing\nto have data scientists not just infra\npeople but also data science people here\nin the panel today so from from what\nneil said earlier about\nreusable code and i think most\nimportantly data governance as well\nwhich one of those topics that you heard\nearlier are really on top of your list\nyeah i think reproducibility for us more\nthan anything else um data governance\nprobably if we are pharma company right\nlike if we do a lot of\npatient data\nhandling for us data governance is i\nmean i wouldn't say it's not an issue\nbut reproducibility is probably a bigger\nissue for us because we have so many\ndifferent tools that we use right and\nall each of those has different versions\nand we need to make sure that everything\nis being\nused correctly and the results that we\nhave let's say if we go back to those\nresults a month later or maybe a year\nlater we know exactly what tools were\nused to generate those results and what\ncode was used obviously to generate\nthose so that's a higher priority more\nthan anything else\nthat's interesting great thank you so\nmuch krishna so niels we heard you\nearlier but i think in this segment um\nplease introduce yourself real quick as\nwell shortly and then you know\ni would love to hear from you a little\nbit about the specifics of flight\nspecifically when it comes to\nreproducibility uh just like what we\nheard from krishna about what you have\ndiscovered and why um flight is a a\nsolution for that problem\nyeah um so yeah we introduced myself i'm\nmeals um\ni have a background in public health as\nwell in biology so i've sort of um\nhave different like layers of the stack\nof complexity of uh you know\ngoing from like if you think of physics\nas a lowest level and then all the way\nup to\nsociety economics things like that\num yeah it's it's a it's a\nreproducibility is a big problem um\nso\nyou know especially in things like\nin public health but even here where\nit's you're working with data and it's a\nlittle\nthe the the problem with working with\nhumans is always\nby observing them you change their\nbehavior so it's like adds more\ncomplexity there but even when you're\nstudying cells and and those lower level\nsystems\nlike\ni think one of the big things that that\nflight offers is\nagain this\nbasically decomposing\nall the dependencies\nresources compute at the most granular\nlevel\nand so you're never gonna\nfoot gun yourself into making\nassumptions that you know about your\nentire pipeline and then one piece\nchanges\nhere at the end or the beginning and\nthen\nsomehow your entire thing breaks\num\nso that's one thing that i've come to\nreally appreciate about flight\nis that you can dockerize at the node\nlevel\nyou know you isolate your dependencies\nyou can declaratively\nsay in the sdk what compute you need\nwhat resources you need um\nand i guess the one thing i'm super\nexcited about is thinking about how\nyou can do that with data as well\num\nso\ni don't want to ramble on but that's\nmy phone\nno i think that's really great insights\nand we're going to get back to that in a\nlittle bit about the data analysis uh\npart of of using flight for that but uh\nnext up is chief chief please unmute\nyourself and introduce yourself and tell\nus a little bit about how did you uh\ncome to discover flight and about\ncompany about yourself\nyeah absolutely thanks uh martin for\ninviting me um happy to be here talk\nabout uh freedom and flight so i work\nmy name is jeeve i work at a company\ncalled freenom we are a very\ncross-functional team\nthat's essentially building\nnext-generation diagnostics for early\ncancer detection\nand detecting cancer early is important\nbecause like that's when it can really\nhave an impact like we have better\noptions for treatment at that point\num and as as a result uh better outcomes\nfor patients as well\nbut um\nearly cancer detection is a hard problem\nbecause you know at that early stage\ndetecting cancer signals really really\ndifficult\nthere's very little cancer signal\nrespective to like normal signal and so\nwe've built a multi-omics platform that\nassays a lot of signals including like\nhumor non-tumor and different\nmolecular uh components like proteins\nand dnase hence the multi and\nmulti-omics to try and get a better\nsense of the cancer um to\nyou know possibly even before like a\ntumor can develop\nand then we do this all based on like a\nsimple\nblood draw so you know uh basically walk\ninto your doctor's office as part of\nlike an annual physical you get your\nblood drawn like you know to test your\ncholesterol or whatnot and uh a similar\nblood draw can you know be administered\num our test can be administered with a\nsimilar blood draw so we want to make it\nas accessible so that the test can be\neffective right a good test is has to be\nboth like good in terms of its\nperformance and like how it's actually\ndetecting stuff and also accessible to\npatients so within freenom i work in the\nresearch platform team this is a large\npart of the innovation engine at freedom\num and we build we operate and we\nsupport the different components like\nthe compute platform data platform and\nautomation platforms\nand our role is to essentially like\nempower our users and our users span\nlike you know multiple teams across\nfreedom including like molecular\nresearch\nyou know computational biology machine\nlearning\nand so on so forth and we want to\nempower them to like basically build uh\nbetter products faster\num and\ni guess we're here today because you\nknow flight is a very large part of\nour research platform it's a very key\ncomponent of it and that's just like one\nof the places in freedom that we're\nusing flight there are also other places\nincluding things like our etl\nas well as our product production\npipelines\ngreat so so i heard you say in the past\nuse the term production great compute\nwhat do you mean by production great\ncompute\nyeah i mean so so there there's so i\nbelieve that like you know there are\ncertain things that\na compute platform should provide out of\nthe box\num that users shouldn't have to worry\nabout right so like for from a user\nperspective it should feel\nvery simple intuitive to use so like\nthey just define\nyou know their intent or their workflow\nin a very simple language like python\nright something that flight kit provides\nand then they ship this workload off to\na system that can be\ntrusted to get this workload done and\ncomplete in some reasonable amount of\ntime in a very resilient and robust way\nand then give results back to the users\nright and at the same time\nyou know if they wanted to ever go back\nand review that work and see what\nhappened look at logs you know\nunderstand like how you know uh the cpu\nand memory usages\nuh\nuh you know were you know with their\nworkflows and stuff like they should be\nable to do that fairly quickly and if\nyou know a year down the line a two\nyears down the line they wanted to just\nyou know go back and rerun that\nexperiment to see if like\num there was any discrepancy\num they should be able to just do that\nvery easily so these are all the things\nthat i think uh uh a strong production\ngateway platform should provide out of\nthe box and obviously you know like i'm\nnot i'm glazing over like the obvious\nthings like scalability and all that\nstuff right um that is just like i think\nkind of assume that uh that we need at\nthis point for a computer platform\nfantastic so wonderful so next is kenny\nkenny you're with uh ledge bio so please\nsubmit yourself and introduce yourself\nand tell us a little bit about the\nchallenges that ledge bio is solving\nyeah absolutely um thanks for having me\nguys so i'm cto latch bio we are a sas\nplatform\nwe service many biotechs in all\ndifferent sizes working with all sorts\nof modalities of biology\nand we we kind of\nwe are the stop gap between where data\nis collected from the machine and where\nit's digested into human interpretable\ninsights by the biologists\nand so a unique challenge we face is\nlike we're rolling out software to many\ntypes of companies\nand so\nperformance and scale is much less of an\nissue for the stage of companies that we\nwork with rather than reproducibility\nand the ability to deploy things on like\nheterogeneous compute environments with\ndependency management and so forth um\nyou know concretely there's just some\nmyriad of tools you'll kind of allude to\nthis point that when you the\nbioinformatics tool is derived from a\nresearch process that is the person\ndoing it isn't really trained in\nsoftware engineering best practices\nand so um\ntool itself is just going to be shadowly\nmaintained on github and harder to get\nup and running on a box on some cloud\ncompute\nso\nthe process of like getting that\ncontainerized and rolled out onto a\nkubernetes deployment and then able\nbeing able to\nclick one button and have that go up\nagain\nis invaluable and then that name um so\nit makes this concrete like we work with\na lot of companies doing crispr\ndiscovery\nand so very useful workflow there is is\ncoming up with like a good guide\nrna and for your crystal therapy to be\nable to go after a gene of interest that\noften it's a 7d type application you're\nscanning against the genome\nwith the same sort of\nfunction same sort of algorithm you need\na gpu to be performing so to get that\nsoftware up and running on a box with\nnvidia drivers um\nit takes a while it's hard\nand so\nflight lets us just roll out that docker\ncontainer to that instance on kubernetes\nand it works like a charm so we we've\nused flight as our core workflow\norchestration engine i've been working\nwith the union ai team for almost a year\nnow\nand um\nyeah it's kind of just uh i know i was\nkind of winded longer than scattered but\nhappy to dive into anything in\nparticular\nno absolutely great overview maybe uh\none follow-up question what kind of like\nworkflows or loads uh\nyou know do you put in the system i\nassume this you know you're in the early\nstages of your company as well but you\nalso look forward and actually want to\nyou know have uh some scalability in the\nsystem itself\nyeah i mean so like like i mentioned\nearlier the vast majority of biotechs\nare not really constrained by how fast\nyou can process data um because you know\ntheir experiment around times are far\nlarger the the minimum between the two\nand the maximum between the two is\nalways going to be the time in the lab\nhowever we do work\nthe largest loads come from genetic\nscreens at the closest to clinical level\nso companies doing uh whole genome\npanels against like multiple different\ncandidates and and\nthat sort of set up you're gonna have a\nthousand to ten thousand\nwhole genome assemblies so each one of\nthose\nassemblies is gonna be a 100 gigabyte\nfile and then the actual operation is\ngoing to require a 96 core machine for\neach one of those files so it's a lot of\ndata moving a lot of computing spun up\num and so we just leverage a node group\non our kubernetes deployment on which\nflight resides that has you know 50 odd\n96 core machines and rely on the flight\nsystem to schedule them on that as\ndatabases and reschedule if the spot\ninstance gets preempted\nwell impressive thanks kenny next up is\njason from simon merchant uh jason so\nplease\nintroduce yourself and tell us a little\nbit about your role and sign merch\nhey martin thank you um so yeah i'm\njason i work at zymogen we focus really\non\nstrain engineering and strain\nimprovement in order to develop new\nproducts\nuh and biology is hard so we try and do\nthis through high throughput methods you\nknow around a lot of experiments\nfigure out what the biology is doing\nthrough that\nand my role at zymogen is i'm a\nsolutions engineer so that kind of means\ni wear a lot of different hats depending\non what's going on from more traditional\nsoftware engineering to product\nmanagement to support to tech training\nall of the above\nand one of the things i was involved in\nwith this role is understanding our\nlong-running jobs and like async\nexecution architecture at zymogen uh\nzymogen went through a period of really\nrapid growth so which like doubled in\nsize from like 400 to 1000 employees\nlike over a year\nand kind of through that rapid expansion\na lot of different uh of our software\ndevelopment teams kind of siloed into\ndifferent platforms for doing async\nexecution and software development\nafter a while this became a little\nunwieldy um\nyou know things became sort of less\nreusable um you know less scalable\nharder to maintain and so my job is\nfiguring out well\ndo we really need all this stuff and you\nknow well if not what do we do about it\num the result of kind of that effort was\nno we don't need all these different\nplatforms they're all kind of doing the\nsame thing\nand they all sort of had a core problem\nin that\nwe couldn't you had to deploy each\nsystem uh with all the workflows uh at\nonce and we're really trying to move\ntowards sort of continuous deployment uh\nand so that sort of made that a big\nchallenge\nand so when we get to the well what do\nwe do about it we sort of surveyed the\nlandscape uh and bound union in flight\nuh and really liked that each workflow\nwas sort of the unit that could be\nversioned deployed independently you\nknow if something breaks we need to fix\nit it's just that workflow we need to\nworry about\nand it's also really easy to work with\nso we've got a lot of\ndifferent kinds of developers you know\ntraditional software engineers to\nscientists that have become software\nengineers to data scientists a\nbioinformaticist\nscientist with a little sprinkling of\npython here that you know want to blow\nup the cpu for a while um and so we\nwanted something that we could make\nthat's really easy for them to use and\nsort of have all these different user\ngroups working within the same system so\nwe could get that sort of scalability\nand reusability um and so far flight's\nbeen been doing a great job at that\nfantastic thank you so much\nso caithin you're next so i'm sure the\ncrowd here knows caithness caithness ceo\nof union\nand uh i think you wrote the first first\nline of codes for flight back at lyft so\nplease introduce yourself and tell us a\nlittle bit about flight the genesis of\nflight and now the adoption the rapid\nadoption in in biotech why do you think\nthat is such a standout uh field\nand segment for\nuh firstly you know thank you i'm\nnot even close to all of you guys like\nwhat you guys have achieved in biotech\ni'm basically your software that's what\ni've done uh\nbut\nit's it's been amazing to hear what all\nof you guys are doing and it's like you\nknow it's almost humbling\nuh\ni'll tell you maybe a short story\nlike two lines uh\nwhen we\nwhen i first started flight\nwhy was solving my own problems really\nright and and the problems were\ni i was actually\nhandling major duty for a\nsoftware a model that a data scientist\nwrote that i had no clue about\nand i was like okay\ni want to change certain parts of the\nmodel how do i do that\nthat was one second part of the story\nwas\nthere was a data scientist who left my\nteam and i lost the model on his laptop\nand i spent three months trying to build\na model\num\nand i had to go to the ceo of lifting\nand constantly explain oh i'm working\ngive me time give me time i'm just\ntrying to rebuild like that and what we\nended up doing which is rebuilding the\nexact same model for three months after\nspending a lot of money\nand then the\nthird one was um\nthis constant uh\nlike you know none of the tools that\nexisted really\nwe actually had this fantastic model\nwhere we wanted to use a million\nparameters or something right for\ntrading some idea\nand we when the data scientists came up\nwith the idea we sat down and we're like\nhuh it's going to take three months for\nus to build that infrastructure can you\nwait we live in provisional machines we\nhave to get spark clusters and you need\nsomething other some other stuff i have\nto write that code and like\nand what ended up happening after all\nthese three is that you're like why are\nwe doing this consistently like i spent\na year of my life doing nothing but like\nyou know kind of throw away pieces of\nwork and like uh\nand really reduce the velocity of this\ncompany\num\nso that was one\n[Music]\nsecondly by career i've worked across a\nlot of different industries and i ended\nup doing similar stuff\neven though like in mapping trying to\nbuild a map you know like running on\nthis is 2012\ni brought down s3\nand one of the reasons and that's how\nyou learn more about the system right\nwhen you bring things down you're like\noh yeah there's a bunch of problems with\ns3 and the reason why we brought it down\nbecause we hammered it so fast\nand i found a\nfun stuff in spot\nwhere like\nif you kind of pin yourself to be eighty\npercent of an on demand cost\nso you outbid everybody else from the\nmarket this is 2012 now they don't let\nyou do this and so i just said okay i'm\ngoing to play at 80 percent i like show\n20 discount and i'm gonna spawn off 10\n000 machines and i'm going to create the\nworld in like 30 minutes and we wrote a\nbunch of software that orchestrated all\nthis stuff\nuh and this the point of the story is\nthat we were just doing like many many\ntimes i was just doing the same thing\ninstead of solving my own problem\nand this\nit was the core genesis of why\ni didn't want to do infrastructure i had\nbuilt cloud software and so on i was\nwhen i joined lift i said i'm not going\nto do it i just want to do this like you\nknow\nmapping and ml stuff but i\nthis problem kind of worked me again and\nwe're like okay let's try and see how we\ncan solve it\nnot going to be open source just for\nlift\nand and and i will be completely i had\nno idea about biotech i've never worked\nwith a bio\nbiotech person had never actually\nmet a biotech engineer\nand it's amazing to see that it's kind\nof helping solve these problems and it's\nactually the right use of a tool like\nthis like a tool\nor an infrastructure piece like this has\nto\nhelp\nnot just make better recommendation\nengines and advertising engines it needs\nto solve real world machine learning\nproblems that's what i think we are here\nfor and i'm like really happy to be part\nof\num\nand\ni heard a bunch of different folks they\ntalk about reproducibility decomposition\ndeclarativeness uh cindy and and it's\namazing we were building we talked about\nall these we actually made a bunch of\ntrade-offs we said\ni like to use kubernetes this is this\nwas a spectrum that was happening it was\nlike kubernetes was just coming on 27\n2018 like you know think about the\ncoconut it was really early then\nlots of\nuh\nskeletons in their closet over there and\nwe we were working with it but we saw\nthe potential you could actually\ndeclaratively create infrastructure in\nconnect pieces\num\nand so we made some choices that meant\nlack of some flexibility right we\ncouldn't do and then we kind of\nengineered our way out of it by creating\ndynamic workflows and so many of the\nthings that you see today are basically\nthings that we were we wrote in one\ndocument of 15 pages but were not able\nto solve and solve it over a period of\ntime\num and so\ni think it's great to see that they\napply to biotech but i i have now talked\nand i've been\nin an interesting position that i've\ntalked with folks in different\norganizations and they have all had the\nsame\nsort of issues and this is not just\nbiotech it's physics biotech machine\nlearning uh all kinds of different\nfields that kind of come together\nto solve a big problem like whenever\nthere are complex problems to be solved\nyou need these other use cases or these\nsort of\nuh requirement from the system so thank\nyou for you know having me hopefully\nthat helps\nabsolutely so i think what we hear here\ntoday is like\nbiotech is actually not just like one or\ntwo requirements there's a complete set\nof requirements\nand uh that usually warrants a platform\napproach to answer because otherwise you\nend up with point solutions point\nsolutions and so so my question for for\nthe panel right now is how long did it\ntake you to discover that the point\nsolutions themselves are probably not\ngood enough you need to have an\nunderlying fabric a platform to really\nmove forward in your journey of\nproviding biotech\nservices in your organization so this is\nan open questions feel free to jump in\ni can go ahead um we my company embark\njust recently went through the process\nof shifting a lot of our production data\nprocessing over to flight\nand there we had a homegrown set of\ntools for our or sort of like\nwe do direct consumer testing so there's\nlike the the customer facing pieces of\nour compu computation that gets quite\nlarge with thousands of samples coming\nthrough today but there's another piece\nthat's like research and we found that\nwe were we were both\nwe had our homegrown tooling that we\nwere spending a lot of effort on for our\nsort of customer facing tools but then\nwe kept rebuilding things for research\nand that was that was one of the things\nthat really made us realize like oh wow\nthis is both painful because we want to\ntranslate stuff it's a whole big ordeal\nbut also we're spending a lot of\nresearcher time and folks who have\nhybrid skill sets they're not just\nthey're people who have like they went\nto school to do biology and they're\ntired of working on infrastructure who\nare struggling through bash scripts to\nlike we've had bash scripts that like\nspin up ec2 instances and do like try to\ndo the the clustered computer for\nsomeone's research work so for us that\nwas a big part of it was seeing that get\nlike duplicated separately from\nproduction in what people were trying to\nbuild for research and realizing like\nthis doesn't make sense that's going to\nharm\nthat translational aspect of the\nresearch\nfantastic and so calvin uh\nuh you know\nwe didn't introduce you but i wanted to\nget to you at the very last because i\nthink embark has actually a couple of uh\nvery interesting stories to tell you and\ni shared a few things beforehand but\ntell us a little bit about embark and\nand you know how embark discover this\nspecifically with those you know fast\nyou know ranging uh changing\nrequirements when it comes to you know\nhaving a platform in your organization\nand and it's it's not a small\norganization i assume it's embark has\nbeen around for a little bit right so\ntell us a little bit about embark about\nyour role and and the needs for a\nplatform orchestrator\nsure um\nso embark's been around for about five\nyears and we've grown pretty quickly in\nthat time we do direct consumer pet dna\ntesting so it's a little different it's\nlike 23andme in the concept but we focus\non on pets\nwe're also doing other things now\nbesides just dna testing we're doing\nmethylation testing and some of the\nthings\num\nso we we originally like i said had a\nhomegrown platform and around\nuh well since the company started we've\nbeen talking a lot about like is that\nthe right solution for this we looked at\ntools that existed like um like airflow\nand some of the sort of like older\norchestration tools and we're never\nhappy with um\nin particular like managing complex\nscience pipelines that stretch across\nteams like here's the team that\nprocesses the raw data and extracts some\nmore featurized version of it and then\nhere's the team that does the\nlike machine learning piece off of that\nhere's the team that predicts the breed\nof the dog or the ancestry of the person\nhere's the team that finds relatives\nlike each of those were very different\nscientific skill sets such that you\ncouldn't collapse all those into a team\nbut managing these pipelines with the\nexisting tooling was was very hard you\nreally need like contracts between teams\nat that point that stretch beyond just\nlike a json spec right like i've done\nthis type of thing for e-commerce and\nthere it's like okay we have a json\nobject and we align on that and here it\nwas like okay we have like vcf files and\nthese big binary files and people need\nto understand the contents and what they\nmean in a much richer way\nso we were doing a whole bunch of\nexploration about six months ago and we\nwere exploring\nflight we were exploring next floor\nspring bunch of these tools\nand what really brought us over here was\nboth the\nrobustness of the like reference task\nstructure where you can have multiple\nteams\ndeploying separately and then composing\nthose things but also the ability to\nto build that contract and build\nintegration tests that stretch between\nteams and having robust like typed\ninterfaces between those teams\ni think that was part of what really got\nus interested and got us excited\nand then we had all those science\nproblems we also wanted to fit into the\nsame framework and it it was one of the\nthings that checked both our production\nboxes and our research boxes nicely\ni think that's it that's a fantastic\nview to have research and production\nbecause i think in biotech we we have to\ncombine you know\nyou know very different teams and skills\nresearch and data science and is one\narea and then the production engineering\npart is a completely different one so\nthat's also an open question further for\nthe panel on on getting teams together\nif you look backwards through experience\nwhat were kind of like the lessons how\ndid you get your data science team onto\na platform like flight or if you come\nfrom from the infrasight uh how did you\nactually you know get your infra team\nonto flight i mean and then you have to\nactually make sure that all of those\nteams together uh embrace the same\nplatform so so looking backwards what\nwas the experience collaboratively\ngetting everybody on the same page\ni can talk a little bit about that\num so at freenom we have this mantra\nthat like you know we don't want to have\na lot of tools in-house like we want to\nhave\na few things that work really well that\nwe really understand that we can you\nknow we're very confident about\noperating keeping up debugging fixing\nand so on so forth and also like you\nknow in the case of flight like uh we've\nwe've built a pretty good relationship\nwith the team and like we feel very\nconfident like taking this supporting\nthis tool like in our production stuff\ni think the the things to take the\nthings to note uh because of this is\nthat like\num we need a very flexible tool to like\nsupport both our research use cases and\nour production use cases and that's\nessentially like one of the main things\nthat we get from flight and that allows\nus to like basically you know go from\nresearch research into production like\nfairly quickly otherwise you're ending\nup like doing a lot of like rewriting or\nlike modifying you know large pieces of\nyour code base as you're transitioning\nand promoting it from research into\nproduction so all those reasons aside\nlike i think that was\nuh that was like one of the like very\nimportant pieces um for us to uh very\nimportant like reasons why we went with\nflight i think like getting people\nonboarded so we started i think like\naround um\n20 20 or so\num you know after like flight had been\nopen source we were very early in our\nadoption\num and like one of the first things that\nwe did was basically like setting up\nessentially like a sandbox internally so\nthat users can start playing around with\nthe sdk and like really you know have a\nvery like isolated playground in which\nlike they can just test things out um do\nregression testing even right like check\nout specific features and so on so forth\nso we spun that up actually fairly\nquickly\num and since then um we've had like our\nengineers were like you know very like\none of the first ones to like start\nadopting like we were at the point\nstarting to write our production\npipelines and stuff um or moving our\nproduction pipelines to flight really\nand so like that that happened fairly\nquickly the research side moved a little\nbit slower um i think researchers are\ngenerally\num\nless more resistant to change like\nbecause you know they don't want to\nfocus on like\nlike you know the ux and like all that\nstuff they just want to like do their\nresearch in like an environment that is\nlike most comfortable and familiar to\nthem and so you know that took a little\nbit more work but so the way that we\nstarted there was to start onboarding\nlike like specific teams and like\nworking very closely with them to like\nyou know see them and to really empower\nthem to be successful on this platform\nand then as we started seeing like more\nsuccess and people started getting\nexcited about it like there's a lot of\nword of mouth going around like within\nthe research organization and then other\nteams started picking that up and then\nwe started supporting those teams as\nwell so it was it was uh it was a little\nbit more a little bit slower i'll be a\nlittle bit more like you know it's like\nvery mindful intentional on the research\nside on the engineering side it was more\nof a no-brainer because of like all the\ngreat features that like life provides\nus\nfantastic thanks chief so chris now\nyou're you're on there so i actually\nwant to jump in there really quick\nbecause\npart of the premise of what we're doing\nis like recognizing the like from a\nsocial engineering perspective the way\nthese companies work is you have folks\nwith different levels of experience with\ncomputation and\ncomputers and programming um like as\njeep described i think freedom's unique\nbecause they have such a high density of\nengineers and that's really an outlier\nin biotech companies at large\nthere is this big chasm that exists\nbetween the people who\nwork at the bench should do the\npipetting are unable to like even run a\npython script\nand so one thing that we've done really\nwith with flight is we've built a\ncompiler to generate react code from the\nthe flight interface so taking the types\ninterface and the the flight ideal\nsystem and we generate front-end\ninterfaces for it so the web lab\nscientists can self-serve\nthat is and then shipping a thin wrapper\nover the flight sdk\nallows people in on the bioinformatics\nside to write their own pipelines but\nthen expose them directly to that\nresearch team i'm actually incredibly\ncurious how freedom was able to\nexpose the python directly to a lot of\npeople who don't have fluency in python\num or maybe\nit was exposing the phi console directly\nto the biologists but this is a huge\nproblem in the space and as you said i\nthink that's spot on there's a huge\ninertia against adopting new tools\nbecause biology by nature is\nconservative because it reproducibility\ncrisis at the bench\nso the people have um you know\nthey're not exactly super eager to pick\nup new computational tools\nchief do you want to respond yeah i'll\nreform briefly and then we can also talk\noffline kenny i think i would say like i\ni do agree with your point i think like\njust exposing those interfaces\nhas been really key so we do have teams\nthat are responsible for building those\npipelines and like essentially providing\nyou know like a configuration layer for\nour molecular researchers to like kick\noff these pipelines and um you know it's\nnot like uh\nthere's not like a react component to it\nyet um but but we do have like you know\nuis that users can basically log in to\nand like fill out certain configurations\nor like modify the configurations\nuh for the pipelines that they want to\nrun and then just kick off runs that way\nso it is very similar to like what you\nwere basically suggesting\nlet's see some of that open store\nsometime\nso i would love to to get um krishna\nback in here a little bit and because\nchris are you coming from the other side\nfrom the data science side so tell us a\nlittle bit about youtube or actually you\nknow getting onto board with the system\nnow you know was was it data science\nmoving and research moving slower in\nyour company as well or were you\nactually kind of like an outlier moving\nfaster than\nwhat we hear\nyeah so i think\num\na couple of things i think with respect\nto that so we like i said we are a small\nteam but we have tons of projects\nsynthetic biology projects going on at\nthe same time and one of the things is\nwe want to self-serve like anyone's\nsaying you know directly to the\nbench scientists and we don't want us to\nbe the bottom like data science slash\nsoftware engineering to be the bottling\nbetween\nyou know those iterative loops of dbtl\nright so learn is a key step in that\nwhole loop design build test learn and\nwe don't want to be\nuh the slowest step learn i mean we have\ntools now uh we can talk about that in a\ndifferent venue where we've actually\nsped up the entire biology process much\nmuch faster so to a point actually where\ncrossing data is the bottlenecker right\nnow at some level\nso we wanted to make that bottleneck go\naway\nso having some kind of self-serving\ntools like can you mention you know\ndirectly to the bench scientists\nexposing those interfaces and making\nsure that everything in the back end is\nreproducible and be able to scale to the\nnumber of samples is really crucial so\nthat that that is why i think you know\npreviously in previous companies that\nwas not a problem you know data\nscientists could take their time turning\nthrough the data or by information\nwhoever could take the attention into\nthe data because\nwet lab was the slowest step\nright so here in our company the\nparadigm is a little shifted so\nthat's what prompted us to go beyond the\ntraditional bioinformatics tools where\nwe were\neach building our own r scripts or\npython scripts and bash scripts or\nwhatever and trying to do the you know\nanalysis individually\neach data scientist has its own you know\nenvironment slash preferred tools we\nwant to take that away and make\neverything unified so that we can go\nfaster that was motivation\ninteresting so i want to get over to to\nneil's uh niels in your intro earlier\nyou talked a little bit about uh you\nknow the conditions uh that are so\nspecific and special in biotech now\nlistening to that conversation here\nright now and then actually you know\nmake it easier for the researchers to\nactually do their work and so on so so\nwhat was your experience so far engaging\nwith uh you know biotech companies using\nflight\noh sorry right\nthe question is what listening to the\ncollaborative aspect and also to\nunderstanding\nhow to get the researchers on on board\nand make work simpler easier for them\nand getting to results as an\norganization faster so you're speaking\nto a lot of flight users out there so i\njust wanted to ask you if there's\nanything that that actually stands out\nwhere you can see a pattern there we say\nlook i hear this all the time but this\none is a very very very specific one\nthat you know that is worth mentioning\nin this context\nyeah you know the interesting thing is\nthat\ni don't i feel like there's nothing\nreally\nthe things that i hear are\nsuch an app like an abstract\nmore computing level that kind of\napplies to all fields to be honest so\nlike biotech will have its own\nspecific manifestations of these\nproblems\nbut\nyou know a lot of it has to it comes\ndown to you have domain experts\nthey don't they they're not software\nengineers they're not trained in\nthey don't they haven't like\ninternalized concepts that we all take\nfor granted like versioning data like we\nhad image files that were you know\nnamed by date something and then final\nand then underscore final and then\nunderscore and then another date you\nknow so\nit's like the way\nthe way scientists do their work is very\ndifferent and\num\nyou know one thought i had here is\ni'm like curious to see what\nthe panel has seen in their own teams of\nkind of like the hybrid people who are\nlike the translators between different\ndisciplines\nand are able to\nsort of navigate the two different\ntypes of stakeholders right like\nscientists versus software versus infra\nand i feel like a lot of the stuff that\ncan't be automated\nare organizational change adopting new\ntools like all these things that's\nthat's like a piece that i'm very\ninterested in learning more about\nbecause as someone who\nhelps people debug their their flight\nworkflows and whatnot that that's not\nsomething i i hear often but\ni have a sense that that is a problem\nlike based on this conversation as well\num\nand you know very much much interested\nin you know kenny's thoughts here as\nwell on you know what do low code no\ncode interfaces\num and he's ended up that already with\nlike the strongly typed\nworkflows that you you know transfile\nover to different uh components using\nreact or whatever other web framework\nwill emerge\nbut you know i'm curious to see what\nthat\nthere's a both a technical component of\ntranslating these things to more\naccessible interfaces\nbut then there's like the softer human\ncomponent of\nhow do you actually get\nyou know\ni used to work at memorial school in\nkettering kansas center in new york\nlike we're very well established cancer\ninstitute\nlike how do you get the scientists and\nthose who adopt new tools like this\nokay is there any\nanyone from the panel that wants to\nrespond\nyeah just briefly i think you know it is\na huge problem the\nlifeblood of biotech is academia so to\nget the old guard to adopt tools that\nhas to be published in a paper\nthe best way to do that is to get in\ntouch with people doing cutting-edge\nbioinformatics and get them to adopt the\nframework that you were using\nand not and you have to do that\norganically so you have to build\nsomething that's incredibly useful and\nintuitive\ni think that's a multi-year process and\nit involves the same sort of user\ntesting and on the boots interaction\nwith these people that you would have in\nany sort of\nother field\ni i those principles haven't really been\napplied to biotech at large which is why\nwe haven't seen something like this\nthere's been previous attempts tools\nlike galaxy that have really floundered\nbut it just succinctly you know\nacademic biotech has to adopt these\nthings and to do that they have to be\nconfident enough to publish them\nand that's how you get these these older\nfolks that use it\nfantastic so we have about seven minutes\nleft i would like to ask one question uh\nto the entire panel here\num now you have you know hindsight is 20\n20 you have made your experiences for\nfolks who are just entering this uh\nyou know the problem space and\ndiscovering you know taking the first\nsteps\nwhat is the advice maybe the one one\nline advice you you want to give\nyou know maybe a lesson you have learned\nthat you want to share with uh you know\nother people in that space i'm just\ngonna hand it over to calvin if if\nthere's a one-line advice from your side\nsure um don't be afraid to use the slack\ncommunity it's a surprising one perhaps\nbut we found a lot of times we got\nreally heads down in a problem when\nwe're like reading the source code and\nthe slack community has been incredibly\nrobust for this even though like it's\noutside of our domain or around like the\nparticular problem we're working on\nit it's probably been one of the more\nrobust uh like this and dbt i think are\nlike the two slack communities that like\nor like discourse in dbt's case like\nthere's very few things where it's like\nyou can go get a ask question get\nanswered in like an hour from like\nsomeone who's involved with the\ndevelopment team so anyway that's more\nthan one line that's all good and i know\nthose people who answer the questions i\nsee them uh sitting next to me\nand uh you know caithness and niels\nobviously are some of those people as\nwell so that's fantastic um so jason\nfrom your point of view the\nlesson that you learned and uh the\nadvice you want to give\nuh yeah i'd say it seems really obvious\nbut uh don't reinvent the wheel um\nthat's something we've we've learned the\nhard way over and over again i think you\nget a lot of really smart people\ntogether and you're in that startup mode\nand yes we can all write our own async\nexecution frameworks um we've done it\nbefore uh doesn't mean you should um\nit's worth you know even though you're\ntrying to move real fast to kind of\nexplore what's out there find something\nlike flight that can meet your different\nuse cases\nuh to understand you know just the\ndifferent user personas that are\nactually going to be using your system\nto not just solve your own problems but\nyour organization's problems\nright fantastic chief how about you\nyeah i mean that's a it's our question\nbut i'll say this like i think um i\nthink open source is like a big uh\nimportant thing uh important thing in\nbiotech um specifically because like you\nknow we are trying to build this like\nfda regulated product right and so i\nthink like\nfor us\nuh the more things that we have to build\nin house you know the more proof the\nevidence we have to show the fda that\nlike you know this is the right way to\ndo things i'd rather invest that effort\num in you know like a really solid like\nopen source project like flight um and\nthen you know grow that product nurture\nthat product for biotech and you know\nfor a future like fda regulated products\ntogether with a much larger like open\nsource community so like having one\nreally nice standard um that you know a\nregulatory body can just look and be\nlike oh yeah there's other companies in\nthe space that are like using those have\nbeen successful using this so it makes\nsense right it would be a much better\nlike uh selling point than saying like\noh yeah we decided to build this thing\nand house ourselves so like agreeing\nwith um\nwhat uh calvin was saying about like\ncommunity and the uh the slack community\nand like you know nurturing source\nproject and also\nuh what uh jason was saying about like\ndon't reinvent the wheel i think putting\nthose two things together and then\nessentially working together as this\ngroup right like to and future\nuh adopters of flight to like make\nflight better and more usable for uh buy\ntech applications\nfantastic no i think that's that's also\nthe main purpose here having the\ncommunity come together and actually\nestablishing you know working on that\nstandard i feel like a great point that\nyou made krishna um your advice for\npeople entering that space\nit's tough to\nimprove on what people have said i\nreally like what calvin said i think\nslack is probably\nyou know\num something that really helped me\nmoving so fast so that's what i would\nreiterate i think\nand then just try it out i i guess i\nmean there's there's some problems um i\nthink someone else mentioned that is\nis um sometimes you know researchers are\nhard to um\ni mean they have inertia so you need to\nbe really careful in how you actually\ntalk about these tools i think within if\nyou are in a larger company with\nbiologists um the things that i've seen\nbecause you know problems with nex flow\nand galaxies i think there's a so\nthere's a communication problem in terms\nof what these tools are capable of doing\nand not doing so having that explicit\nconversation both with data scientists\nprobably uh software engineers and also\nbiologists is probably an important\nthing before you actually venture into\nadopting tools like this making sure the\ncontracts are explicit yeah\ngot it got it and kenny\num obviously great insights from you and\ndutch bio so what what are the lessons\nor maybe two lessons that you learned\nand advice you can share here\nuh yeah absolutely if you're an engineer\nget your hands dirty getting the source\ncode don't be afraid of like peeking up\nbehind the curtain and see exactly\nwhat's going on in all cases i think too\noften we have a tendency to blackbox\nthese tools um let's not like mistake\nourselves we don't know we're not the\nbiophysicians here we're infrastructure\nengineers and it's our duty to like\nreally understand how these things work\nso um yeah you know i've read a lot of\nthe lines of the source code of flight\nyou guys did an amazing job at union\nlooking forward to keep on working and\nlearning with you guys this team\nfantastic and katherine i'll hand it\nover to you\nto uh take those uh inspirations as well\nin the feedback and um it gives us a\nlittle bit of um you know maybe a one\nminute overview about what's next here\nfor for union and for for flight and and\nhow important uh you know the the\nbiotech is and will be\nwell thank you martin and thank you all\nfor a wonderful discussion\ni learned a lot and i think uh\nat\nunion at least on our side\nwe are we have seen a huge\nuh\ncommunity being formed on the biotech\nside\num you know fantastic work by kenny and\nsteve and\nyou know others who have actually helped\nkind of see the community\nand we would love to\nlike we are here to help but\nas i said i have no real idea about\nall the things in biotech and what i\nwould suggest\nis we should probably have a working\ngroup\nuh of of some sorts where people can\nand to a point where g made and i think\nsomebody else it's like an extremely\nflexible tool but it's just\nand it was designed because of the\nflexible nature of clients\num and so\nwe\nwould love for uh like you know\nwe give up the tutorial section in the\ndocs is maintained by\nuh folks within union and there are a\ncouple other folks who have contributed\nto it we would love that\nsome of you actually add you know bio\nwith flight or a section like that and\nthen you know help like add so that\nfolks are coming in can become really\non day one itself can get\nused from it we would highly highly\nappreciate that and recommend it\nanother thing would be if there are\nspecific features that i think we need\nwithin the\nyou know flight core or\neven like kit and so on that actually\nhelp uh biotech in any way we would love\nto know that right because\nthere's no there's no way\nfor anybody to know the problems unless\nyou find the issues issues nobody feels\nbad about issues and i've said this to\nmultiple different people because some\npeople have said can i really find an\nissue like this\nissues are you know that's how we are\nsharing\nproblems and ideas and you know even\nbugs are important to be filed so please\nfind issues and and i i would love if we\ncan\nyou know going forward probably every\nquarter once maybe have like a small\nbiotech\nuh section within\nthe open source community saying like\nspend 15 minutes just talking about\nproblems and what do you guys foresee\nand like what would you guys want to add\nto the core\nand let's make you know fight for bio uh\nthe\none of the most important tools that\nexists um so that\ncompanies like you and more\nuh can actually\ntake uh\nget used from bio uh forget get used\nfrom flight timing and yeah we we we are\nhere to support we are uh\nextremely community focused if you have\nnoticed that and we we would love to\nnurture and build this community so\nthank you\nand thank you for trying like the the\npioneers are the ones that actually\nbuild that community and so thank you\nfor all of you being the finders\nfantastic thanks catherine thanks our\nspeakers here this was very very\ninsightful very great kind of a very\ngood conversation so we're going to\nshare this conversation online as well\ni like the idea okay then of having a\nwork group and i think we should pick\nthis up and you know one of the open\nsource buildings here as well i think\nit's a fantastic idea so with all of\nthat i just want to say thanks have a\ngreat day and we're going to publish the\nslide deck as well with the office hours\nand the upcoming event so we'll see more\ninformation\nas soon as we get it post thank you very\nmuch"
    },
    {
        "title": "Fireside Chat - July 12 2022",
        "transcript": "so um now we're moving on to\nsomething new that we haven't done here\nbefore so it's a very exciting part\nabout community sync we are going to do\nthis once a month\num so i would like to welcome uh mathias\nrobin mike and krishna\nkethen as well uh so gentlemen if if you\ncould introduce yourself real quick um\nour scope for this fireside chat today\nis it's really about how did you or your\norganization you know get into\nml orchestration what was your journey\nwhat was the pain you had and so we're\ngoing to talk a little bit about this\nand we're going to talk about some\ncomparisons about it\nand about orchestration how those two\nthings the organizations relate but i\nwould love to welcome first uh\nthe panel members the fireside chat\nmembers so please uh hand it over to\nmathias so maybe you want to introduce\nyourself real quick and then robin mike\nand krishna\ndon't have any audio so if you're\nspeaking we we don't hear anything so\nyou're probably gonna need\nno i don't think so let's go on\ndo you guys can guys hear me yeah yeah\nnow we can hear you\noh okay perfect\num\nso\num hi everyone my name is matthias i'm a\nmachine learning engineer at herb it's\num\nit's a brazilian company it's a travel\ntech tech travel company from brazil\num\ni'm i'm from the machine learning team\ni'm actually our sole\nemelops engineer for now um we are right\nnow trying to\num develop a stack of\nmachine learning tools so that everyone\non our team can use and like\num\nfacilitate the process of developing on\nend-to-end machine learning systems\nand\nyeah that's it i'm\ni'm with them for like eight months now\ni believe and\nyeah\nawesome thanks matias robin tell us a\nlittle about what you're certain about\nentropy\nhey so i'm an engineer with entropy\nour main product or our only product you\ncould say is\nwe give you more information on bank\ntransactions for example on your bank\nstatement you might have\ndifferent transactions and there's some\nkind of transaction string description\nand what we do is figure out who like\nthe immersion was and what kind of\ntransaction it was\nfor example maybe you bought fast food\nsomewhere\nand uh yeah so that's\nour main product and\nuh basically uh\nmaybe a couple months ago we wanted to\nstart using pipeline systems and i\nsuggested airflow first but a colleague\nwasn't into airflow he suggested trying\nsomething else so\noh yeah that's where we try it flight\nawesome fantastic\nmike uh zhong from embark\nhi yeah thanks for having us today uh my\nname is mike i'm a software engineer at\nembark veterinary so we are a direct to\nconsumer dog dna testing company\nand basically uh the customers will\npurchase swabs they'll swab their dog's\ncheek\nship the swab to a lab somewhere the lab\nwill process the dna\nrun it on a microarray chip and then\nsend us data in s3 and that's kind of\nwhere we take over um i've been with\nembark for about a year and a half now\nand when i joined embark had at the time\nkind of brewed their own workflow\nplatform and orchestration system using\nyou know sqs and sns and some some other\nyou know ec2 auto scaling some more raw\nmaterials from aws\nand the team was much smaller at the\ntime so we were able to kind of manage\nthe pipeline end to end\nas the teams grow\nas you kind of start to build up\nyou know kind of better scoping of the\nindividual jobs in the workflow it's no\nlonger manageable for uh for a small\nteam to manage everything so we look\ntowards solutions that would allow\nmultiple teams to own the jobs iterate\non the jobs and kind of update their\njobs independent of the whole pipeline\nso when you have these very bespoke\nhomemade systems you need a deep\nunderstanding of everything in order to\nmake these changes um and we also had a\nlot of these implicit dependencies where\num you know everything was captured in\nside effects whether they be database\nrights or s3 rights um and these side\neffects were not\nthey were not apparent from an interface\nlevel right so you could easily break\ndown stream jobs unintentionally um we\nunderwent this kind of trade analysis\nwhere we explored a bunch of tools\nin my own personal background i'd use\nnext flow heavily so i advocated for\nthat actually\nbut we ended up finding out that flight\nhad a lot of really great features\nspecifically what we really liked was\nthe strong typing of the input outputs\na lot of other tooling your inputs are\neither\na value or a file and that's all they\ngive you right um and then everything\ngets wrapped around that um but but\nflight's really strong typing and the\nability to introduce custom types uh was\nsomething that that we really enjoyed um\nand something that we've really liked so\nfar in our adoption\nso we're we're currently moving our\nentire pipeline system over to it right\nnow um and really enjoying the process\nfantastic thanks mike and uh krishna\nfrom uh\ninfinom is that the name that's right\nyeah yeah okay sir thanks for having us\nand thanks for putting together this\nawesome product um yeah so uh at infinom\nwhat we're trying to do is we're a lean\nbioengineering team\nso we do a lot of synthetic biology\nprojects and lots of our data is very\nsimilar i think to what mike was\nreferring to it's ngs or next generation\nsequencing data\nand we have at least three commonly used\npipelines that we put these data through\nover and over again\nand what we wanted to do i've been only\nwith inferno for the past four months\nnow\nbut what we wanted to do is increase our\nreproducibility so in the future if you\nwanted to go back and say hey how was\nthe data treated\nhow were those input outputs generated\nwe wanted to have some some kind of\ntrace back to how those data were\ngenerated and as you probably know with\nnext generation sequencing data these\ndata go through a lot of different tasks\ndifferent software tools so you need to\nmake sure that all those versions match\nup perfectly so\nso having that legacy is is really\nimportant for us and also the\nscalability because right now what we're\ntrying to do is only run a few samples\nper week but we envision that as we have\nmore projects\nand as we generate more data we need to\nhave a system that scales up really well\nwith the number of samples\nand having flight as a kubernetes first\nsystem i think is perfect for this\nbecause we can scale up really well so\nthose problems i think\nso we did a little bit like mike said we\nplayed around with um with next flow it\nwas fine for some tasks but i really\nlike where flight's uh helping us right\nnow it's the early days i've only been\nusing it for a month now\nbut at least one pipeline that we trans\nuh transferred over to flight seems to\ndo really really well with the very\nlittle refactoring of our code which is\nperfect\nso yeah thanks thanks again for putting\nthis together\noh absolutely i mean this is fantastic\nto have you here and um\nwork with flight so i was wondering\nabout um where you or your team were you\nclear about your requirements when you\ntested your first solutions obviously\nand then or did this evolve over time\nwhat your requirements are i mean it's\nalways you know an open-ended uh\nsituation you learn more so what was\nyour situation\nuh you asked me\nanyway oh yeah i can go first yeah so\nit's kind of like an open-ended i think\ni'm not a machine learning engineer\nso i'm a data scientist so i'm learning\nthis as\nas i'm but if i had more experience\nmaybe i would have\ncome up with requirements\nbut because i'm a data scientist and not\na machine learning engineer i had to\nfigure out things\nwhile i'm still you know trying to use\nthese things so that's been my\nexperience\nin my past one of my past previous jobs\nwe used prefect so that was kind of like\na good experience understanding how this\norchestration workflows look like\nsoftware it looks like but other than\nthat i think it's the latter where we\nhad to come up with requirements as we\nwere\nbuilding these tools\ninteresting and then uh mike uh matthias\nrobin on your side did you know when you\nstarted journey with flight what you\nexactly wanted or was it just kind of\nlike a discovery process as you went\nalong\num\nso for us uh i guess when we started\nwhat people were doing is that there are\ndifferent scripts that they ran on data\nor they had\nmaybe they were training a model in\nnotebooks and so on so our goal was uh\njust making that more reproducible and\nobservable and all that stuff\num and yeah this kind of pipeline\nsystems are perfect for that right\nbut other than that we didn't have any\nhard requirements on the system itself i\nthink\nso for that i guess airflow would have\nalso done the job but you might as well\nuse the nicer framework right that gives\nyou more features flight has a lot of\ncool features that we're using\nrobin\ni can go next yeah i think we had a good\nsense of the requirements at the start\num and those requirements mostly came\nout of the challenges working with our\nhomemade system so over the years of\ndeveloping and iterating on the homemade\nsystem we had noticed uh some things\nthat were very uncomfortable for us so\num the kind of black box behavior of\naws's auto scaling for instance was\nsomething that we didn't really like uh\nwe found the scale in to be rather slow\nuh it could take you know five to 15\nminutes for a cluster that scaled out to\nyou know several thousand instances to\ncome back to zero even when there was no\nsignal for it to be that high um not\ngreat right\ni think what really helped us here um\nand something that we really liked about\nflight was the kubernetes native system\nand uh you know kubernetes of course\nopen source has a has a very rich set of\ntooling um so we're able to leverage\nthings like you know prometheus and\ngrafana whereas before we were forced to\nuse um cloudwatch right for for\neverything aws\none of the other things that we were\nconsidering was um how can the existing\nengineering teams you know collaborate\nwith the scientists and what sort of\nlevel of engineering support would be\nrequired to transition scientific\nworkflows that are oftentimes just bash\nscripts right let's let's be real um\nwhat would be required to kind of bring\nbring these workflows that may not\nnecessarily fit neatly into a into the\nflight definition language\nhow can we support them and bring them\nin\nhow can we write our own kind of user\nmanuals internally so that people that\nare new to flight can kind of onboard\nquickly um and kind of start writing\ntheir own workflows quickly um and and\nyou know we found the process so far to\nbe very nice\num and then of course the the kind of\nbig requirement i think everyone's\ntouched on this is data reproducibility\nuh data provenance and data lineage um\nthat was something that we you know we\nthink flight provided very well for us\nawesome\num\nbut i'm not sure if you have uh uh\nif any other points to that topic\nyeah um yeah i'll touch on them very\nquickly it's just that the our main\nrequirements when we\num\nwhen we started working on\nflight and\nwhen it started developing our envelope\nstack or main requirements like okay we\nwant orchestrations too that we can like\nreproduce reproduce our code like\nretrain we we already have like airflow\nwe have a very other other orchestration\ntools on our other um on other teams\nlike on data engineering team uses\nairflow a lot but we noticed that it\nwasn't very suitable for machine\nlearning projects so we\nwe were focused on like searching for a\ntool that was it was easy to it was very\neasy to use to adopt for machine\nlearning engineers for data scientists\nto start using it so we tried a little\nbit of different tools um\num as i said prefect was a tool that we\nactually tried a little bit like we\ndidn't actually put in production or not\nor something like that we just tried a\nlittle bit of\nprefect flight and other tools and we\nnoticed that flight was like had this\nvery good um\nunbalance between like being a very\nstrong a very powerful um\ntool and at the same time having this\nhaving a small um\n[Music]\nlearning curve for machine learning\nawesome fantastic so\na question for uh for the panel here so\nif you would be if you would go back in\ntime maybe half a year or whatever six\nmonths nine months\num and you would have to do the process\nagain\nand\nwhat are the learnings what would you do\ndifferently or another way to ask what\ncan you give as advice for people who\nare similar situations\nyou were maybe you know some time ago\nwhat's what's advice that you want to\ngive\nuh i can start um\nso when i started there were a lot of\nquests different questions i had that i\nknow i couldn't answer without just\nusing flight basically\num\nso that's what i did and now yeah a\ncouple months later i have answers first\nof all those questions like uh\nfor example we're using a mono repo and\nwe have different environments like\nstaging and products one and uh\nthere's a question about how how to keep\nthat in sync with the different flight\nenvironments for example so\nquest questions like these\nuh i i couldn't really answer before so\nif i had to redo it i would redo the\nsame thing because there's no way to\nfigure out those questions without us\ntrying it i guess\ngot it\num anything on on i think i heard it\nearlier i think somebody mentioned i\nthink was humatis\nor um\npotentially\nmike\nabout\nthe teams and getting you know the the\nscientist side together with engineering\nside and i think this is always a little\nbit of a struggle you know when you have\nyou know two teams working completely\nfrom two different ends uh and and it's\nyour advice to bring those teams\ntogether to begin with or just roll out\nthe platform getting them used to but\nwhat kind of like steps would you\nrecommend when you get actually when you\nintroduce platform like flight in your\norganization\noh that that's a great question so um at\nembark we kind of started by bringing\nthe the two sides together so we have a\nlot of kind of smaller teams that are\nyou know two engineers and two\nscientists um and then we have a a\nlarger infrastructure team and when we\nwere kind of deciding on the tooling and\nsettle on flight we actually\nwe didn't consult the other teams too\nmuch because we kind of felt that um\neveryone would kind of bring their own\nopinions to the table\nand more importantly everyone would kind\nof bring their past experiences to the\ntable meaning somebody that had for\ninstance worked at the broad institute\nand used cromwell for a number of years\nwould say hey we should use cromwell\nbecause of xyz um and and that's kind of\nwhy we did the trade analysis right we\nkind of wanted to be objective about\nwhat we wanted to achieve and what we\nwanted to accomplish so sure we asked\nthe other teams um what do you need from\nthis platform but at the same time we\ndidn't say well what do you think would\nbe a good solution right um we kind of\ndecided to be opinionated about that\num to your original question about what\nwould we have done differently i think\nthe only thing we would have done\ndifferently is uh get involved with the\nflight community a little earlier um we\nwe went through the documentation very\nearly on and started started learning\nabout flight but we didn't immediately\njoin the slack channel\nwe didn't immediately start asking the\nthe flight team some of the questions we\ntried to answer them on our own\nand uh\nand sometimes it was fruitful sometimes\nwe were able to do it um but i think\nsome of the the the best learning that\nwe had was when we actually interacted\nwith the team when we were very open\nabout our questions and about our lack\nof understanding um we found that the\nresponse times were great\nyou know all the documentation that they\nlinked us to you know was accurate up to\ndate um and so that that's probably\nsomething i would give advice for\nanybody else's if you're considering\nadopting flight and you're working on a\npoc for instance um you know\nintroduce yourself to the community um\nask any questions that you have because\ni think you'll find people are very\nwilling to help you out\nthat's awesome\num we also have a case and ceo of uh\nunion ai here so keith and i wanted to\nask you um community so tell us a little\nbit about the office hours and all the\nstuff that we're doing and then how do\nwe engage with the community\nyeah um firstly you know thank you\neverybody for joining and mike\nabsolutely\ncompletely agree we have\nwhen you're working in an open source\nproject and um\nand we treat\nflight with the the most open\nyou know uh like the highest level of\nopen source uh\nspace that it is we treat it with that\nuh level of respect and you'll never see\nus\nselling any of our other things\num so yeah so it's very important it's\nvery hard for us to know who's using us\nokay like i remember when mike\nand i\nwe got introduced we had no idea that\nembark was even using flight to be\nhonest\nuh\nand\nand even today i i every day learn\nabout new companies using us\num and sometimes i even learn about\ncompanies who are using us went through\na little bit of a struggle\nand i was\nit's too late now because they've\nactually gone over the hurdle and i'm\nsure there are some other companies that\nare trying to use\nprobably give up they're like oh yeah i\ndidn't find this one doc and give up um\nso i the only thing i would say is that\nwe\nthere's no excuse for not having good\ndocumentation and we try our best and we\nare trying to do whatever we can to\nimprove it community help will\nabsolutely\nhelp us improve it\nbut\nthat's why we have a slack channel that\nwe try to keep really you know\nhelpful presence over there\nand not only us i think other community\nmembers from different companies are\nwithin the flag channel\nuh so please join it and ask questions\nsometimes\nwe respect that you take like weeks to\nuh you know bang your head on a problem\nbut sometimes it's probably just\nbetter to ask it's okay no question is\ntoo small no question is too um\nyou know out of the woods uh as long as\nit pertains to flight please ask it uh\nand to that effect we actually\nabout i think about five four months ago\nor something we started doing office\nhours um three times on wednesday\nit's mornings afternoons and evenings in\npacific time\nand\nwe've seen a lot of people come in and\njoin and ask questions and you'll find\nus absolutely and some people actually\njust come in and ask us questions just\nin general about like whoa\nwhat's happening in machine learning and\nwhat do you think about it and all of\nthose uh\nare okay like we we love to meet\ncommunity members we love to know who's\nusing us\nand we would love to hear more we would\nlove to hear what you would like in the\nsystem what do you think is missing\nbecause there's just no way within this\ncommunity to come up with a consensus\notherwise like unless people speak up\nand thank you martin for doing this and\ngetting more people involved today and\nyou know asking them\nto voice their opinions\nthis is not the only reason why you\nshould voice your opinions feel free and\nyou know just stuck in\nthe slack channel even or this community\nthink every other week that we have\ndefinitely come up and say like for\nexample the other day somebody was\nsaying\nwe want to use ammo flow for tracking\nand i know mathias or somebody else has\ndone this work and i'm like i\ni know there are people who are doing\nthis but i don't know how to connect you\nso if you come to the community i'm sure\nyou will automatically enhance somebody\nelse's\nuh\nyou know\nexperience so\nplease uh you know voice what you would\nlike\nif there are like some i think there's\nanother team arctic wolf they asked\nabout\nsagemaker and then immediately some\nother company\nfolks jumped on it and said like hey we\nalso would like this\nthis is how it works\nwith spotify they talked about ray and i\nthink shopify and like my dad's at four\nother companies jumped on it and it's\njust like and now all of it is happening\nthese companies are working together so\nwe need we need this is why community\nexists literally\num and so please use it and i think help\nus spread it to like help us spread the\nword get more people on it if you can\nwrite a blog or something that really\nreally really helps the community so\nthank you it's awesome i think thanks a\nlot catherine i feel like that's that's\nimportant too i think mike you\nabsolutely nailed it i think engaging\nsooner is is really the the key office\nhours are a great tool here i wanted to\nactually ask one more uh question and\nthen we have you have probably five more\nminutes or so to to go over that and\nthat is for creation and i think you're\ncoming from the research from the data\nscience side and so do i i mean i'm a\ndata scientist i'm not an engineer um\nand and i was always interested um when\ndata scientists work on find their\nresearch demands and then we go and talk\nto our ml engineers and we figure out\nhow do we get this thing to the point\nwhere actually we can learn what it does\nin the inference and so on in production\nso\nmaybe you have a unique view about you\nknow your experience here and um and\nmaybe unique advice about how to engage\nwith ml engineers specifically when it\ncomes to orchestration topics yeah\nyeah one thing\nthat i found sticky with even prefect\nback in the day when i was using prefect\nwas\nyou know the seamless\ntranslation between dev environments to\nproduction environments right\nso that i think is sometimes lost in\nthat whenever i mean\nthe data scientists talk with mission\nlearning engineers you want something\nthat\nobviously eases the burden on the data\nscientist to develop things without\nbeing overburdened by\nyou know extra things that they need to\ndo to get things to production i think\nthat's the key and with flight\nsurprisingly i mean i wasn't hoping this\nwould happen\nbut it was actually very i mean i\nwouldn't say completely seamless but\nsurprisingly less uh onerous than what i\nthought it would be so that was awesome\num so that's i think one thing that i\nthink\npeople go in naively into these meetings\nassuming you know data scientists and\nmachine learning engineers talk the same\nlanguage\nbut we come from dell right we we like\ndeveloping things and\ni'm assuming machine learning engineers\nlike things in production right\nso yeah so that i think needs to be more\nclear more\nexplicitly communicated yep\nawesome fantastic all right any any\nother i just want to open up one more\ntime um mike matias robin any comments i\nhave a thing to suggest so i don't know\nif you uh oh folks so\num that's very new to flight uh\nsometimes i think probably if you have\nyou know smaller videos\nwalkthroughs as in this is how this\nshould be done kind of setting a tone to\nthis is the best practice to you know\napproach this and probably this could\ncome from a lot of folks who have\nalready gone through this you have gone\nthrough the pain of figuring out oh you\nshould probably should have done this uh\nin this manner rather than that um so if\nuh you know\ncollaboratively something could be uh\ntriaged and\nif smaller videos could be made\num so that can fast track adoption of\nflight and\njust like uh\ni believe someone here was mentioning\nthat they tried uh you know\ntrying out things on their own and then\nlater on getting connected with the\ncommunity was helpful so\nit could be uh you know\nhard for people to sometimes come and\nask\nbecause they might think i have not gone\nover all the documentation if i just\ncome up and ask i might just\nseem\nuh\nless sincere of\nyou know what i should have done so\ncoming from there i would say if you\nhave some videos of walking people\nthrough that could be helpful\num just as an idea trying out\nabsolutely you know i think i think\nthat's one of the things\nthat that we feel like would be super\nhelpful also medium posts\nuh you know walk through some some\nexamples links to youtube or whatever\nyou're gonna use there i felt like this\nis kind of like how we learn the first\nstation i feel like exactly where you're\ncoming from is like you don't want to\nactually bother somebody and you want to\ndo your homework first and not you don't\nwant to get the response of hey\nhave you looked there or you're like oh\ni could have found this right so i\ncompletely get you and so it's up to us\nto produce those uh videos and\ndocumentation as uh casey said earlier i\nthink there's an area where we have to\nimprove i think there's no\nno doubt about this but i think really\num spreading even if you think it's it's\nnot that important everybody knows that\ni think the rule is don't assume that\nright\nyeah and i would i will definitely say\nso we've we've added a few videos and\nwhat we did is\nall of these meetings that we do if\nsmall snippets from there we take it and\nwe try to put it in various parts of the\ndocs we probably have seen those\n[Music]\none thing we would love if\ni think knowing the problems is also\nthis is the\nyou know i talked to all four of you\nyou've now been using flight for a\nlittle bit you've probably forgotten the\ninitial pain points of like where are\nthe these like these crazy oh package\nand register and like blah blah it's\ncrazy right like we would love for you\nto if you could remember if you're early\nin that journey if there is that pain in\nthe friction log\nshare it with us share it with the\ncommunity because that friction log is\nis is golden right\nit it only happens once\nand at the end of it you either are with\nthe community or we have lost\nalready so if you if you have that i\nknow it's a sometimes it's a lot of work\nbut if you have even you know few points\nin there\nplease share it it will help\nus improve and i promise that we will\nimprove because that's what we've done\nover the last one one and a half year\nwhich is constantly improved\nand we want to do more of that\nfantastic things\npain driven development is a very\npowerful thing so you can either\nyou can either internalize the pain or\nlike let it go into the\nthe ether of the slack and uh someone\nwe'll we'll pick it up\nyeah give it on slack it's fine\nit's gonna work okay cool awesome so i\nthink we have a few more things so i\njust want to say encourage you to\ncontinue this conversation on slack\nabsolutely right um uh bring bring\npeople in that you know might be\ninterested in and and flight and uh have\nthem engage with us i think that's\nreally fantastic"
    },
    {
        "title": "Flyte Community Update 018 - July 26 2022",
        "transcript": "so with that let's uh dive into the\ncommunity updates\nso one of the big important pieces we\nhave started in july was get to know\nyour contributor\nso the contributors of the month are\nzugatu\nray who has been i think on stage last\nweek i lost two weeks ago we have robin\ncarlo and babies i hope i don't butcher\nthis name\nkyocitis\ni would assume that's correct so\nthis is a shout out to all the flight\ncontributors we appreciate your hard\nwork we appreciate your contribution if\nyou want to get in touch with the flight\ndevelopers slack is the best way to to\nconnect with flight and of course uh the\ngithub repo\nall of those um you know links are being\nposted at the end we're gonna post this\non the web as well so you know where to\nuh contribute and how to engage with us\nwe also have office hours i'm going to\ntalk about slater a little bit so you\ncan talk to like athen um and haythan\nand katrina um and that's when the team\nis really available so i would take\nadvantage of this sometimes when i'm\nhere in the office and see that you know\npeople call in i feel like this is\nreally exactly what this whole thing is\nabout so i encourage you to take\nadvantage of the office hours we're\ngoing to speak at the very end when the\noffice hour so with that let's move on\num next slide\nso we had a fantastic uh twitter event\nlast week i just want to speak quickly\nabout this twitter event and um\ni want to make sure you see the\nrecording you see the url so this was a\n60 minute session\nlast friday with kelsey hightower for\nthose of you who\nassume pretty much everybody is using\nkubernetes knows kelsey hightower but um\nif you haven't heard about kelsey kelsey\nis is really the leading person figure\nin eventualizing kubernetes and\nis a fantastic person to host\na topic like machine learning production\nthat's what he did last week we had\noverall\nthousands of people looking uh you know\nat the recording and of all\n200 and hundreds of people participating\nin the live session uh i thought some of\nthem very very important quotes which is\nin the next slide i to you know\nhighlight a little bit uh fabio grad's\nphd from a company in germany called\nmerantics i i think just nailed it with\nthis quote that uh we highlighted here\nin you know i'm just reading out in\nclassical software engineering\nyou can see this what way is going here\nbut um the only thing is that changes in\nthe are the code and the health system\nhas changes too but the really the\nimportant part is the infrastructure\nhas is changing too and needs to be\nversioned and that is the key takeaway\nfor having a system an ml orchestrating\nsystem like flight to give you assurance\nthat you know you have everything\ncaptured i thought this was really an\nepiphany for many of the people\nunderstanding how in ml ops is what i'm\na losses and what ml orchestration is\nand orchestration includes more than\njust the code for for data sciences and\nml engineers and the data itself but\nalso the whole infrastructure setting so\nit was a really important takeaway"
    },
    {
        "title": "Flyte Community Update 017 - July 12 2022",
        "transcript": "welcome to the flight community sink\ntoday is july 12th my name is martin\nstein and i'm your host today we have a\nsuper packed uh agenda\ni think it's gonna go probably like\nlonger than 45 minutes but we'll see a\nfew housekeeping notes as always we're\nrecording the session and we'll be\nsharing it on social media\num but with that let's just um hop to\nthe first uh topic to the agenda walk us\noutside real quick through the agenda\ncenter can you move forward so we're\ngoing to have community highlights we're\ngoing to talk about ecosystem projects\ntoday we're going to do a little bit of\nroad mapping today not too much we have\ndan\nuh from union ai doing a demo today this\nis going to be pretty awesome and thanks\nthen on your time off uh to come in and\ndo this demo we're super excited and\nthen we have a fireside chat today first\ntime fireside chat it's going to be very\ninteresting we're going to talk about\nemma orchestration uh we've got people\ninto ml orchestration and we have\nprobably about 30 minutes and uh five\nguests today uh together with katherine\nuh ceo of union i and myself so looking\nforward to that all righty let's go to\nthe\ncommunity update um get to know your\ncontributor so we do this on a monthly\nbasis once a month we introduce our\ncontributors uh today we have uh zugato\nray um and with\nwith that uh let's have zugato uh\nintroduce himself cigarcho uh what i\nwould love to to know is maybe you can\ntell us a little bit about yourself and\nyour interest in flight and how you\ndiscovered flight\nnot sure if cigarette okay yeah these\ngo ahead sugar\ndon't have any audio\nokay let's do this later once the audio\nstuff is resolved um let's go to next\nslide that's the guide to slide\nso\nuh so god let's let's do this at the end\nuh or if we don't get it to today\nbecause we've packed agenda we're gonna\ndo it next time okay next slide\nuh community blogs um sorry i was not\nable to come in earlier it was not\nallowing me to unmute oh oh i'm sorry\nokay let's go back so we got to stages\nyours please tell us a little bit about\nhow you discovered flight and and what\nyou're doing at the university of\nwisconsin um\ndiscovering flight uh that was from one\nof the\nposts made on linkedin\nabout union ml\ni first got to union ml\ngot saw that post from niels\nand\nthey kind of liked the ideas and how\nit's structured\nand\ni've been working for a lot of packages\nin condo forge adding you know packages\nto condo forge from pie pie for a while\nnow i'm maintaining close to about 60\npackages\non condo forge\nand it seemed like adding union ml to\nquantum forge would be nice and then i\nalso saw there was uh you know these uh\nopen issues\nand went ahead and added the package uh\nto condo forge\num in doing so i also ended up updating\nsome packages like flight kit and flight\nideal\nthis was the beginning and\noften i've seen is when you're adding\npackages to corner forge\neven though the libraries the python\nlibraries are otherwise made you know\nproperly the packaging is where often\nlibraries would miss out a few things\nso\nthere were a few things for even union\nml we\ngot that ironed out and\nyou know that was another pr2 in an ml\num\nmy research in uw university of\nwisconsin milwaukee as a\nphysics phd student\nuh has been in\nuh 3d infrared imaging\nthe specific problem that i worked on\nwas about a\nprecision a precise positioning of the\nsample\nwhich we could not earlier do for\nboth translation and rotation so we can\ntake images from multiple different\nangles and do non-invasive\nyou know\nrendering of images such as cat scan\ndoes but that's with x-ray here we are\nusing infrared\nand in doing so i ended up using deep\nlearning because at some point it was\neasy enough for you to see that you can\nexplain what the problem is but you\ncannot really put a mathematical\ndefinition to that and use heuristics uh\nyou know other solutions for that\num so that's kind of in a nutshell uh\nwhat i've done so far and uh i\nreally love uh contributing to open\nsource i've learned a ton from open\nsource and for anyone who's listening to\nthis now or\nwould see this you know\nvideo later on on youtube maybe\num\nif you have not started contributing to\nopen source please do consider doing\nthat i was on the other side of the\nfence for long enough and i used to\nwonder who are these people who just\ncontributed these things um\nit is often easy\nto think and you know maybe i'm not good\nenough for doing that but maintaining a\nlibrary is not easy there could be you\nknow typographical errors in documents\nor there could be a things that are not\nworking properly and so you may not have\ntime to come in and you know put a full\nrequest to make that update but at least\nidentifying that and leaving a note in\nan issue is also helpful so\nthink in terms of contributing in\nwhatever capacity you can and that'll be\ngreat so\nawesome that's fantastic and i'm sure\nneil's you're super happy\num having cigar to help you on union ml\nyeah totally i mean it was um kind of\nout of the blue you know as these things\nusually go\num and super grateful for to him for um\nyeah cleaning up a lot of the\npackaging stuff that i learned a bunch\nof uh\nkind of conventions and how to do things\nas well so\nthank you shigato\nawesome\nthank you very much looking forward to\nhaving you\num be a big part of the community super\nexcited about happy new years you got to\nawesome all right let's meet you let's\ngo on to\nabsolutely um so\na quick\nlook at our community blogs so our own\nsamita has written a very interesting\npiece about\nthe airflow provider from flight and\nbasically how to extend air flow with\nflight very very important topic uh\nsometimes people feel like you know we\nuse one platform we want to extend to\nthe other we want to use both\nthis uh blog post is really about how\nyou can actually leverage both platforms\nso super excited about this you can find\nit on uh\nblog.flight.org and uh we have\ni think niels speak that's next slide\nneil speak uh this week at the scipy\nconference in austin niels can you tell\nus a little bit about what's going on\nthere\nyeah this will be my second time\num actually third time speaking at\nsci-fi but super excited to chat about\nflight basically introducing it um in\nthis venue\nso\ni'll go through some of the basics but\nreally the crux of this\ntalk will be about\nall the nice things that we get from\na lot of the flight opinions uh the the\nprogramming model of flight\num so reliability reproducibility\nrecoverability and i couldn't find\nanother r to make the alliteration work\nso you know auditable\num machine learning so\nyeah i'll\nbasically be going through the basics of\nflight and then digging into\nthe um specific properties that make it\nreally good for production and yeah i'm\ni'm trying to\npopularize the main meme of uh\nyou know flipping the testing and\nproduction meme and making it uh\nproductionizing in test\nsee how that works fantastic do you know\nif there's any recording or live\nstreaming uh from the conference if\ncommunity members can watch this\nyeah it'll be it'll be recorded there'll\nbe some you know a month or so for\npost-production of the videos but they\nshould be on youtube\nyou know within a few months of the\nevent\nall right great thank you very much that\nsounds fantastic so let's go on and um\ntalk about one more thing from niels you\nset up a union ml\nopen source software planning so tell us\na little about what's happening there\nand who do you want to actually\nparticipate in the planning event\nyeah we're super excited about this\nformat and um just to give you a little\nbackground on it we want to open up our\ninternal planning meetings where we\ngo through road map items do some\ngrooming\nassign issues to people\nand usually you know so far it's just\nbeen myself in eduardo hammering away at\nuh\nunion ml building but we wanted to open\nthis up and you can find our first\nmeeting in this youtube link below\nand you can subscribe to the calendar\ninvite here it happens every other\nwednesday so not tomorrow but uh next\nweek wednesday will be our next one\nand\nreally all\nall levels of open source experience\nhere so\nthere's a bunch of holes in the\ndocumentation i'm sure they're typos\nthat we missed\nthere are higher higher lift issues here\nof implementing new features and you\nknow cleaning up some bugs so\nreally anyone is welcome here\nto to join and yeah we'd love to have\nyou and um\nyeah if you're if you're interested in\nhow these\nthese things are run and um\nat maybe at a higher level what we're\nworking on please do join\nawesome and in a nutshell for new\nmembers union ml is just about what\nyeah it's a opinionated framework ml ops\nframework that\nreally tries to reduce the boilerplate\nand simplify the\nend-to-end experience\nreally for for building and deploying\nmodels\nso it's built on top of flight\nand the programming model is very\nsimilar to something like flask or fast\napi\nand um you can dig into the docs i don't\nwant to take too much time here but the\nwhole point of this is to\nprovide a standard interface for\nexperimenting building deploying models\nfantastic\nawesome\nall right thank you very much um so\nlet's go on to the road map\nsantro so flight\nversion 1.2 has been on the roadmap for\na little bit i think there shouldn't be\nany big surprises we're going to have\ndan talk about\na component of that i think the big part\nuh we're going to speak in august is ray\ntasks and ray integration so we're going\nto have a lot more happening at the race\nside\num sandra let's move on to real quick to\na few housekeeping things we showed it i\nthink a second uh before the office\nhours um\nthat was\nwe talked about this and then\nso that's uh you know haytham and\nkatrina and katherine so that's another\nway you can do this you can do slack you\ncan do both uh so you can\nfollow us there so it would be fantastic\nand then uh for the next\nuh sink on july 26th so maybe there's a\nbiotech theme so i'm sure i'm going to\nsee you again\nfrom the biotech\ncommunity there so we're going to do\nthis for the first time that we have a\nthematic focus and talk really uh in the\ncontext of what is specific about\nbioattack and how can flight help there\nuh we have matt rasmussen there and\nkrishna thanks a lot for joining us for\nthat conversation as well um you can\nfind uh these recordings on youtube and\nthe youtube channel you can share them\nas well\ni just want to say it's it's amazing to\nbe working with all of you i think\nflight is is an amazing community so\nthanks to everyone for you know\ncontributing participating and spreading\nthe boot and with that i hope you have a\ngreat day"
    },
    {
        "title": "Flyte Signaling and Gate Nodes",
        "transcript": "so uh we're discussing flight signaling\nand gate nodes here um for those who\ndon't know me my name is dan i am a\nsoftware engineer with the union team um\ni work mainly uh on back end type stuff\nflight propeller flight plug-ins uh kind\nof everything encompassing actual\norchestration of running workflows\num so so first kind of dive into\ndefining signaling seeing what signaling\nis um\nwe're referring to the ability to pass\nkind of arbitrary data to a running\nworkflow\n[Music]\nthis work has been driven by by human in\nthe loop use cases\nfor most of which is approving data\nbefore any kind of subsequent processing\nso things like you have a task task a\nfor example that produces a data set\nthat's then processed by task b\na user with some kind of esoteric\nknowledge can come in look at that data\nset\nsee if it looks good or you know value\nas a result of task a\nand see if they want to continue running\nthat workflow or if it's just not worth\nit um similar use case could be labeling\ndata for supervised learning um if task\na outputs a data set you need somebody\nto come in and label uh different roles\nor data in there before some sequent\nprocessing um you can do that both of\nthese use cases are passing just a\nboolean value of like approving or\ndisproving it um but we've kind of taken\nthis a step further to say we want to be\nable to pass arbitrary data we want to\nbe able to pass things like collections\nof data or maps of data\nof any kind of data type so a use case\nfor that could be something like\ndynamically defining task parameters um\nyou could\nyou know a user could conceivably uh\nat runtime of the workflow say hey let's\nrun this with this value downstream um\nafter seeing some of the output of\nprevious tasks and there's just a ton\nmore use cases for this work\num a very brief outline here i'll try to\nmake this as quick as possible\nproviding the most information\num but but we'll dive into the back-end\nimplementation uh very lightly i think\nit's important to do this because um\nright now this has been\nfairly clear and straightforward we're\nworking on kind of the user experience\nand the ui work and so\nback end is is mainly done\num we think it provides a very flexible\nand extensible support and we're kind of\nlooking for some level of input on\nflight kit api proposals uh how we want\nto present this to the user to make sure\nthat we are encapsulating all of the\nfunctionality we have\nand it's flexible and extensible enough\nfor for users use cases down down the\nline\num so kind of the foremost\nimplementation on the back end for this\nis the introduction of a gate node into\nthe flight workflow uh into flight idl\num so flight constructs a dank for those\nwho don't know\na dag of all the operations these are\ndifferent nodes within the system we\nhave things like dynamic nodes\nbranch nodes for conditionals\ntask nodes execute any kind of plug-in\nthat you're running um and then start\nand end notes anchor this so this work\nadds a gate no definition that is a\nfirst-class citizen and gate note is\nreally just contingent on a single\ncondition it takes inputs just like any\nother node and it writes outputs\nbut one executing that node\nthere are a condition of right now\nsignal or sleep that defines if that\nnode is successful or not or if it\nshould keep going uh the signal\ncondition we define as just a unique id\nto the workflow uh when it's run in the\nglobal scale we concatenate that with\nthe workflow execution id to make it\nunique um and then a data type we are\nstoring that as the literal type in our\nflight idl proto message and so it's\nimportant to note that this supports all\nkinds of data primitives like you know\nintegers or strings\nsupports collections of data if you want\nlists of values uh maps\nand you can nest these things as well\nyou could have a map where a value is a\nlist of data so very very flexible on\nwhat you can pass into the workflow\nas portion of a signal\nand then the sleep condition just\nrequires a duration this would be\nsomething just like here's a node uh we\nwant task a to run and then five minutes\nlater we want task b to run so in\nbetween those dependencies we can put a\ngate node with a sleep condition\nto get signaling to work we've added a\nsignal service onto flight admin i think\nthat this is a perfect spot to to add it\ninto our framework flight admin is kind\nof control plane for executing and\nmanaging execution of tasks\num and so the signal service is just a\ngrpc service just like every other uh\nservice that's offered in flight admin\num a quick kind of dive through about\nwhat execution of a gate note with a\nsignal would look like here we have\nflight propeller in the bottom right\nthat has some kind of workflow dag with\na gate node um\nwithout diving too deep into flight\npropeller uh it we iterately iteratively\nprocess that workflow uh when it gets to\na gate node it will contact flight admin\nto check if that signal exists based on\nwhat the id is\num\nflight admin is able to store all these\nthings persistently in a database\num and if it exists\nmeaning there's a value set for that\nsignal then it can use it and progress\ndown the workflow if it doesn't it just\nwaits until the next periodic check of\nthat workflow\nso users are able to to kind of trigger\na signal using a number of different\napis being a grpc service is very slick\nand seamless\num\nthere's support for http endpoints\nthrough that they integrate very easily\ninto grpc um and then additionally we\ncan contact like flight console\nwhere there are our specific buttons in\nthe ui to say okay signal with this\nvalue to this node um and then that on\nthe back end goes through and contacts\nflight admin and sets it as well\num so so the back end we think is\nextremely extensible very very flexible\nthe problem that we're running into\nright now and i wouldn't say a problem\nmore so an active discussion is what is\nthe correct level of abstraction correct\nway to present this to users\nto ensure that there's there's full\nfunctionality but not being you know\nunbelievably verbose in that we've kind\nof come up with two different ideas that\nwe've fleshed out over over quite a few\ndiscussions\num specific syntax has changed but just\ntry and look at like the overarching\nideas\nbetween this async await proposal and\nchaining i o proposals\num so an async await uh idea of\nimplementing signals and gate nodes in\ngeneral\nkind of attempts to emulate popular\nasync frameworks these are extremely\nextremely useful in things like python\nmy experience is with rust on the back\nend\num but the idea is that\nwe can define signals explicitly\nand then those have typed values so in\nthat workflow the first workflow signal\nworkflow we see we run task t1 on the a\ninput we then define a signal that's of\ntype boolean there's a timeout and a\nname if we want to specify that\nand then we're able to in this\nconditional say okay if v is true v dot\nweight that's what's called as part of\nthe async api then down the line we\ncould process that b value this would\nsay process you know execute task t1\nwait until a user\napproves that value and then process\nthat down the line\num similarly for something like a sleep\nwe can use uh promise chaining which is\nalready available in the flight kit api\nuh we can call flight sleep on something\nand wait that would return a promise of\nlike a nun value or something like that\num and then for the t1 tasks to execute\nuh we can use promise chaining with v uh\ninject kind of a dependency there where\nwe say uh we have to sleep before b is\nuh executed\num\nthe one problem with this that we're\nseeing is is this maybe not enough\nabstraction for use cases it's certainly\nvery flexible um but we don't want to\nhave uh an api that's uh two verbose\nwhere users don't understand what's\nhappening\num and and just kind of thereof\nexpressing that cleanly and\nergonomically\nanother kind of competing one is\nchaining i o\nuh so the flight kit api if you look at\nit from a high level it kind of\nimplicitly defines the workflow data\nusing io flows\nyou execute a task that returns a value\nyou pass that value to some other task\nand that defines what the dependencies\nbetween all of this data is\num so in this one\nyou have something like a flight dot\napprove\nover the data that takes then a value as\npart of that function call and returns\nanother value\num so we'd execute task t1 returns b we\ncall approve on that and then we can use\nthat conditional again to say okay was b\napproved or was b not approved um and\nthen this would very very explicitly uh\ninject that dependency into the the\nflight workflow dag of saying run t1\nthen have a gate node with a single on\nit and then we can process downstream\nfrom there\nsimilarly with the the sleep call on\nthat sleep workflow at the bottom we\ncould have something where sleep and\nyou'd pass it a value to say you know\ndon't you know don't make this value\navailable until we've slept on it and\nthen we have t1 call that is then over\nthe result of that sleep call\num and there's a little bit of concern\nwith this as well is\nis there a lack of flexibility here um\nhow do we integrate support for\narbitrary type signals i think that\nthere's a lot of power in being able to\nsay hey i want to pass it a map of\nstrings to boolean values or or\ncollections of data um and i think\nthere's some things that we can do in\nthe api to make this a little bit\ncleaner so\nmaybe this is too much abstraction and\nmaybe we need to you know expose those\nthose internal apis a little bit deeper\num i'll talk about it a little bit more\nat the end as far as input from people\nbut we're very much so looking for input\nand seeing what kind of use cases people\nwant\nand make sure that we can capture this\ncleanly in an api that that for you know\n90 95 presents a very ergonomic api\nbut then covers feature complete for the\nrest of it maybe a little bit more\nverbosity but but still very powerful\num we'll look at in flight console\nthere's a brief preview of kind of how\nthis may be presented um\nkudos to the front end team they've\ndrawn up some some very beautiful things\nthat i've just butchered uh take little\nsnippets of them and throw them into my\nslides here\num but the idea is that if you have a\nlot of workflows with signals we don't\nwant those to get lost i think it'd be\nvery easy to say hey you know you don't\nknow what's running or what's blocked\nand then those would time out and fail\num so right on the project page in\nflight console there\nat the top you can have this paused\nexpandable\nbutton here\nwhere that would fill out here are all\nof the tasks or all the workflows that\nare actively being paused\nand waiting for a signal to be sent to\nthem\nprobably can't see it but in the right\none where it's expanded there's a little\nbit of a play button right here um and\nwe'll talk more about that in the next\nslide\nwhere\nsimilarly in the note and task view\nyou'll be able to dive into a specific\ntask and say okay this task is paused um\nin that upper upper graphic\nuh and then there'd be a description\nthat would say hey pause down signal\nwith whatever the signal id is waiting\nfor type\nwhatever the type is and we see in this\none that this is actually a sub workflow\nso that uh label of being paused can\npropagate up to this to the sub workflow\non top because overall that sub workflow\nis being paused as well\num and you have this similar button that\nis a resume button there on the right uh\nwhen you click that you would get\nsomething like a pop-up that would say\nokay here is the task that's being\nweighted on um go ahead and enter an\ninput value\nuh and then you can send a signal to the\nback end through the ui that way\nand\nprogress with the execution\nuh so quick road map for this work the\nback-end implementation is is functional\nidl\nstandard lid propeller admin there's\nthere's prs out for all of that so\ni kind of snuck away from doing a demo\nhere it would be a lot of terminals that\nmay not be terribly interesting to\npeople so something a little more\ngraphic i think was a little bit better\num the ui work is in active progress i\nthink these these\nthese previews look fantastic i think it\ncaptures the idea well\num and presents it in kind of an\nergonomic function\nand then the flight kit api like i said\nwe've been going back and forth with\nkind of what is the best way to do that\nand and for me it's sometimes difficult\nbeing a mainly back-end guy that maybe i\ndon't have the uh the experience um on\nhow people are going to use this or what\npeople want out of this api so i would\nlove kind of any community input we can\nget the rfc is a great place to to make\ncomments on that um otherwise if you\nwant to talk about it in future\ndiscussions a little bit\num\nbut yeah we would love to discuss this a\nlittle bit further\num\nquestions\nif we have time\nwe do for one\nif nobody has a question i have a\nquestion for you dan so where do you see\nthis uh in in the future in a year from\nnow i mean what do you hope people are\ngoing to do with it and maybe tell us a\nlittle bit more about the vision beyond\nthat\nsure sure absolutely um you know like i\nsaid\nthe main reason for implement this has\nbeen a feature that i think people have\nbeen looking for for quite a few years\nif you look at the actual issue for it i\nthink it's like three years old or\nsomething like that\num but but just the human in the loop\ntasks just being able to approve data or\nor passing parameters is very easy um\nand very useful but uh moving forward we\ntried to implement this in a way that\nyou know with this gate node that is\ncontingent on a condition\num as part of the flight ecosystem we\ncan integrate you know different\nconditions into that\nand hopefully kind of build out some\nmore robust functionality in this gate\nmode than just sleeps and signals um\nthere are conceivably a number of\ndifferent use cases that might be useful\nfor for blocking a workflow\ngreat awesome so wonderful i think it's\ngood to have the future discussions on\nthe slack channel so i would just say\nwhoever sees this uh watch this on over\nsocial media engage with then\ni think there's um\na lot\nto dig in and i'm looking forward to\ngetting this implemented and rolled out\ncompletely"
    },
    {
        "title": "Flyte Airflow Provider",
        "transcript": "all right uh the last one uh we have one\nmore presentation and that's amita and\nuh she's been cooking up something\ninteresting um for a while now uh we've\nbeen\nwaiting for this to be merged um so this\nis\nthe flight airflow provider\ntake it on\nyep thank you\num let me share my screen first\nall right\nhey everyone my name is samhita i'm here\ntoday to talk about the flight airflow\nprovider\nso um airflow's typical use case is to\nconstruct etl pipelines in the case of\nmachine learning pipelines we kind of\nneed more powers to facilitate building\nand deploying the machine learning\nmodels\nuh which airflow doesn't provide or\nisn't uh you know efficient enough but\nflight is and\nit also has all the required features uh\nbecause flight\nhelps run resource intensive tasks it\nhelps construct dynamic tags uh manage\ncustom dependencies and also run\ndistributed training jobs and you know\nthere are more features\nso uh the flight airflow provider with\nthe flight airflow provider one could\nconstruct etl pipelines in airflow and\nthen machine learning pipelines in\nflight and thereby trigger the machine\nlearning pipelines or in general flight\npipelines from within airflow so this\nthat's the typical use case of this\nprovider and uh we think this is going\nto be helpful to the airflow users who\nwant to leverage the best of both worlds\nairflow and flight\nif you want a deeper dive into the\ndifferences between airflow and flight i\nrecommend you to read this blog post\nwherein the author talked about airflow\nand flight in detail compared the two\nand also has included some example use\ncases to uh\nyou know to prove the differences\nand now let me just uh give a brief\noverview of the\nprovider the design of the provider so\nthis is the repository where the\nprovider is hosted\nand this is how you'll have to install\nthe provider it's just flip install\nairflow provider flight\nand these are all the configuration\nparameters that you can use to initiate\na connection from airflow to flight so\nhost is the required argument and the\nothers are optional there's also extra\nwherein you can pass a json dictionary\nwith all of these keys the\naccepted keys\nand yeah these are the two modules that\nare available in this provider one is\nflight operator which is the one that\nyou'll have to use to trigger a flight\nexecution be it a task execution or a\nworkflow execution so in here you'll\nhave to give the flight connection id\nwhich is the id that pertains to the\nconfiguration parameters you give here\nand you will also have to give the\nlaunch plan name or the task name\nis how you can import flight operator\ninto your tag and then there's flight\nsensor so if at all you need to wait for\nan execution to complete you can use\nflight sensor and this is how you can\nimport sensor into your dag\nnow let's just look at a couple of\nexamples to understand the code syntax\nbetter\nso here i have initialized an airflow\ndac\nwithin the airflow diag i have again\ninitialized flight operator which is an\nairflow task\nand in there i have se i'm sending task\nid which is a unique identifier to the\ntask and then there's flight connection\nid project domain launch plan name\nservice account and so on so when i run\nthis tag what happens is uh airflow\ntriggers a flight execution at the\nflight end it triggers a flight\nexecution and yeah that's pretty much it\nit just triggers a flight execution and\nthen it succeeds but what if you want to\nwait for an execution to come\nsorry can you hear me\nwe're losing you a little bit tomita\ni think my internet connection is flicky\nyeah it's okay now\nyeah yeah\nokay so in here uh this is an example\nwhere if at all you want to wait for an\nexecution to complete then you can\ninitialize flight sensor\nso uh in flight sensor you can send task\nid the execution name the execution you\nwant to wait on and then there's project\ndoom\nand flight sensor by default post the\nexecution\nevery 60 seconds so that's the default\nuh number that's the default number and\nuh there's also um\nlike what what if you wanna say run on\nrun an execution after the sensor you\nwanna say that you you wanna define that\norder uh\ncan you hear me i'm sorry\nwe can we can\nokay okay all right\nuh so if at all you want to define the\norder of the execution then you can use\nthis operator and this is how you can\ndefine the dependency between the tasks\nand what if you want to wait for an\nexecution a long running execution say\nthe execution is going to take a day or\nmore than a day in such a case you can\nset the mode to reschedule so\nthe default mode a spoke uh but if at\nall you want to wait for a long running\nexecution make sure to set the motor\nreschedule i\nalso increase the interval so here i\nhave increased poke interval to five\nminutes so this takes the execution\nevery five minutes and i have also set\ntimeout and software these are the\nadditional parameters that you can give\nand yeah\nthese are the examples and this is the\nexample that i'm going to run now this\nexample basically imports nyc taxi data\nfrom s3 and then uploads it to craigdb\nwhich is a distributed sql data\nwe lost you again the uh s3 data which\nfetch the s3 data and then uh\nuh yeah and then they upload or insert\nthe data into createdb\nuh here i have initialized the dag again\nand then i'm calling all the\naforementioned functions\nand finally i have initialized a flight\noperator to call the flight execution\nafter the data is inserted into createdb\nso here i am\ncalling this particular workflow and i'm\nalso sending an input and i'm i'm asking\nairflow to execute this flight operator\nonly after inserting the data into\ncreativity and this is the flight\nworkflow in here i have defined a sql\nalchemy tasks to fetch the data from\ncreatedb and then uh\ntrain a machine learning model\nand then finally generate the\npredictions so that's that's the typical\nflow\nflow core\nwhich i'm using in conjunction with the\nairflow code so here i have\ni have included all of these libraries\nnamely cyclic learn flight provider and\npostgres provider\nand\nthis is the uh repository this is the\nyou know the directory structure that\ni've used uh ignore all the other\ndirectories so they pertain to the\nprovider just consider demo demo for now\nin here i have included all my dags\nunder the dac folder that's folder and\nthen there's\nrequirements.txt which has all the\nrequirements so yeah that's pretty much\nit and before i run the code i just want\nto go through all the commands that i've\nrun to set the airflow environment up\nfirstly i have downloaded the astro cli\nand then i created a new project\ndirectory and run astro dev in it to\ninitialize a new astro project and then\ni've written my tags updated\nrequirements.txt\nand then run astro dev start to build\nthe containers and run\ni see after i i've run this command so\nthis is the final ui and nyc taxi is the\nworkflow that i want to trigger now let\nme just trigger that\nall right so it's triggered meanwhile\nlet me just go through the connections\nuh\ni have set so this connection basically\nconnects\nuh to fly from airflow to flight so\nthat's the connection i've set up\nuh here the connection type is flight\ni've set the connection type to flight\nand i've said host login password and\nextra so the in extra i've said\nautomotive client credentials for\npeople who are new to airflow the\npassword is uh available but it isn't\nvisible on the ui\nso yeah and i have also created a\nconnection from airflow to createdb\nin here i've set the connection type to\npostgres i've set the host uh i'm using\ncreatedb cloud so this is the host and\nthen\nthis is the login i have i also have a\npassword and i've said ssl mode to\nrequire\nnow let me just go to tags\nyeah\nso the execution's done\nit\nsorry\nyep so it runs successfully\nand now let me just go to the flight\nconsole to see if that's\nthat run\nyour network is really struggling i\nthink\ni don't know\nit's kind of unstable now\nyeah so this execution got triggered\nand it runs successfully\nare you still there\nnow let me just trigger the sensor\nexample\nsamita you might want to turn off your\ncamera\nokay\ni think you can still share your screen\nwithout the camera on it might save a\nlittle bit\nyeah yeah\nall right um\nso let me just trigger the sensor\nexample now\nso the task are triggered let me go to\nthe flight console again\nyeah so\nit's the running state and let me just\nterminate this task this execution to\nsee if that event is propagated back to\nairflow so i'll just terminate it\nand let me go back to this now this is\ngonna take about a minute or so because\nthe polling happens every 60 seconds\ni also wanted to mention that if at all\nyou terminate the execution at the\nairflow end after the execution is\ntriggered\nand before the task succeeds then that\nevent would be propagated to flight and\nthat error would pop up so\nlet me just refresh it\nall right\nso i'll show you the error message so\nthis is how the error message looks like\nif at all you terminate the execution at\nthe air flow end and let me just go back\nto the\nperipheral view so yeah this failed\nbecause i aborted it\nat the flight end let me just go to the\nlog\nand yeah we can see the error message\nthe execution got aborted\nyeah but it's about it i guess yeah\nthat's about it\nif anybody has any questions you can\nshoot them now or you can post your\nquestions later on slack\nthank you\nit's fantastic i think uh\nprobably samitha did not even expensive\nif you have legacy pipelines or any\npipelines your entire data team is still\nusing\nairflow you can start using flight with\nthis today uh you can\nyou can basically go piecemeal like use\nwhatever you want from flight and the\ncool part is you can use flight as a\ntask orchestrator alone completely so\nuh\nyou don't have to run workflows you can\njust write pass so you can run it for\nspark and now with ray and das and all\nthese things that are coming in flight\nwith a singular interface one operator\nto get all this\nto go\nhopefully this is useful and helpful to\nmany other people\ni know lyft uses this operator for all\nour full art their legacy\npipelines\nall right\nokay if there are no more questions i\nthink we are at time and\nhopefully this was a helpful meeting it\nwas\nreally cool to see all the work that's\nhappening\nand thank you folks for joining in"
    },
    {
        "title": "PyFlyte Register",
        "transcript": "talking now we'll go on to demos and\npresentation i want to hand over to e\nwho will talk about my flight register\nand the like\nuh sure\num i will\nshare my screen\neveryone can see this right\num sure yeah so this is a quick little\npresentation uh just one slide\num so i hope i'm assuming everyone here\nis familiar with the pi flight command\nthis is the python command that\nwill do the parsing of user code and\ncompiling them effectively into the\nflight entities that we all know and\nlove\nthere are two commands that you're\nprobably familiar with one sub commands\none is package this operates on your\nentire repo\nand produces the\nuh like a flight tgz file that flight\nctl can then use to ship off to admin so\nit's kind of like a two-step process\num and is meant for\nuh basically like pretty like large\nfull repos\nof flight workloads\num\nand we recently introduced a command\ncalled run that is starting our kind of\nattempt or journey on bridging the gap\nbetween a more iterative experience\nversus like a production workflow get\nops type experience so it only handles\nsingle files\num it\nunlike package it does use the network\nconnection to talk directly to admin\nand we added some more\nbetter naming conventions\nand we've we personally internally have\nfound it very useful because it kind of\ncombines all the steps into one thing\num like you just run one command and\nthen you get a little link and you can\nsee it immediately\num so it's it's super helpful for us uh\ninternally for testing and it's super\nhelpful if you're just starting out so\nwe wanted to combine those two\nconcepts into a new thing called\nregister this will work on again light\npackage on the entire repo\nbut like run it will also handle the\nnetwork\nuh it has the same naming conventions\nand then it will it also does fast\nregister\num\nby default the non-fast version of it is\ncoming\nand as always you can still rely on\ncustom images\num\nyeah so just very quick demo on this so\nyou'll see we're in the\nuh\nbasics part of the\nof the cookbook examples you can just\ngive it a couple files or you can give\nan entire folder\num and specify all the\nthe arguments that you typically would\nspecify on\nflight\nctl like the runtime parameters\neffectively\num there's a couple things like\ncurrently it produces this thing which\nwe should probably clean up uh this is\nthe fast register\nzip um but then you will see that we we\nshould have\nregistered\nuh the new\nthe new workflow and all the subsequent\ntasks\num in in whatever files or folders that\nyou specified on the command line\nand it names it with the full\nit does the same thing as run it walks\nup the tree\nto find a first\nfolder that doesn't have an init file so\nthis doesn't work if you use namespace\npackages\num but so\nthis gets\nthe name expanded\nyeah that's it any questions\nhey i had one question so what happens\nif uh\nnow with register does it also handle\nthe case where your multi-file\nuh\nlike you showed an example where you\nadded uh\nfolders.pi or files are five\nwhat is that files that pi has a\ndependency on some other python\nfiles does it automatically discover\nthat\nuh yes\neverything should everything should get\nzipped up\nokay\nfantastic\nuh it's assuming\nthere are some limitations in it\neverything gets stepped up under if it's\nunder the the same root directory\num it does not traverse your entire site\nfactors obviously\nyeah\nand what are the goals for documenting\nthis and sharing this with in general\nin our\nmethod\nit will be documented\nokay cool"
    },
    {
        "title": "Flyte Community Update 016 - June 28 2022",
        "transcript": "yeah if anybody wants to introduce\nthemselves and tell us where you're\ndialing it from that would be\nawesome\nnobody\nthank you Josh I'm uh I'm in Kirkland\nnear Seattle\nit's 9 A.M Pacific time here today\nI can go I am jeeve I work at freenom\nand I'm dialing in from the Bay Area\nnice to meet all of you\nfantastic same here thank you\nuh sure I'm Rahul I'm dialing in from\nCambridge Massachusetts uh machine\nlearning engineer at theorem nice to\nnice to see you all\nlet's save your own thank you now we\nhave people from different places that's\ngood\nanybody else\nI saw Bernard\nI'm gonna hide you on this fight hey why\nnot\nforeign\nwho else wants to go anybody else\ngood morning thank you\nthis is Mike I just joined uh the\ncommunity I don't know Friday just a\ncouple days ago\num yes just uh we're a company uh does\nML model monitoring um and just looking\nat integration so kind of keen to just\nlearn more about\nthe platform and kind of what you guys\nare up to so it's pretty cool\nthanks for Welcome to the community Mike\nand thank you for joining in we you know\nthis is uh just to remind this is just\nan open Community think that we do every\nother week or Tuesday morning 9 A.M\nPacific time so we are open to changing\nthe time so you know\nI we we do post all the videos on\nYouTube and many people prefer to watch\nthem on YouTube but I I do prefer you\nknow active participation where people\ncan come in it really helps us figure\nout uh what are the right next steps\nwhat what we can focus on as a community\nand so on\nso again as a call I'm gonna say this\nout probably like a broken record you\nguys want to have this in a different\ntime zone at different times please\nrecommend\num there's no easy way to do this but\nyou can drop a message on slack just in\ngeneral or DM me\num and we would love to accommodate\nbecause I know so many people are in\nAsia and\nvarious countries\nall right I think we can get started\nit's 905. let's do that uh Sandra do you\nwanna\npeople I I guess I'll start it but we\naren't\nit's uh June 28 2022 and\nthank you everybody for joining in uh\nfor the flight Community sink\ntoday's agenda as usual is we'll go\nthrough a few Community highlights\num uh we'll talk about uh you know again\nUnion ml as the ecosystem project but\nthis is a part of our flights ecosystem\nso any project that wants to be included\nhere please come in write about it share\nyour information if you would love to do\nthat\num you know you can give updates and\nthis is your area\nfrom a roadmap point of view we'll go\nthrough some of the exciting features\nthat we're working on uh just a high\nlevel overview and couple of them will\nDeep dive a little bit there are a\ncouple rfcs that are open so I wanted to\nbring that up I'm not including all the\nrfcs so pardon me if I'm not introduced\nsmoker RFC so I've tried to\nand then eventually we have two demos\nand both of these uh contributions uh\nWithin\nflight gate itself or an auxiliary\nproject\nall right from Community highlights so\nthe community has been growing and thank\nyou again we are extremely grateful for\nall of you guys trying our flight\ngiving it a shot and improving it every\nday by contributing to it uh every type\nof contribution is extremely extremely\nimportant whether it's documentation\ncode\nblog\ntweets even right\num one of our biggest uh some Community\nsome members have really expressed that\nthe biggest problem with flight is you\nknow lack of awareness and we we really\nwant to help you know the community to\nimprove on that\nso three things here well we we've been\ndoing contributors for the month and for\nthis month uh I guess it shouldn't be a\nsurprise Matthew Griffin\nuh with the awesome pterodactyl skk and\nthe dino node.jsck\nuh David uh for his blog about uh\nPipeline and\num a documentation about plugins and\nflights and also a couple contributions\nto flight CTL and so on so David has\nbeen busy\num and thank you David for your\noutstanding work I I know he cannot be\nhere at this point because he worked\nfrom Japan I think\nuh Robert Everson from Lyft and uh he's\nbeen adding support for arm architecture\nso the goal is that\nwhen you run workflows you could Target\nuh alternate Hardware like arm uh and\nbecause of Dockers multi-architecture\nsupport it should be pretty seamless and\nflights natively integrating it so that\nwould you know uh on many of the cloud\nproviders either ways cost effective\nmachines and it might be useful for many\nmany use cases\num today we have and we kind of met uh\none active contributor uh Bernard he's\nactually he quickly told us for a year\nand a half when I first met him\num he's\nhe's been a fantastic uh contributor you\nknow take spreading helping us spread\nthe word adding great features\nand overall you know just what I love\nabout this community so Bernard why\ndon't you uh share something\nyeah uh hi again\num my name is Bernard I'm\ncoming originally from from black circle\ncompany in Austria that there's a large\nscale geospatial data processing\num it was the company that came backing\nMicrosoft flight simulator and they have\nbeen on uh I think also the open source\nsync at some point\num and about a year ago we were facing\nthe the position for going from our\nin-house workflow engine to something\nthat someone else could manage because\njust a lift of having your own\nwork finding was a bit too much\nand that's when we found flight and and\nsince then it has been quite the journey\nI we've tried to break it in any way\npossible I would say um especially using\nlarge Workforce or use case was mostly\ncentered around big fan arts of one huge\nworkflow and again a big thank you to\nall the the great guys if like you've\nhelped us a lot um\nand we've like had lots of chats and we\nmade everything work which is really\nnice\num\nI'm now a pajama a company that tries to\nbring a bit more transparency to carbon\ncredits and just through the whole\nvoluntary cover Market\nuh doing a very similar thing of\ngeospatial dictator processing of remote\nsensing data and yeah we're also using\nflight there\nis\na part of what uh what I wanted to to\ngive back and also a bit of what we\nmight want to use a pajama I'm currently\nworking at a desk plugin so so that's\nwhat I'm most interested in because\num some of our data scientists and the\nwhole\nthe whole pengu Community\num they're setting a lot\nonto dusk\num yeah and that's what I'm doing right\nnow so there should be a back-end\nplaying coming it is taking a bit of\ntime because I actually have a day job\nand it's not my main part because we\nhave a great fight um for keep plugin\nworking for dusk but\nthere should be something up in the\nupcoming one\nthat's awesome uh but I had a question\nfor you so uh what's been\nlike why did you decide to choose flight\nand what is the one thing that you would\nsay\nis you know different or is good and\nhelps you in your day job or you or your\nteam and what's one thing that we should\nimprove on\num\nso the reason why we chose flight is\nbecause it seemed quite mature in the\nway it does scheduling we really liked\nthe type safety bits of things and we\nwere already relying on kubernetes so we\nhad like kubernetes in-house knowledge\nand then all that kind of stuff\num\nyeah we like that it relates to protobuf\nand just that the general architecture\nof it\num\nso far it has also been really really\nstable to us so so when we could get\nthings to work\num they were like on the flight side of\nthings and scheduling type of thing of\nthings if things don't go crazy wide on\nthe task on a workflow that was great\num that brings me to the second bit\nthat's probably the things we've\nstruggled the most with is\nthat it's sometimes hard to it's getting\nit has been like the past year a lot of\nthings have improved a lot um especially\nin the beginning the documentation was a\nbit sparse so we dug around a lot that\nhas improved greatly\num\nand sometimes also yeah regressions in\nin like new deployments but it also has\nbeen like we see that it has major\nimprovements in in every direction there\nbut probably that documentation and also\njust\nit's hard to find answers on stake\noverflow and things like that\nso especially it's a bit hard\nthat is absolutely Fair well that's I\nthink documentation has been an area\nif you if you know the wideness of the\nproject\nuh if you guys have a better structure\nfor documentation please recommend uh if\nyou see we have tried a bunch of\ndifferent things we keep on moving it\naround our search we're one of the\nproblems with flight documentation again\nno justification but like it's too many\ntrucks too many different Reapers right\nso sometimes I just wonder if you just\nmake it mono repo and maybe that's a\ncommunity question should we just cook\nit into a monore and there are lots of\nproblems with monolith history and\ncommits and like but there are also\nbenefits of model people so\nuh but that's that's fantastic good to\nhear yeah we've also I think the\ncommunity and uh my my team at Union AI\nhas been very very focused on making\nsure that every release is\nis better uh and we try to minimize any\nkind of issues but as we migrated\ntowards 1.0 there were some issues that\nhappened\num and yeah hopefully very very less so\num our goal is that you should be able\nto deploy the back end and flight kick\nindependent that's really the goal of\nthis project okay make it really fast\nfor platform teams and user teams to\nkeep on updating their software\nthank you Bernard that's uh completely\nand I'm looking forward to the dashboard\nagain uh you you'll see something in the\nfight deck today that we'll talk about\nthat that uh some of the other folks are\nworking on and it kind of correlates\nwith that but I'm really looking forward\nto it\nokay there's a question for Bernard in\nthe chat oh sorry I missed it um\noh that's whether it's open trust so\nit's like the flight plugin uh it's not\nopen source yet the\nif I get one the backend plugin that\nthere is that they work in Focus there\nis open source um the reason why the\nflight kit plugin is not open source at\nthe moment is because we have some\nin-house additions to it um that are\nreally specific to the tooling that we\nuse what I would like to do is if it's\nof any value I'm happy to to open source\nit I'm also from the companies that\nwe're happy to Source it we want to\ncontribute back\num\nprobably we would like to give it a\nmonth or so more just to see how it\nperforms um because we're currently\nramping up the whole flight plus task uh\nstack and if that works we're happy to\nput something out it might be like a bit\nof a reduced version that our parts will\nbe retracted but then the general is\nactually should stay\nyeah anybody uh open to contact or you\nknow collaborating with them that would\nbe awesome because you know when there\nis another set of eyes it usually\nbecomes easier to to get the life you\nknow it's mostly about the API it's\nobvious about the API\nI did have a couple of questions but I\ncan reach out offline I don't want to\nderail the meetings no no go for it you\nknow it helps a lot a lot more people oh\nyeah well the I mean the the simple\nquestion is like is the flight kit\nplug-in handling the management of the\ncluster and the auto scaling\nuh yesterday so at the moment it's using\nuh the flight the task operator that\ncame out recently\num and we're also quite closely in touch\nwith the with I think of Tomlinson I\nthink is his name remember correctly\num\nwho is managing the the task operator\nand it's the operator is a bit of an\nexperimental stage so it's not um 100\nfeature complete\nbut it's good enough for us and so the\nflight keep plugin actually uses the\noperator to to manage the cluster so the\nwhole life cycle was managed to\nreoperate gotcha and then does that mean\nthat there is persistence across like\ntask retries so like if\num you know like the the the driver task\ngot preempted or something like that I\ngot interrupted when it starts back up\non another uh process like does it reuse\nthe same Das cluster or like does it\nstart from scratch\nit would start from stretch okay gotcha\nokay cool\ngood to know yeah that's uh that's uh\nthose are those are pretty important I\nthink like that's kind of where the the\nflight\num like a flight plug-in would really\nreally help right on top of like cool\nthank you fantastic I think yeah I I'll\nbe talking about some roadmap items and\nand I think this plays really really\nwell together with what we're doing so\nfantastic great to hear\nall right uh Sandra next slide please\num\nso uh another uh important thing and I\nthink Ryan Kim couldn't join but I've\nseen him active on the channel he's been\nhelping people and like doing some\nfantastic work uh they're uh I think he\nworked for the company called Q raft in\nKorea it's uh\nit's a hedge fund from Korea and uh yeah\nthey wrote A Blog in Korean uh that\nexplains my korean's a little weak so I\ncouldn't read it all but\num you know if anybody is willing to\ntranslate or uh is interested in taking\na route uh look at the Korean blog\nplease take a look\nand thank you Ryan\num I highly appreciate this\nas usual we still have office hours\nwe're doing them everywhere let's say 7\nA.M 1 30 p.m and 9 00 PM Pacific I think\nthe 9 pm one doesn't have as much\nattendance nowadays because I think the\n131 has taken or most of the work from\nNorth America\nuh please let us know if the 9 pm one is\nuseful if you should continue having it\nI'll be there most of the times I don't\nmind having a chat so\nall right so we actually were going to\ndo the arms\num talk today but for some reason Robert\nhad to postpone it and so we push it to\nnext uh saying it's other things have\nbeen pushed forward as well\nbut\num so that's coming up on July 12th\nwe'll also talk about signal nodes uh\nthat's Dan working on something really\ncool uh we think it's going to be a\nfeature that will help a lot of teams\nyou know we're really really amazing uh\nnew you know products for their\ncustomers\nand\nokay I want to hand over to\nNiels who's hopefully on the call uh to\ntalk about yep\nyep uh thanks Caitlin hi everyone I'm\nNiels Ben thielen\num and yeah I'll just go through really\nquickly give you some updates on the\nunion ml flight ecosystem project\nso for those of you who don't know what\nunion ml is it's a higher level machine\nlearning specific framework built on top\nof flight\nand it's a way for us to\nprovide you know some nice conveniences\non top of flight that help you to write\nmachine learning models and deploy them\nmuch more easily it's a little bit more\nopinionated than flight\num but yeah this is my first hand at the\nflight meme propaganda so please pip\ninstall Union ml try it out if you're\ninterested\num\nthere are links down here uh for the\nwebsite the documentation the repo but\njust to give you a little bit of an\noutline on the\num\nthe roadmap\nwe have a few things here that we want\nto\num build out so we're prioritizing the\nthe prediction and serving story uh\nbehind Union ml so adding support for\nstreaming batch prediction and\nadditional serving Integrations\num\nI see mother attendant asks is it\navailable in Condor Forge yes I'll get\nto that in a second\n[Music]\num\nthe next the following release will\nfocus on model tuning and experiment\ntracking\nand then\nimproving Opera interoperability with\nflight\ncurrently you can use Union ml with\nflight quite well but\num I think there is a lot more there we\ncan do to make it more seamless\nand then finally we're going to double\ndown on model observability data quality\nand kind of ux around Jupiter\num and the next slide should be my last\none cool\nso if you're interested in contributing\nuh if you go to the union ml read me\nread the docs site there's a\ncontributing guide where you can\num you know get started in in various\nways\num from lightweight to heavyweight\num like improving docs to filing bugs\nand feature requests and pull requests\non the bottom there you can see that\nthere's a link to the roadmap and\num we're planning on starting uh OSS\nplanning meeting so it'll be different\nfrom this one and that it'll like\nliterally be a develop developer meeting\nwe'll go through the roadmap by default\nit'll be myself and a few of the folks\nin the union AI team but we're going to\nexperiment with just having an open\nstanding meeting with uh that's open to\nthe public\num\nso you can participate you can give your\nfeedback you can ask for features there\nyou can interact with us there\nand if you want to Fast Track a feature\non the roadmap\nyou can check them out in the the issues\nlink on the union ml repo you can join\nus for those planning meetings the first\none is going to be in July 6th and then\nyou know as Shia LaBeouf says you can\njust do it uh just make a PR if you want\nto implement something yourself and\nyou're excited about a feature\nuh oh sorry very uh the very last thing\nI'll mention here is a union ml\ncontributor uh sugatu array he's a PhD\nphysics PhD candidate at the University\nof wisconsin-milwaukee\nhe literally just made condo Forge\npackages for Union ml flight kit and\nflight IDL so shout out to sugato maybe\nwe'll we'll have him on here next time\nif he's available\nabsolutely I think this deserves uh this\nhas been one of the most asked feature\nat least for Flight 2 uh the condo Forge\nand I think for Union ml fantastic we\nwould definitely want to add it as a\ncontributor of the month so thank you\nsocato\nand I think this answer is madhur's\nquestion\nall right\num\nroadmap uh so what we're doing is like\nyou know we always try to do roadmapses\ntechnical dive deeper into the roadmap\nbecause there are a couple very\ninteresting features and we did uh you\nknow some discussions on task so I'm\ngonna just continue on that line\nso one of the motivations and I hope all\nof your folks have tried the spark\nintegration if you're not definitely try\nit from a ux perspective\num and and some people have said this I\nlike I like the spark integration right\nbut I uh you know simply like the Spy\nspot code and try to handle the cluster\nmanagement\nuh but many people say I do not want to\nwrite twice I want to write python uh\ngood and so Ray or desks are much better\nfits in that scenario\num so what do we do uh what what\nhappened in the spark plugin in a\ncentury right code flight will bring up\nthe cluster and the way it brings up the\ncluster is to the spark operator\nor it could use just simple spots a bit\nuh to bring up the driver and the\nsecurity parts and we are doing the same\nfor Rey\num and this is in collaboration with\nSpotify and uh pythons and Shopify where\nessentially you can write a task with\nRay code in there\num and flight will bring up the right\ncluster now this has another interesting\nproperty if you run it locally\nit will run it locally in a single node\non your local laptop if you set one\nenvironment variable that's our goal um\nthen it will automatically connect to a\nlaboratory cluster and send all your\nworkloads to that remotely cluster and\nflight will handle uh things like uh\nrestarting retrying again uh multiple\nRay clusters across different executions\num and even but the actual management of\nthe ray cluster which is you know\nincreasing Auto scaling and so on is\nhandled by kubrey which is a fantastic\noperator written level like that and\nthere's an rfce in progress for this so\ndefinitely click on that and check\nthrough\nnext slide please\nbut there's a problem right like now you\nhave one Ray cluster and uh for every\nview right job you have to start a new\nrate filter that's that's an overhead uh\nso let's say for example we're doing\nsome data preprocessing and now you want\nto do training on it and you want them\nto be two separate steps that's the\nideal workflow uh but uh Ray uses this\nthing called the plasma store which is\nwhich is a common\nout-of-core storage which is actually\none of the powers for it right um and so\nif you tear down the ray cluster and you\nbring it back up the plasma store is\ncompletely torn down\nso what if we could reuse the plasma\nstore across multiple executions\nand this is what we wanted to solve and\nTackle within the ray integration uh or\nyou know this is a general concept that\nwe want to bring into flight is\nessentially flight can handle resources\nat a workflow level and it can bring up\nthese clusters and pass them down to\nevery task and it can be done implicitly\nbased on name matching so for example on\nthe right right hand side if you see the\nexample\nthe WF workflow has resources R1 and R2\nthey could be Ray clusters or they could\nbe spark clusters eventually or Das\nclusters and so on uh and then as D1\nrequires array cluster that's implicitly\npassed in\num it's based on name matching and if\nthere is a sub workflow that needs a\ncluster that will be implicitly passed\nand if you don't want to implicitly pass\nthen you can also say don't implicit\ndevice so this allows us to create\nmultiple different ways of managing Ray\nclusters and the ray cluster lifetime in\nthis case is bound to the lifetime of\nthe workflow now an interesting question\nis what happened to the rate cluster\ncrashes flight will manage to uh bring\nit back up\nand uh but there are some problems that\nwe have heard from Community folks about\nusing\nrunning multiple tasks on the same or\nreusing requesters so that's why we have\nput this in phase two but we think this\nidea has a huge Merit for all different\ntypes of big data or uh compute\nintensive workloads and this will be\navailable in the core\nand this should be reviewable across\ntasks\nand we just heard from Bernard that he's\nworking on desk\num the same sort of phases will be\nshould be available industrial venture\nnext one\na second uh big feature that we're\nworking on is signals and sleep and I\nthink Dan will cover this in more detail\nbut I just wanted to give a little bit\nof a teaser\num so a question that you often ask is\num let's say you have a workflow and you\nwant to pause for an external input\nmaybe it's manual or a system provided\ninput type think of the cases where you\nwant to have an approval a very easy way\nto you know visualize this is expense\nprocess so you want to do expensive and\nyou want to stop in between for\narbitrary amounts of time till somebody\nopens and approves your expenses maybe\nwrite a note maybe send some other data\nor changes the value and then it goes on\nso\nuh so that's one type of an example\nanother one is like maybe you want a\npool gate for doing you know uh\ndeploying your Machinery models uh to\nproduction so for example one thing that\nwe have seen in the past is uh\npopulations get experiment where people\nwill deploy a model for X percentage and\nthen wait for\nsomebody to actually approve before it\ngoes to 100 other parts that deployment\nand then you may want to pause uh for an\narbitrary period of time between\nconsecutive steps so it's like startup\nworkflow and then sleep for hours\num and and all of this needs a lot of UI\ninteraction like this makes sense only\nwhen it kind of works as a unit\num so you should be able to provide\ninputs to the UI or the web API\nso a solution for this is we are going\nto add gate nodes to workflows\num and gate nodes allow you to can be\nopened by external signals or they can\nbe timed and I'll give you an example of\nthis in the next slide\nso let's take an example of you know\ncreating a signal that has an external\ninput in in the right hand side top\nthat's one of the uh these are the\nproposals for the API and so you could\nintroduce a new uh special\ntask or flight dot weight which has a\ntimeout of course and you can say you're\nwaiting for a signal and it's a named\nsignal which can accept an arbitrary\ninput type\nnow this type could be uh\nnot in protect sorry this is the signal\ntype and it could still accept inputs\nthat can show in the UI and so on\nwhat experience here would be is that\nyou would go to flight remote if you go\nto the third example at the bottom uh\nyou go to flight remote and give it the\nexecution name give the signal name and\npress the value as a dictionary yeah\nand if that executions are active or\nrunning then it will get the signal and\nyou know unblock and support another\noption is just to introduce a new\ncontent just like sleep right\nflight.sleep for X number of hours and\nor minutes or seconds and we'll sweep\nfor that or pays even and you can sleep\nfor that period of time the persistence\nwithin flight should allow it to last\nfor as long as you want these workflows\ncan last forever and because of the\nimmutability guarantees a workflow will\nnot be uh you know changed in mid-air\nnext like this\nso how is this done this is done using\nuh\na new node type within slide for those\nwho are interested slight has a bunch of\nnode types uh one of them is a test mode\nthat runs your tasks one of them is\nBranch conditional node another one is\ndynamic uh some tasks that actually\nsplit into multiple tasks and another\none is sub workflow so nodes that have\nworkflows underneath you're going to add\none more which is called the gate node\nand this is explicit where whenever we\nusers use a flight kit site API or you\nknow in like a Java in all other\nlanguages it all transforms into one\nsingle representation which is called\nthe gate node\nand the gate nodes are they follow the\nsame data dependency or task dependency\npattern that you express in the code and\nand flight propeller when encounters a\ngate node waits for a signal and the\nsignal is actually sent through the\ncentral control panels like that\nnow this signal can be Central flight\nconsole so there's going to be work to\nbe done in Flight control that as an UI\nthat shows you there's an approval\nwaiting or you can put an input and that\nwill be included and of course\neverything in Flight is always through\nan API also so there is an API now you\ncould use like it robot or just use the\ngrpc restful API to trigger the signal\nand because the signals are stored in a\npersistent database all of this is\nrecoverable and everything is captured\nin that history with the auditory\nnext please\ncomment like subscribe on the issue and\nthe RFC uh there's a PR as well and the\npr is with the backend side and this is\nin progress so please please comment I\nthink this is going to be a\nuh we've seen much we've heard about\nmultiple use cases from multiple\ndifferent teams if you think this is\nuseful to you let us know\num if you think there are some other\nmodifications that we could do please\nlet us know we would love collaboration\non this thank you\nyeah and there's more uh of course I\nwanted to give a teaser on a couple of\nfeatures but we've we found uh that web\nAPI plugins that have\nextremely long synchronous runtimes uh\ncan take\ncan slow down uh flight so we are going\nto allow them to be on asynchronously\nand this allows us to scale to arbitrary\nnumber of web API plugins\num and and this was discovered by\nSpotify so thank you and we are working\nwith them\nuh also there's work going on on user\ndocumentation in the UI itself and of\ncourse flight tech was released in the\nlast few days if you have not checked it\nout please check it out it allows you to\nwrite custom\nrenderers\num so that if you are using let's say a\ndata frame and you want to show the\ndistribution of your data or capture the\ndistribution of your data you can do\nthat in a static way and it's always\nstored and shareable etc etc all the\nflight tech properties can be done by\nsimply adding one function so we highly\nrecommend\nadding flight deck and I think this is\none place where Community can really\nhelp so thank you"
    },
    {
        "title": "Flyte Community Update 015 - June 14 2022",
        "transcript": "all right hey so welcome to the flight\ncommunity sink today is tuesday june\n14th\nuh it's not as hot in seattle as it is\nin austin uh so i think we can say this\nfor sure um but we have a packed uh\ncalendar packed agenda my name is morten\nstein i'm your host\nand let's take a quick look at the\nagenda\nsanta can you thanks so we have uh\ncommunity highlights we added a new\nsegment called ecosystem projects we're\ngoing to talk about this we're going to\nhave unions uh niels\nspeak about union ml we're going to talk\nabout the roadmap and we have two\npresentations uh actually one demo and\none presentation today uh which i\nabsolutely look forward to if there's a\njavascript sdk prestation from matthew\nand then from our own eduardo we see\nflight decks so that's going to be\npretty interesting so let's kick it off\nwith the community highlights and we're\ngoing to start with a new segment in the\ncommunity highlights it's called get to\nknow you contributor\nand also your new slack member here on\nthe channel so we have two new members\nto the flight community there's george\nand there's mihai\nlet's\nintroduce george so that's going to be a\nif george is here george i think he is\nhere i hope so so george why don't you\ntell us\nwho you are and what got you here to the\nflight community and what you're trying\nlooking to get out of that community\nhere so please\noh thanks much for the introduction\nuh so um i'm george my other name is\nsteve george as you can see in the\nin the zoom section\nso yeah so i have a background in vet\npractice\nbut currently i'm\nshifting to\nmolecular medicine so\ni'm currently studying molecular biology\nat masters level\nand uh i'm much more invested in\nbioinformatics\nin uh solving biological problems using\nuh\nusing uh\nusing coding skills\nuh\nbasically python\nso currently i\nam an ambassador to the latch bio\nlatch bio is a system built on flight\nand uh it's it's my first introduction\nto knowing about flight and how the\nflight works\nand uh\nbecause we use the flight syntax to code\nuh to code for the tools that we upload\nat the latch by usdk\nthen\ni had to learn\nhow flight works for me to work well\nwith the large bio\nso besides working with the large bio my\nmajor interest in knowing about flight\nis because uh\ni wanted to build some of how to build\nsome tools\nbeyond large bio tools that that that\ndoes not depend on the large bio syntax\nuh i'm looking at tools that\nfocuses solely on the flight syntax i've\nread quite a lot about the flight\ndocumentation and i've understood how\nthe the workflows are being constructed\nand i can see it's uh\nit's a pretty much simple\nsyntax to use and i think i can\nwrite good workflows using the system\ni'm also interested in uh\nin in in next flow but next flow doesn't\nuse\nflight\nyeah so that's that's\nin general great fantastic so thanks a\nlot george so look we'll look forward to\nsee\nyour feedback in the slack community\nchannel and obviously\num see tools being built upon uh flight\nthat's gonna be fantastic so great\nthanks a lot for joining us so let's say\nhi to\nlet's say hi to mihai uh mihai please\nintroduce yourself and tell us what got\nyou uh join the flight community\nsure and thank you for having me my name\nis mihai todor and i'm a principal\nsoftware engineer at optum which is part\nof united health the big health\ninsurance consortium\nand my background is software\nengineering so i've been at this for\nclose to 15 years now\ni work on data streaming all things\nkubernetes\nbenthos is an open source go project\nthat i contribute to and\nas it for my work\nit lets you\nread data stream it through pipeline\nmanipulate the transformatory please and\nthen emit it\nuh and then you know you know like\nthings like monitoring the posts of\ninfrastructure cloud and on-prem etc so\nthis is kind of my day-to-day work\nuh but i do have an ambition of getting\ninto bioformatics and i would like to\nlearn as much as possible about\nimmunotherapy for cancer treatment aging\nepigenetics these are topics of high\ninterest to me\nand i found flight and i think\nnot sure if i was introduced to it\nthrough large bio or i also saw it in\nother places as well\nbut i would like to learn more about it\nand see if you know there's any\nopportunity to contribute to it i'm an\nopen source contributor i'm happy to\nlike\nuh\nweigh in my go experience and try to\nhelp in any way you know that would be a\ngood way of meeting other people in the\nin the bioinformatics space since i see\nthat latch uh that\nlarge pie exists and also flight seems\nto be a good alternative to um\nnext flow which yeah to me it's kind of\nhard to work with because it's like\ngroovy and i can't like\nif it's jvm i don't want to touch it\nyeah oh it's fantastic to have you so i\nthink it's pretty interesting uh and i\nfeel like the group in bioinformatics is\na really solid and strong group so you\nmight have to do a one special session\nfor just bioinformatics uh if there's\nenough interest we should set something\nup and maybe do a group meeting uh maybe\neven in person at some point but a\nvirtual first for just that topic so i\nthink that might be a good story yeah so\nmartin about that i also wanted to see\nif we should do like uh\nuh like sigs like the kubernetes style\nso that we basically have like specific\nfocus areas and like breakouts within\nthat surface area because\nyeah like we have like so varied set of\npeople i'm sure if like uh you know bio\nbioinformatics folks would be like oh\nwhat are we talking about in imaging and\nin autonomous vehicles and so on but\ni think we should break it out so that\npeople who are interested can you know\nget to know more of them within the\ncommunity that'll be awesome i think\nthat's great let's discuss it in the\nslack channel maybe we can uh start a\nconversation here with the team with the\ncommunity and then make decisions of\nthat i think that would be fantastic\ngreat awesome thanks a lot thanks mihai\nand let's go to the next one so blogs so\nthere was quite a lot of activity so we\nhad\num our own samita write a blog post for\nthe mlaps community\nit's titled the conversions of workflows\nbetween machine learning engineering\nsuper interesting topic uh\nwe always hear that mlabs is kind of\nlike the\ncombination of machine learning devops\nand data engineering semita so so tell\nus if we all know this why it's still so\ndifficult uh why do we still talk about\nthe convergence why is that a topic\nnot sure if samita is here\nso if she's not here then actually the\nanswers in the blog post\nso i think that's that's a a great blog\npost you wrote take a look at this\num it's on the mlaps community we have\nit here in the sliding we're going to\nshare also with the newsletter update so\nyou find the link um\nnow i'm just going to be careful and ask\nif if david is here david are you\njoining us today no david is in the\njapanese time zone so i don't think he\nwill be okay\nso if cleveland is not here so david has\nwritten a fantastic uh blog post about\npipelines flight versus kubeflow i think\nthis is a really really good blog post\nuh to to take a look at um it's really\nabout if you ask a few questions about\nif flight is a tool you should be\ninterested so it's really more for\npeople who haven't really considered\nflight and actually are more aware of\nkubeflow and kubeflow and i think that\nis a\nvery very important you know comparison\nto make and there's a lot of good\ntalking points in there and so we want\nto take the story further forward and we\nwant to make sure that david can join us\nin the community and actually get more\ninformation out to people who don't know\nabout flight so i think this is a really\nimportant article about it\nokay\nuh link is in there too office hours\nuh so we have our standard office hours\ni'm not going to say too much uh but we\nhave haytham katrina and katherine uh\nbeing available for you uh let me just\nask real quick catherine uh is your late\nnight mlab's office hour packed or do\nyou want to have see more people what's\nwhat's the status on your side there\nso i think it used to be way more packed\nrecently i haven't\nseen that many people there's one or two\nlike maybe you know folks in japan or so\non show up so i was actually considering\nif the timing is right or no you know if\nusually folks from gojek or somebody\nelse joins uh but\nit's not been packed for the last couple\nof times\nbut i'm sure the other ones have did so\nso katrina i'm just not sure if you're\nuh here but how about yours is that\npacked\num\nthere there's a few people to drop by so\nif you're in a us time zone kind of come\non over\nall right\nso i think it's it's a\ncall to action to join hatham's\nkatrina's and katherine's uh office\nhours i think it's always interesting\nwhen i'm in the office and i hear uh you\nknow the conversations i feel like uh\nthis is something that everybody could\nand should take advantage of it's really\ncool to get so close to uh and see what\nyou're doing and you know getting\ninsights and helping tips and tricks i\nthink it's awesome all right\nuh with that let's talk about the next\nupcoming sync so we have robert everson\nfrom lyft talk about added arm support\nso that's under\nstill in the works i think and uh\nprobably is gonna see the\nit's gonna be out there available in\njuly i think i spoke with robert a week\nor two weeks ago so i think it's going\nto be a very very interesting topic to\nsee if there's other topics other\nuh you know flight activities\nthe community wants to share\nplease hit me up on slack and we're\ngoing to add you we're going to chat\nwith you real quick and we'll see if we\ndo a demo or video presentation i think\nit's really cool to share your work and\nif even if you're just at the very\nbeginning you don't have a result\nanything say hey look this is where i'm\nat right now that's what i want to do\nwe would be very happy to share that\nwork so just uh hit me up on slack\nthe other stuff uh calendar invite\nyoutube i'm not going to talk about this\nyou see this you know this\nlet's move forward and talk about uh the\necosystem projects so\nwe basically started the section\necosystem to showcase solutions built on\nflight so we had already won uh which\nwas the tower the uh project uh case and\nyou had you presented this i think back\nin time with who was it who did the demo\njeeve\nyeah\nfrom free gnome yeah yeah that was\npretty cool and so that was kind of like\nthe first time i think we did an\necosystem presentation um and we thought\nit's a really really great concept to\nsee what is built on top of flight and\nhow can we grow this ecosystem and so\nthis time we we feel like today we're\ngoing to introduce and actually talk a\nlittle bit about union ml not too much\nwe have spoken about this in the past\nbut\nmlab's world was last week and we had\nour union ai\nengineer niels there so niels can you\njust tell us real quick you know\nwhat union ml is and uh where people\nfind more information why why most\nimportantly why they should care\nhey martin thanks uh just doing a mic\ncheck here\num\nyeah hi everyone i'm niels um\nyeah so we created union ml essentially\nbased on feedback around\nfrom our flight users saying hey i just\nwant to i just want to train a model\nand deploy it\nfor prediction somewhere\nand you know you can do this with flight\ntraining training your models and\nwhatnot\nbut at the end of the day you needed to\nuse other solutions to\ncreate some kind of prediction endpoint\nand so\nunion ml\nreally is this interface that tries to\nunify all the different frameworks for\ndata and ml work\nand you know the tagline is that it's a\nway an easy way of building and\ndeploying machine learning microservices\nso we're really trying to reframe the\nprocess of building machine learning\nmodels\num instead of\nas data pipelines or graphs as kind of\napps that where you\nimplement\ndeclaratively and also imperatively\ncertain\nfunctional entry points to key pieces of\nthe ml puzzle\nso don't want to go too much into this\nyou can look at the video that's\nembedded here\nyou can go to union.ai unionml to learn\na little bit more about it\nand the documentation is\nunionml.readthedocs.io\nthere's a bunch of information there\nthere's also a slack channel in our\nflight slack\ncalled\necosystem-union ml you can check that\nout there also\nask questions let us know if there are\nany issues there\nand yeah that should be your first entry\npoint into this project um super excited\nabout you know what\nfolks will build on top of it\nsee how uh\nyou know\nyou react to really what is um maybe a\nvery kind of different syntax\nthat then you might be used to\nawesome fantastic and are you looking\nfor more contributors\nyeah so this will be tbd but we will we\nare you know the short answer is yes\nagain reach out to us on slack and you\nknow if you're down to contribute\nwe're still shaping up our issues and\nroad map\nbut please do reach out to us there if\nyou\nare you know excited about a certain\nintegration\nup you know maybe as a teaser we are\nlooking into integrating with other\nserving solutions like bento ml\num\nselden k serve\nso if that's something that's appealing\ndo reach out to us\nwe are going to build out\nmodel tuning experimentation\ncapabilities into union ml so as part of\nthe microservices right you'll have a\ntraining prediction and then you know\ntuning microservice\nthat's great awesome\nvery cool and you also wrote a a blog\npost that's listed here i think there's\nat the end of the blog post a table\nabout you know further in the creations\nand the creations you just mentioned so\nyou can take a look at that table i feel\nlike that's really interesting to see\nhow we actually get to on the machine\nlearning side to an end-to-end service\nwhere you have like not only like the\npipeline and the model training and\nmodel ser but also model serving also\nhyper parameter tuning so i think that\nall together the ecosystem that we can\nbuild is actually pretty powerful so i'm\nsuper excited about seeing that awesome\nthank you hey martin i'll add one more\nthing so from not purely from a union\ni'm a point of view but i heard uh\nor steve\ntalk about things is essentially\nthis is also a way to build higher level\ndsls and languages and sdks that don't\nlook like the typical workflow syntax so\nyou know union ml is not only for ml\nit's also like a showcase to essentially\nbuild a arbitrary new sdk um that allows\nyou to that targets more your use cases\nright for example if it's bioinformatics\nmaybe you only care about writing shell\nscript so you can create a completely\nnew sdk uh so we would recommend\nlook at union ml uh and it shows that\nhow tiny amount of code that you have to\nwrite to essentially achieve um what it\nwhat it did\nvery very good point okay ethan\nokay cool so that's the\nflight ecosystem project for of this uh\nsink union ml so let's look at the road\nmap and we have uh unions uh katrina\nwalk us through you know the two slides\non the flight road map\nsure um so 1.0.2 is out even though it's\nonly a patch release there's a ton of\nbug fixes and features that are part of\nthis including 10 bug fixes and\nhousekeeping items\num thanks to nick from black shark um\nfor adding full interruptible slash like\nsponson support i create execution time\nso now you can kind of dynamically\nchoose whether or not to run your\nexecution on smart instances and\nsave money that way\num things you raj for adding in the buff\nintegration for flight idl\num and then the console we have a lot of\nui improvements that are part of this\nrelease as well too\nso some changes on the task details\npanel you can now collapse and put some\noutputs and also look at different task\nversions\nthere's also support now for structured\ndata sets as inputs and outputs when you\nlaunch your workflows you can render\nthose um and then we also have\nimprovements for retry status for\nindividual map tasks um so hopefully\nthat helps improve debugging there and\nyou can rerun a single task execution as\nwell too from the console directly um\nbut you can also do that now from a\nflicker using pipeline run as well too\nuh so a variety of different ways to\nkind of debug you know individual tasks\nwithin your broader workflow um if\nthat's useful to you i'm just gonna\ninvite you to check that out the release\nis uh published um and all the latest\nhome charts should include that\num and then so our upcoming release of\n1.1.0 which were codenamed hawk by\nkeeping that flight thing going um so\nflighttech's long-awaited we've been\ntalking about this for a while that'll\nfinally be released and these are kind\nof customized visualizations that you\ncan write for your tests um then\nconclude input or output rendering\nanything you want that we'll embed um\nthis kind of like you know the custom\nvisualization\num in addition we'll have a fast\nregistration support coming in uh like\nit uh flight remote\num which should hopefully improve\nregistration time there\num and then support for a launch line\nand level matchable attributes um so\num if you want to you know accept custom\ntask resource overrides uh execution\nmetadata cluster assignments all that\nalready exists within like project or\nproject domain scopes um will not be\nsupported in uh launch line level\num also have uh asynchronous uh human in\nthe loop test um so thanks dan for\nadding support for that um that'll be\nreally cool to see and we're excited to\nsee how people end up using that\num and then of course as always uh you\nknow improvements bug fixes housekeeping\nitems um and uh just focusing on\nstability of the core platform\nawesome very cool so that actually is\nalready the keyboard uh flight deck so\nwe have eduardo\ntalk and present and do a demo about\nflight decks i'm super excited about\nthis edwardo take away"
    },
    {
        "title": "Flyte Decks",
        "transcript": "do this so um flight decks this\nfeature that we're adding to the flight\necosystem to fight kit more specifically\num\nwhat\nlet's first first talk about like\nwhy are we even doing this right like\nwhat are the use cases that we think um\nwill be covered by by having this\nconcept of like decks\nnow available to to the um flight\nfight kit users\nscribe some of the features do a demo\nthere's like a lot to cover so i'll try\nto be quick not use a lot of the time\nfor for this meeting and um\nthere's a lot of future work planned for\nthis so um\nespecially around like you know um\ncontributions from the community\nthese uh\nlet's let's first talk about like why\ndecks so um\nmachine learning has a sorry what's that\nsorry to stop you are you sharing your\nscreen by the way\noh my god i'm not oh\nthat's what zoom is\nuh that's\nhow do i\ni wanna share my whole desktop i'm so\nsorry\nzoom\ncan you can you see my my screen now\nyeah we can\nawesome you didn't miss much this is\nlike a bunch of you know\nhousekeeping um\nslides so let's just keep going why are\nwe adding this why why do you even get\num\nwhat was missing in the flight gate like\num\nexperience at this point so\nas we all know and love like flight has\nhas\num has been\nfocusing on becoming like the machine\nlearning platform or the de facto\nmachine learning um\nplatform for you know for teams and\num\noftentimes you you want to be able to\nvisualize steps of of your workflow\nnot in terms of like you know what the\ninputs are and what the outputs are but\nmore like add some custom visualizations\nlike in this example here we're just\nlike\ngetting a data set and producing um\ncustom metrics for the this is a very um\nwell-known\ndataset the iris data data set that you\nlike multiple libraries have it like\nkeep learning what not\nso\nflight so far has this idea of like\nletting you know what are the inputs and\noutputs are but we we haven't really\ninvested in like\nletting the users specify customs custom\nvisualizations for for\nspecific tasks right so we asked\nourselves hey how could we do this and\nthis is what we came up\nwith\nthis content concept of\na\ndeck or a flight deck so\nfrom a um\nconceptual standpoint\na dac\nis a a kind of meta output attached to\nspecific task\nexecutions that allow users to customize\nvisibility into their tasks\nso\nthis is a very abstract\nnotion let's get it more real let's raid\nby this so the idea is\nnow you're gonna be able to um\nadd custom visualizations for inputs and\noutputs and what i'm calling here steps\nwithin tasks\num each task execution gets its own set\nof tags or decks and\num what are these\nvisualizations that we built so far\nso um\nwe have summaries of canvas data frames\nwe have a markdown renderer which means\nyou can now output markdown from within\na task execution\num\nwe also built a box plot chart using\nplotly so now you can get all the\nlike those um\nsimple um\nsummaries of you know canvas data frames\nor some some other sort of like tabular\ndata like quant quartiles and um\nquantiles and like mean and max and the\nmedian and whatnot right\num we also built a\num a renderer for the\nprofile report that's exposed in the\npandas data data frames themselves so\nthat's\num\ntake a look at the demo now um i\nwrote\nuh can you guys see this\nyes\nyes yes\ngreat amazing so um\nfor those who have\nnever written\num\nflight code using flight kit\nthere's not a whole lot of mystery\nreally it's super simple like we have a\nworkflow that's composed of tasks right\ndecks are attached to specific tasks so\nin this case\nlike we\nare\nnot really doing anything um super\nmagical i just wanted to introduce the\nfeature\nhere we have like a workflow that\nbasically\ndoubles the input and um stringifies the\nthe\nthat value and in a later task which is\nwhat what this means right here\nstringifying\na thing that's best\nbest um we passed 42 to it so we got 84\nand we returned the string 84. so how\ndoes that look in the console now\nso um\nthis example one i just read it i i\ndon't want us to you know spend time\njust waiting for\nthings to run\nzoom in real quick yeah\ninsane resolution\nhow about now is it better\nyep\nyep i wonder if it translates to the\namazing i would have to do this great so\num remember uh i guess so\nwith these side by side\nno\njust look at the output of of the\nstringify\ntask now we introduced\num a new button to the task execution\nflight deck\nthis is like for this very simple task\nit doesn't do much right you get\num\na visualization for your inputs in this\ncase there's a single input\nthere's a single output so\nnot super exciting but let's do\nsomething more interesting\nlet's go to example two now we have a\nproper dac\nand we're using the markdown renderer so\nread this from bottom to top like you\nhave a workflow that contains a single\ntask that produces some some markdown so\nhere i'm just you know writing some some\nmarkdown texts\nusing the\nthe headings but\nnot super exciting but\nthis is the where the magic happens\nright like we are\nadding\na markdown\nrenderer tag or some text to the\nmarkdown renderer\nand we are appending that to the default\ndeck for this task\nthis is how the api looks like you get\nthe context get the dac for for this\ntask and then you append\num\ncan i call them renditions\nuh of\neach of specific renderers\num i'm returning the text here just so\nwe can compare like how all these are\nrendered but this is how this thing\nlooks like so\nrun it again um\nit's a workflow containing a single task\nright and here's how the deck for this\ntest look like we have a default deck\ncalled well default and\nit is marked down so it's compared to\nthe actual text here like it has like\nthe\nheading 1 heading 2 and heading 3 and\ncompared to the you know how we render\nit we also get the inputs and outputs\nthere's no inputs in this case but\nthere's an output so\njust to compare you know we're returning\nthe\nmarkdown text that was rendered in the\ndefault deck\nthat's one example um\nlet's keep going things will get more\ninteresting now\nso\nlet's take a look at this this workflow\nso here we have three tasks right to\ngenerate normal df generate normal\ngenerate annotated normal df and box\nrender so\nthe first one generate normal df it\nreturns a data frame\nthe context of the directory was not\nsuper interesting i just wanted to\ncompare the outputs of these these two\ntasks so in the first case we returned\nthe\nnaked data frame whereas in the second\ncase we're saying hey i i want to get\num a slightly different kind of summary\nfor this data frame containing at most\n100 rows\nand the third task um\nit's a\nit just\nuses the box plot renderer and it um\noutputs the c-pawn length as um the\nsummary\nright so let's take a look at the output\nof each one of these\num so example three\ntake a look at the output\nof\nthis\num task\nwe're getting you know a bunch of\nmetrics for\num that\ndata frame\ncount mean whatnot\nand but in the second case in the\nannotated case um we\nactually get\nthe values of the data frame and i said\ni wouldn't at most i wanted to show at\nmost 100 rows so\nwe\nhave\n500 rows total in this this data frame\nbut notice like how here in the middle\ni'm\ni am only showing the top 50 the the\nbottom 50 rows\nright so imagine this could be a data\nframe that's super complex it has\nmultiple columns but you can say hey\njust show me the\ntop 10.\nyeah and then uh whoops box rendered\nas i said we\nalso have a box plot trender already\nimplemented so\nthis case you get like the max the\nmain median and two quantiles\num this thing uses plotly so you can\nlike\nlike zoom in\nlike pan and\num\nwell go back to it\nuh and finally\nyou can\nmix and match these\nthese examples like are these renderers\nso in this case\nwe have another workflow that combines a\nbunch of renders right we have um the\nbox plot renderer that i just showed and\nsome markdown and you can imagine like\nyou can have like other kinds of renders\nattached to this this test execution\nright\nso\nthis is how this looks like so\ncombined renderers like dax and we have\nthe the box plot that was um which we\nhad earlier\ndefaults like some markdown and then\nboots and upwards but\nimagine like this could be as\nrichful as as you can\num\n[Music]\nthink or come up with\nand uh it's a final\num\nexample i just wanted to show that you\nare not\nyou're not tied to only visualizing\nflight decks\nin inside of the console right you can\nlike\nlots of people are\nthey feel more\nfamiliar with um\nwith jupiter notebooks\nso\nwhy not expose jupiter notebooks you\nknow or flight decks to people who are\nwho feel at home when they are\nprogramming inside youtube notebook so\nwe also did that for\nfor dax so um\nthis case here like the code's not\num super important but like take a look\nat this we're defining a task\nand um\ni'm just appending\nin this case it's the pro\nthe data frame profiler reporter\num from pandas that we haven't shown um\nsame idea have a task you know you\nappend a renderer to to send back in a\ntask\nand here's how you use it so you you\ninstantiate a in your context\nexecute your your task\num\nyou had a small problem with you know\nthe\nthis is how you're supposed to be\ngetting all the decks attached to a task\nexecution you just do\ncontext.getdeck\num this is like bug there we're gonna\nfix it but this is the proposed api\nso i'm forcing the the html to be\ndisplayed in this case but you know that\nthis is not what we expect users to be\ndoing\nand\nlo and behold you get\nyour dac inside um\na panda or a jupiter notebook\nso in this case it's the this profile\nreport which\num\ncan be\nway smaller if you configure it but here\nwe're just showing like everything that\npandas gives you so like correlation\nlike there's a bunch of\nanalysis and variables and whatnot but\nthe next step really is like you can\ncombine this um this visualization here\nand drop the notebook with um\nwith fight remote right you can\ntake\num or leverage flights to not only um\nrun your tasks\nbut you can you can use um jupiter\nnotebook alongside with flight decks\nto visualize um\ntasks that were executed in your flight\ncluster\nso the sky\nreally is the limit for flight decks we\nonly have a small number of renderers\nimplemented so far\nso\nsome future work as i said um we want to\nextend the support for renderers\nimagine you had you know\nan option to have like pandera checks\nembedded like for for inputs for tasks\nor um\nperformance data you know imagine like\nhaving um flame graphs attached to some\nsome um task execution that that uses\nyou you are investigating some some um\nperf issue\nit's a bunch of like other mark down\nlanguages that we don't have support for\nright now that would be great because\nthey're used in the community like you\nknow restructure attacks latex um\nif we go back to the beginning of the\npresentation i said that\nmy flight deck is a kind of a meta\noutput but right now um the internals of\nhigh like how how flight taxes\nimplemented um\nthe meta output concept is not a plus\nfirst class citizen in in the flight\nsystem\nso um i want to introduce this this\nconcept of you know a proper meta output\nthat you can attach past\num\nwe also want to add decks\nfor fail tasks again for like debugging\npurposes imagine you have you know your\nworkflow for whatever reason you have a\nparticular test that's failing you want\nto know\nwhat's happening there and right now\nthat decks are only attached to\ntasks that\nsuccessfully executed and\nand then as i said like the sky really\nis the limit like we can have that dex\nfor other kinds of of tasks like map\ntasks imagine\nbigquery tasks having you know decks\nbefore\nanyways here's uh\nif you have more ideas you want to\ncontribute to this please\nthis is\nmore than welcome um\nwe need help to make sure that access is\na feature that is successful\nand um\nkatrina already mentioned this\nthis feature is going out\nin the next release the 1.1 release go\nhawks\nyeah i want to acknowledge the person\nwho\nactually did all the heavy lifting for\nthis feature kevin not sure if you're in\nthe call if you're not\nthank you thank you so much\num now i open for questions\nif anyone has any\ni just have a comment um i think it's\nfantastic i i think this is great to see\nmore vendors coming\nalso in life sciences where data frames\ndon't lend itself in life sciences to be\npresented as tables of data frames\nbecause there's a lot of contextual\ninformation on top so i could see very\nspecific renderers for bio\nuh coming because they have a very\ndifferent way of visualizing things i\nthink there's a whole lot that will\nhappen in this area so i just want to\nsay this is amazing kudos to you and i\ncan't see more and i would love to see\nmore renders soon\nyeah and actually uh probably eduardo\ndid not show the renderer synthetic is\nreally really simple it's just one to\nhtml\nfact so yeah please\nand i know there are a lot of people\nwho've been waiting for this feature so\nnow you have no excuses\nadd renderers\nthat's right"
    },
    {
        "title": "Pterodactyl, Javascript SDK for Flyte",
        "transcript": "um\nright so\nuh mentioned it's a javascript sdk for\nflight uh the name is pterodactyl uh\nhopefully that'll make sense in a few\nminutes\num but\njust high level overview here we\nbasically took flight and slapped the\ndino runtime on it and made a\nway for you to run javascript inside of\na flight cluster\nreally conveniently in a way that i\nthink feels pretty comfortable\nas a javascript user uh more so than\nlike you know raw containers in a flight\nkit would feel\nso\nwith that i'll get to a quick agenda of\nwhat i'm actually going to talk about\nobviously we need to cover what is it\ndoesn't make sense to talk about if we\ndon't know what it is uh we'll talk\nabout how to use it you can use it today\nif you want to uh and then a little bit\nwe'll talk about\nhow\nsome of the various pieces that exist in\nthe flight ecosystem today that you\ncould use if you wanted to\nyou know contribute but also if you were\ninterested in making your own sdk for\ndifferent languages\nthere's other pieces that exist today\nwithin flight that make that pretty easy\nand then i'll also briefly cover\nwhy it makes sense to do that uh for\njavascript specifically but then maybe\nalso your language if you care about\nthat if you have a favorite pet language\nuh and then uh i'll cover some current\nlimitations of uh\npterodactyl as it stands today and some\nfuture work that uh\ni hope to be able to do and bring more\nfeatures into this\nso with that we'll just get to what it\nis\nso\njavascript sdk for flight and what that\nmeans uh is what flight kit is to python\npterodactyl hopes to be to javascript so\nif you're already comfortable writing\nworkflows and python maybe it doesn't\nmake sense to look at this but if you\nwant to write workflows but you really\nwould rather do that in javascript\nhopefully pterodactyl provides you a way\nto do that\nas well um it wraps up some of the\nuh\nthings that typically are handled by\nlike five flight and flight ctl in terms\nof serializing and registering your\nworkflows to flight so there's just a\none registration script that you run\nagainst your\njavascript workflow and then\nhopefully everything goes correctly and\nyou have a workflow registered in flight\nso see that in a second uh just to\nmention it utilizes the dna runtime for\njavascript and typescript which is\npretty similar to node if you're\nfamiliar with using node\na couple of differences and\nbetween that and node and the biggest\none is that primarily\nfocuses on reusing\nsort of existing web apis so\nwhat that means is if you're pretty\ncomfortable with sort of web javascript\nyou should be pretty familiar and\ncomfortable with javascript in the dino\nruntime of course it's open source\nso\ncontributions and interaction are\nwelcome\nso excited to see that and then just to\nexplain the name a little bit if it\nwasn't apparent from the logos dino's\nmascot is a dinosaur\nobviously flight what kind of dinosaur\ncan take flight the only one i know of\nis pterodactyl so hence the name\nso\nfor now that we know\nwhat it is\ni can't find my mouse uh we'll talk\nabout a little bit how to use it and\nhopefully it's not too small\nuh here\n[Music]\nokay cool so\nbasically four main things that you're\ngoing to need to have in order to be\nable to use this you'll obviously need\nto have dino installed the runtime so\nthat you can actually run your\njavascript\nuh you'll need to have a flight cluster\nand hopefully you have access to one of\nthose by now if you don't uh fly sandbox\nstart i think is the probably the\nquickest way to bring one of those up\num\nyou want to write some tasks in composer\nworkflows so this is where we talk about\nhow hopefully this looks and feels\nsimilar to\nuh\nyou know\nwriting a workflow in python or\nany other language\nbut just import task and workflow\nand this is the raw\ngithub link but there's actually a\nshorter\nversion link that you can access now\ni'll cover at the end of the\npresentation\nand that's also\nthe new link is version tag so you don't\njust get whatever's on me\num\nbut\nso same way that you would typically\ndecorate your functions in python with\nlike a task decorator or you might\ndecorate a workflow with the workflow\ndecorator\nhere we're basically uh\ndoing exactly what python does under the\nhood which is\nwhere\ncalling the function task on our\nfunction sum here\nand then assigning that to\nuh\nsum\nof course you can also assign that to a\ndifferent name if you wanted to refer to\nyour task by a different name than its\ntask name but\nbasic functionality here is you\ncall task on function you define\ndo what you want to do with your input\nvariables\nand\nthen you take that and utilize it in a\nworkflow function\nthat defined very similarly to the task\nfunction\nyou just use those\ntasks that you've defined\nactually don't know if you guys can see\nmy pointer can you see my mouse\nokay yep so\nhere\njust call workflow on my workflow\nfunction\nreceive the x and y parameters\nand then we'll send that to the square\ntask\nand then we'll also send y to the square\ntask and we'll set the result of both of\nthose to the sum task\nonce you've gone and defined a workflow\nlike that you can take advantage of the\nregistration script and here i actually\nshow the newer shorter link\nat denoland\nsame as you would with uh flight kit or\npi flight you have the packages flag to\npoint it at a\nhere you can only point it at a\njavascript\nworkflow but for pipeline obviously a\npython workflow\ntell the project you want to register on\nthe domain that you're pointing to\nversion you want to use uh\ntell your endpoint so where your flight\ncluster is located where you can hit the\nhttp endpoints\nand then you tell it an image to\nactually register that with\nsomething cool i think to point out here\nis that because i'm using a\nscript that's remote so this script is\nhosted on\ngithub\ni actually don't have to build that\nscript into my container\nso it's kind of like a cheap script mode\nfor\nhigh flight but\ntakes advantage of somebody else's\nhosting infrastructure\nand then if you're successful everything\nchecks out you'll see these messages\nhere at the bottom you've registered the\nvarious tasks uh workflow and then a\nlaunch plan so that you can actually\nstart that workflow\nand we'll see what that looks like in\nthe flight console\nso\nyou get the same result this is my\nworkflow\nas you would expect if you'd register\nusing pi flight with a flight kit\nworkflow uh some some differences that\nwe'll get to and point out but you know\nit looks pretty similar we have my\nworkflow here or in the flight snacks\nand actually this one i registered to\ndevelopment instead of\nstaging like in the example above but\nmore fun is that when we actually run it\nwe look at the tasks we can see now\nthese type\ntasks are javascript task instead of\npython task which hopefully is fun for\nyou as it is for me\num\nand then again when we go and see the\ngraph view of that execution we can also\nsee that it's a javascript task\nso\ni mentioned before that\ni was using the\nscript that was hosted\nin\ngithub\nif for whatever reason you don't have\nsome repository that's accessible\nby your task executions\nso you need to actually bundle those\nthings\ninto your\ntask execution images\nuh that's totally doable\nyou can just take advantage of docker\ncontainers to build the\nworkflow script that you're\nrunning with as well as even the\npterodactyl execution and pterodactyl\nannotation\nscripts\nyou can bundle those all into your\ncontainer\nso that it doesn't need to reach out to\nanything remotely when you go to access\nor execute those\nand so\nthat looks pretty similar to\nthe example before\nexcept for now i'm using the\nold longer link here\nand now instead of packages pointing to\na remote script it's just pointing to a\nfile i have locally on my computer which\nis the same as what i've copied into my\ncontainer\nso\ni mentioned there's some slight\ndifferences between what you might\nexpect to see and\nwhat you get\nand\nthese options cover some of uh\nhow we get back to sort of feature\nparity with what uh flight kit and\npython has to offer\nuh so\none of the first things that uh you may\nhave noticed if i go back a couple of\nslides uh\non this workflow you can see that the\ninputs are input zero input one and\noutput is output zero so\nthey're kind of just named pretty\ngenerically\nuh when they get to flight obviously\nwhen you're using them in your\njavascript you can use them as x and y\nand whatever they were defined as\nbut javascript doesn't provide a really\ngreat way to get the names of parameters\nso\nif you want to just override and say hey\ni want this to be a parameter named name\ngo ahead and define a list of parameter\nnames\nif you want your output to be named use\nan option here\nfor naming your output\nand then a really cool thing about\nflight that i like is that you can cache\nyour function or really task executions\nand so we exposed a similar\nfunctionality here in pterodactyl with\nthe cache version\nuh\n[Music]\noption\nwhere if you go ahead and write a cache\nversion in here\nand it's already executed on some inputs\nsay that we already executed this\nfunction with\nmatthew\nthen\nthe next time somebody goes to execute\nthat with task with the same uh cash\nversion they'll just get back hello\nmatthew again\nso they could avoid\nall the startup and launching and\nexecution time\nparticularly important if your task\nisn't just hello um somebody\nuh\nso that was for task we have those param\nnames output name and cache version\nsimilarly for the workflow you we have\nthese options for parameter names and\noutput name\nthere's not a cache version for workflow\ni actually haven't really dug into uh\nwhether i can do that or not but\nif i can i'll add caching of workflows\ntoo\nso just to show what the results of\nactually utilizing those look like um\nwe had that function\nthis task\ncalled step one here\nwhen we actually go to register it now\nthat we're using those options\nyou can see once again it's a type\njavascript task\nonce we succeed in executing we'll see\nthat the result of the execution gets\nwritten to a cache\nsee now the\noutput is named response\nand then when we go and we look at the\ninputs we can see\nthat the name of the input is name\nso one thing you might be wondering if\nyou're pretty familiar with javascript\nis like how do i use this with async\nstuff because half of javascript seems\nto be async and most of the useful stuff\nis\nand the answer is hopefully in a way\nthat feels pretty natural\nso you can just define an async function\nand send that to the task or workflow\nfunction\nand obviously those async functions need\nto follow the rules so they sync\nfunctions so\nif you\ntry to do an await in a synchronous\nfunction it's going to yell at you\nso on and so forth\nso hopefully that feels pretty natural\nand you can see that within this\nworkflow when i run that task i still\nhave to await it here\na couple of weird gotchas around that\nthat i don't really have too much time\nto go into now but\nhappy to answer questions around that if\nyou have them later on\nuh and then\nother things to point out here is if you\nreally love your arrow functions you can\ntotally just define tasks as arrow\nfunctions\nand then the other fun thing is that if\nyou really love\nrunning your tasks locally you can add\nthis if import meta main\nso that you could just grab\nrun your execution locally\nand the\nworkflow that you get back still should\nfunction mostly like a normal\njavascript function\nnext up task references if you've\nalready got a bunch of tasks defined in\npython or some other language\nand you want to make use of those\nwe expose that functionality here\nthrough this task reference import\nuh basically you just tell it the\nproject domain version and name\nand then you'll need to call it with uh\nkeyword parameters so keyword arguments\nhere\nso for example my takes float function\nthat i defined takes a parameter a\nand then this gen float function takes\nno inputs so it's just an empty object\nthat i send to that and then this\nworkflow would you know generate a flow\nand do something with that flow\nhopefully usefully\nso\nto quickly get into a bit of the reason\nwhy\nwhy would you want these things i think\nit's pretty obvious that you know it's\nhelpful to bring more people into an\necosystem and definitely i think it's\nfun to bring more languages into the\necosystem\nso i pulled this graph that's just\ncherry-picked data that proves my point\nthat javascript is the most popular\nlanguage\nyou can find data that supports your\npoint uh\nif it's different but you know\njavascript is huge there's a lot of\nyou know huge ecosystem a lot of tools\nalready in that ecosystem to be able to\nreuse those and to have the expertise of\nthose people\nin the ecosystem i think is huge\nthe other like really selfish thing\nabout making this is that i just want to\nuse fetch to make web requests i don't\nwant to have to use requests\nand so\nthis this really helps me in my like\ncurrent work because i can just make web\nrequests with fetch instead of having to\nlearn requests properly\nuh but then you know really any other\nweb api this makes super comfortable\nuh and then just to compare like all\nthis is technically possible with the\ncontainer task in flight kit\num\nbut the\nlevel of effort required to get to\nuh you know doing this comfortably with\ncontainer tasks\nyou need a whole python tool chain in\naddition to your javascript tool chain\nuh so i think that this just removes\na huge amount of friction around that\nand then something really\nforward-looking that i hope uh can be a\nthing\nis uh jsx uh flight decks so hopefully\nyou could define flight decks with you\nknow jsx syntax like you would in a\nreact app\nbut\ni need to\nyou know hopefully work together with\nsome flight deck people and get that\nimplemented\nuh just to quickly do the how\nbasically copying flight kit helps a lot\nbut there's also you know reusing a\nbunch of existing tools so if you're\ninterested in making your own sdk\nflight co-pilot is huge you can just\nreuse that to get your upload and\ndownload working in basically no time\nthen flight idl\nas well as the grpc gateway\nuh\nmake actually\nmaking requests to flag super easy\nbecause if you're ever confused about\nhow to make a request you can just read\nthe flight idl\nand then the container interface that\nexists in inflate\nis also a huge help you can really\nbasically do whatever you want as long\nas it runs in a container\nnow to get to the limitations so\nsimilarly to flight kit workflow\nfunctions are really a dsl they look\nlike\nlook like javascript\nand basically are running javascript but\nthere's some things that probably aren't\ngonna work quite the way you expect like\nyou know if you run a a real function\ninstead of a task\nuh it's not gonna give you like\nit's not going to run that task at uh\nworkflow execution time it's just going\nto run the function bit\nwhich can lead to interesting bugs\nuh\none\nmajor limitation i hope to overcome soon\nis input and output are currently\nuntyped\nand there's a slide in a second that\nshows what that means\ni do plan to add typing and hopefully\nintegrate with a typescript to make that\nseamless\nand then currently sort of a weird\nlimitation is a task and workflows have\nto return something\nuh so if you just want to like\nmake side effects\num it's not really a great use case yet\nthat should be easy to fix\nthen with having to return something we\ncan only return one something at the\nmoment\nso another limitation is\nyou know only one output but we hope to\nlift that as well\nand then task references can't be run\nlocally\ni think that makes a lot of sense\nbut\nif\nthere's a way to get around that and\nsomebody has ideas be happy to hear\nso on the input output limitation\ncurrently untyped basically\nany input where the thing that you want\nis equal to json.parse of json.stringify\nof that thing you'll be good\nuh anything else\nyou'll probably have problems\nand then obviously this doesn't really\noperate well with\nthings in the ecosystem\nso hopefully we'll add typing\nsoon and be able to\nnot have this limitation\nso some future work obviously top of the\nlist is strict typing i think a huge\npart of flight is making sure that your\ntasks line up and you get the right\ninputs and outputs going in and out of\ntasks\ni really love dynamic workflow elements\nthings like map tasks i think are super\nuseful\nparticularly when you have a lot of data\nto scale to\nso i'd love to add features like that\nand then uh\ndefinitely excited to get to some\ntypescript integration i think that it\nmakes typing in javascript feel\nsupernatural\num and so\nreally excited for that\nand then uh you know obviously just want\nto have more feature parity want to be\nyou know a really useful sdk the same\nway flight kit is\njust for javascript\nuh\nwhere to find these resources so if you\nwant the tagged releases they go to demo\nland\nslash x pterodactyl\nand then a tagged\nreleases like add that tag name\nyou can also find those on my github\nhere\ntag releases\nsource and all development is taking\nplace\nin this repo here\nand then you can find me on the flight\nslack\nand just just add me\ni'm there in a lot of channels uh and if\nuh if we need we can make a channel for\npterodactyl so\nthat's it that's all i've got uh\nthanks for letting me take so much of\nyour time uh\nhappy to share and answer any questions\nfor anyone who's still still hanging\naround\ngreat shop i i i think that's amazing i\nthink we could absolutely find a channel\non slack and uh that could be even a you\nknow java script sdk eco system channel\nor something to that extent so i think\nthat would make a ton of sense so any\nquestions from from whoever's left\ni think it's fantastic to see that\ni think this is amazing uh one question\ni had is this usable because dino is it\nusable in the browser directly now\nyeah so this is actually very close to\nusable in the browser i'll say that uh\nyou know some of the registration script\nwhich is what you would really i think\nwant to use\num certainly on the execution side it's\nusable\num but on the registration side it's\njust not architected quite right so we\nneed to you know\nhoist a couple things into functions\nthat are sort of just like script level\nbut once we move those into functions\nyou could totally just import those\nfunctions and then have like working\nregistration from your browser with\njavascript\noh wow\nthis is a huge step you probably have\nnot realized it yet but it's going to be\npretty big\nthank you so much matthew it's amazing\nyeah thanks for having me"
    },
    {
        "title": "Transforming Satellite Data with Flyte @ MethaneSAT",
        "transcript": "yeah so thanks for that introduction i'm\ngoing to talk about what methane set\nis and our mission because a lot of\npeople might not have heard of us\nwhy we're choosing flight\nand how we're using it and plus some of\nour favorite features and then\ntalk about some of the initial results\nthat we've\ngotten by running flights and then have\ntime for questions at the end\nyeah so methane set as an imaging\nspectrometer our goal is to reduce\nmethane emissions\nfrom man-made sources worldwide focusing\non oil and gas initially\nand we're a part of the environmental\ndefense fund and we're a completely\nphilanthropically funded\nmission\nand the the project is already underway\nthe satellites already being built\nthe software team is is i'm part of that\nso i actually skipped that part so i'm\nthe lead back-end engineer for\nthe data processing pipeline which i'll\ntalk about\nand so why methane is gonna talk about\nthis very briefly just in case people\naren't familiar\na lot of people hear about co2 and\nreducing co2\nwell methane provides a\nway to\nbuy us more time to decarbonize because\nit has a way more potent effect on\nglobal warming initially\nit's like i think 80 times more\npotent than co2 but it only lasts for\nabout 20 years in the atmosphere so\nchanges we make now we see the benefit\nsooner\nso that's the focus of\nthis mission\nand then the quick comparison in terms\nof you might have heard of some of these\nother satellites that have launched so\ntropomy\nis a satellite in the eu that tracks uh\nmethane among other things that's kind\nof like a global mapper so it can\nliterally map the entire world's methane\noutput\nuh and then there's location mappers on\nthe other side kind of these point\nsource mappers that look at very small\nareas\nand can get\ndown into the weeds in terms of where\nthe leaks are coming from we're kind of\nin that middle spot where\nwe can still image a large area but not\nthe whole world um and then we hope to\nbe able to\nhave pretty high resolution as well as\nidentify\nand quantify these different\nsources of emissions\nand then so let's see where the data\nkind of comes in so we\ndid that the data is you know collected\non the satellite obviously and then\ndownlinked to\na base station and then that's relayed\nover to our data processing platform\nwhich is where flight comes in uh and\nall this processing that we're\ndoing and that we're working with\nharvard and smithsonian astrophysical\nobservatory\nand\nnew zealand and other partners to\ndevelop this this pipeline that i'll\ntalk about shortly\nand then we're displaying all that\ninformation on the ui that our front-end\nteam is building to you know hopefully\ndrive that that change\nso you know why flight you've probably\nheard a lot of these\nthings before you all here because\nhopefully what you like flight um\nthere's a lot of things that we like\nabout it as well\nwe're running on kubernetes so being\nkubernetes native highly scalable it's\nobviously a big deal\nand then having the backing of rigid\ncoming out of lyft and and all these\nother partners like spotify if you know\nothers that are\nusing it it gives us confidence that\nuh you know running five million\nworkflows at spotify okay so you know\nhopefully our use case is handled we're\nnot running as many workflows we're\nrunning only about 30 to 50 a day for\nthe different targets that we collect\nbut each one requires a ton of\nprocessing thousands of cpus running for\nhours doing these really\nheavy math and physics computations so\ndefinitely no small workload\nwe needed different containers custom\ncontainers per for tasks because each of\nthese i'll get into the different\ncomponents but there's different\nscientists and different science teams\nworking on the algorithms for different\nportions of our processing pipeline and\nyou know that they use different\nlanguages including\nfortran\nso we have to support\nvarious languages and try to shovel that\ninto one container is kind of\nprobably possible but not what we're\nlooking to do so being having the\nflexibility of\nrunning different containers for tasks\nis really great um caching obviously no\nbrainer but you know our tasks are long\nrunning two hours\nor more potentially\num or for you know for these things and\nuh if you\ntook four thousand cpus and then all of\na sudden some downstream component\nfailed because the container couldn't\nstart or whatever the reason is we don't\nwant to have to rerun\nwe love the flight file abstraction not\nhaving to deal with\npassing files back and forth is great\nbetween the tasks\nand this is the last but definitely not\nleast is the the community here is that\nyou you know\nthat's been built is\nis phenomenal we\nwouldn't be where we are when in terms\nof testing without without you all and\nasking a million questions in slack and\ngetting answers from everybody that's\nbeen\nsuper helpful\nthis is kind of a brief\nlike overview of our\narchitecture so you can see\nthere's various pieces uh the green box\nkind of where the data gets saved and\nserved up to the front end\nthe yellow is is our kind of backbone\ninfrastructure we're deploying to google\ncloud\nand\num then you can see the blue boxes are\nkind of the science different science\ncomponents we call them\nlevel zero to level four\nand that's a nasa designation like we\ndidn't pick those the levels they have\nvery specific designations like level\ntwo for example is methane concentration\nor a concentration and then level four\nis a flux rate so kilograms of methane\ncoming from out per hour from a\nparticular source\nand then to get from raw to level four\nwe need to organize all these tasks\ntogether and that's where flight comes\nin it's our backbone for\nfor orchestration and task management\nso some of the features that we really\nlike so far and i've been leaning\nheavily on these aren't necessarily the\nsuper high tech or anything but they're\nthey've been really helpful for us is\nyou know combining data classes and\nflight files\nso\ni'm sure everyone's familiar flight\nfiles but it's a really nice abstraction\non distributed platform to just say yeah\ni need this file\nand you know flight takes care of\ndownloading it uploading it and only\naccessing it when we need to\num and we we're generating large binary\nfiles these netcdf format\nso not having to worry about\ntransferring and copying those files has\nbeen really nice\nand then we're also leveraging data\nclasses\nwhich allow you to combine\nuh\nyou know to structure a cloud data class\nand define what your your fields are and\nthen flight automatically serializes and\ndo serializes it and uh\nrecently and most recently in the last\nfew months i figured exactly one but\nthey added the ability to allow flight\nfiles and flight directories to be part\nof those data classes and that was just\nlike a shining moment for us because we\nwe have a lot of different inputs and\nvarious parameters that we want to\nassociate with the flight file with the\ndifferent outputs and the example i have\nhere on screen is kind of a trivial\nexample like you could easily just have\nthose four\nfields as the parameters to that level\nthrough gridding function but you can\nimagine if you have more and more\nparameters and things you want to group\ntogether\nthese data classes are really nice for\nthat\nuh and we're also leveraging pretty\nheavily dynamic tasks and um because our\nalgorithms can run kind of on a per\npixel basis or uh on these sub workflows\nand\nand so we're leveraging dynamic tasks to\nkind of\ninspect the inputs and based on the\ninputs make decisions and on how we want\nto\nsubset the image into different chunks\nfan out to do parallel processing and\nkind of combine at the end\nwe\nlooked at map tasks originally for this\npurpose but didn't quite suit our needs\nbut i know there's been a lot of\nrework around them recently and\nperformance improvements and all these\nthings and now ui support\nso it's really exciting so we'll\ndefinitely have to take a look i think\nit could be\nvery useful for for us as well\num but if you're not familiar with\ndynamic tasks\nthey basically allow you it's kind of\nthe best of both worlds between a\nworkflow and a task\nand a workflow you you call out to\ndifferent uh tasks but you can't\nactually\ninspect or make decisions on the inputs\nit's kind of a promise object\nwhen\nin a task you just have you know your\nstandard data inputs um\ndynamic tasks lets you call out to\nadditional tasks if you need but also\nyou can inspect and actually look at\nthose inputs so in here you can actually\nsee what the number a is\nand and loop over it to call out to that\nt1 task\nso then we're putting all that together\nhere using launch plans and uh yeah you\nhave a nested workflow so we have a\ntop-level workflow for our level zero to\nlevel four pipeline\nand then\nwithin that we have sub workflows for\nthe different\nlevels and we're using dynamic tasks and\nyou know all these features the\ncomposability of them\nreally allows us to do\nto suit our use case and i'm sure many\nuse cases which is\nfantastic we really love the flexibility\nit gives us and then expand out on this\nlevel two tests you can see\nwe\nhave split the image and then we have\nactually twelve hundred and eighty\none for each column in the image\nuh these retrieval tasks like i call\nthis is where the heavy cpu actually\nfortran code gets called\ndoes the processing\num\nso we we've been running that on our\nplatform this this kind of workflow from\nlevel zero to level two to start\nand um we tried it like it's shown here\nwith using launch plans\nand we really like launch plans\nbecause it lets us obviously\nspecify\nwhat workflow we want to run but also we\nlike the idea of having fixed inputs\nversus defaults so eventually when we're\nrunning in production and the satellite\nis\nup in the air collecting data they're\ngoing to be analysts who are\nreally you know\nexpert in the\nyou know\ngeospatial gis sort of world but won't\nsav programmers are familiar with flight\nso kind of reducing the amount of inputs\nthat they can change and\nonly giving them the knobs the few knobs\nthat they'll need to look at it and\ntweak is is going to be really valuable\nbut we tried it like this with you know\n100 level zero files which are you know\n10 seconds each and we actually kind of\ngot bottlenecked\nand we're only able to run\nup to 500 of our available 3000 cpus so\nthat didn't quite work and this is when\nthe\nflight community came uh\ncame in handy and\ncame to the rescue and we\nended up saying well why don't we just\nsplit it into\nyou know have a launch plan per task get\nbetter\nthroughput and also better observability\ninto kind of what is running so that's\nwhat we've been doing and we've been\nable to utilize all 3 000 of our\ncpus\nthat's just because that's the limit\nthat google has given us at the moment\nit's not\nobviously a flight limitation we're\nactually working with google to get\nmore cpu so we can\nscale it even further\nso that's been a really nice\nreally nice feature\nso yeah so far we're deploying to two\ndevelopment clusters in gcp using helm\nwe've scaled out the 3000 cpus\nand\nwe're still definitely figuring out how\nto optimize fight with our algorithms\nand just our flight deployments in\ngeneral but we're really happy with what\nwe've been able to accomplish so far and\nyou know our goal is a product to use\nflight is our production\nuh task orchestrator you know we're\ngonna have 10 000 plus cpus that we plan\nto use every day processing the\nthe raw data there'll be 30 different\ntargets approximately that we're\ncollecting data on every day and it's\nthat's about 200 gigs of raw data and\nthen\ni forget what our estimate is for final\ndata we're probably like two terabytes\nor so on the output so a lot of the data\nprocess and we're leaning heavily on\nflight to make that happen\nand this is some\ninitial results that we've seen\nso we have a methane air operation where\nwe actually\nstuck a very similar sensor\nto\nmethane sat on a plane\nflew it around you can see this is kind\nof in the corner of texas and new mexico\nin the permian basin and\nwe flew this kind of back and forth\nmowing the lawn pattern and call it over\nthis area and collected a lot of really\nreally great intel and you can see this\nis the\nthe output and you can see the color\ncoding of where the methane\nconcentrations are and we're on the\nright side here i don't know if my mouse\nwill show you can see\nthat's kind of one of the really heavy\nleaks that we detected\nand then you can also see there's kind\nof this blob of\nuh what we call diffuse sources where\nthere's a lot of emissions it might not\nbe able to be identified to one\nparticular source but we identified that\nyou know we can quantify the total\nemission there\nand to put it in perspective this\nall the blue area here is where we flew\nover a course of a few hours\nand this white box is the equivalent of\nwhat methane's at we'll be able to tar\nto to view and scan in about 30 seconds\nso\nuh fraction of the time larger area\nso we're really excited about the\ncapabilities that\nthat this project will enable us to\ndo and hopefully drive the\nproduction and methane from manmade\nsources so and flight's going to get us\nthere so i'm really excited about it\nany questions i think that's the last\nthing\nso this is amazing this is fantastic so\nhow big is your team that has been\nworking on that so far\nuh on the back end team there's myself\nand three others\nwell\nand then we're working with scientists\nuh and then the front-end team is about\nus equivalent in size and then we have\nuh partners at harvard and the\nsmithsonian astrophysical observatory\nand others that are providing science\nexpertise and uh example code that we're\nusing for\ndoing these computations because that\ngoes completely over my head\nwow and then looking back on your\njourney so far what was the biggest\nlesson that you have learned\nuh that's a that's a good question um\ni think\nfrom the flight perspective it's\ndefinitely trying to\nbreak things up into smaller tasks uh\nusing those multiple launch plans um\nsmaller workflows so we can kind of get\nthat composability has been huge um\nand uh yeah that\nwe've gotten a lot of benefit from doing\nthat\namazing\ni think this is fantastic to see that\nproject i mean you know if you i mean\nthere's so much that\nthat there's a bigger you know benefit\nto the society i mean i wonder what\nkeeps people up at night when they run\nsuch a big project and so\nso obviously you have a good solution\nand i feel like you can sleep very well\nthese days\nyeah a lot to do but uh yeah i'm i'm\nconfident in the technology in our team\nto get us there but yeah a lot going on\nawesome\nany other questions from the class\ni just want to say one thing that\nwe feel\nproud to be even be associated with this\nremotely in any way so the community is\nreally there to help you in any way this\nis one of the biggest challenges\nfor this century or more for humanity so\nyeah thank you for doing this work yeah\nand thank you for helping us because we\nwouldn't be able to do it without you so\nthat's one question um\ndid you experience any friction\nusing fortran with flight\nand i'm also curious like what that\ndevelopment looks like that like life\ncycle of\ninterleaving or adding support for\nfortran in within your flight workflows\num\ni would say there was there was friction\nwe're just running fortran in a\ndocker container in general but then\nonce we kind of got\nkind of kind of containerized and\nand running there hasn't been any\nfriction with\nwith it calling it from a flight\nworkflow or flight task it's\num\nwe essentially just call it as an\nexecute like we have a flight test\nthat's python gets stuff set up and then\nit just calls out to the\nthe fortran executable um\nwe have we would like to maybe i think\nthere is a fortran\nway to call it more um natively from\npython but we haven't done that we're\njust actually just doing a\nlike a subpro uh what is it subprocess\ndot\nrun command at this point to call the\nexecutable\ncool\nthanks\nand besides um python fortran is your\ndata science research team using any\nother languages\nuh yeah they're using r as well\nyeah case and smiles because\nyeah and meals too because i think um\nthere's uh obviously in geo spatial uh\nyou know community with\nthere's a huge huge support of r and i\nthink when you go back to the\nyou know back in time to mr sid formats\nand wavelet compressions for geospatial\nimaging and and then also the data\nscientists on the geospatial side i\nthink r has been a really stronghold\nthere so\nmaybe maybe we can get\nsomething started on on an rsdk and open\nsource that would fantastic\nyeah i'm happy to chat about that okay\ngreat\ni think any other questions i don't see\nany in the chat\nnope all right so we're a little bit\novertime but nicholas this is fantastic\nthank you so much so let's make sure\nthat we can get the word out for what\nyou guys are doing it's really really\ngreat so kudos to you"
    },
    {
        "title": "FlyteConsole UI Updates, May 2022",
        "transcript": "let's do demos and presentations so\nwe're going to kick it off with\nunion ais jason porter so jason you have\nbeen working on the flight console ui\nupdates why don't you show us a little\nbit more\nyeah hi i'm jason porter i lead the\nfront end team here at union and yeah\nwe've been working on a lot of stuff\nlately and i know we're pressed on time\nso i'll make this really fast but i want\nto walk you through some of the cool new\nfeatures we've released in flight\nconsole\nso let's start with a little little\nslide show here okay\nso\nfirst up uh just you know updates across\nthe board on the site and so one of the\nthings that's been asked for a lot is\nsome updates to the project dashboard\nand so you can see now we've added\nthings like workflows tasks and just\ninformations about your domain\nconfiguration\nand don't worry we're going to demo\nthese in real life in a second but it's\ngoing to want to go through these real\nfast\nwe've also added support for dynamic and\nnested graphs hooray finally\nso now you can drill into a graph and\nsee all the different tasks all at once\nwe've also added timeline view which is\nas an execution progresses we can see\nthe duration of each individual task\nuh we've also added support for mapped\ntasks this is a really really asked for\nfeature\nand so we're really happy that this is\nworking and we can show you guys this\ntoday\nand then uh\nrerun uh we've also added the ability so\nsometimes like in a really big workflow\nuh in the debugging case for example you\nmight want to just figure out why one\nspecific node isn't working what this\nlets you do is that it takes the inputs\nfrom the workflow that occurred and lets\nyou just rerun that task individually\nso\nlet us oh and last thing i am so sorry\nuh we've all also added support for\nstructured data sets um and this is\nreally cool feature\nand we're gonna show this off too\nso let me jump into this\nokay so let's go to snacks\nand so\nhere's the product dashboard not a lot\nto show you here to be honest but uh we\ndo think this will be really useful\num going into the graph stuff so let's\nsay we want to show off how the nested\nstuff works\num we can come in here\npick one of these executions\nand you'll see that on this\nuh sub workflow it's actually a nested\nso it's a it's a workflow and a workflow\nagain\nso we can come in here now on this and\nnow we can click on this button\nand it shows us\nuh what was inside that and you can go\nback and view the container\nor back like that\num\nand i know that's you know not super\nimpressive but if we go to a much more\nadvanced workflow\nlike this one\nwe can come in here\nand so this workflow has a lot going on\nlots of things\nso when we come in here\ni don't know why it loaded like that\num\nnow this gigantic workflow\nlet's say we want to dig into one of\nthese well now we can we can click this\nand now we can see\nthat we are inside n3 0 and 3 and we can\neven go deeper\nand we can even go deeper\nand so we can nest all the way down and\nif you want to go back just click where\nyou want to go on this little breadcrumb\nand that's uh\nthat's how that works\nuh one more really cool feature but i\nthink this is a great workflow to show\noff is our timeline view\nso\ngive a second to load but again it's\nthis ability to zoom in and kind of see\nhow long each individual\ntask took\nso you can drill into this\nsee what was loaded from cache\nall kinds of things\nso\num\nso yeah that is the graph and the\ntimeline view\nnow let's go to maps tasks let's go back\nto snacks here\nactually wait hold on a second i want to\ngo to pod\nhere we go\nand so\nlet's say we want to drill into this see\nwhy this is not working\nuh now we have we can see on a map level\nthe\nindividual tasks\nand you can navigate\nthrough them using this new breadcrumb\nthat we built\ni want to find a more interesting one\nthough let's try\ni think it was this one\nso now we can also show\nuh the statuses of the constituent tasks\nso we can see the ones in here that\nwe're kind of stuck in running\nor again we can go to the failed\nand again you can use the breadcrumb\nso that's pretty cool\num\nmoving along trying to keep track of\ntime here uh i want to show off actually\ngo back to here to show off the rerun um\nso let's go back to create\nthis is that example of these gigantic\nworkflows and in the debugging use case\nyou don't want to start the whole\nworkflow over just to test if your fix\nworked so in this case we can imagine\nlet's say we wanted to come down here to\nupdate and\nyou know resolve customer account id so\nwe can see the inputs and outputs here\nbut what's really cool is if we click\nrerun\nwe see those same things here and if you\nclick this this will just execute just\nthat one task to help with debugging\nand then finally\nlet's go to structured data sets\nokay\nso you see in here\nnow on the inputs and outputs we give\nyou this\nformatted json and this is there's some\ncool things about this uh the big one is\nthat you can just copy really easily any\nof these little purple icons you can hit\ncopy and now to your clipboard all that\ndata and i mean if it's just three data\npoints not a big deal but it was bigger\nyou can imagine this is helpful\num\nand we've actually also used this on one\nmore small update i want to show which\nis an update to the\ntask detail page\nwell not that one\nlet me just copy and paste this\nit was that one\nthere we go\nso now in the task detail page we also\nuse the same json formatting\nso anyway uh that's just a few of the\nfeatures we've been working on uh some\nof the highlights some of the big ask\nfor features um lots more cool stuff\ncoming but i will hand it back to you\nawesome"
    },
    {
        "title": "Flyte Community Update 014 - May 31 2022",
        "transcript": "okay cool\nall right\nwelcome to the flight community sink my\nname is morten stein\ni'm with union and i'm your host yes i'm\na new face that's my first time hosting\nthe community six i'm super excited\nabout that if you have any feedback\ndon't hesitate\nslag it to me direct message me the\nfeedback looking forward to\nmake this very engaging and with that\nlet's go\nto the agenda today we have a packed\nagenda i think it's going to go 50\nminutes today community highlights it's\nabout 15 minutes we're going to have\nkaithin talk about the road map\nand then we have about 20 to 30 minutes\non them with some presentations\nlet's talk about\nthe next slide community highlights and\nuh\none more and get to know your\ncontributors so we want to do something\nvery new which is highlight the\ncontributors in our community sync\nthis time we actually selected the\ncontributors of the month eugene and\nnick and we're going to give both a\nminute or two to basically talk about\nthe contributions\num what you know where they work why\nthey got to start it\nwith a flight and basically what they\nhave been on lately uh specifically to\nthe contributions so let's get started\nto move forward to have eugene eugene is\nactually our software engineer at union\nai hey eugene are you contributor of the\nmonth so tell us a little bit about what\ngot you into flight and what was your\ncontribution to get you the contributor\nof the month\naward\nhello yeah thanks for this opportunity\nuh\nyeah i just joined union ai recently am\ni graduating from\nuniversity of washington with computer\nscience degree and\nyeah i just found flight online and then\nstart contributing to the open source\ncommunity and i think 5 is powerful and\nthey also support a\ngood ui with flight console to make the\ndebug and also monetize the\nall the flight tests that i think is\npretty interesting so what i've been\ncontributing is\njust making the\nuh the console dashboard become\nmore user-friendly to improve the ui ux\nand i will continue to work on that so\nif you guys have any feedback on the uh\nflight console uiux\ncan also\nfeel free to ping me on that yep\nawesome hey jean thanks so if there's\nany any questions for eugene reach out\nto him happy to have you there and with\nthat let's go forward and talk to nick\nso nick um your side\nobviously tell us a little bit where you\nwork black shark ai i think the\ncommunity knows about black shark ai\nwhat you're doing there and\nwhat got you started with flight number\ntwo and what was your contribution go\nahead nick\nyeah hi uh thanks for the invitation uh\nas i already said i\nwork at black shark a guy for the people\nwho don't know black shark\nwe basically generate a\n3d\ndigital twin of the whole world\nfrom mostly from satellite imagery\nand we heavily rely on flight for that\nfor our machine learning pipelines and\nour processes\nand i've recently joined blackshark i\njoined it about two months ago\nsince\nwe have decided as a company that\nwhile\nwhat is already provided as the open\nsource version of flight is amazing\nthere is still a lot of changes that we\nwant to do to flight and we figured why\nnot just have someone dedicated to that\nworking inside the company\nthat's improving flight contributing\nback to the main\num open source repositories and just uh\nyeah not not just making uh use of the\nopen source community but giving\nsomething back so i have the very um\nvery i'm in a very nice situation that i\ncan can do some open source\ncontributions while being paid for it\nwhich is the optimal way\num so yeah i'm a go developer\nby trade i\ni will mostly be working on the the go\ncomponents of\nflight\nitself um and yeah for as a starter i\nthe the main contributions i'd say was\ntrying to\nimprove\nthe\ngetting started experience as a\ndeveloper of flight like i tried to\nimprove the local setup of light\ncomponents i tried to improve some of\nthe\ndocumentation generation\nand i also added a little\noverride so you can\nmark your tasks as interruptible\nuh for a single execution because that\none is that is something that we've been\nusing or will be using um on spot\ninstances\nthat's awesome so i assume people can\nreach out to you and get a hold of you\non slack\nyeah absolutely i'm both as my\nfull name and the my github username on\nthere as well awesome great very cool\nthanks a lot for the contribution we're\nsuper happy to have you dedicated also\nto open source that's fantastic great\nokay cool let's go forward um\nand talk about uh samita's blog post\nabout envelopes with flights amita why\ndon't you tell us a little bit about\nwhat the blog post is and who should\nread it\nyep sure\nhey everyone\nso this blog post dives into the\nstruggles that production ml users face\nand explores how flight could be the\ngo-to tool to bridge the gap between\nmachine learning and engineering so to\ngive you all a rough idea the blog post\nstarts with what devops is and why ml\nops isn't the same as devops or is a\nsuperset of devops in there i outlined\nthe expectations and challenges of\nmachine learning models during the\ndevelopment and deployment phases and\nthe next part of the blog post is about\nflight or dive into the ml specific\nfeatures it offers and how it could be\nused to\nengineer the ml pipelines effectively\nand\ni have also included a step-by-step\nguide on how one could adopt flight for\nthe machine learning workflows so yeah i\nwould like to thank martin and matthew\nfor helping me out with ideation and\ncopy editing\nso the article is on emelop's community\nblog do give it a read and let us know\nwhat you think of it thank you\nawesome great great articles amita and\nyou're going to speak uh you're going to\nwe're going to get to this in a second\nat chennai at the conference so it's\ngoing to be a little bit more who's in\nindia who can actually speak with it\ngets a lot more insights there cool um\nso let's talk um about union ml and uh\nwe have niels here so niels uh give us a\nlittle bit of an overview about uh\nwhat's next to what what union ml is and\nuh you know obviously\num where you're going to speak uh with\nregards to mlabs world so give us the\nscope\nyeah thanks martin um hi everyone i'm\nniels\num yeah i'll be presenting\nat the envelopes\nworld conference in toronto next week\non the union ml project that i presented\non it maybe a few weeks back here on\nthis\nflight oss sync\num so really i'll talk kind of quite at\na high level at first of union about\nunion ml\nand kind of dive into what\nthe code looks like since everyone wants\nto see the code\nbut yeah just to give you a summary\nreally the\nthe nugget of inspiration here was\num\nwhat does the interface look like that\nhelps people\nbuild\nand deploy models from an ml ops\nperspective so there are a lot of\nmachine learning frameworks out there a\nlot of data frameworks out there\nand really\nas someone who really likes kind of\nto think about the ergonomics and the\nhci component of machine learning\nhow do we make developers more\nproductive\nwhat does that interface look like and\nyou know the inspiration here was to\nmake an analogy\nyou know you have the http protocol and\nthen you have http methods which\nstandardize\nwhat you can do with that protocol\num in machine learning i think we're\nconverging and you know to to comet\nabstractions like fit train\nevaluate predict\nso you know really this project is\ncentered around what does that\nwhat does that interface look like\nnot just from a theoretical perspective\nbut something that actually works and\nsomething that people can\ncan use so this is you know kind of like\nuh\nbuilding off of flight and all the cool\nstuff we've built on top of it and\nis a machine learning focused\nmeta sdk or like higher level sdk for\nmachine learning\ngreat so you would say\ndata scientists would feel very\ncomfortable\nusing that\nhigh level yeah that yeah that's that's\nthe goal um\nyou know we're gonna have a call to\naction in the conference itself um\ndetails will be upcoming there but yeah\nwe we really want to build a community\naround this and um\nyou know\niterate on it not just based on our\nopinions but everyone else's opinions\nand thoughts on how to how to actually\ndo this\nawesome great okay there's a lot more\nlet's do a\nnext topic here that is about the vs\ncode extension uh matthew we have you\nguys working on\nthat vs code extension why don't you\ntell us a little bit about the vs code\nextension and give us maybe a little bit\nof model about uh you know i think the\nnext meetup where we have you talk about\na different topic\nyeah so uh can you hear me all right yep\nokay cool so\nuh really happy to be here thanks for\nhaving us uh i think jake's here as well\nbut uh just happy to share our work here\non this extension\nuh\nbasically our thought process behind\nthis was you know\nuh\nflight is pretty easy to use and adds a\nlot of nice features but there can be\nsome friction just getting things\nuh up initially if you're not\nexperienced you haven't used the tools\nin the command line\nyou know maybe your data scientists and\nyour organization want these features\nbut they don't know\njust quite all the\nyou know command line copy base they\nneed to do to get into\nuh production uh and so\nwe just sort of automate that process\nthrough the sort of getting started\nguide of you know i need to set up a\ndocker to push to here i need to set up\nthis registration step i need to\nserialize my workflow and then i need to\npush it and so\nwe just take those sort of three basic\nsteps you know configure my docker push\nto here configure uh or do my\nserialization and then register my\nworkflow\nuh and then we also just add a little\nstep to initialize a new workflow uh if\nyou're starting from scratch\nuh and so we put those three things into\njust the command palette of visual\nstudio code\nso our data scientists can just hit ctrl\nshift b\ninitialize workflow get a workflow from\nnothing\nand then ctrl shift p again register\nworkflow select that workflow\nselect the\nproject select the domain of that\nproject\nand then just kick off automation that\nwhen it finishes it'll give you a little\nlink you can go to that webpage and\nbring up the flight console you'll see\nyour workflow there you can go ahead and\nrun it\nso\nthat's really exciting stuff that we're\nhappy to share uh jake nyer and i worked\non that uh happy to share that\ncoming mostly from stride works and then\nnow\nfor next week or next presentation i\nguess in two weeks\ni'm also happy to share something that\ni've been working on\nto\nmake\nessentially an sdk for javascript\nworkflows so if you\ndecide that python is not really your\nspeed or you know you prefer to work\nwith javascript and you really love\ntensorflow.js maybe\nyou can\ncreate register and serialize javascript\nworkflows and then use them just the\nsame as you might with say\nflight kit\nin python\nawesome\nvery cool we're looking forward to to\nthe community thing in two weeks and i\nthink for everyone else here don't\nforget uh to start the repo so uh super\nexcited about that um\nvs code extension let's take a look at\nthe upcoming conferences i think we\nspoke about two already sandra if you\ncould move forward\nwent back\nso\nso we have the kubernetes community days\nin general so you're going to see samita\nthere she's going to talk about\nobviously everything flight not just\nemma ups with flight but you know you\ncan\nreach out here and learn a whole lot\nabout our flight experience and\nspecifically\nusing uh you know getting machine\nlearning models into inference into\nproduction it's going to be very\ninteresting to see that so that's on\njune 4th\nand uh we heard niels earlier talk about\nthe ml ops world in canada so he's going\nto talk and show i don't know if he's\ngoing to talk neil's but we're going to\ndefinitely present\nand have contributions about union ml so\ni think union ml is really for us uh\nhere a highlight of the month and i feel\nlike\ncoming from a data science background\ni'm super excited about you nina well\ni'm going to say this a few more times\nin the future all right let's go forward\nand look at the office hours we have\nhaitham\n7 7 30 a.m we have katrina there he's\ntaking the mid day\nand of course katherine is taking the\nthe late night uh i feel like it's\npretty late night so if you\nuh you know want to attend would be\nawesome to have your tent ask questions\num you can ask the flight maintainers\nanything in those office hours so i\nthink it's super\ngreat to have them and i want to say\nthank you to all of those of the three\nwho are actually hosting those office\nhours okay\nwith that the upcoming sync\nmatthew spoke about the javascript sdk\nfor flight there are a few more links in\nhere about how to\nsign up to the community sync you can\nalso find our youtube channel in the\nnewsletter and i also want to make sure\nwe should put in the you know github\nrepo for flight as well and don't forget\nto start that one so we're really happy\nif you get your stars as well or develop\nthe contributor stars who haven't\nstarted so far\nnext kaythan so let's talk about the\nroadmap and uh you have a few things to\nshare with\nus um\nyeah a few things i decided to do\nthe roadmap a little differently\nthis time i wanted to explain\nall the things that are in progress\nthese are only the things that i\nknow of for i probably remember\nand if i missed somebody i'm sorry that\nwas not the intention so please let me\nknow if i can add your name i would love\nto know what you guys are working on\num but i know flight decks has been a\nvery very highly requested feature and\nwe\nwe are\nit has been merged in flightgate but uh\nin fightcon so it's going to be part of\nthis month and that's one of the things\nthat uni i have been working on and we\nare looking forward to what people can\ndo with it i think\ni just got a brief demo of it and it's\npretty awesome so thank you\num\nand continuing with all the improvements\nin streamlining flight kit and how to\nregister and load there is\nlast or a month and a half ago we\nreleased five flight run\nuh which allowed you to run a script\nimmediately without uh\nthinking about serializing registering\nand so on\num but for larger reports registration\nis still the preferred way so pi flight\nregister instead of flight cli register\nand wi-fi serialize all of that is going\nto get condensed into pi fight register\nand this is again union eyes uh working\non this part\nand we would love to\nmake sure that all of you guys know\nwhat's happening uh in in pipeline\nregister try it out give us comments\nuh\nthis has been a very requested feature\nnot only by union and spotify\nfew other companies like tesla and\nshopify and etc it's human in the loop\nworkflows that means if you\nare running a workflow and you want to\nsuddenly pause the workflow\nuh wait for an external signal like\nan approval or pass some data maybe\nyou're doing labeling etc\nthat will now be supported when flight\nand we are working on that\nwith spotify\num there's a huge use case and modifier\nas well\npachama has been slowly chugging along\nand\nadding flight plus task integration they\nhave it working internally they will be\nopen sourcing it at some point\nuh i'm working i'm trying to\nuh get a date i don't know exactly the\ndate when they're open source but i\nthink it's awesome potential and awesome\nintegration so dash is a python library\nthat allows you to scale out um\npython computation then flight will\nmanage the cluster so that's the best of\nboth worlds\nso a user can get a serverless\nexperience while using fl dash\nthe next one is dvt plugin i think this\nhas been\nstained for a little bit so i'm\nfollowing up with project but\nuh the plugin works there's like a minor\ncouple things and then we'll get merged\nthis is cbt so you can run dvd\npipelines within flight\nanother one is flight on azure i just\ngot an update today black shark making\nteam actually steph from duck shark\nhad it working on azure\nthey found issues with um the fspec\nlibrary so they actually rebuild the\ndata layer\num and i think helsing that ai is\nanother one that that's working with\nthem\nflight plus y logs this is uh\nif you want to get more\nlike more with visualizations and\nvisibility into data distributions etc\nso that's why logs is working on this\npart\nwe are working on a bunch of different\nml types suppose starting with numpy and\nthen you'll see four and after a couple\nof them we'll probably open it up just\nfor everybody in the community to add\nsupport for\ndifferent ml types like tf tensor etc\nwe are also working on flight plus onyx\num so that you don't have to really\nthink about how models move this may\ntake a little longer still thinking\nabout the extremely good experience that\nthat is right for the users\num\npropeller performance this is always a\nback burning thread that we have we keep\non working on uh parts of it we've not\nbeen focusing as much\nfrom all our uh you know\nfolks but recently urinary and spotify\nhave been diving deep and improving\nthings\nspotify has been running more than five\nmillion workflows a month um so yeah\nit's on one cluster so it's getting\nreally really hot so we're trying to\nmake sure that it keeps on\nscaling uh lyft has been doing something\nfantastic multi-architecture support for\num\narm x86 etc etc so\nand this is extremely useful for\nreducing cost\naws if you are an awf shop\nthey have graviton support which is\ntheir arm-based uh self-fabricated dies\nthese have about one-fourth the cost of\na regular on-demand instance so\nlift is uh make adding it to the\nto the workflow as a primitive so you\ncan actually just target a workflow to\ngo on to an arm machine\nwithout really doing any\nmuch changes of course you have to build\na container slightly differently but\nit's not much\nand when we are working on a lot on\nflight consoles and i think you'll hear\nit a little bit\nnext one\nso uh we also wanted to say that we are\nkicking off a few different projects and\nif you guys are interested in\ncollaborating contributing getting\ntogether um discussing more please join\nin one of these\nuh projects i think eventually these\nwill become six like you know folks who\nare interested in leading specific\nthings\nbut\nthere has been some interest in flight\nand data hub integration across\ndifferent companies\num there is a channel for this and even\nin the data hub side there's some work\nand volt is leading the charge\nand we are just helping union data is\nhelping them and jpmc actually has a\nprototype for this and so\nall of them are coming together and\nthat's fantastic to see in a community\nuh similarly\nuh there has been a heavy amount of\ninterest in flight plus ray uh\nintegration and ray is this again just\nlike das it's a way to scale out python\nprocessing\nuh slightly different from tasks so in\nterms of the architecture but they\nsuffer from the same problem that you\nhave to first create a cluster just like\nspark and so on\nwith flight we are trying to bridge that\ngap make it possible that all these\nclusters are automatically created\nbut we are also trying to create a new\nthing where\nyou can reuse the same created cluster\nacross multiple workflows and this will\nlay down the foundation for using pods\nand spot clusters and whatever\nin the future so that amortizes the cost\nof running the workflow and bringing up\nthe fml clusters multiple types\nyeah and then\nwe we would love contributions on\nmultiple different things there are few\nfew of these are issues already fired by\npeople a few of them you'll probably be\nhosting hackathon or something\nyeah uh one thing that i wanted to do is\ncreating a prioritization list that few\nfolks can you know\nvote on or\ntalk about and help us prioritize at\nunion or across the community\num and\nin the past though like you know\nprioritization lists don't always get a\nlot of different\nopinions it's always important to get\nthe community involved in this sort of\nstuff so\nplease let me know if you have ideas of\nhow we can do this that would get the\nmost people to participate um\nand we would start off with something\nbut you'd love\nmore participation in that\nthat's it so those are the two things we\ntry to do this every now and then\ninstead of talking about like\nimmediately what's happening in the next\ntwo weeks we are going to talk about\nthese are the big things uh items that\nwe're working on\nand you'll see lots of new interesting\nstuff and how folks are contributing so\nif you want to contribute\nit's an open source project just\ncontribute\nthank you\nawesome\nhey clifton thanks a lot\ncool um i think that's great i'm looking\nforward to the to the ray uh project as\na former spark user i think this is\nsuper interesting as well so see what\ncomes out of that"
    },
    {
        "title": "Flyte's Script Mode - pyflyte run",
        "transcript": "so my name is eduardo and today we're\ngoing to talk about this this new\naddition to interacting with flight this\nthing that we we deemed um to call\nscript mode or\nit's basically a new sub command to\ndefine flight um cli\nquick agenda uh i'm gonna talk about\nlike what is this command some of the\nfeatures that we implemented\nsome of the feature work that we're\nplanning and you know open for open up\nfor a q a real quick\nso why a new command um feedback time\nand time again you know we listen from\nmultiple people that\nespecially like when you're getting\nstarted like flight is really hard to\nuse\num\nso we devised this this new command\nrun aka script mode it lets you run a\nspecific like a single\nfile if your file contains your audio\ntests or your overflows you're good\nyou're good to go to use um pi fight one\nthe rest of the options are not super\nimportant we're gonna dive into some of\nthose throughout the presentation\nso\ncompared to how we asked users to\ninteract with the system at the\nbeginning\nhow do you even run a workflow to begin\nwith is this five-step process right you\nhave to build an image you have to\npackage your code you have to register\nit and finally you schedule it using you\nknow the ui with flight console or cli\nvia flight cli or ctl or\nyou can use a programmatic access via\nflight remote api\nfinally you get something to you know\nthat executes on a flight cluster so\nwe compare this to how you actually run\nusing by flight run\nsingle command and encapsulates\nall these these steps in a single\ncommand you do pi flight run dash dash\nremotes to indicate that you want to run\nit on a remote um\nflight deployments\na file name that contains your workflow\nspace\nand then your workflow\nand then you can pass the parameters\nthrough the cli and that in itself is is\nan interesting problem that we're gonna\ntalk a little bit about\nobviously um in order to to do this\nspike flight one has to be extremely\nopinionated about some of the stuff that\nyou have to provide when you when you\nuse like the regular way of of um\nregistering packaging your code for\nexample um here\nin in these\nbrackets you can see that\nyou have to provide an image and you\nhave to provide a version to actually\nlike type things together right and in\npi flight run um we kind of cheat a\nlittle bit because we provide a bunch of\nimages that are pre-built so we don't\nhave to worry about like the first step\nwhich is building an image\nwe packaged the code\nusing\num fast registration i think we already\ntalked about fast registration here if\nfolks are not using it they should use\nit more by fight run makes heavy use of\nfast registration\nthe version is another parameter that we\nasked from the users um now we derive\nthe version from the code itself so we\nhash it and use like\na bunch of parameters around that but\nlike you it's another prompt that you\ndon't have to worry about um as i\nmentioned again\nalready uh the the code has to live in a\nsingle file all the tasks and workflows\nthat you are registering using by fight\nrun they have to live in a single file\nobviously if you already have like tasks\nand um\nall the things that you're referencing\nin your workflow like they they can in\nthey are already registered\nfeel free but like\nsimplify the use if everything lives in\na single file you're good to go it's\nalready touched on um workflow and task\nparameters which\ncan be passed from the the cli\nand um\nyou can use this the same\nux like it's the same command to run\neither locally or or you know or\nremotely which it\nhelps you know\nin in the iteration but um\nlet's talk about some of the features\nfive flight run has a wide support for\nfor input types we're gonna dive into\nlike some of those\nin in a bit um you can run from anywhere\nthat's also another um cool feature that\nthat we implemented like sometimes you\ndon't want to be\nclose to the fire that you're going to\nbe executing you can actually point to\nthat file from anywhere in the in the\nfile system\num\nthe help\num\ntext\nis generated on the fly so it's\nself-describing it lets you know what\nare the types and the names of the\nworkflow parameters so that you don't\nhave to like\nswitch between your editor and and like\nthe cli when you're running um when\nyou're about to run a workflow\nand um we also like provide you know a\nvery simple way to like\nwe provide you with a snippet that lets\nyou access the result of your execution\nprogrammatically using flight remotes\nlet's go over um each one of those uh\nas i mentioned white support for for\ninput types as you all know flight has\num\na type system so pi flight run supports\nall the simple types and that includes\nits floats strings booleans duration and\ndate time\nyou can also pass lists dictionaries\ndata classes in painters data frames\nyou you have to ask like\nhow do you pass a data frame from the\ncli so we made this conscious decision\num as long as the\nthe the format of the data frame is a\nparquet file and it's a file in your\nlocal file system you're good to go so\nlet's take a quick look can you guys see\nmy screen still\nyes yes\ngreat amazing okay so\nis the\nthe font size okay\ni can\nincrease this a little bit is it better\nnow\ni'll go with this better for the chord\nyeah not for the cli yeah i'll increase\nthe phone from the cli later um so let's\ntalk about the code real quick on the\nright hand side we have this very simple\nworkflow\num we take you know a bunch of\nparameters like in strings lists\ndictionaries data class and appends a\nframe and i'm not doing much i'm just\nlike printing each one of this so like\nall the tasks here they just print\nthe the actual inputs but again this\ncould be more complicated right um\nworth noting here how the data class\nlooks like\nit has only two fields and into this\nstring\nnow let's take a look at how we can\nactually run this in the cli so you do\nlike pipeline run let's is it is that\nokay font size maybe a little bit more\nmore zoom in okay oh i guess that's the\nis it better now oh my god this looks\nhorrendous\nwe uh let's parse this real quick so by\nflight run dash dash remote as i said\nand then i'm pointing to a file like\nthis file that we just described like\nthe one that describes the workflow um\nnotice how i'm naming the workflow here\nwhich is the exact\nname of the\nthe workflow um functional definition\nand then i'm passing the parameters so\nwe have\nan int\na string\na list see how the list is is described\nlike it we're just using like a a json\nformat a dictionary which is you know\njust like a simple\nin in\njson format same with data classes so\ni think this one is just interesting and\nfinally the the last parameter the data\nframe so\num parking tests is a file in my my\nlocal file system i'm just doing you\nknow pointing to it and\nlo and behold after you run it you get a\nlink to the flight console with your\nexecution\nwait by um dude you know\ni guess that's wrong\nlet's try this again\nyeah see\nlike you don't have to like remember any\nexecution id you don't like to type\nanything you get a url\nwith that points to the actual execution\num\nrun from anywhere that's also um\nanother feature real quick so bye\nclean\nso take a look at my\nmy example here i have\na\nfolder structure that like especially in\nthe master case i have like a\ndirectory called master that has a sub\ndirectory called workflows and another\ncalled directory and finally inside of\nthat i have\nmy example.pi so let's\nopen um\nexample.nested\nworkflows directory it's simple\nagain not oh my god\nnot doing a lot\nbut um um sorry can you can you zoom in\nmore on the cli side of things\noh really\nuh yeah how about now is it better it\nlooks much better\namazing cool um\nwhat else yeah so\nwe're trying to run this workflow that\nsits inside like a three level deep um\nhierarchy right\nso\num my flight\nnested\nso this thing takes a single parameter\nso i'm i'm i'm sitting at the root of\nthe of the\nof the project now\nsee how i'm pointing to it\nusing like the relative paths of nested\nworkflows directory and then examples.pi\nbut um\nso it ran\nbut if i go to nested workflows and i\npoint to it\noh yes i'm missing\nah\ndirectly here\nit also works\nso run from anywhere you can even be\noutside your project it would do the\nright thing\num the way it does this is that has this\nvery um\nit's kind of a simplifying assumption\nbut like we we walk up the\nthe folder structure trying to find the\nfirst directory that doesn't contain an\ninit file\nthat's what we consider the\nthe the root of the project\num\nworkflows and tasks\nyou know i that the two examples that i\nran are just like workflows but it works\nthe same way with with tasks\num the self-describing example that's\nthat one is is an interesting one so um\nlet's use this example that we just ran\nso if we do\nfive flight remote\num\nworkflow dash dash help you see\nwe have a parameter called msg\nthat is required that that is a hint\nthat you have like um default values for\nparameters too so for example if i did i\ndon't know\nabc\nnow you see like it's no longer required\nand it tells you hey you have a default\nvalue for abc so that works across all\ndeeper the input types\nand\nfinally we have programmatic access to\nthe execution snippet\num\ni'm not going to demo this but\nit works by passing this extra flag\ncalled that\ndumb snippet\num\nnotice how we\nemit this this\nsmall you know python code that lets you\nusing flight remote investigate and like\ninspect the results of you of the\nexecution\nthis is good for this is amazing\nactually for like these these more\ninteractive environments like jupiter\nand\nlike stuff like that\num\nfuture work we want to support uh\nmultiple workflows and definitions seen\nin like different files um\nthinking about you know adding auto\ncompletion of inputs so that you can\nactually you know just like tab all your\nway\nall the way to the execution of the\nworkflow um\nwe have\neven though i claim that we have a wide\nsupport of input types um we can always\ndo more you know there are some types\nthat like people are interested like the\nthe support for data frames is kind of\nlimited we we only allow for pandas data\nframes using parquet right now\nand um\nwe only have basic images that contain\nlike fight get installed in like very\nfew um like libraries so we want to also\noffer\num other images like you know so so that\nwe can allow people to run quite watch\ntensorflow and like other more complex\num\n[Music]\ninitial workflows\nand with that how\ni open for questions and\ndoes anyone have any questions\nhey eduardo this was a wonderful demo\nreally exciting stuff um i may have\nmissed this earlier so apologies but is\nthis um is it possible to use a uh\ncustom image for these like if we have\nan internal image that's like an\ninternal registry somewhere yes yes um i\ni skimmed over it but one of the\nparameters to pi flight run is actual\nactually\nimage so you can pass like multiple\nimages but yes\nokay wonderful makes sense\nyeah um firstly\ncan i put a meta comment in here i love\nthe\nuh interaction and thank you edwardo for\nmaking it energetic this is awesome\neverybody is a small wing thank you for\nall the chats and comments it really\nhelps us makes us feel that we are doing\nthe right thing too\nthank you\ncan i ask one question\ngo for it um curious if you have\nthoughts on how you can signal to the\nuser like what's happening post submit\num like right now you're just including\na url but wondering if we want some sort\nof like a wait and exit after\nsome event or something like that\ninteresting can you say a bit more\nyeah so like imagine you have a\nlightweight first task that does some\nvalidation of inputs beyond what you can\ndo with just the type system\num how could like spec validation for\nlike ml purposes um\nhow could you potentially do that spec\nvalidation\nwait until that spec validation happens\non remote before actually exiting with\nyou know exit code zero\ngot it yeah no\nthat's\nno we haven't thought about it that's\nthat's a\ngreat use case i'll you know\nmake a nation of it that's amazing\nthanks\ncan you make an issue yeah and also\ndefinitely\nno of course\namazing\nhey lordo\nwhat's up um\nwhen are we going to be able to run this\non a directory\nmultiple scripts\nwait you want to run\nworkflows pointing to a directory is\nthat what you're asking\ni want then workflows and tasks just in\nmultiple files that's what it is\noh we multiple\nthis is not coming the next release but\nit's something that we're actually\nworking on\ni i think it's an important thing\neduardo that probably we did not touch\nis the mechanics of wow how the fast\nregistration is working underneath\num just for the folks who are interested\nin the guts and i think we should talk\nabout the cuts here sometimes\nso uh this is you utilizes 1.0 new\nsupport for signed urls underneath\nuh that means the code is getting fast\nserialized\nand uploaded\nand all of that is happening through\nthe\nassigned url that's getting sent by\nadmin so the you the user does not need\nspecial permissions to write to s3 gcs\netc\nit's all handled by the admin uh service\nitself\nand we wanted to have a unique version\nand so on and that's why we decided to\nuse one single file\nnow doing multiple files is absolutely\npossible it's just that it's more work\nwe wanted to get this one streamline use\ncase going\nand and get that\nyou know evangelize that and see how\npeople like about it i mean and we\nformally also wanted to update the\ngetting started that's really was also\nthe the goal of this\nbut you know we realized that this is\nsuper powerful multiple files is\nrequired\nyes we could agree more\none other thing i wanted to mention here\nis since we have this new way of running\nscripts\ni don't know how people interact with\nflight snacks or like the user guide and\nthe examples but\num we're working on a thing that\nyou can kind of just download the python\nfile and then flight run it\nyou know um with some commands there so\nit's like super easy to interact with\nthe examples instead of just reading it\nyeah\nwe've tried to made all flight snacks\nwork with wi-fi run there are couples\nstill that don't work oh and by the way\nthis should work across any custom\nimages so for example in spark\nyou want to have a custom image you\nshould be able to use hyphen iphone\nimage and we do publish these images for\nflight sex same thing for distributed\ntraining same thing for uh\nspecific gpu specific images\num\nand to be honest they don't even need to\nhave the code in them that's why that's\nwhat eduardo was saying we were thinking\nof publishing stock images now the\nproblem is i think maintaining stock\nimage is extremely hard and that's why\nwe need community support would be\nreally really fantastic\nnice um\nif you have more questions please feel\nfree to you know reach out um we are\nsuper pumped about this feature and we\nare going to\nlean on it so to speak"
    },
    {
        "title": "Taking Flyte at Freenome",
        "transcript": "cool so\num you know some of you know me some of\nyou may not my name is jeev i'm a\nsoftware engineer at freenom\num and this this dog has been a long\ntime coming it we adopted freedom almost\nlike over a year ago and i'm very\nprivileged to be able to give this talk\ntoday uh just to kind of showcase like\nhow we use flight at freedom uh and some\nof the interesting things that you know\nwe think we've built over the last year\nall right oh so you can figure this out\nnever given a talk on zoom all right so\nuh quick agenda we'll talk a little bit\nabout who uh freedom is\nuh and hello flight so where freedom\nactually meets flight um and i want to\nintroduce this concept or we'll talk a\nlittle bit about this concept of\nreactive workflows with tower\ni know that's something that actually\nhad a blog post kind of related to this\ni want to touch it out a little bit more\nand then we'll leave some time at the\nend for questions\nso who is freno so\nfreedom is actually a very\ncross-functional team of you know\nmolecular biologists computational\nbiologists machine learning engineers\nsoftware engineers like medical industry\nhealth tech veterans regulatory\nprofessionals so on and so forth\nand we're here to build uh next\ngeneration blood tests for early cancer\ndetection\nand why early detection you may ask\nso to direct you to this chart on the\nright here\non the x-axis we have a bunch of\ndifferent cancer types on the y-axis we\nhave survival rates\nthe black bars\nindicate like localized cancer and the\nyellow bars indicate like metastatic\ncancer and what's apparent from this\nchart is that you know once your cancer\nmetastasizes your survival rates\nactually drop significantly across\nmultiple cancer types\nso if we're going to have an impact via\nintervention\nwe want to detect cancer early when it's\nstill localized and not\nnot metastatic\nright\nand so uh how do we do this and so\num you know this is actually a pretty\nhard problem\num because if you want to detect cancer\nearly the signal is very very low right\nbecause there's a small amount of cancer\nsignal in like basically drowned out by\na whole bunch of healthy signal\num and so the approach that we take and\nthere's a number of companies that are\nactually like working on this problem\nbut like the thing that makes us stand\nout i think is that we are taking a\nmulti-omic approach combined with\nmachine learning\nand i say it's multi and i say\nmulti-ohmic because you know we're\nactually looking at more than a single\nbiological molecule uh we're attempting\nto like uh characterize quantify and\nextract biomarkers from like many\ndifferent types of biological molecules\nincluding proteins and dna and more\nand this helps paint a really clear\npicture\nof this patient so that we can then use\nthis information to you know apply our\nmachine learning\nand make a call about whether this\npatient has a cancer signal or not\nand the really exciting part about this\nis that like we can actually do this\nfrom\na routine uh blood draw and so no\ncomplicated testing procedures like no\nyou know like days or weeks or whatever\nin the lab or anything like that you can\nwalk into your doctor's office um you\nknow as part of a physical or walk into\na lab get your blood drawn and actually\nhave this uh this test administered\nsuper exciting\nuh and where does flight fit into all of\nthis i mean we're trying to we're trying\nto do something important\num and you know a lot of it is like you\nknow really running workflows\num and i want to share some of the\nplaces where we use flight in this\nentire platform so\ni actually thought i would start with\na quick summary of like how we're using\nfriday frino before we dig into why we\nchose flight and so\num you know we are a biotech company and\nso you know we're doing there's a lot of\ndata flow there's a lot of\nbioinformatics we're also a machine\nlearning company so it's a lot of\nfeature engineering and machine learning\nand so you know everything from like\npushing data from our production systems\ninto research right where data\nscientists can then operate on it\nextracting data from external sources\ninto our internal data stores\nyou know running bioinformatics really\ncomplex large\nand heavy bioinformatics workflows on\ndata coming out of our wet lab that our\nmolecular biologists are generating\nuh you know like processing data from\nexternal sources there's a wealth of you\nknow public data available\num using that to like learn more about\nlike you know what the hallmarks of\ncancer are and so on and so forth\num you know processing pre-processing\nour our molecular data into a form that\nwe can uh run feature engineering on\nlike generate features\nuh and then run machine learning right\nso like hyper parameter optimization\ntraining and inference\nand then a small subset of all the work\nthat we've done in research up at this\npoint you know makes its way into our\nproduct pipeline\nand\nthat's where like you know we're\nactually running against real test\norders and so flight as it turns out as\nyou can see like you know this is weave\nthrough many of these components like\nflight actually\num is used in every single one of these\ndifferent components so it's like across\nour entire data engineering and data\nscience stacks\num and so it's not an understatement to\nsay that flight is really a workhorse at\nfreedom\nand so you know we've and you know this\nis something that we've actually built\nor\nuh set up and you know had going over\nthe course of the last year\num and i think that like you know we've\ninvested a lot in you know uh in things\nlike a sandbox right like the stuff that\nyou uh that's available in flight cuddle\nsandbox\nuh in templates and stuff like basically\nyou know inspired by the work that union\nhas done but inspired by the work that\nlatchville have done\nand we really wanted to like you know\nget flight adopted quickly across the\norganization but most importantly i\nthink\num\nyou know flight made sense uh for you\nknow from many from many different\ndifferent perspectives at freedom and so\ni think that really drove adoption at\nfreedom so i want to talk a little bit\nabout uh why we chose flight at freedom\nand hopefully\nyou know as a bi-tech company right why\nwe chose flight and hopefully that\ninspires uh you know other viatech\ncompanies to also consider flight uh to\nrun their stack\nso why flight\nuh so you know we're a kubernetes shop\nlike all of our workflows everything\nruns on kubernetes uh kubernetes is\nbasically ubiquitous now it's all over\nthe place and it's making like you know\nthe the running workloads like and and\noperating and monitoring like cloud\ncloud workloads like really easy\nand so we uh one of the important things\nthat we wanted was a kubernetes native\nlike workflow engine and there's\nactually uh you know less than a handful\nof them around uh and flight was one of\nthem\nuh the important thing for us was also\nthat like we needed you know a workflow\nengine like this since it's like so core\nto the stuff that we do at freedom we\nwanted it to have like a sense of\ncredibility uh you know to be battle\ntested and so on so forth uh and so you\nknow flight was developed at lyft uh by\ncadence team\nand you know at lift it was like up you\nknow running more than a million\nworkflows per month across 500 users uh\nand lyft has a really rich history of\nopen source software including like you\nknow uh i guess envoy and amundsen and\nso on so forth so we felt very good\nabout adopting technology coming out of\nlyft this way\nit has an extensible plug-in system this\nis like this was really important for us\num you know we're running all of our\nmost of our workloads on kubernetes\nright now but flight is really execution\nengine agnostic and so like if you\nwanted to run spark workloads if you\nwanted to run pytorch workloads uh or\nwhatever else like you know aws batch or\ngoogle ai platform or something like\nthat like we can basically write a\nplug-in uh that would work and you know\nsome most of these plugins already exist\num\nbut uh it allows us to like grow right\nas our needs evolve like we can still\nkeep the\nvery basic ux for our users uh and then\nlike run on all these different engines\nthat like you know have their own pros\nand cons and so on so forth so that was\nreally nice for us um it already has an\nsdk in python so that's less work that\nwe had to do and it's a great sdk flight\nkit has been amazing\nuh there's been a lot of development on\nit and we're very excited to use it\nwe're using it basically everywhere at\nfreedom right now\nuh it's fully open source with open\nsource at kubecon\nuh and sorry i should have updated this\nslide it's a graduated project in linux\nfoundation ai and data umbrella not\nincubating anymore so that lends even\nmore credibility and so as a biotech\ncompany attempting to get like you know\nfda validation for our product you know\nhere are all like the things that we can\nsay uh that we can use to defend our\ndecision to go with flight\nand then most importantly\nuh you know an open source product open\nsource project is like not really great\nwithout a really strong community you\nknow things like getting prs approved\nand like getting help and support and\nthings like that has been like really\nseamless uh within the flight community\nso\nyou know we have a lot of power users\nhere we have lyft spotify go jack woven\nplanet latch bio you know morantik so on\nand so forth like i missed a whole bunch\nbut like they're on the flight website\nuh and everyone has been like super\nhelpful and has really helped us like uh\nsupercharger adoption so we're very\ngrateful for that\num so a little bit to dive into some of\nthe features that we really care about\nabout flight i know that you know maybe\nthis audience like i've seen this so\nmany times but like maybe it's a good\nthing to just kind of reiterate why this\nis very useful for us so it's python\nright like um and you know i mentioned\nearlier that we're a cross-function\ncompany we've got molecular biologists\nuh computers biology and so on so forth\nso we've got like a wide range of uh you\nknow like engineering capabilities and\nwe want everyone to be able to feel\nempowered to use flight and so like you\nknow having\nuh just like having to write a python\nfunction to run like workloads on flight\nis really really nice for us and the\nbeauty of this is that you know you can\nrun it locally so it's just like running\na python function and then just ship\nthat remotely and run that on at scale\non the cluster and that's really nice as\nwell\nand in biotech and probably in other\nplaces as well i think with anything\nrelated to machine learning\nreproducibility is like a huge thing\nand it's like really important for us\nbecause we want to be able to defend\nlike you know certain life-changing\ncalls that we make uh for our patients\num and flight has this concept of like\nan immutable transformation right where\nthey're running within like uh tasks\nrunning within these like containerized\nenvironments and they're versioned uh\nand as it turns out they cannot be\ndeleted and so like having this like\nimmutable transformation is a really\nnice abstraction to have\num for our data engineering stack\nbioinformatics workflows also tend to be\nvery heavy and very expensive and so you\nknow you don't want to run the same\ntasks with the same inputs like multiple\ntimes since the flight allows us to\nbasically memoize and cache the output\njust by introducing like you know like\nyou see here\na cache\nargument to the task decorator\nuh it helps us overload uh it allows us\nto override like resources um you know\nwe might have a simple like cheap task\nand we might have like a really\nexpensive like alignment task right and\nall of them can run within the same\nworkflow just by specifying these as it\nturns out you can also override these\nlike at runtime as well and so like if\nwe if the workflow sees a really large\nfile like it can basically adjust its\nresource requirements like on the fly\nfor the child tasks that need to run so\nthat's been really nice for us as well\nand given that given the scale at which\nlike some of these tasks run like you\nknow it can get really expensive in\nterms of compute and so just being able\nto like add\nuh you know an interruptible argument to\nthe task\ndecorator\nfor certain tasks has been really useful\nin like you know cutting back our costs\na little bit as well\nso some really nice features coming out\nof the box like batteries included with\nflight so that was really nice to have\nuh and this concept of extending tasks\nright so like we talked about writing\nlike uh like plugins earlier and you can\nalso write like python plugins right\nlike python tasks as a way to kind of\nlike extend the flight sdk a little bit\nuh and so here i show an example of a\nsql alchemy task but like for\nbioinformatics you can easily have like\nyou know an alignment task or a variant\ncalling task and stuff like that where\nyou just wrap internally uh the logic\nassociated with passing like parameters\nto like you know an aligner of your\nchoice or a varying call of your choice\nand stuff like that so since it's just\npython it's really nice to build uh it's\nreally easy to build like these\nabstractions right on top of the\nunderlying uh binary or tool that you're\nusing\num and this makes it very easy for\nusability so like other teams can just\npick up and they don't have to like\nfigure out this underlying alignment\ntool like they can just work off of this\num this custom task\nworkflows um so i mean obviously this is\na workflow engine\nand like you know we want to be able to\nstring together these tasks in a way\nthat makes sense and the beauty of uh\nwhat flight gives us is that like type\nvalidation occurs at registration time\nso we are guaranteed that the tasks are\ngoing to play well with each other when\nwe're registering the workflows so\nthat's really nice\num sub workflows this is something that\nwe were very excited about um and so you\nknow again with reusability and trying\nto keep code dry\num being able to like share workflows\nwithin you know python libraries and and\neven better i think is that we're\nactually able to like call into\nreference workloads that are not even\npart of your project uh that are that\nhave been like pre-registered with\nflight and actually run things like you\nknow featurization or whatever right\nfrom like within your own project so\nthere's a lot of room to really share\nthe work that you've done that your team\nhas done in terms of writing workflows\nwith like other teams and stuff like\nthat so it gives us it really increases\nlike room for collaboration and stuff\nand we really love that type of reno\nuh dynamic sub workflows this i think is\nprobably one of the most important\nfeatures for us uh there's actually not\nvery many workflow engines that can do\nthis and so this was like you know this\nwas a this is a big deal for us\nbasically and so you know with\nbioinformatics you can have like files\nin the order of like terabytes sometimes\num and you know one thing one you know\nlike a simple operation that you might\nwant to do on this is essentially like\nto chunk this file up into smaller\npieces process them in parallel and then\naggregate the results at the end and so\nuh you know a feature like dynamic tasks\nlike helps us do that really really\nseamlessly without like having to you\nknow add crazy hacks\nright so um and here's an example of\nlike a simple simple like chunking of\nsome inputs running it and then\nreturning some values that can be\naggregated later\nso fan in find out type workflows very\ncommon in bioinformatics uh and we\nreally appreciate this feature\nso coming back to like\nyou know this diagram that i shared\nearlier right so like there are all\nthese like features in flight that made\na lot of sense uh for us at freedom um\nand you know like the\nlike using flight in like research\norganizations so like research in the\nresearch context has been like really\ntalked about a lot i think you know uh\nwith respect to like feature engineering\nand machine learning there's a lot of\nexamples that have been shown\num\ni wanna i wanna talk a little bit about\nlike running flight uh as part of a\nproduct pipeline because that's not\nsomething that has like traditionally\nbeen talked about at this meeting and i\nwanted to introduce like the concept of\nreactive workflows in the context of\nlike running flight and as part of our\nproduct pipeline because\nyou know when we're running it in like\nproduction um there's not going to be a\nhuman pushing buttons like nobody's like\nfiring off workflows nobody is like you\nknow pushing play or anything like that\nthings just kind of need to happen right\nlike files drop things need to happen\nmessages get received things need to\nhappen\nuh you know schedule\ncron triggers things need to happen and\nso on and so forth and so we wanted a\ncompletely like no human in the loop\nlike hands-free approach to running like\nall of our you know pro our flight\nworkflows that we're running as part of\nour product pipelines\nand so uh you know we talked a little\nbit about this in the past but we\nactually built a small framework a light\nuh layer around flight that helps us\nessentially like orchestrate\nuh these reactive meta workflows using\nflight so i want to talk\ni want to take a little bit of time to\nkind of talk about that but also\nintroduce this concept of like reactive\nworkflows and the power that it has\nuh you know in production like you know\nas part of a production line as part of\nthis like conveyor belt that runs\nuh in production\nso so we built a little a little tool a\nlittle wrapper called tower uh and it's\nbasically an air traffic control tower\nyou know flight air traffic control\ntower uh and so it's it's almost like a\nway that uh that like we\nuh we can orchestrate you know very\nself-contained like uh flight workflows\nor modular flight workflows uh into you\nknow much larger amount of workflows\nthat run as like you know that might\nwalk a sample through the entire product\npipeline all the way from like you know\nreceiving raw inputs to generating a\nreport for the patient\nright so so what so what exactly is it\ndoing like so in in it very simply like\ntower is basically receiving some events\nuh either from object storage so like\nwhen files get i mean files get created\nor dropped or uh or modified uh from pub\nsub where you know other other uh\nservices might be sending messages\num that might trigger flight workflows\nalso cron schedules\nuh and also this concept of like entity\nstate updates and so flight and towers\nbasically\nconsists of two different components it\nhas handlers um that you know respond to\nthese different events and it has like\nthis concept of entity which essentially\nis a business like unit of work right\nand so event handlers um are written to\nessentially process they're basically\npython functions and they're written to\nlike process events coming from all\nthese different sources\nand these handlers can basically create\nbusiness entities or essentially\ntransition them from one state to\nanother and we'll talk a little bit\nlater about what these business entities\nare\num and when when entities are\ntransitioned to a certain state and in\nthis case like it's running uh we\nbasically like kick off flight workflows\nuh and these light workouts will finish\nand then they'll either succeed or fail\nand like update the entity state\naccordingly\nso um\nso what so what are entities right so\nlike i said before entities are\nbasically business units of work they're\nextensible state machines and so uh you\nknow it's a it's basically a uh\nsomething that was kind of like copy\nfrom flight right where like every node\nhas a is essentially a state machine uh\nthis is built on top of sql alchemy so\nit's very light um and you can basically\ndefine like you know attributes and\nrelationships to other models\num and and like you can you can define\nthis like get shops back uh method that\nwill basically kick off a specific\nworkflow that is done a workspace\nsolution that is like you know composed\ndynamically\num you know when this thing is ready to\nrun essentially\nso by the way like feel free to stop at\nme at any point in time to ask me\nquestions i know this is uh this is a\nlittle off here\num and artifacts uh you know we have a\nfew different types of like event\nhandlers that we're supporting so like\none is artifacts uh and so like the the\num you can basically like trigger these\nhandlers on one or multiple artifacts\nand it has support for like wild cards\nand so this is what the syntax kind of\nlooks like right so you can like an app\nis basically just a tower app and i'll\ntalk a little bit about that later uh\nand then on artifacts you can just\ndefine like multiple artifacts that you\nwant to kind of listen on oops my bad\num and what that and what the what the\ncode will do is it'll extract the actual\nvalue that was passed that was like you\nknow received it'll provide that to the\nto the handler and also will extract\nthese wild cards out and provide that as\nlike adders and so you can use this you\ncan use these handlers to like basically\nsay okay now that my sequencing run is\ncompleted let's kick off like a flight\nworkflow that does that processes the\nsequencing run\nright\num and another type obviously is like\nmessages and so we have these message\nstreams that we can define so here's a\nmessage stream that basically\nlooks for uh you know within the payload\nof the message like it looks for typical\nsimple message essentially and then all\nthese messages are passed to this\nhandler and then this this handler can\nbe used to like you know create a\nworkflow sorry create an entity or\nbasically like find an existing entity\nand move that into uh\nyou know a different state\ncron jobs uh this is something that we\nyou know like we don't you really use\nthis a lot but like there's we thought\nthat there was some value here we\nbasically like you know run handlers uh\nperiodically with just a simple cron\nsyntax\num and finally the this entity state\nupdates and so\nuh when entities like you know we talked\nabout business entities and how those\nentities estate machines and so like as\nthey are transitioned into like running\nright like fight workflows kick off but\nwhen they're transitioning to like you\nknow succeeded or processed we can\ncreate like we can basically run uh you\nknow additional handlers to to maybe\nlike extract some information about the\nqc right and then like transition this\nuh this entity into like qc pass or qc\nfail state and stuff like that\nso\nand so the reason why we built this\nright i think this is my last slide i\nwant to leave some time at the end for\nquestions but the reason why we built\nthis was because our production pipeline\nis actually like a really really complex\none right so so what's happening is that\nlike we receive these blood samples\nthese blood samples then get like you\nknow we extract our uh our our molecules\num from this uh different sort of uh\nanalytes from these uh from these blood\nsamples and then these analytes all\nbasically get processed asynchronously\nso you know they're going off into like\ntheir own track of analysis of\npre-processing and so one once all these\nfinish you know possibly at all\ndifferent times uh their data will land\nseparately in\nlike in gcs or in object storage right\nand so we're really kind of like kicking\noff independent analyses of these\nanalytes um when they arrive but then\nwhen we're when we're actually doing the\nmachine learning we need to wait we need\nto make sure that like all of these are\npresent right before we can like do some\nfeature engineering uh generate the\nfeatures and like actually pass it to\nour classification pipeline and then\ngenerate the report so there's a lot of\num coordination that needs to happen and\nthis coordination is actually like over\ntime right um and so and so this system\nthat we've built like basically helps us\ndo that like it helps us basically you\nknow wait for things that need to happen\nand then check to see if like all the\nother dependencies have been met before\nlike you know proceeding forward and so\non and so forth so in a way it's like\nalmost like a workflow right but it's\nnot like a static dag it's not like a\nsynchronous workflow that's happening\nit's like happening as\nuh the state is basically blooming as\nthings are happening\nright and and this is what i thought i\nthink like this was a very important\npiece of like driving our product\npipeline um and again you know this is\nnot like final right like this is our\nfirst iteration at taking an attempt at\nreactive workflows and so we wanted to\nintroduce this as a concept uh and you\nknow bring this to the community and see\nif like we can have you know an actual\nlike a community driven effort an open\nsource effort to build something that\ncan support use cases like this moving\nforward\nand so you know we've built it obviously\nnow it's like kind of like a django\napp-ish type framework right where like\nwe have models and\nhandlers instead of like models and\nviews but uh you know an argument can be\nmade that like this can be built into\nflight and flight has the ability to\nkind of like watch for events and\nrespond to like a workflow which is\nalready a concept that exists in flight\nright so like there's a lot of room i\nthink\nto um to work on this concept reactive\nmetal workflows\nso that's all i have i want to leave\nsome time to take questions\num\nso opening it up to the floor\nyes\nfantastic work with tower are you guys\ngoing to open source it or\num so that is still kind of like up in\nthe air uh we are considering it but we\nhaven't like you know taken any concrete\nsteps towards this but like we can talk\nabout it like we can talk about it in\nthe context of the larger community and\nsee if it makes sense\nright now it's built very very\nspecifically for us right like we didn't\nwant to over engineer it so it's like\ngcs only it only works with pubs up and\ngcs and so it's very specific but like\nyou know we can talk about what it takes\nto maybe make it open source and maybe\njust start a discussion about like how\nwe can build something better for\nreactive metal workflows because ideally\nlike you know we don't want to be\nsolving these problems uh this can be\nsolved at the platform level and it\ncould be useful for like you know many\npeople\ni actually think it should doesn't need\nto be part of like it can be something\nthat works with flight within the\necosystem right like i think one of the\nthings i have learned over here is to\nkeep this app as small as possible and\nyou can keep on expanding\nand then\nour surface area to break other use\ncases kind of increases um so um so yeah\nso but that doesn't mean you know i\nthink we would love to support freedom\nin any way if they want to open source\nthis under their umbrella don't even\nthink about over engineering think about\nlike you know getting gcs and google\ncloud support done and i think the\ncommunity here is\nawesome enough that other things will\njust land in if you don't\nif there is\nif if it is open so\nand we would love to support it right on\nthe flight orgs ecosystem i don't know\nif you guys have seen that there's a new\necosystem tab where we are adding\nprojects as they're getting created\nacross the different companies so cool\nyeah i haven't looked at that yet but\nthat sounds exciting so i mean we we\nbasically built this on top of flight\nremote so you know\nit's like it's a light layer on top of\nflight kit that's working really well\nfor us right now but um yeah this could\nbe exciting to\nyeah that's what union ml is also it's\njust a light layer on top of light kit\nbut i think the point of view matters uh\nto be honest right you know\nand the use cases matter so\nwhen you build an open source tool\nthat's extremely\nlightweight it has a huge a lot larger\nadoption right to be honest so\num we would we would love to help you\nevangelize that absolutely\ncool let's talk\nuh any more questions that can help\nanswer uh hopefully hopefully uh i don't\nknow if there's a lot of like biotech\nfolks here in this call but you know you\nknow maybe this this video will make it\nto youtube and people watch it and\nthey'll feel inspired to like you know\nlook at flight for biotech applications\nyet to be honest you guys have already\nlike kind of you know just by putting\nthe name has helped a lot of biotech and\nthen latch bio talking about us and\nbiotech is probably one of the most\nactive\ncommunity is\ninterestingly looking constantly at\nflight and a lot of production\ndeployments now so\nand we hope to grow that even further so\nthis will really help\nthank you\nawesome\nso it sounds like everybody everybody\nthought this was an awesome talk and\nthere's no question so i'll uh stop\nsharing\nthanks everyone\nthe talk was awesome the recording will\nbe published later today but anybody\nhave questions\ndon't be shy ask them yeah or feel free\nto reach out um on flightselect we're\nhere there's actually quite a few\nfreedom folks on the call right now\nas well\nif you don't mind me asking how many\nflight clusters do you have you might\nit's like you know i guess just for\nother people who don't know you guys\nmanage clusters separately right like\nyou don't use the multiple domain\nprimitives within flight you just manage\nthem separately yeah so um i mean this\nis our initial talk but like we're\nhoping that we'll have a few more talks\nto give let like dive into some more of\nthese things but briefly\nyeah so we um we run a single flight\ncluster a single flight deployment per\ngke cluster\num and so like we don't have you know\neven staging uh staging is like a\nseparate cluster with a separate flight\ndeployment and so on so forth so that's\nhow we kind of run it right now um but\nwe like the flexibility of saying like\nyou know having a centralized control\nplane with um with like uh with\nmulti-clusters uh we're just not using\nit right now we're also the other thing\nthat we're also kind of doing\ndifferently is that like we are\nuh we are pre-creating like all of our\nprojects with the right permissions and\nthe right infrastructure and things like\nthat um and you know thing with features\nlike pod templates and stuff that that\nreally helps us like you know really\nplatformize our uh our pla our platform\ni guess\num but\num\nso so it's very like it's very cons like\nit's it's very it's a very tight ship i\nguess is how we're running on flight\ndeployment it's not like people are just\nnot\nyou know creating their own projects\nwith whatever and like running things\nthere um and so far it's been working\nfor us but but it'll be interesting to\nsee like how this model evolves as we\ncontinue to scale\nhey chief this is martin i have a quick\nquestion for you maybe a little bit of\nan advice that you can give a new user\npotentially watching this on youtube\nlater what you would what would you\nrecommend them to do you know when they\nget started with flight what's your\nnumber one tip how to get started with\nlife\nthat's a tough one i mean well this is\nlike this is actually not that difficult\nfor me i mean we like talked about this\na long time ago i think we uh like the\nfirst thing that freedom did that i did\nat freedom while like introducing flight\nto the to the internally as well as like\nyou know also uh in in concert with like\nuh caitlyn and hate them is to introduce\nthis like sandbox right like give people\na really easy playground to like start\nplaying around with like throw some\nexamples in there uh and essentially\nlike let them play around with things i\nthink that's the best way to like adopt\nit learn the features and you can always\ncome back to the sandbox like if you\nneed to reproduce a bug or if you need\nto like you know revisit or like dive\ndeeper into a specific feature and stuff\nlike that so that that playground\nactually you know like it pays off over\nand over again um and so i would say\nlike\nthe first thing you need to do to get\nstarted like don't even bother with like\ngetting all the deployments up and going\nlike i would say just get a sandbox up\nand running get familiar with the api\nget comfortable test the features out\nsee the you know look at the features\nthat make sense to you and then take it\nfrom there\nand that's what we did yeah\nthanks you're welcome\nall right joe\n[Laughter]\nsigning off here"
    },
    {
        "title": "Flyte Community Update 013 - May 17 2022",
        "transcript": "um so welcome to the may 17th edition of\nthe flight community sync thanks again\neveryone for joining us uh we have a\npack presentation today we have ebardo\ndenver dublin flight uh pie flight run\nand jeep talking about um uh flight at\nfreedom but before we get started\njust a few community updates and then an\noverview of roadmap and what features\nare in store\nso community highlights\nuh so we just released flight 1.0.1 this\nis our first patch update after our big\n1.0 release so this includes a few bug\nfixes to fix the issues such as workflow\nparameters not sporting empty defaults\nfor collection types\nwe've updated some of the admin\nsubstitution for execution time params\nuh we've also fixed uh stowe which is\nthe day storage layer um for uh working\nwith google access id\num we also have some cool new features\nin flight kit that sharp part of this\nrelease as well too you can now do task\ntraining without explicitly calling\nmaking function calls you can just use\nthe right shift and left shift operators\nlike so\num and we've also updated some improved\nimage config validation to make it\neasier to use for registration\num and if you're interested you can\ncheck out the full release notes\nof the release link below\ni've also been busy publishing a few\nblog posts in the meantime\nso samina has published one on how to\nfile uh event driven neural style\nwell uh\ntransfer application with aws lambda um\nreally cool blog neat example of using\nflight with landon encourage you to\ncheck that out\num and we also have an upcoming ml ops\ntalk um with a flight uh going up with\nthe convergence of workloads and machine\nlearning and engineering and that will\nbe uh later this month\num and in case you weren't able to make\nit niels recently spoke at data council\naustin and that reporting is now live on\nyoutube if you want to check it out run\nover data safe uh processing and machine\nlearning with flight and pandera\nhighly encourage you to watch the video\nand we also have some upcoming talks as\nwell too uh caitlyn and i will be\ntalking at data at scale on uh this\nwednesday\nit's virtual conference for your\nregistration we'll be talking um the\nsame thing on making data quality title\npart of developing machine learning and\ndata products\ngoing over flight and panera as well in\nthat talk um so please check that out\nin addition more talks um so sunita will\nbe talking\nlike kubernetes communities and chennai\ni'm going over i will also flight\nit will be a virtual event on june 4th\num and then niels will be speaking again\nat emil ops world talking about machine\nlearning and production\nwith union ml which he demoed last week\nof open source singer last two weeks ago\nthe open source sync up i'm going over\nunion ml in more context in detail\ntalking about how he built micro\nframework for machine learning\napplications and this will be in toronto\nand canada on june 10th\num and as always if you have any\nquestions we are always available on\nslack but we do have um in-person office\nhours um i hate them at 7am on\nwednesdays uh late night ml offs with\nkatheyn at 9pm on wednesdays and um we\nare just introduced new kind of more u.s\ntime zone friendly based uh off stars on\nwednesdays as well too 1 30 to 2. if you\nwant to come chat with me um i don't\npromise i have answers but we can talk\num okay please come with all your\nquestions um great in the meantime we're\nalways available on slack like i said um\nin\ngithub um\nyeah please reach out\num we'll have a regular occurring sink\nin two weeks\nnicholas from methane sat will be\ntalking about how they use uh flight to\ntransform satellite data um\nand uh to support their mission of\nspotting and reducing mapping emissions\nglobally overall\num so feel free to add the yourself to\ncalendar invite and if you can't make it\num all the talks are recorded and\nuploaded to youtube and we also have the\nflight monthly newsletter if you want to\nkeep up to date that way\nall right right now so what's coming up\num so a lot of features are in pipeline\nright now um we're working on adding a\npi flight run support for a task\nexecution in addition to workflow\nexecutions that'll be on the next\nrelease\num thank you nick from blackshark for\nadding interruptable support at\nexecution time so now you can specify\nthat then as opposed to only\nregistration time it's awesome to see\nthis come through\nwe will be working on adding optional\nvalue types in tasks not just workflows\nupdating some of the apis for fetching\nnatural attributes um\nfor at the project level and the launch\nplan domain\num flight tags we've been talking about\nthis for a little bit um that should be\ncoming soon as well too which is kind of\na way for you to embed the custom\nvisualizations and summaries as part of\nyour task outputs um\nalso working on improving the flight\nremote experience which includes things\nlike fast register and of course bug\nfixes issues crop up\num and also working on some propeller\nimprovements as well too which includes\nthings like a better articulation"
    },
    {
        "title": "UnionML Introduction",
        "transcript": "so let me get started i'm niels ventilan\ni'm an ml engineer in the union ai team\nand i'm super excited to show off um\nthis new thing we've created called\nunion ml\num\nand the tagline is it's the easiest way\nto build and deploy machine learning\nmicro services\nso uh\nnecessary caveat here is that you know\nunion union ml is still in its early\ndays um\nso\nyeah with that out of the way let's just\ngo to the the main question we're asking\nhere so\nso this is something we've been thinking\nabout um\nand i'll just i'll just read this out\nloud uh which is uh where we've been\nthinking of this question of is it\npossible to define a standard set of\nmethods for machine learning\nthat i can reuse across many different\ncontexts\nso that goes from model training to\nprediction and batch and streaming\ncontext to even things like model\nexplainability things like that\nso this is sort of the core question\nwe're asking with this project\nand\nto give you maybe a rough\ncrude analogy here right we have this\nprotocol\nthat we all know and love\num called http and has various versions\nand extensions and whatnot\nbut it really you know there there are\nthese\nmethods many of which i actually didn't\nknow uh i only really knew get put\npost and maybe a few other of these\num\nbut you know this is a standard protocol\nand\num the question again here is what what\nwould this look like for ml you know and\nagain it's a crude analogy it may not\nthere may not be direct direct mappings\nto these concepts\nbut at a high level you know\num\nin the python ecosystem at least there's\nthere's flask and there's fast api and\nthey they are implementations of this\nprotocol they have certain opinions on\nhow to write\num\nan application\num and\nso you know this is this has been going\non for for many many you know years and\nprojects pop up now and again to\nre-implement this thing to make it maybe\nmore modern you know quote-unquote\nmodern python or you know things like\nthat\nso\nunion mla really is\nour\nour take we took a crack at\ndefining what this might look like for\nmachine learning\nso what is a\nwhat is an app\nthat exposes various endpoints\nor microservices\nlook like for for machine learning\nso i'll just take you through i'll kind\nof work\ni'll start from the syntax and maybe um\ntalk about what some of those you know\nstandard methods could be and and if if\nyou're familiar with machine learning\nall of these are really not new these\nare kind of well established\nyou know functions or methods\nso\nwith with um a union ml app you start\noff by importing a bunch of dependencies\nnotably the union ml data set and model\nand so these are the\nthe two core components of a union ml\napp\nthen you define a data set and this is\nreally like a specification of the data\nset you're giving it a name\nyou're giving it a few things that if\nyou're in data science or machine\nlearning like test size\nimplies like you know the test size of\nyour test set of your data set you want\nto shuffle these your data set for\ntraining\nand then targets is just the target\nvariable of the data set that has yet to\nmaterialize\nthen you define a model\nand you give it a name as well\nyou say what the\nkind of the model object is so in this\ncase we're using sklearn logistic\nregression in this init\nargument that basically just says how do\ni initialize a model\num and then you bind it to this data set\nand so this is a core but an important\nthing to note which is models are bound\nto data sets\nmeaning that whatever data set you give\na model it already sort of constrains\nimplicitly or explicitly constrains the\nform of that model\nokay so the first method here is the\ndataset reader\nand this is\nreally just a way of\ngetting data from the outside world or\nyou know in this case it's just using\nthe load digits function in the sqlearn\napi\num but really like this could be a sql\nquery this could be anything you know\num\ni'll tie this into flight eventually but\nfor now just look at this as with fresh\neyes and don't even think about flight\nso this returns a data frame you know\nstrong typing and all that good stuff\num then next you define a model trainer\nand what this is is\nas you notice this is a purely\nfunctional syntax for defining these\nwhat we'll call\napp components or entry points\nand um the the signature of this is\nfairly flexible but the common thing is\nthat the first argument is a\ninstance of whatever you gave the model\ninit argument up here\num and then it takes in features and\ntargets and we're just calling you know\nestimator.fit nothing fancy\nand we're returning the trained logistic\nregression model\nthen we have to specify okay now that i\nhave a model\nvia trained or untrained like how do i\ngenerate predictions from it\nso predictor is just that it just takes\nin\nthe model\nthe model object and the features\nand reduces in this case a list of\nfloats\num of the predictions of the uh\nthe digits\nand then actually i found a bug here but\nanyway that aside finally you define a\nevaluator function and this again just\ntakes in an estimator\nand then your features and targets and\nthen in this case computes the accuracy\nof your model on some arbitrary\npartition right so this has no opinion\non this is the training partition or the\ntest partition or validation or whatever\nis just\ngiven the feature target mappings of\nsome data\nhere's the performance\nso this is you know you're it's a little\nbit more verbose than you know your\nminimal flask cap but this is the\nminimal union ml app you define four\nmethods\num and it right currently only really\nworks well\nin this minimal sense with data frames\num but you know we're gonna we're gonna\nkeep iterating and working on this so\nthat it's super easy to work with\nother data structures\nuh local execution is supported out of\nthe box all of these all the functions\nyou saw up there in the previous slides\nare just python functions\nso you can execute them all locally\nand we've made it super easy to create a\nserving microservice with integrating\nwith fast api\nso you know the syntax up here might\nlook fairly familiar but\nyou know you say union ml serve\nyour app script and with it separated\nwith a colon on the other side is um you\nknow the variable name of your fast api\napp\nand then you specify some model path for\nit to load beforehand\nand then you can use you know record the\nrequest library or any other\nthing\nto generate predictions\nagain all the syntax here as uh i want\nto say is you know\nthere'll be a kind of a bumpy ride here\nbut um you get the general\num i hope you get the general gist of\nthis so we're loading features from that\nload digits data set and we're feeding\nit in as json into the predict endpoint\nthat's automatically created for us by\nunion ml\num so\nwith that i'll\nstop and uh see if there are any\nquestions so far\nokay\num\noh awesome did this okay so this this\nthing succeeded i wanted to show you\nthis quick demo\num that we showed off in pycon\ni made a few modifications and\nunfortunately\nuh it may or may not work so please bear\nwith me so so here we have a\nquick draw application\nso this is this is um\nan app that basically takes in an input\nof a tensor input of pixel values that\nyou've drawn\nand it produces a prediction of what\nyou've drawn you know so like kind of\npick like pictionary\num so here i'm just gonna\nexecute my model locally and this shows\nyou you know the the way how you\ninteract with this um\nyou know on a jupiter environment um if\nthat's\nthe id you're used to and here i'm just\ntraining this thing locally so this is\ngoing back to that local\nexperience i'm using weights and biases\nhere so\nwe can go here to see the model run\nso\ni think this is my the wrong account\nokay let's try that\nagain okay\ni guess um\nthese are\nthese metrics are not being logged but\num i'll just show you this\none\nthing this one execution i'm running on\nunion ai playground\nthis is a nice new little thing that i\nnoticed\nit's supported now these logs\nand we can see\noh boy\nokay\num okay so this demo is cursed um\nand now i'm gonna i'm gonna i'll try\nthis and if it doesn't work\ni apologize\nand\nopen it up for more questions if there\nare any\nso this is uh\nshowing off the remote train\nfunctionality so you can take that same\nunit ml model object and call remote\ntrain\num with a lot of hand waving on a\nconfiguration but this executes\nthe training microservice basically\non the union ai playground and this\nshould execute really quickly since it's\ncached\nbut it looks like it's re-running so\noh right never mind here\ni need to change this to\none epoch since that's the thing i just\ncreated\num but as you can see\nwe're trying to really\nintegrate union ml\nin the remote\nwhen you want to scale your\nyour\ntraining jobs up and hook it up into\nflight so that you know all the benefits\nof flight are\nenjoyed by someone using union ml\nhowever you can still use union ml\nlocally and in whatever system you're\nusing\nbut currently\nthe the scaling journey is that you have\nto use flight and that'll be the case in\nthe foreseeable future\nyou know we have talked about you know\nhaving some\nkind of way of extending it so that it\ncould work with other\ncloud systems but for now it'll be\nfairly strongly coupled with light\nand um yeah well as we wait for this\nuh are there any more uh questions\ni i have a quick one here niels\nso the remote train function right now i\nmean that's kind of hard-coded to work\nwith the union playground\num as i understand it\nis there a way that we can uh you know\ndo that remote train to just like\nwhatever flight cluster that you have\ndeployed you know if users have their\nown flight cluster can they use union ml\nand connect to that and what does that\nlook like\nawesome\nuh great question dan thank you for that\nthe\nthe way you attach a back end to a union\nml model\nis\nby calling model.remote\nand\nwhat this does is you can\nit gives you basically the\nconfiguration for you know the docker\nbuild which union ml under the hood\nhandles automatically for you\num\nand this config file path is the flight\nctl config which can point to any\narbitrary\nflight deployment\nand then your you know very familiar\nproject and domain which are flight\nconcepts\nso yeah that's\nyou can\noverride this you know you so you can\ndefine a model once\nand then\num override and change this from the\nremote remote configuration you know as\nyou go\nawesome thanks yeah this is really cool\ni'm excited\num\nhey hey neils\nyes\ni i think the cached execution has a\ndifferent uh\nbatch size looks like it was 4096\njust sent you the\num the link to the cached execution\nif you want to\nfinish the demo that way but uh\nyeah i guess that's that's why you threw\nyour hand\nokay\num yes i missed that\njust terminate this real quick\nlet's see\n4 000\ngosh 96\nokay\nhatha this should run without having to\nspin up anything right\nuh yep it should be all uh cast\ni think that was the one you just closed\nyeah\nyeah okay\nmoment of truth\nwill this load because they're yeah\nthey're like issues with like gpus and\nstuff so unfortunately this is uh\nstill the case so anyway uh just imagine\nthat there is a\nthing on here where i can draw uh little\nimages and it'll predict\nthe uh the label for those for the\ndrawings\num\nbut yeah so this is very this is we have\na zero zero one release\num\nthat\nkind of is yeah it's still very much in\nbeta um so there are many rough edges\nwe're still smoothing out but i hope you\ngot the general gist of\nbasically creating\na union ml app like this\nand this is basically you know your set\nof reusable components that you can\nuse the same set of you could use the\nsame model you can override the data set\nthat it's bound to so you can you know\nwrite a different data set with a\ndifferent reader that grabs data from\nsome other location say s3 or another\ndatabase um\nand you can deploy that\nfor both training and prediction\ncurrently\num but\nwe do have you know big plans for this\nyou know uni ml is kind of like a\na very ambitious name for this project\nand so we're planning to\nbasically solidify the core\ninterface and then start building out\nother\nservices like i mentioned earlier like\nmodel explainability and just have you\nknow observability\num\nlike\nbuilt into like basically all aspects of\nit\nso\nthank you for your time and"
    },
    {
        "title": "Flyte Community Update 012 - May 3 2022",
        "transcript": "thanks or thanks for joining us today\nwelcome to the may 3rd edition of the\nflight community sync\num some exciting demos up ahead but just\nto recap we'll go over what's been going\non in the community and what's up ahead\non the remote of the roadmap\nso community highlights so um in case\nyou haven't seen we've released flight\n1.0 which is a huge major milestone\nrelease um we want to thank the entire\ncommunity for all the help in testing\ncontributing everything you've done to\nhelp us reach this point\num the 1.0 is kind of like you know our\nrecognition that we've reached a stable\napi we think flight as a platform is\nstable but this certainly means that\nthere's it just means that there's a lot\nmore to come we have a lot more features\nin store um but we're really excited to\nannounce stable release of flight\num and some of the highlights of this\nrelease include like config application\nso you'll notice now that flight kit and\nflight ctl inline application all share\nthe same config\nso you don't have to constantly change\nbetween those uh flight kit tasks and\nworkload names are automatically fully\nqualified\nwhich is great when you're using pi\nflight run you don't have to worry about\nwhat directory you're registering from\na ton more features in the pipeline like\ni mentioned and thank you all for\nhelping us reach this point\num in just recap some recent events um\nso a couple weeks back uh hayden mcneils\npresented the emma watts community\nmeetup a couple of talks there on how\nflight work is straight tests and\nworkflows and what we learned building\nend-to-end ml vacations on flight we\nalso went over the hackathon winners\nthere as well too um if you weren't able\nto make the sync up you can watch the\nrecording at the link in the slides\nalso a union was representing flight at\npycon over this last weekend it was\ngreat talking to you folks out there um\nwelcome any new uh community members um\nwe had a great time talking about flight\nand meeting with the python community\num as always we have off stars on\nwednesdays\ni hate them at 7 a.m and k then at 9 00\npm\nuh informal feel free to show up and\nhang out if you have questions or even\nif you don't um and uh yeah if not we're\nalways on slack if you have any\nquestions to come up in the meantime\nwe'll be having a regularly scheduled\nsink in just a couple weeks on may 17th\nshe will be taking us through flight of\nfreedom\num and as always talks will be recorded\nand feel free to add yourself to the\ncalendar event with the link on the\nslides as well\ngreat and then roadmap so what's coming\nup in the future\num so we'll be releasing or we'll be\nworking on flight decks uh which is uh\ncustomizable visualizations for your\ntasks so you can do things like embed uh\ntraining metrics\num any kinds of visualizations that are\nappropriate for your tasks this will all\nbe stored as kind of first class\ncitizens of your test um outputs um and\nthey'll be rendered in the ui or your\nnotebooks as well too\nwe'll also be iterating on pi flight run\nuh right now the process has been really\nstreamlined for running with a single\nfile but we want to improve that process\nso it's as easy to register and run\nmultiple files as it is for a single\nworkflow right now\nand we'll also be cleaning up python\nregisters more\nwhich will be coming as a single command\nin python um so you can go ahead and\nserialize and register all of your\nworkflow artifacts in a single command\nwithout having to\nmuck with like serialize and register\nseparately as you do now\ngreat and then i'll hand it over to nils\nfor demos he'll be going over the\nexciting new union mlsdk"
    },
    {
        "title": "Leveraging Flyte for Frictionless Upgrades at Spotify",
        "transcript": "this is our talk about how we were using\nfly to\nleverage\nfrictionless objects\nand this is a generic title but\nfeature this we have\n200\nemployees doing data workflows\nin different teams in the company and\nfor\nsome reason or another they they're\nusing some tasks they need to create\nso\nthis is our dilemma\nhas been a pain for us to do it in our\nprevious orchestration\nand\ntooling\nand i will present\nhow we're using flight to facilitate\nthose tasks\nokay let's talk\num\nthis is a spotify\nmaybe you\nalready know this is a streaming company\nwe have\nin hundreds of markets\nin the world we serve music and\nrecommendations\nto\ndesktop mobile\ntablets and so on\nwe are on one of the biggest in the\nin the business of streaming yes\nand who am i uh i'm nelson anderson\nand i'm originally from venezuela living\nin sweden for\nnine years maybe now\ni am senior software engineering spotify\nand\nwhen\nbeing here spotify that my time i would\nsay have been divided evenly between\nworking on the spotify event delivery\nsystem and now\ni'm working on the\nteam that does\na\nscheduling an orchestration of data\npipeline\nand\nour team a few years ago we selected\nflight our next\norchestration\ntooling and we wrote a blog post about\nthat you can take a look on\nthat link is in our engineering blog\nand\nwe have been working in flight we\ncontributed uh\nflighty java and scala\nand we did also a lot of work to\nto make flight work on gcp at least at\nthe beginning and we also have done a\nsmall things here and there\nso er what is our agenda um\ni will explain\nmore details why doing friction operate\nis a big deal for us spotify\nthen i will do i will explain how we\nhave been\ndoing\nthose upgrades with uh luigi this is\nalthough we are using flight uh we are\nthe\nmigration path will be long uh\nmulti-year projects so i hope we will\ntalk more about that in uh\nmaybe in a year or so when we have\nmigrated more people to\nto fly\nbut we are not talking about that today\nso we still use a lot of luigi i will\nexplain how we do these upgrades with\nreally\nand then i will switch to how we do it\nwill fly with\nthe features that flight provides\nso why doing friction upgrades are a big\ndeal\nfor us at least\na spotify\nsome companies work this way let's say\nthey have\ndifferent departments uh teams tribes\nwhatever it's called\nand\nusually there is a\na team or organization dedicated to data\nand they are in charge of all the data\nneeds in the company so they interact\nwith the user what you need\ni produce it\nbut\nwe don't do like that in spotify\nwe\ndistribute our engineering force\non the things they need\nthe expertise\nso\nthey say there is a search team they\nneed data engineers so there will be a\nthing working that our teams are\num\nmultidisciplinary\nso we have uh\nhundreds of engineers and teams\naround the company\nand we don't want them to bring bend the\nwheel every time so we have a smaller\nplatform organization there several\nthings working there the\nmission is to produce tools and services\nso\nour engineers are more efficient when\nthey do\nwe want to then be self-sufficient we\nprovide platform let's say i want to\nthese data be anonymized and we produce\na\nplatform that\nbased on configuration we provide a\nanonymization for them\nthey say we say i want this pipeline be\nscheduled\nthis many\ntimes per day and we will provide the\ninfrastructure to do that\nso this is a given some numbers we have\nmore than 600 people between data\nengineers and data science\nthey produce\nmore than\n15 000\nworkflows that are being scheduled every\nday\num\nyeah we have uh only daily\nsome weekly and some yearly workforce\nand as a platform we we do a lot we\nschedule execution data discovery\nlineage profiling rotation\nevent delivery as a service uh yeah we\ndo well\nso a on this environment now let me go\nto the\nmean or basic use case uh\nas i mentioned we try to be a\nself-sufficient infras so we will have a\ncookie cutter template that people will\nsay okay i want to create out a workflow\nand then with the button they have an\nesg project\nwith this\nbasic\ninfra and tuning already in place so\nlet's say they have a workflow\nand they need to the workflow have some\ndependency data so\nwe have some tasks that\nlook into the upstream dependencies to\nsee if they are there\nfor that\nwe have a backend service that is in\ncharges\nto say okay the date is present or not\nand giving a name that this is the\nthe data they are looking for and the\npartition it will tell you here the data\nand here is the url\nand in some discussion we have called\ndns for data for giving a name\nand\nand not only that\nwe have that back-end service but also\nwe use tasks embedded in our workflows\nthat connect communicable with these\nservices\nas you can imagine\nthese tasks will be the same for every\nuser they want to connect to the service\nso we provide these tasks as a platform\nclass\nand\nthis is one thing we provide this okay\nlook up i want to pull this data i want\nto know where to write when i'm back\nuh\ncreate my new data where i need to write\nso all these tasks we call it platform\nclass\nthen we say okay we need to start the\nworkflow execution let's say this is a\ndata flow um\nor maybe it's a big\nquery or and when the execution is done\nuh\nthe most probably the paper will\ncompute some data and we could\naccommodate some counters and we want to\nvalidate\nthose countries saying okay we are\nexpecting this amount of data we are\nexpecting\nuh we tolerate this amount of invalid\ndata whatever whatever\nso\nand not only is the user code they need\nto run we provide platform data to make\nthe user\na job easier\nso in this basic example i'm counting\nalready\nfive platform teams and this is a\nsimplified version we have\nmore platform teams involved in the\nday-to-day\nworkflow\nthat we run in the pub\nso\nlet me go back to this picture so you\ncan picture this we have 50 000\nworkflows and this workflow not only\ncontain tasks written by the users\nthey contain platform tasks\nthis does written far\nby us as uh\na platform team a platform or in the\nspotify\nand\nwe don't write perfect code i guess\nnobody does so\nuh we need either to cheap\nbug fixes or even better\nto provide new functionality\nthat means that we have the problem that\nwe need to\npush the new version of this task\nthrough these\n600 developers\nto reach those 15 000\nworkers\nthis is\nwhat is a big deal for us we\nour scale\nwe are way beyond going manually\nhave one-to-one interaction developers\nwe need\nan infrastructure\nthat helps us doing our work\nso\nhow we do\nupgrades of workflows in reading\nso\nluigi\nlet me\ngo back uh explain a little bit here\nis a orchestration framework written\nby spotify a long time ago\nbefore my time joining the team\nand it's based on python so\nit's all\nclient side there is like a\nserver side that they care most about\ndistributing locking but\nyou can run ready all locally without\nconnecting to any packing\nand\nyes python is very flexible\nand this is what we have been doing\nall these years\nso uh\nhow we do\nworkflows with reading so\nthen resolve will be a workflowy maps\nthat this is our orchestration service\nsorry our scheduling service will take\nour schedule seriously say okay please\nrun this image\nevery hour or every day\nso how we produce that image well we\nhave a base image of course\nand that's a basic message produced by\nus we have some instrumentation that we\nput there we care about\nwhat operating system and so on and so\non\nand then\nreally being a python library we specify\nokay this is ouija this is the version\nnot only the region but also\nevery platform task will be a library\nbecause these\ntasks are produced by different teams so\nwe will collect all the version all\nthose things that we will put in the\nrequirements\nand those goes into the image and then\nat the end the\nuser code\nso\nwe have user code\nand sometimes it's very small compared\nto all the platform\nfunctionality we are bringing into the\nimage\nand once that\nworkflow image is\nbuilt then the version of the libraries\nget frozen in time get deployed to our\norchestration a scheduling system and\nthat's it\nso why this is a problem\nnow i already think that we\nwe built the cameras and those python\nversions although version of the\ncelebrity get frozen\nso\ndoing any great involve doing a pull\nrequest to that repository to bump the\nlibrary that the people are referencing\nreferencing their build in the\nrequirements\nand\nthen\nthe problem is scale how\nwe contact 600 developers to do that\nyeah we can send emails select\ncommunication but this is one scale\nand what we do is we send automatic\nautomated pull requests\nso\nif this enough\nis not enough\nso\nuh\ni haven't computed what percentage but\nmany of the federal personnel or the\npull request\nand yeah this is uh we are reaching\nother people's\nteams they have their own priorities\nmaybe\nthese workflows are not they are not\nworking on that or maybe\nthey will replace that with something\nelse soon so they don't see the need to\nspend time on creating a platform\ntask\noperating a platform that for them is\nnot a priority they want to deliver\nvalue this is a problem why should i\ncare\num and\nuh\nsome other thing\nthe data is critical enough they have\ncomplex testing requirements so they\ndon't want mesh any automatic vr they\nwant to do\ntheir sandbox with\ncontrol inputs and output and make sure\nthat data they produce is the same as\nthey want running production\nbut okay that's understandable but\nthere is also a problem\nand this problem is that\nin luigi that doesn't really libraries\nand those\nplatform tasks are complicated they have\ncomplex dependence\nfor example they need grpc to talk to\nother conservatives so they use google\ncloud\nlibraries because they need to do i know\nsubmit jobs to bigquery or whatever\nand\nmaybe they are forced to\nbomb their their dependencies maybe the\nbigquery interface is deprecating this\nversion and you need to bump and that\nat the end could create conflict between\ndifferent teams with their dependencies\nand it's not\neasy to detect those it's not easy to\nhave a\ntest infrastructure they're able to\ncatch all these\npossible connections so\nthe sad part is that some users\nare\nhesitant to merge this pr because they\nare afraid that their worker will fail\nin production because\nthese dependencies collection\nso we can trace that and how we really\nworks by\nthat's being achieved as libraries\nthere is no isolation between these\ntasks\ncool\nso\nuh yeah the other sort is we have a\nreduced feature penetration we release a\nversion and we don't know\nwe'll know some will merge some will not\nuh yeah we have different\nsometimes we see a pipeline there is\ncertain version behind and yeah\nthat's a sad story\nwith regia\nso when we were selecting a new\norchestration system we really want to\nsolve this problem we don't want to\ni mean we know these\nfat clients with libraries not something\nwe can continue doing\nat this scale\nand we select 5\nbecause of that\nso\nhow we are organized so\nwe say okay it will be easier if we\ncreate a centralized platform projects\nwe call it flight warehouse\nsometimes we became creative with naming\nand\nflight warehouse sounds like oh very\ngood because you have all the spare\nparts of your planes\nfor flight but then of course it's\nconfused sometimes confuse people with\ndata warehousing so\nin any case it's a platform project so\nin this platform project we centralize\nall the platform does so\nnow instead of being in libraries in\nthat people will install in their repos\nwe have passed\nin these projects\nso all other constraints and by being\nor concentrating this project then it's\neasier to grant permission to\nother people\nin the cookie cutter we say okay\nyou will need permission to read and\npull in the artifacts of these projects\nso\nhaving that\nmake us easier to to manage the scale\nand then we leverage the remote tasks so\nwe ship\nnow a library with\nremote as a reference\nso this is this library is really a thin\nlibrary it's only dependent on flacky\nand there is no\nconflict because we are not pulling in\nyour pc or anything it's just\na reference to\ntask is and why we do this because\nwe find it easier for a user to say okay\ni import this library this task and they\ndon't need to care to\nwhich\nproject is there\nlooking for or which version we will\ntalk about version a little bit\nlater\nso\nthis thin library is uh doesn't have the\nsame troubles as three\nbecause it's only\na\nremote as reference\nof course\nas a bonus uh flag is language and\nnothing so\nbut this is unrelated to\nto how we do objects\nand\nso we have a centralized project we do\nremote tasks\nand uh we cannot talk about the easiest\ngrids without talking about propeller\nplugins this is uh\ni would say one key feature that we saw\non on flight extensibility of\nof flight and\nyeah right now we are running two custom\npropeller plugins uh\none related to fling clusters\nand other related to doing\nbigquery\ncourse\nand between both uh\nuh just look up quickly and we have done\nmore than 40 updates\nof those plugins\nand meaning that we deploy a new version\nof the plugin\nand\nyeah because everything in the backend\nis 100 coverage no user intervention\nand yes this is\nexactly what we want\nuh sadly\nis that know the whole story more than\n60 percent of our tasks are not blocking\nus\nit's hard\nharder to write a task as a\nplugin than just use the\nflight kit of lucky yama to ride\npassing python or java\nso in our case we have more than 60 of\nus\nas a container task meaning that\nyeah we need to do something else\nand what we do is that\nyes we say okay let's explore this\noption let's say that\nuh\nor\nduring build time we have a pull request\nduring build time we have\nour theme library\nactually fetch what is the latest\nversion of the\nthese remote tasks in flight\nand then we fetch that version and we\nwire up in the workflow and then we\nregister\nso\non every build time say that again\ni will fetch what is the latest version\nof the remote task for this specific\ntask and then make my workflow user\nokay there is a trade-off\nfirst is ski the platform keys make sure\nthe\ntasks are always backward compatible\nthe\nkey here is the interface that cannot\nadd really nearly any a new\nparameter to their task\nand also users need to be aware they\nwill relinquish control of which a\nspecific version\ntheir\nusing their workflows so we want\nthey will say okay i will do this lookup\ntask\nand\nyou guarantee that your interface will\nbe\na staple that won't break my code and\nthen\nunder that condition i will relinquish\ncontrol and let you\ntell me what version to use\nso um\n[Music]\nthe downside is that we still need to be\nbackgrounds periodically but we have a\ndifferent project is taking that\nso\nbut now the key difference here is that\nthis these automated prs and i will\nexplain that later\nare easy\non users\nand let's talk about this so\num\nyes i mentioned that\nplasma developers need to maintain that\nwork compatibility\nbut sometimes\nit will be some day will be the time\nthat\nbreakage will need to happen\nand the vision what we say okay\nhow we keep\nmake possible to create a new version of\nthe task without being a new version of\nthe test\nright uh and\nthe solution is that we do like matter\nmajor version in the name of the pass\nitself\nso you can imagine\nthat\nplatform task a v1 will become platform\ntask a b2\nthat will keep the fetch latest\ncontract still happening because\ni will still be able to fix the last\nversion of platform task av1\nright and i will\nretrieve whatever latest version it\nis and\nthen it will be that the question okay\nthen as you say tell me that you are\ngoing to\nevery time a new version you are going\nto keep\nmaintaining more\nokay\nso um\nwe don't want to do that we want to\ndeprecate for more of the earnings\nand the way we are planning to do that\nis that\nwe were able to control for how long we\nare allowing these two parallel versions\nto be\nuh\nalive at the same time\nand because workflows in flight are\ni can inspect what paths they're using i\nwill we will able to notice that these\nuh\nprofile users are using the old name for\nthe old version\nand we could then send notifications\nsend automatic pr to those\nand\nthis is a process that\nwe haven't deprecated any yet so\num\ni guess when the time comes we will\nthis is our plan and we will\ntell you later how how it went\nso\nthis is the end of the my presentation\nand i want to do a recap\nwhy frequent\nupgrades important for us is because we\nhave a decentralized organization and\nthe schedule that we manage\nwe produce platform tasks and we need to\nachieve\na\nof both visits to the platform and those\npeople related to all those workflows\nwe\nis the rigid model is not good for us uh\nshipping flat libraries\nscalable\nand\nthe fraud the versions are frozen at\nbuild time and\ndevelopers are hesitant to merge their\npr's\nand a big part of that is that\nthey're afraid of persian colliding\nand because they're\nfat libraries\nwith fly\nwe use remote\nso we take advantage of the task from\nisolation one from the other\nand\neverything we have in a common\ncentralized\nproject\nand\nwith the combination of a\npropeller plugins\nplus fashion the latest uh version of\nthe attacks\nuh\nwe this is how we decide to go\nwith\noperating our users\num\nwe\nwhat we want is to have the propeller\nuh plugin version for everything but uh\nwe\nwe know that is\nprobably not feasible and we find these\nspeculators with the\nprocess how to handle\nand\nbreaking the the interface and doing\nupgrades is how we hope to\nto work with this\nthank you all of you\nand i open for question with you coming\nthat was an awesome presentation\nuh yeah\nwhen you read it this way it makes it\neven\nmore impactful to understand why we\nbuilt it\nbecause\nwhen we built it we we had the same\nproblem with\nmachine learning libraries specifically\nthey're really really terrible like you\nknow if you\nimport uh tensorflow and pytorch in the\nsame year you're kind of almost dead\nin the container the particular site\nloads up and things like that and then\ngpu dependencies and so on so we built\nit using that but you know we found that\nwithin data so there were lots of use\ncases for this\nso it's great to see that you guys are\nseeing value\none thing i noticed in your thing was\nthe\ntwo things i wanted to add one was uh\nthe deprecation or\nversion management\nuh\nfor tasks\nand uh i\ni call it interface evolution right and\nthis is a common problem in any\nrpc or rest or any services right we\nhave the same problem once you publish a\nservice api yeah you have to live with\nit\num\nbut we do get some interface evolution\nwithin the service apis at least\ndepending on the protocol that you use\nand so thank you for latches recent work\non union support\ni don't know if you've seen that nelson\nbut no i haven't\nasked\nthey just added it\na month ago so you can\nwe haven't yet enabled it in the\nplatform but we would love to work with\nyou guys and see\nwe can now introduce\nnew\nattributes to a task\nor new parameters to a task\nwhich are union\nand the union can be optional you can\nunion like one of the sub types of union\nis optional so you can say none value or\nan actual value and we can make that\noptional value\nthe auto substituted by the platform\nas none\nand then users can hand or you know\nplatform builders can handle none as a\ndefault and you know use the right\noperation for that none that that's\nactually pretty\nbecause of latch now it's pretty easy to\ndo so we should be able to do that so\nthank you\nokay it's uh\nyeah that will\nreduce a lot\nbeneath to come up with a new\n[Music]\ntask version a task with a v2 because we\ncan we're able to add these attributes\nyep yep and this will be done by the\ncompiler\nat compile time itself so no\nruntime errors so that's like another\nadvantage but yeah we would love to work\nwith you on that i think this is pretty\neasy to do at the moment\num another one\nwas back-end plug-ins\num\nwe\nthere's a bunch of work at least in\nunion that's happening for\nlike\nkeeping the back-end plug-in but\ndecoupling it from propeller like in a\nway that it can kind of spawn it out\nlike services\nuh yet you get the same flexibility\nfor back-end plug-in\num and and i think uh\nand uh probably nelson you and i also\nhad a chat about this in the past\ncloser to getting that and we would love\nto see\nand one one advantage of that\npotentially is that\nwriting them will become simpler\nuh in a way that you\nwill have a predictable simple api\nso let's see if we would love to\ncollaborate on that as well but this is\nfantastic stuff so thank you"
    },
    {
        "title": "Flyte Community Update 011 - April 19 2022",
        "transcript": "all right hey folks um thanks for\njoining us for the latest oss sync um we\ngo ahead and get started i think it's a\nlittle bit five past hour now and\nhopefully things are rolling in we're\nalready trying the meeting\ngreat so um hey\ncan you hear me okay\nokay awesome thanks for coming\ngreat so for um today's thing um we'll\ngo over a few community highlights uh\nupcoming road map um\nand then i will have a demo from the\nfolks over at spotify\num so\nsome of the things that have been\nbrewing in the flight community over the\nlast couple weeks\num so i don't know if everyone saw um\nbut we officially announced cnn ai\ncoming out of stealth\num and i think keith maybe had a few\nwords he wanted to add for what this\nmeans for the flight open source\ncommunity um and how we're hoping to\nkind of use that partnership between\nunion uh to improve uh\nhey hey katrina i think i'll go in a\nlittle bit can i just mention you uh i'm\ndriving today sorry\nyeah we'll remove this this time um but\ni guess uh summary is though um you know\nwe hope to kind of basically massively\nkind of you know\nuh contribute to the open source\ncommunity still um nothing is really\nchanging we just want to use the power\nof the flight community\num great and then uh upcoming community\nevents um so uh niels and hayden will be\nspeaking at the envelopes community sync\ntomorrow uh they'll be talking about\nwhat they learned building antenna uh\nmachine learning applications on flight\nand the winners of the hong kong will\nalso be announced there as well so we\nhighly encourage everyone to join\nmeeting will be at the same time uh just\ntomorrow you can register at the billy\nlink on the slides though\num as always office hours are on\nwednesdays uh 7 a.m 15 at 9 00 pm with\nkathy\num come if you want to hang out if you\nhave any specific questions or if\nthere's anything you want to chat about\nflight or mls in general um everyone's\nalways welcome no sign up or anything\nrequired whatsoever\nand then a sprinter we have our site\nagain in two weeks on this 10 years\nwe'll be chatting a bit about parking\nwhich is exciting\nand we'll also be done moving pipeline\nas well\num\n[Music]\nand uh we mentioned this at last sync\nbut uh we're working hard on releasing\nflight 1.0\nand that should be imminent um and some\nof the new features that you can look\nforward to in the\nrelease are flight signal binary uh\nwhich will increase the speed in these\nin which you can set up your entire\nflight back in deployment and make it\nreally easy to contribute to fight as\nwell too and test out your contributions\num and it takes a little bit and a half\nto sign up so we're really excited to\nkind of invite the getting started\nexperience with some binary\nuh one thing i will also include\nscriptwork\nwhich is a single fan and pipeline uh\nwhich you can use to register and run\nyour workflows whenever you don't\nnecessarily need to rebuild your image\num this kind of takes out a lot of the\nmodeling with impact light serialize and\nwith flights to register it just gets\none single fan\nand it's super simple easy to use\num on top of that we have a lot of ui\nimprovements that will speed bundle in\nthis release uh this includes project\ndashboard with an overview of all your\nproject level settings do some\nexecutions\non top of that improve math test\nrecording and dynamic class rendering as\nwell too so a lot to look forward to and\nlike we said we're hoping we're hoping\non getting this release up very very\nsoon\num and just a preview of what exactly uh\nis in that release so i mentioned this\npipeline kind of scriptment\num so not a particularly thrilling\nscreenshot but it will have a more\nelaborate demo in the next couple of\nweeks um as you can see here you have\nthis one high flight run command it's\nall you need in order to run any\nworkflow to find anywhere within your uh\ncodebase you can run it remote\nwhich will show you here you can see it\nthis will render um console link where\nyou can go check out your execution you\ncan also buy new workflows directly just\nby calling the python workflow function\num and you can also uh\noh hey okay that is here\nand uh it comes fully uh out loaded out\nof the box with support for\ninputs uh discovering workflows um\nthat's kind of one command line tool\nthem all uh since games here let's\nrevisit the the union ai\nannouncement is that\nyeah\num\ni think\nfirstly i wrote this in slack and i've\ncompletely\nbeat this up\nwe wouldn't have been here without the\ncommunity i think the\nall of you folks are just wonderful\ni'm more than happy every day\nto work on a product that you know we\nstarted but i really\nwant to take\nto you know where we think it can go to\nbut most importantly work with all of\nyou folks and you know to make sure that\nthe vision is achieved\nwhich is amazing uh thank you for you\nknow we started this community thing\nabout a little\nless than two years ago\nuh initially it used to be like four or\nfirst or something was to join in and\nslowly sorry like last\nlast couple weeks ago we were about 55\nor 60\nthat shows\nthat you know we are growing as a\ncommunity\nagain that 60 is not a representative of\nall the users which is just amazing so\ni am i am actually humbled\nto be here\nwhat the funding really means is union\nai\nis the company we started which has been\nif you have noticed has been\ncontributing heavily to flight we will\ncontinue doing that\nyes we are working on a\ncommercial version but\ni would highly recommend that you guys\nread the blog and i wrote about union ai\nthe reason for us to start union ai\nessentially\nto make\nuh you know one to help promote flight\nas an open source product just\nget many many users to ignore\nthe you know the problems that they get\nsolved with it and how to use it and\nwherever possible uh help them uh get\nalong the way\nbut also there are some cases in which\nwe found\nover the last\ncouple years is that there are teams\nthat do not have big infrastructure\nteams as they're\nuh supporting\nuh them and so we want\nthese teams also to\nget advantages from flight and as well\nas you know get on board very quickly\nthat's one of the reasons why we started\na union ai the commercial offering\nwhere the goal is that we become the\ninfrastructure partners for teams that\ndo not have them\num and i think together with the open\nsource community and union.ai\nuh partnerships we can have more users\nusing flight and the reason for having\nmore users using flight is essentially\nevery user that comes onto the platform\nhas so many amazing inputs which\nactually improves the platform overall\nfor everybody else and that's why i\nthink a large community\nis is a fantastic goal that we will have\nand\nmore\nbecause of in india ais funding we are\nof course accelerating more on areas\nthat we really need improvement like for\nexample\nwe would be helping people\nand we help you with the java and scala\nsdk that spotify folks have been driving\ncompletely on their own\nwe\nwe have we have large investments in the\nui\nwhich actually we are also we are\nworking with stride works and the\nspotify folks for\nunbundling the ui in a way that people\ncan embed it within their\nown uh user experiences which\nfurther makes\num\nit's possible for users to deliver\nthe experience that they want it's like\nbecause again the goal of flight is not\nto be\nthe center but be the supporting actor\nin whatever platform you are you know\ntrying to build\nand\nyeah so uh and and and it also helps us\nto hire more engineers to work on a lot\nof backend features that we wanted to\nhire work on\nuh and\nflight 100 release today\nis just\none step in that milestone\nagain there is no breaking changes\nuh\nwe\nwe are\njust creating a one dot door to signify\nthat we are confident about the apis uh\nand we are\nfurther going to invest in the internals\nso you know one do one not also doesn't\nmean the feature complete we just mean\nthat we think we are stable\nthat everybody can use us in production\nbut we will keep on improving the entire\nsystem\nuh i also wanted to bring up two other\nimportant points last week we had a\nsecurity vulnerability\nthat we discovered and we\nwe had a very quick turnaround from the\nunion air team\nand thank you for everybody who jumped\non and helped with it\nwe've tried to communicate with\neverybody within that we knew of\nuh but of course there are many many\nother users that you don't know of\ndirectly so if you're using flight\nconsole\nwhat we did is we yanked older versions\nof fight console\nplease upgrade your version of record\nsurvey\ni cannot explain all the details of the\nsecurity vulnerability because of the\nembargo at the moment\nbut uh we have a cve in progress and we\nwill be publishing that\nonce we have a confirmation from most\nusers that they have upgraded\nabout uh for folks that i reached out\nmost of them\nhave upgraded and it would be fine for\nthem\num\nyeah and and\nanother uh point about office hours that\nwe actually were talking yesterday we do\nan office hours in the morning and in\nthe evening on wednesdays of tomorrow\nyou're thinking of doing one in the us\nhours in the afternoon please recommend\ntimes\nand that would be wonderful so that's it\nfor mine and thank you again for trying\nyeah thanksgiving\nso i just wrapped up the preview of pi\nflight script mode so we'll be talking\nmore about that in the next upcoming\nopen source sync i'll have a full demo\nthere\num and that wraps up all the updates for\ntoday so i'm headed over to nelson and\noffice we'll talk about um how the\ngame's uh\nspotify"
    },
    {
        "title": "Orchestrating MLOps With Flyte - Striveworks",
        "transcript": "all right so i want to thank you all um\nfor letting me come in here and talk and\nget set up for uh about 30 seconds there\num i just want to say thanks to this\ncommunity you know i think it's it's a\nbig part of what makes\nan open source project so good and i\nthink this uh particular community does\nit does it right\num\nso\num some things i'm going to talk about\ntoday you know who are we at strive\nworks and what are we building what's\nchariot\num\ni want to tell you guys like why we\nchose flight\num\nand how we're using it and some things\nthat we\nare adding to it to make it uh better\num so who are we\num we like to think of ourselves as a\npioneer for operational data science um\nand particularly like two verticals um\none being national security and and the\nother highly regulated industries\num so\nby operational data science what do i\nmean by that um that's sort of like\nbeing able to transform decisions or\nlike business outcomes by using data\nscience as opposed to just supplying it\nlike theoretically or in some fashion\nlike that\num and these\ntwo particular verticals that can be\nkind of challenging oftentimes because\nthe you know the deployments of these\nthings are bespoke and\nthe environments they're operating in\nare kind of a steer\nso you know that's where we're kind of\ncharging through and breaking down\nbarriers in those verticals\nwe also offer consultant services\nmostly around computer vision\nand i think like most excitingly and\nkind of what i'm going to base this talk\naround is is how we're building out our\nmlaps platform called chariot and how\nwe're using flight to help that\num all right so what is chariot um\nwe sort of call chariot like vertically\nintegrated mlaps\num i think like a better analogy is like\nget lab for ml\num so what we're trying to do is capture\nthe whole life cycle of the ml problem\num into kind of a uniform platform\nand make that accessible to different\nuser groups so high code user groups and\nload code user groups\nso you can kind of see an example you\nknow we have a\nui\nfor that more low code user\nand then of course we have like sdks\napis other integrations for\nfolks that are\nfriendly writing code\num so and as you can see here kind of\nthe goal you know be able to explore\nlabel catalog data\nyou want to be able to train and serve\nmodels and then\num sort of what i'm talking about uh\nspecifically in this talk is being able\nto run workflows at scale so using those\nuh those models\nto actually affect you know business\noutcomes like i mentioned before\nand uh for that you know we picked\nflight and we're kind of opinionated\nabout\nwhy folks should use that\num so um\nyou know one of the first reasons we\npick flight is it fits into\nour sort of\nwell i guess lots of focuses uh idea\nthat everything should kind of be cloud\nnative kubernetes first um like i said\nwe plan to\nuh and we we are installing chariot into\nkind of austere environments so you know\nthat might look like a large-scale cloud\ndeployment\num or it might it might like take\ndifferent forms as well and as long as\nwe have a standard sort of\nframework to deploy these things onto\nwhich is kubernetes we're in good shape\nanother important thing is the\nmulti-tenant architecture that flight\nprovides\ni think\nespecially in these like regulated\nspaces uh multi-tenancy is sort of\nobviously important where you need to\nseparate uh users and and resources and\nand things like that um amongst each\nother within the same organization\nand i think\nuh most importantly one of the biggest\nreasons we picked flight was it\nis ideologically kind of aligned with\nwhat we think all mlap systems should\nhave and that's strong lineage\nguarantees\num so you can kind of see in this\npicture here uh side by side comparison\nso like flight lineage obviously this is\njust like snapshot from the ui but you\ncan get\num all types of great lineage\ninformation from flight\nand kind of combining that with chariot\nyou can get sort of a full picture of uh\nwhat is going into workflows what is\nwhat are workflows using and\nwhat are the conclusions that are coming\nout of them\num\nall right so\nthis diagram here um\nthis is sort of how we see\nchariot uh overlaying into like the\nbroader sort of ml op space\nand this picture here i want to preface\nthis is not like\neverything that that flight can do\nbecause i think flight can probably do\nuh you know\nmore than potentially is just\nhighlighted here but this is like how we\nare fitting flight into our system and\nwhere we see it fitting in\num so you can see like\num if i scroll back here like obviously\nwithout flight we're clearly missing\nsome like important parts here where\ndata orchestration um kind of like how\ndo we get stuff uh out of out of this\nthis middle phase here of like curating\ndata labeling data training data\ninto some sort of like\nproduction pipeline that is reusable and\nextendable um and so you can see like\nflight fix those boxes pretty nicely for\nus\nalso you can kind of see it sort of\noverlays on a lot of these other pieces\nas well as sort of\nyou know a data engineering engine an\nexperimentation engine you know we can\nyou can train models obviously in flight\nand then\nalso using that to\ndeploy models to production\nall right so\num\nin this example here this is like\nperhaps a not so contrived example of\nhow we're using flight um so\nin this particular workflow um you know\nlet's let's say we're indexing faces for\nsome like similarity search that we plan\nto do later on\num so like typical workflow like split\nvideo into frames\nget detections of faces perhaps in each\nframe\ntake those detections kind of you know\nclip them get embeddings of those faces\nwhich is like you know a vector\nand then save those embeddings to some\nyou know database or state store that is\noptimized for that you know like a like\na vector database or something like\nmilbus\num\nso\ni think like\nthis is why flight's so important this\nis why like we think flight is great is\nthat you know in this particular example\nhere um\ni have deliberately like sort of\ninjected a bad model into this workflow\nthat i showed you before um and you can\nclearly see like this task here has got\nno output uh detections so this detect\nfaces task has no outputs\num\nand\nyou know because of that the next task\nin the workflow fails\num and so we can quickly see like the\nway we use flight is oftentimes we'll\ninject model uris which are immutable\ninto inputs um and so we can clearly see\nlike which models were used in this\nparticular workflow\nin this case you can see it's you know\nhopefully you guys can see that like bad\nface model from checkpoint\num\nso\nfrom that you know you can we can go\nback into our chariot system and quickly\nsee like here's the model that was used\nin this workflow\num maybe someone is nice and left to\ncomment on it that like this model is no\ngood\num oftentimes that's not going to be the\ncase and so\nfollowing that lineage pattern\num we can\ngo through here and see exactly like\nwhat is the metadata of this model um\nin this case like which training run\ncreated this model\nwhich hardware was it like trained on\num and then particularly like which\ncheckpoint was used to\num create what we you know call a model\num\nso from there you can track it back to\nthe training run\nagain using the lineage um you can get\nlike a\na summary of like you know what is the\ntraining data that this model used what\nis the validation data\num on the right here you can see some\nmetrics and you know i i train this\nmodel so the the metrics are absolutely\nterrible so you can see that oh clearly\nthis is not a good model um so we know\nthat like this production workflow that\nwe used from flight we were able to\ntrack down through this lineage all the\nway back to this training run that um\nyou know i clearly uh did not do a very\ngood job in creating uh\nso\nwhat i what uh what also i think is\nreally cool with flight is that we can\nsimply inject a new model into the same\nexact workflow that i just showed you\num using some model that you know\nhopefully someone else trained besides\nme and that works and that's validated\ninject that you know as an input\nparameter rerun the workflow as you can\nsee here we get like output embeddings\nand then uh hopefully you know the rest\nof the workflow succeeds as as well\num so i think what's you know it's not\nnecessarily like there's nothing\nrevolutionary here um but i think it\nlike highlights you know\nwhy lineage is so important and like how\npowerful flight can be with just like\ninjecting a new uh model um without\nchanging any code or anything like that\nand being able to rerun and and get\nfixes and\ninto production um\nvery quickly\num all right so\ni've mentioned some additions that we're\nmaking to flight\num there's a few things in particular\nthat we're working on\num\nright now i think like making\nuh flight workflows easier to register\nis is a big thing i was pretty excited\nabout that road map that katrina uh\nteased with um i think\nuh one thing we're doing at you know at\nstride works right now is we've uh\ncreated a vs code extension called\nflight wingman um that sort of makes\nthis easier internally for us\nand this will you know initialize\nworkflows by creating boilerplate um you\ncan register workflows\nyou can get code snippet like you can\nget like hints in your in your ide for\nfor generating workflows so i have a\nquick video hopefully i can share and\nnot ruin my screen share\nto show you guys\nperhaps\ny'all see that okay\nyep yep\nawesome\nall right so here's flight wingman in\naction\num so you can see here\nthis will just hook right up to the\nflights api\num you can select which projects\nyou want to use you can register\nworkflows um the beginning happens\npretty quick i don't know if you saw\nthat but we initialized the workflow we\nregistered workflow this will go through\nall like building the container\npipeline steps the flight ctl steps to\nregister this as well\nso if there's any interest in this let\nus know um we are very open to like open\nsourcing this and making this kind of\nreadily available for everyone\num\nthe next thing we're doing right now is\nsort of combining the flight console\nwith our ui um and a big thanks like\njason porter and nazia\num\nfor coming up with like a design dock to\nhelp make um\nyou know flight console kind of\naccessible to lots of folks through a\nplug-in architecture\nso hopefully all of us as a community\ncan kind of work together to make flight\nlook and feel the way we all kind of\nwant flight and look like to look and\nfeel\nand get the same functionality out of it\nand then lastly um you know\ni think probably most people get the\npoint by now but like we think\nlineage is really important um and we're\nworking to extend that slightly within\nflight um so one thing that we've been\nwe've been doing\nis essentially adding like a uh a\nsidecar proxy to our workflows which\nwill\num basically collect network information\nand then integrate and aggregate that\nwith flight lineage information from the\ndata catalog\nand so\nwhat you can see here is like an example\ntask um that will or an example workflow\nthat will output\nthis is essentially the output\ngraph of what that network and\nflight lineage information looks like\nonce combined and into a graph structure\nand each one of these nodes here will\nhave specific details that correlate to\nwhat is going on so you can see here\nlike this is a task execution\nyou can see that this task is making\nsome requests to our inference server\nwhich is where we host you know our\nmodels so you can see that this does\nsomething with this model and returns\nsome output back to this task which then\nhas some output\num so really working um on enhancing\nlineage as much as possible and this is\nsort of one way we're doing that\num\nthat's all i got it was kind of quick\num are there any\nquestions or anything like that\nfirstly thank you jake for sharing all\nof that that's pretty awesome\num i think after you asked if there is\nany interest i think a bunch of people\nsaid\nyes absolutely and i think\neven me i think we love it would you we\nwould love if you could share the vs\ncode extension\nyeah yeah absolutely happy new year\nis that written in python or is that in\njavascript or\nyeah so it's all in um\njavascript right now um just a vs code\nuh plugin api um but then it it'll make\nsome calls too uh we try to use the\nnative tooling so we don't have to like\nkeep you know\nkeep up with upstream so it is just\ncalling like flight ctl and pi flight\nunder the hood\noh okay\noh it's actually calling the cli\nyeah at times it depends so like when\nregistering we use like the grpc api to\nlike get the project info but then for\nlike red uh for registering and building\nthose will just call like you know your\npodman or your doctor or\num\nyour flight cto\nthat's fantastic because uh so the thing\nthat katrina shared she did not give a\ntimeline it's april 15th\na bunch of uh\nso we've been completely revamping the\nregister experience and fight robot so\nyou'll not even need to use the cli but\ndirectly look into the programmatic apis\nor the friendly programmatic ap is not\neven go to the rpc directly so we would\nlove to work on this because this is i\nthink you folks would love it uh\ncompletely\ncool\nyeah are you allowing like custom cookie\ncutters like you had showed creating the\nnew project um and thought that was\nreally cool but wonder how that kind of\nlooks on your end\nyeah um that's a great idea right now\nwe are not using custom cookie cutters\num we sort of have like the basically\nthe same template that you'd get from\nthe getting started guide uh from from\nthe flight page\num we just have this that templated out\nbut you know i think adding cookie\ncutter templates to that and making\nthose customizable would be totally\ndoable\ncool thanks it looks awesome\nthanks\ni think if you open sources i'm sure the\ncommunity will help make it even better\nso\nlove that\nanybody else any questions\nany questions about just in general the\nupcoming roadmap\nitems\nhello everyone\nhello i am marvin it's my first time\nattending the\nflight meeting and i am here to\ncontribute to the project\noh great to have you here irvin thank\nyou so much\nyou are not authenticated\noh hello can you hear me\nyeah\nyeah i'm just saying you know thank you\nfor joining in uh\nfantastic to have you here\nand uh go through the issues if there is\nany help you need to\njump on the slack ask questions don't be\nshy\nthank you\nanybody else have any questions\nif not then i'll just give a little bit\nof a spiel about the upcoming release\nit's we are calling it flight 1.0 again\nno alarms we are not breaking any user\ncode anything but\none thing is happening is\nyou if you want to continue using flight\nyou will have to upgrade the back end to\n1.0\ni guess it's not a completely but any of\nthe new features that you want to use\nthat come with 1.2 will need a back-end\nupgrade\nbut once you upgrade uh it should\nshould just work uh for all the existing\nstuff\nthe biggest\nthing that's getting added is um we are\nreducing the requirement for users to\nhave to get a permission on their site\nto push their you know bundles or code\nbundles\nfor fast registration that is\nautomatically handled by flight now\nafter this point um\nand\na lot of ui and other updates and ux\nupdate roots like it\nand we are also setting the stage for a\nlot of fun stuff coming down this year\nso\nuh\nwe actually want to start doing uh\nlike a\nleaderboard of features uh and\nwe would love for folks to actually vote\nand see which ones should get\nprioritized\nso any ideas of where we should host\nthat how we can get more of the\ncommunity involved how we can get\nfeedback about what you guys are liking\nwhat you guys are not liking would be\nfantastic and we would love to do that\nall right\n20 plus people are silent maybe more\nthan 20.\nit's 50.\nall right\nsilence is taken as\neverything's okay that we're doing so\num katrina did you have anything else i\nlanded towards you\nuh no i don't know what's up for today\nso i guess we'll see you everyone in two\nweeks and see you on slack in the\nmeantime\n[Music]\nthanks for joining\neveryone thank you"
    },
    {
        "title": "Flyte Community Update 010 - April 5 2022",
        "transcript": "um okay well i guess we can get started\nthen um for past the hour um so thanks\nfor joining us for today's community\nsync um we have an exciting demo from uh\njake from cyborgs coming up um but first\nwe'll go over some community highlights\num airplanes open and then we'll hand it\nover to digi\num so community highlights um so just a\nreminder we'll be uh joining the emma\nlops meetup on april 20th uh we'll be\ndoing a deep dive going over our flight\nand also presenting the hackbawn winners\ni'm really excited to see all the\nprojects that folks have been working on\num and if you're interested in learning\nmore uh you can check out the emma watts\nuh conference talks playlist um and the\nflight recording should be uploaded\nthere as well too\num that ml ops folks also have a slack\ncommunity welcome to join\num\nhope to see you there\num we'll also be a flight will be at\npikeland on salt lake city at the end of\nthis month if you're there come by and\nsay hi\num we'll have a booth set up at the expo\nhall\num we'll also be participating in the\njob fair and um we're really excited to\nuh hopefully demo fight\nand see if all uh see anyone who's there\num\nnext office hours uh schedule remains\nunchanged if you have any questions\nabout maintaining play getting your\nflight deployment up and running\nfeedback complaints comments suggestions\nuh please come hang out office hours\nhate them at\nand um kate then at 9 00 pm\non wednesdays\num\nand uh our upcoming oss in two weeks uh\nwe'll have nelson and baba's uh from\nspotify talking and expanding on that\nblog post you might have seen that they\npublished recently talking about their\nflight deployment at spotify um just how\nthey run flight there\num and we'll also be announcing the\nengineering labs top-down winners um so\ni think the hackathon's been a while\nrunning down so it'll be exciting to see\num all those projects come in\ni'm excited to see who ends up winning\nas well too\num and as always feel free to add the\nopen source link up to uh your calendar\nour youtube channel will publish all of\nthese talks after\num the sync ups and our playbase letter\nwill uh you can subscribe for updates on\nall the latest and greatness in flight\nand finally our roadmap so what's coming\nup\num so as we talked about before uh\nflight 1.0 release is imminent calling\nthis phoenix\num and some of the new and latest\ngreatest features that will be is in\npart of this release um we'll have an\nupdated fast register experience\nwhich should improve the ux make it a\nlot simpler to run through fast register\nand easier to use as well too\nwe have a lot of ui updates in the\npipeline as well\nwhich includes a revamp project page\nwhich will be a kind of project\ndashboard include all of your settings\nrecent overview project level executions\nnumber workloads tasks everything you\nwant to know about your project\nalso updated rendering for\nthe dynamic workflow graphs so you can\nfully expand those\nuh and\nkind of\nhook around with the rendered kind of\nworkflow itself\nand uh timeline views uh also been\nreleased this includes gantt charts that\nkind of show the duration of your\nindividual tasks\nthroughout the entire timeline of your\nworkflow get a better sense of what\ntasks are running when uh during the\nconstitution\num\n1.0 release will also include\nflight kit remote going ga it's been\nlike alpha data for a while now the\nexperience has been totally revamped it\nshould be super simple to use as well\ntoo\num and then oso flight kid\nconfigs are now shared with flight ctl\nso that should simplify uh all of your\nkind of like remote registration\num anything you do where you need to\ntalk to flight uh that config experience\nhas been completely updated and\nsimplified as well\num and then in if i could uh flight\nconnect deck 7 merge flight decks are\nkind of an improved\ninput output rendering uh jc kind of\nlike graphical\nuh like visualization of your inputs and\noutputs um and those will be renderable\nand super notebooks coming up too\num give you kind of like more\ninteractive experience um with your your\nindividual tasks within the methods\nalso working on upstreaming the airflow\nflight operator uh which will be a great\ntool to kind of um you know if you're\nmigrating from airflow flight or if you\nwant to integrate with existing airflow\ndeployments that should simplify that\nprocess\nand we're also updating\nuh the entire kind of getting started\nprocess um for onboarding onto flight in\ngeneral uh so this includes like script\nmode which lets you you know package all\nof your dependencies without building a\nnew container uh upload and register all\nin one command um without having to kind\nof like multiple settings this should be\nsuper simple to use\num as well as single binary which\nbasically runs the entire flight back in\ndeployment in a uh as a single binary so\nthat you can simplify you know like all\nthe work around standing up communities\ninstances and deploying individual\nservices onto it uh this should just\nmake getting started with super smooth\nand a lot faster as well too\nand then finally in the back end we also\nhave improved uh map task experience\nwhich includes all sorts of things like\nher individual map tasks retry support\nand caching as well as improvement\ncurrency controls\ngreat and then now on to the demos so\nhand it off to jake um we'll talk about\nuh envelopes with lightning at starbucks"
    },
    {
        "title": "Flyte Weather Forecasting Demo",
        "transcript": "so\nhow it's made the overview\nvery\npi level\nis we make a request to the noaa.gov api\num there's a workflow here for getting\nthe training data\nthat produces\nsome training batches\num just for\nthe past week\nworth of daily temperatures\num there's a little other stuff here but\nyou know if people have questions we can\ntalk about it that training batch is is\nsent over to the get latest model\nworkflow uh what that does is it updates\nthe most it gets the most up-to-date\nmodel and then updates it with\nthe current data that we just got for\ntoday\nand then it forecasts for in this case\njust a week a week's\nworth of forecasts of mean daily\ntemperature\nand then some metrics\nthose forecasts are then read in by a\nstreamlined app um streamlit is it's\nlike\nmaking it's like jupiter notebooks\nbut a python script that you can just\nrun and it's it renders like an app for\nyou\nreally nicely so i'll show you what that\nlooks like\nuh\ndigging in a little bit to the data how\nthe data works and i yeah this\ni'll just talk through what this is\ndoing so in the get training data we're\nkind of zooming into it and looking at\nthe subtasks but get training data\nitself is a dynamic workflow\nand\nit\niterates through\nthis for loop of\nyou specify a start date so you can give\nit some prior knowledge so if you start\ntraining the data model today\nyou\nbasically give it you specify with the\nstart variable here\num how much prior knowledge do you want\nto give it you want to give it you know\na week's worth of training before today\nor you know three weeks or months\nso it iterates through here and\nthis is an interesting thing that i\ntried and it's you know happily it works\nbut um\nthe data that noah returns is just a csv\nit's a giant csv for a particular kind\nof date range\nand so\nyou don't you can compute beforehand\nwhether that csv file has all the data\nor whether it's going to be updated\ntomorrow with some new weather data\nand so here this branching path\num if the data is complete\ni.e there's going to be a new csv file\nand that this the current one i'm\nreferencing won't be updated you know\ntomorrow or the day after\nyou can assume that that csv file is\ncomplete and i'm going to just cache\nall the data associated with this\nparticular training instance\nif it's not complete i get a partial\ntraining instance in which case i don't\nwant to cache it so i'll fetch it again\nwhen it's updated tomorrow\num\nthat produces a list of training\ninstances just batch and that goes into\nthe model so that's that's a little like\nfun thing that took a little bit of\ndebugging to get to work but\ni'll show you later why i did this\nmodel\nis pretty simple\nthis is also dynamic forgot to annotate\nthis but it just looks through the\nbatches and updates the model\nfor each of those days\nuh pretty straightforward this is also\ncached so you could you inv you kind of\nlike have\nlike cached\nand tracked versions of all your model\nupdates over time\nand for forecasting\nit's a similar thing that i'm doing\num you're i'm just iterating from now to\nlike however many days i want to\nforecast for\nand i do a similar thing where\nif i already got that training instance\nfrom the training run\nor previous training run then i don't\nhave to recompute and get all this data\nand in the case that the data is not\ncomplete i just get a partial instance\nthen i create a bunch of forecasts\nand um\nfor this prototype i made three launch\nplans one in atlanta which is where i'm\nat uh seattle which is where a bunch of\nthe team is and then hyderabad i think i\ni might have missed a few team members\nbut this is just a fun thing to just\nlocalize all of us and\nget a sense of\nyou know i didn't want to do the entire\nworld so just had to\npick some heuristic\nand um\nnow i'll just want to show you a quick\ndemo so this is a streamlight app that\nthere are some issues with the hyderabad\nlaunch plan\num so right now there's just a line in\nseattle\nbut as you can see this\ni haven't really trained tuned this\nmodel very well it's just producing kind\nof similar outputs for atlanta seattle\nat least regionally it makes sense so\nseattle i think the predictions are\nlower\nthan\nthan atlanta so atlanta's hotter so like\non you know on average the model is\nproducing\nkind of sensical behavior but still it's\num\ni haven't looked at the metrics yet but\nyou know\nthat said uh\nthis app is creating a flight client and\nit's just reading the latest executions\nfor each of these launchpads\num so when the control plane\nuh stuff that i'm also working on is\ncompleted you know the the code\nbacking this app will be a lot cleaner\nit's kind of i don't want to show it\nbecause it's kind of a\nrickety house"
    },
    {
        "title": "Flyte Spot Instances, Intra-task Checkpointing & Flyte Decks",
        "transcript": "today we will kind of focus on a\ndifferent sort of the work that we've\nbeen doing um\na lot not a lot of folks here within the\nuh\ncommunity are using the spot instance\nsupport but there are i know i know two\nor three teams that i've been using\nand and they have had great success with\nit uh i know lyft uses it i know\nfree gnome uses it and i know\ntry uh nothing looks like black shark is\nusing it i don't know if others are\nusing or not like never really spoken to\nthem about it\num so we'll we'll talk about spot\ninstances what\nwhy you should consider spot instances\nuh and then some problems with it and\nthat will lead to the next topic\nintercostal pointing and then from there\nwe'll talk about\nsome cowbell that we are adding and why\nwe are adding that and\nand some more stuff some sneak peek\nall right uh\ni guess to this community i don't need\nto explain that machine learning is\nexpensive once you really start printing\nmodels at scale you will be like oh this\nis you know costing a lot\nit's also very compute intensive\nunlike many other processes\nit's compute intensive from the user\nside like\nthere's no easy way to build a model\njust by writing a query\nand\nand hence building a model needs a lot\nof compute\nresources it's very compute intensive\nit's also very experiment driven that\nmeans it's ad hoc and constantly\nchanging requirements you want to you\nwant to update and change\nthings constantly before even production\nand for these three reasons\nwe actually build flight in the way it\nis it allows you to quickly iterate and\nwe are working on some of the other\ncoolest things that will further improve\nthat hydration speed\nbut you have to remember that\nwith every improvement in the iteration\nspeed you're probably going to make it\nmore expensive because there are more\nresources being used\num\nso\nhow does flight do that like basically\nallows centralization of infrastructure\nso let's assume you're a company of tens\nof data scientists where\nyou now use\nsome sort of compute infrastructure for\nyourself\nand\nand you want to\nstart building model there's one option\nevery data scientist gets their own\ncomputer resources\nthat's great right like there is no\nqueuing everybody has their own\nresources it goes pretty fast\nproblem\nis now\ni will you know it's a general question\nhow many how often does every data\nscientist build a model are they doing\nit constantly do they need the resources\nconstantly\nand then do they also need one machine\nor hundreds of machines\ndo they need gpus or not you know don't\nneed gpus\nso then the questions that's happening\nis like okay if we start provisioning\nresources for\nuh\ndata scientists we actually end up with\na scenario of like underutilized lots of\ncompute which actually\nleads to a bunch of problems of you know\nspecifically cost\nand as we already said it\nis costly\nso\nhow do we handle that\nthat's what flight does like give you\none central plane to opt you know to\ncentralize all your\ncomputations it doesn't centralize the\nactual management\nof your models itself but it centralizes\nthe management of the infrastructure\nwhich is actually pretty cool like you\nknow your ml envelopes teams or the\nplatform teams can now own the\ninfrastructure well the user teams can\nsimply use the api to execute their\nstuff\nbut then this has another problem you\nknow as more and more and more now tens\nto hundreds of users start coming in\nand\nlots of contention increases for\nresources\nnow one option is to elastically just\nhorizontally scale and add as many\nmachines as you can even add more\nkubernetes clusters but supports all of\nthat\nbut\nif you add a number of machines you\nbasically are going to increase the cost\nuh it will not be as much as you know\ngiving each person's own individual\ncluster but it's still going to be\nhigher than\nat some scale it starts becoming very\nvery expensive um that lift for example\nit was one of the most expensive\nservices we used to run\nand so\nhow do we reduce the you know\ncontention\nuh\nto reduce contention\nbesides condition there's another\nproblem right like we we make cause some\nunfairness within the system that you\nknow one user who's coming in\nand hogs up all of the\nuh available capacity and the next user\ncomes in is actually has experiences\nwhat we call as head offline blocking so\nflight provides uh\nor end users things like resource quotas\nand resource pooling\nto prevent head-on-like blocking and\nimprove fairness\nand in the future we'll also be adding\nuh things like\nuh\nthings like preemption to improve\nfurther\nbut\nuh as i said you know if you increase\nthe cluster size in total you'll keep on\nincreasing the cost now one idea is to\noverprovision resources by reserved\ninstances and if you're not doing this\nhow we highly recommend this\nuh what this means is\nuh provision your cluster with flight\nwith like a max bound\nuh you of course set the you know the\nminimum to zero but the maximum is let's\nsay\nuh 1000 machines in the cluster and\nyou probably want to set that 1000\nmachines and buy two or three different\nskus and and buy them and and configure\nthem\nin the cluster we actually recommend\nbuying one large machine as the sku\nbecause it improves uh\nhow the machines come up though it might\naffect depending on the amount of load\nthat you have at an average you might\naffect the\nthe utilization the total utilization so\nyou know you have to do a little bit of\nmath over there\nbut\nthen you know if you buy reserved you\nstill cannot ever buy the complete peak\ncapacity and you should not so then you\nspill over to on demand\nand if you spill over to on demand\nit's really really expensive the chart\non the right actually shows you on\ndemand is zero percent\nuh discount assuming that there is some\ndiscount but let's see the zero percent\ndiscount\nuh\nthere's no commitment though you can\ndirectly scale up and scale out that's\nthe promise of cloud computing\nand it's amazing\nbut it's you know it's pretty expensive\nwhen you use reserved you actually get\nabout 40 to 60 discount depending on the\norganization that you're in and\nyou can save you know you but you have\nto give commitment of usage\nand you can save a lot of money\nthe last option is to use this thing\ncalled spot machines and spark machines\nuh\ngive you the about 50 to 90 discount\ndepending on your bidding strategy\nand what this actually does is\nit gives you benefits of beyond resolved\nalmost\nbut without any commitment\nbut you know the other side of it is aws\ngives you no commitment as well or aws\nor even gcp called preemptable this\nisn't azure also has an equivalent so\nthey give you no commitment what that\nmeans is they will take over your\ninstances at any point in time without\nyou know much of a notice\nbut the if it's cost is an extreme\nconcern then spot instances become\nvery very lucrative\nand uh we actually think for mature\norganizations or you know as you start\nscaling up when you're doing more\nexperimentation spot machines actually\ngive the best value\nbut\nas i said remember we are\nthings will go away and how do you use\npart first of all\nit's extremely simple you can go to\nflight and turn interruptable for a task\nwhy is this an opt-in feature there's an\nopt-in because you know as i said with\nspot machines the machines may go away\nanytime so\nyour tasks may not inherently be\nokay with losing machines\nfrequently so it's better to opt in and\nyou can mark it as interpretable equals\nto true\nanother option is you can actually\nmake the entire workflow interruptable\nthat will make all the tasks within it\nrun on infra interruptable hardware as\nlong as it's possible\nokay\nis this it no you have to do a couple\ntweaks in the back you have to actually\ngo\nto flight propeller and update either\nuse a node selector or interruptable\ntolerations so the way you do it you\ncreate two poles non-interruptable and\ninterruptable nodes in the back end\nand then have specific labels on those\nmachines so that flight can target those\nuh the interrupt machines whenever the\nuser predicts or i'm sorry indicates\nthat they are interested in using\ninterpreter\nso this is an example configuration\neither of those or together could be\nused\nbut\num you know interruptable\ngives you all of these benefits\nas i said it comes at a cost\nthe machine will be taken away at any\npoint in time\nwhat this could be i don't know if you\nknow thrashing this actually is a\npicture of tragic crashing means\nyou these are rice\npaddy or i think i think we eat i forget\nwhich one and and thrashing comes from\nthis agricultural dome where you\nactually take and you\nbang a bunch of these\num\ncrops\nto extract the grain\nand\nnow imagine uh this this person who's\nscratching is actually aws and it's your\ntask that's running\ninvariably\nyou'll find that when you're about 90\ncomplete\nis when your task gets thrashed or moved\nout\nuh\nyeah we could start again and again you\nwill be 90 completed again and this is a\nnever-ending cycle potential\nso flight does give you one interesting\nsolution in there and it automatically\ndoes this for you so if you have and\nretries it will automatically be try\nn minus one times on spot or\ninterruptable hardware and then the last\ntry it would directly move it to a\non demand or reserved hardware\nthat way\nit gives it the best chance of survival\nbut that still is expensive right let's\nimagine you're training a model for\n10 hours and it's nine hours and it\nkills and again you do it and your five\nretries so you essentially end up\nspending 50 hours potentially\nand and it would have been much better\nto just put it on reserve instance in\nthat case\nso how do you solve that problem\num\nthat's one way you can of course break\nit down into smaller and smaller chunks\nand and you should do that and that's\nwhy flight is designed in the way it is\nit is um\nworkflows are a simple way of\ncheckpointing you can break it down into\nsmall tasks and every task will be just\npointed independently and these are\nlogical checkpoints um\nbut\nin terms of like things like model\ntraining and as i said you know in some\ncases let's say you're replicating a\ndatabase to another database it's like a\nlong-running\ntask\nyou can't really build a workflow for\nevery single\nrow in your database that would be too\nmany workflows and and it's not really\nscalable\na better solution would be that you\nbasically bring up a task\nthat runs for a long period of time and\nit it itself kind of checkpoints itself\num\nevery you know periodically so that next\ntime if it crashes or something you can\njust begin itself\nfrom the previous successful checkpoint\num and this is a pretty well known\narchitecture for you know checkpointing\ncomplex systems\nuh now it does need user involvement\nbecause you have to you know make your\ncode\nresumable\nand it does need a lot of infrastructure\nsupport to actually make those\ncheckpoints durable automatically pass\nthem through\nuh make sure that\nthey don't corrupt each other make sure\nthat\nyou can always resume from the previous\ncheckpoint\na lot of different things right\n[Music]\nso it is great it would work\nbut it needs a bunch of\ninfrastructure and user effort\nso\nflight makes that extremely simple using\nintratext checkpoints uh\nwe we provide a very very simple usable\ninterface called checkpoint\nuh where you where you have a bunch of\ndifferent methods\nlike you can actually checkpoint entire\nfiles or directories or paths\num\nor you can actually checkpoint just bite\narrays and in this example on the left\nuh i'm showing how you can use it in\nyour task\nso let's say\nyou have a checkpointer and you want to\nspecifically uh you have a range of\niterations that you want to do let's say\n100\nand you want to checkpoint every 10\niterations so you would probably\ndo a modulo algorithm and see like\nwhenever your iteration hits comma 0 10\nand then you would write a checkpoint\nand then every time you start up you\nwould try to read the previous\ncheckpoint and if the previous\ncheckpoint exists uh\ninitialize it to the actual value in the\nprevious shift\nin this case\nyou can automatically resume uh there is\nsome user work to be done but you don't\nreally have to think about how the task\nis getting retried how the checkpoints\nare getting moved between them how are\nthey getting persisted how they are\ngetting durably written how they are\nensuring that there is no corruption on\nthem\netcetera etcetera\nuh and and that's it if you write this\nuh you will be able to checkpoint any\nspecific long-running task\nbut\nuh how does this work in the back so uh\nfor those who are uh aware of like\npropeller is the thing that actually\nruns your task in the back end\nuh it it\ntoday already creates data sandboxes for\nevery single execution and data\nsandboxes are the way to ensure that\nfrom a user point of view it looks as if\nthis is exactly once uh\nprocessing but in reality there is no\nexactly one processing in flight right\nthere is just no way when we run a task\nit's possible that\nthe network might partition and we might\njust\nuh lose\nvisibility into the task and so then at\nthat point we start another task\nbut in these cases uh what fight\npropeller does is it allows you to\ncreate data sandboxes as well as\ncheckpoints and boxes and then\nfor checkpoints it does something\nspecial as as it restarts another\ninstance it reuses\nthe previous checkpoint and\nautomatically passes it to the next\nand flight propeller does all of this\nautomatically\nuh okay so as i said checkpoints are\navailable but what's what's coming in um\nso currently if you run checkpoint it's\nonly within one execution uh across\nexecutions you cannot recover from a\nprevious checkpoint but uh that's\nsomething that we're working on it's uh\nit's it's actually not that hard to\nbuild and this actually fits in very\nvery nicely so for example if you're\ntraining a model and crash in between\nand the entire workflow crashes even\nafter retries but you're like oh this\nwas just a system error or you know i i\nknow if i recover it will just work so\nyou can hit recover and it will it will\ncontinue from the last checkpoint\nuh\nanother thing we want to actually make\nit even easier for users to\ncheckpoint and make this more of a habit\ninstead of like a afterthought uh and so\nflickkit\nwill have a set of plugins for\nuh\nmodel checkpoint\nwrappers\nand most popular libraries like keras\nand python lightning etc have a\ncheckpointing callback\nand these higher level wrappers will\nessentially implement those apis\nuh we would love community contribution\ni think there were a couple community\nmembers who actually said that they\nwould be implementing this i'm looking\nforward to working with you guys uh\nwould love contributions here uh\nanother thing is today in flight all\ncheckpoints are synchronous that means\nyou when you write a checkpoint you\nactually pay the penalty of durably\nstoring the checkpoint\neventually we would like to make them\nasynchronous now there are a bunch of\nproblems over there because you know\nwhat if there's one checkpoint is being\nwritten another checkpoint\nis to be committed now it causes a bunch\nof queuing and we wanted to avoid those\nproblems at the moment that's why there\nare synchronous checkpoints\nall right um so that's checkpoints now\ntalk about\nflight decks\nbecause we gotta have more cowbell\ni don't know if you guys know this was a\nsnl\nthing some time ago which actually\ntalked about\nuh\nadding more cowbell to their music\nand it just adds that extra\npizzazz\nto the to the song so\nuh but on a serious note you know\nmachine learning is a very visual\nprocess oftentimes\nwhen you're training a model or\ntransforming data set\nyou you would you would want to\nvisualize many aspects of it because for\nlarge data sets\nlooking at the data set is actually\nuseful visualizing gives you quick hints\non\nmaybe the data distribution or\nvarious aspects of it\nso this in this case you know it's an\nexample of visualizing a data set\nand in this case actually i think this\nis the\nsome\nuh\nmnist maybe and it looks like\nyeah\nbut flight actually already gives you a\nlot of visualization visualization is\none of the core reasons why you fight uh\nyou you basically try to visualize the\ninputs and outputs of every task get the\ngraph of the uh\nof the\nthe of the workflow itself visualized\nahead of time\nbut\nfrom flight's point of view a task is\nopaque you can only see the logs\nit doesn't know anything else that\nbut that's happening inside it\nwhat if\nwe could have custom visualizations for\na task\nand that's what flight decks is uh we're\nstill working on the name please if you\nguys have uh\nsuggestions of a better name we would\nlove it we we like flight text just from\nthe point of view that you know this is\non the deck uh of the flight um\nit's upon uh intended so\nbut we we did hear from a couple of\nfolks that\nyou should be called report you wouldn't\nlook at this feature unless you like you\ndidn't you wouldn't know what you would\nwant what you were looking at for\nexample\nso are searching for\nso yeah uh anyways the name aside but\nthis is uh the programmatic way that you\ncan enable custom visualization within a\ntask on the right hand side is an\nexample\nyou can write a simple task\ncreate a new you know import a default\ndeck append to the deck\nor create a new deck and simply\nappend an html element to it\num\nand and these are all static\nvisualizations uh it could be plotly\ngraphs which have some interactivity but\nthey're limited to everything that can\nbe embedded into an html\nuh\nand also along with this will be a\nset of renderers as we call them these\nrenders are essentially a\nsimple\nuh protocol that allow you to convert\nany data set into a renderable\nhtml file\nand and the reason to create the\nrenderers really is to create a library\nof bunch of renderers that so the users\nit minimizes the user's need to build\nrenders themselves\nand here's an example\nof a rendered output so\nthis is in a pr today we're still\nworking through as we said naming\nconcerns and so on\nuh but\nwe will always automatically visualize\ninputs and outputs but we will keep it\nsecure that means you if you have a\npandas data frame it's not gonna get\nvisualized completely if you want to\nvisualize the pandas data frame you can\nuse the custom visualization and add\nthat\nadd a frame renderer in this case you\ncan see it's you know somebody\nimplemented a pandas profiling report on\nthe right and\ni think the project team and on the on\nthe top is just a simple visualization\nof the pandas uh\ndata frame as an html\nin the future we definitely wish to\nexpand this to other visualizations\nvisualization\naspects of a task like you know amount\nfor peak resource usage etc\nso i think this is good this will get\nexciting in the future\nplease comment on the pr the api is\nstill in a little bit of a flux but we\nwant to stabilize this so community\nsupport for building renders is\nextremely important like i think i think\nthis is something where the community\ncan really really contribute and drive\nup the\nthe library of renders that people have\nokay\nuh the last segment stuff that's coming\nsoon or next you know\nnext episode preview rather\nuh\nwhat are we working on so we are working\non\nfast register i think most we've heard a\nlot from people people like fast\nregister they love it because they can\neasily write code and\npush their\npush their code without rebuilding a\ndocker container\nbut they don't love it as much because\nyou know you now have to open up a\nconnection to your backend blob store\nwhich means potentially giving your\nusers access\nto a backend web\nstore but what if i said that your users\ncould simply use flight authentication\nto automatically get access to\ntransparently upload the fast\nregistrable artifact right\nwhat would that make that would make\nthis configuration this big\nconfiguration\nuh which has at the admin section\nuh which is you know where to find\nflight which is required and a bunch of\nstorage section which is\ndefinitely not required to go away\nso\nthat would result in this smaller\nconfiguration and now you could just say\nflight traces a flight ctl register\nfights fast and not worry about\nhow to connect to whether it's minion or\ns3 or gcs or portable how to connect to\nit and so on\nand uh\nthis is what we are working on uh and\nkeep it a secret uh as of now however\nyou're working on uh not really a secret\nwe are using\nuh you know signed urls uh as a thing to\nactually upload\nnext thing that we're working on is\nflight remote we have also learned a lot\nabout like you know how people have been\nusing fighter board thank you for uh you\nknow\nstaying with us through the beta period\nof flight remote but we think we are\nready to graduate to be out of beta\nuh\none of the things that we're doing is\nupdating how you initialize flight\nremote\nuh we want it to be completely\nprogrammatic and extremely simple\ninstead of like fiddling around with\nfiles\nso in this example you can see fight\nremote can just be initialized for an\nendpoint\nand you can now uh the example also\nshows you know fetching a task from\nremote\nbuilding a workflow from that fetch task\nand also intermingling with the tasks\nthat's local\ncaution here that these tasks are\nnon-container tasks in the case when\nthey defined locally such a query\nand then you can just simply execute the\nworkflow pass the input and\nit should just execute\nyou can also of course retrieve prior\nexecutions and you know hd outputs so\nall of this can be done through jupiter\nenvironment and\nthis is the fight remote will get more\nand more uh\nfriendlier for jupiter usage\nuh\nanother complaint and specifically\nthrough the\nthrough the\nhackathon that we've been doing and all\nthe new users\ntoday\nflight sandbox is essentially this this\nuh i don't want to call it house of\ncards but uh\ncrazy assemblage of\nkubernetes and docker in docker and\nmultiple different containers and\nmagically working\nto give you this local flight running in\none box experience\nuh\nand we've been extremely grateful for\nyou know uh\nworking with jeeve and other folks to\nget this thing out off the block but\nand it's gotten us this far\nbut we've learned that you know\nall of our dinner images are actually\nhosted in north america\nso when people are in other parts of the\nworld when they start pulling it takes\ntime every time you stop the sandbox and\nyou restart it it again pulls all the\nimages\nand\nand there's no easy way to pause and\nresume a sandbox\nso listening to all of this we are\nactually working on something even uh\nwith a single-minded focus that you want\nto get your sandbox up in less than 30\nseconds for the first time and then\ncontinue to keep that\nrestart time\nlow to like maybe even less than 10\nseconds whenever possible\nso for that we've minimized all the\ncomponents and we're trying to build one\nsingle binary for all of flight\nuh and the single binary is what you\ndeploy uh or it's part of your container\nimage already we still need menial so\nthere probably won't be a way out of\nthat but i think video is probably\npretty redundant across the world in\ntheir hosted solutions\nand\nyeah and and it should look exactly the\nsame but just be way more snappy and get\nstarted this is something that we're\nworking on\nthis has another benefit by the way uh\nif you realize today if you are\nalong with the no pulling container\npositive stream all of that stuff there\nis no postgres in the default uh\nlocal installation that means equal\nlight so that it can really really start\nto work with me\nwhat if you could just take that single\nbinary\nand deploy it to\nec2 that instance with a static ip point\nit to an aurora database an eks and an\ns3\nyou would have a production grade flight\ncluster\nwithout really fiddling with ingresses\nand load balancers and etc etc and and\nwe've seen a lot of people\nreally really struggle with that stuff\num\nand so that's that's one of the\nsingle-minded motives that we started\nthis project with\nuh to really aid the community to get\nstarted with flight extremely quickly\nand also to scale it to an ekf\nproduction cluster in relatively short\nperiod of time\nnow this is not really a full production\ngrade h a kind of a deployment for that\nyou will need more than one instance but\nthe moment you add more than one\ninstance and it's possible the single\nbinary can be just drawn\nitself three times\nand it should work\num\nbut for that then you'll need a load\nbalancer but besides that this just\nsuper simplifies how you get started\nwith like\nuh and finally\nwe are working on something called a\nslight script word\nso you take the example on the right\nhand side you write this workflow and\nmost people are like okay now i have to\ndo package and i have to do register and\ni have to build a docker image this is\nmultiple steps that you have to do and\nactually merantics and all of these\nother people have kind of showed us that\nthere are other ways of doing it uh\nbut specifically the current flow is a\nlittle\ntiresome\nso we started with like what's the best\nway to get started for specific simple\npython workflows um and so we are\nworking on a thing called s551\nand you can just give it the name of the\nfile with the name of the\nworkflow to run and room which just\nthat's running\nyeah you can pass it will always start\nwith a default right kit image and you\nwill use fast register to\ninstantaneously run things\nuh another thing is like if you have a\nlarge enough uh\nproject like uh flight snacks\nyou can make fly flight run fully\nrelocatable go to any folder anywhere\nrun wi-fi run for that file and do which\njust runs so that's the goal we are\ngoing with uh it will come through\ncouple iterations but\nwe are we are thinking that the first\nversion will be out pretty soon\nall right that's it\nuh a huge\nsorry i talked a lot\nbut i'm gonna open up for q a\nand let me stop sharing\nhopefully that was useful\nany comments questions concern\ni see sorens my link so\nyeah i just just want to want to add\nsomething\nabout the\ncheckpointing and api it's the one\nmore more of a comment so one other\nthing where this is really usable or\nwhat i i found very helpful is that you\ncan\nalso write out\nyour tensorboard\nmetrics during checkpointing because\nusually\nyou do that while\n[Music]\nsaving and evaluating anyways right and\nthen you write it to a checkpoint and\nthen you can\njust use\n[Music]\nthese checkpoints during training to\nfire up\na tensorboard instance\nand connect to the bucket and um see\nyour training progress so this is kind\nof kind of kind of useful also\nyeah i we never really thought thought\nabout tensorboard when you were doing\nthis but yeah you could today we don't\nhost tensorboard but i think and flight\ndeployment probably\nsomeone could help us like you know host\nthat thing elsewhere and\nit will be very useful to folks\nfantastic\nuh\nyeah also surren you are contributing an\nexample using checkpoints right\nyeah yeah\nuh\nthis is\nit's not keras but the the hugging phase\ntrainer api so it's basically this is\nanother wrapper around tie torch but um\nyeah yeah of course uh there's\nit's basically the same right they have\na fallback\nand um so\nperhaps\nwe could add\na plug-in or something like that\nyeah i was actually thinking if there's\na loose-coupled way of doing these\nplug-ins where you don't have to install\nhugging face at\ntensorflow then you could put all of\nthem in like this one plug-in folder\nuh\nwe could probably just like you know do\nbased on import and try catch in there\nin that plugin\nuh but yeah but if you are\ni think\nadding them will really help many many\nother users because not everybody is as\nsavvy as users so\nyeah yeah\nuh oh sorry another question sorry i'm\nsorry okay not a great\num so one question about the checkpoints\nbecause when i was using the trainer api\nthere's actually a feature\nthat you can like only keep\nthe latest or the best checkpoint\num but\nand\ni was storing the whole the whole output\nfolder basically where all the\ncheckpoints are in because the trainer\napi basically manages that\nbut now\nbecause it's written to the bucket\nin the bucket of course it's not not\ndeleted right and so if you have a lot\nof large\nmodel check points it's gonna\nbe a lot of storage\nso i wonder if this is something\nmaybe we could\nadd something to to to clean up\ncheckpoints or\ndon't know\nyeah i i think\nwe should\nwe should talk about like just gc in\ngeneral\nof the data because a lot of data\ndirector uh\nand and then i'll give you what we did\nwe just had bucket lifecycle policies\nessentially\nfor and which we kept it at like 90\n[Music]\ndays or\nmaybe 180 days and then just we just\nperiodically just deleted all the data\nand and then the understanding was that\nthe flight works was then only or the\nexecution history just\nis a reference history it's not really\nthe data so if you want to redo the data\nyou have to re-run it um\nbut this has a couple problems and we\nactually should\nwork on adding just gc in general into\nflight i think um so\nhopefully this year at some point we'll\nstart working on that and we'd love\ncommunity collaboration on on the gc\ncurrent\ncool cool yeah\num also for your uh like i think if you\nuse the\ndirectory api sorry and i guess that's\nwhat you use it does copy everything but\nyou can use a file api\nso then it won't copy yeah yeah copy the\none file yeah i get i guess i i was i\nwas too lazy i guess i i i think it\nshould be possible to just actually\nextract the\nthe latest or the best um\nthe best checkpoint from the api and to\njust write that out probably that's\nthat's the way to go i guess yeah\nyeah yeah i don't know i think so that's\nwhy we have the read and write which is\nby the way yes\nabout that\ncool\nanother one um g has a question\nlove the idea of flight scripts and it\nmight play nicely with notebooks as well\nor you think you're using pickling ah\nfantastic\nso\nuh\nsingle script mode as is what it is it's\na script\nit is not a notebook uh it's not ipy nb\nfiles it is not pi files\nif you use jupytex by the way uh tip\ninstalled 2p text then you can open\npython files.5 files in jupyter\nand you can basically use flight with\njupiter and have a terminal\nbut\non that front\nwe are working on\nwe are working on something that will\nuse pickling\nuh\nand that's where flight remote comes in\nso flight remote is the jupiter land in\nour head it's like you know this\ninteractive world where you can like do\nthings and of course it comes with its\nown set of problems uh and so we will of\ncourse advise people that\ndeep only as much as you are comfortable\nwith like\nif you know how to swim\nchive if you don't know how to swim stay\nin the shallow part\num\nso\nbasically the way we are thinking is\nlike uh invite remote when you uh\nregister you can pass a task preference\nand that will automatically pickle and\nuse fast register under the hood\nto send the pickle artifact and\nrehydrate the pickle heart effect at one\ntime\nuh and that's completely doable today\nhe says that it's doable within no time\num i doubt it so uh but but true they're\nprobably\nwe're hoping to get something out in\napril\nuh and this is our our you know\nthe experience is what we care about for\nthe next two to two and a half months\nyou really really want to even hone in\nfurther on the experience"
    },
    {
        "title": "Flyte Community Update 009 - March 22 2022",
        "transcript": "all right we'll get started uh\nhello and welcome everybody\nthis is uh march 22nd and we are uh here\nfor our buy weekly sync again i think\nmany people ask us how do we connect\nhow do we\nattend these things it's\nspecific time 9 a.m we are open to\nfeedback if you have other\npotential choices we could alternate\nso please recommend\nand all of this is available on\nflight.org and just for the record i'm\nactually just going to walk through here\nand show it uh so on the flight.org in\nthe community section\nyou can go to weekly office hours and\nthese are the weekly office hours that\nand i host on wednesday morning 7 a.m\npst and 9 pm pst\nsimilarly the community sync is right\nabove that if you click on that that\nleads you to add event you can follow\nthe calendar add it to your own\ncalendar and\nkeep updated\nanother great\nresource that we highly recommend is the\nis the\nnewsletter which is right right on the\ntop here so just put your email address\nand you should get a newsletter every\nmonth\nabout what's happening and\nuh links to the resources and so on and\nwe have about four or five of those\nand in case you want to uh watch the\nolder ones all of them are always\ncaptured in youtube right from the early\ndays\nand this is this is just linking to the\nplaylist\nso you can see community update\nbut you can also of course browse\nthrough all the\nall the various videos a bunch of them\nyeah so that's just a quick uh recap on\nhow do you connect and\nplease please connect\nit it really uh helps us when you and\nalso reach out if you cannot find\nsomething or if they're unable to\nconnect and definitely join the slack\nchannel\nall right so\nfor the presentation as i said it's\nmarch 22nd\nas usual we'll go over the community\nhighlights uh do a quick\nroad map i guess uh or maybe we'll skip\nthat section today and jump on to\nthe talk\nso\na few community highlights uh a couple\nof our folks will be at data council\naustin it will be heythumb and uh meals\nneil is actually presenting\nuh about type safe data processing and\nmachine learning pipelines with flight\nand pandera so please if you are if you\nare a data council austin and definitely\nyou know uh\nbeing here on the slack channel and say\nhi to\nhim and neil's we do not have a\nunion.ai it doesn't have a booth or\nanything over there but yeah we would be\nhappy to meet and discuss use cases etc\num and i see i've looked at the\npresentation slides it's really\nfantastic definitely\ncheck it out\nwe hit a milestone this\nuh week we\nwe we we don't know really how to get\ngithub stars so\nwe thank you everybody for helping us\nreach a 2000 star milestone um that it\nreally helps in awareness of the product\nand\nif you if you can please spread\nawareness please help us\nuh reach to new users around the world\nand thank you for the continued support\nas i said office hours again this is a\nreminder uh it will be day after\ntomorrow morning 7 am and 9 pm pst\nuh\nalso the upcoming sync so like two weeks\nfrom now definitely join for\nuh\nstride works presentation they will they\nwill be showing how they use\nflight and\nhow they have built uh their chariot\nplatform on type of flight and it's\npretty pretty cool they've actually\nbuilt a full end-to-end machine learning\nplatform commercial machine learning\nplatform on top of like and it's uh it's\nreally amazing\nuh they'll also be showing some work\nthey've been doing on jupyter notebooks\nand such\nuh we also if you are not aware there\nhas been a hackathon in progress with\nmlaps community\nuh using\nflight and\nthank you again for all the participants\num and they will be presenting\nat the next\nsink as well\nand as usual all the links are in here\nthese slides will be shared\nuh\ni don't okay uh we\nwe we just released sorry we just\nreleased 0.19.3\nuh\nhuge uh\nuh changes coming in uh the type\nannotations uh this basically\nmeans for every type you can add\nadditional metadata that is captured\nuh again ui part is not done but it's\ncaptured it's in the system now you can\nuse that to do multiple things\nuh there's a bigquery plugin\nthat's coming\nthat's already in um\nthe bigquery plugin is on python and\nit's like uh and of course there's a\nvacuum coming for it\nuh you can now\ncustomize your caching behavior for\nextremely large objects\nso\njust as a refresher light does not\nautomatically\nhash your\ncontents of a large object it's too\nexpensive\nwe used\nreference based caching so that means if\na file\nthat we are referring to has not changed\nthat is how we automatically\ntransparently cache and we assume every\nfunction has a differential transparency\nwhich which means you are not changing\nthe\nfile behind the scene\nuh configurable uh so also in some cases\npeople are like you know you're sending\nreally really large workflows so now you\ncan configure the grpc message site for\nhandling extremely large workflows\nthrough the network layer\nand\nthere are a lot of sdk updates\nstructured data set as a reminder will\nbecome the default for all data frames\nstarting 5k\n0.32 so if you are using\n0.31 when you upgrade\nevery workflow should\nautomatically start using structured\ndataset this does not really\nneed any user side change specifically\nthe only thing that you need is the\nbackend needs to be upgraded to\n0.19.1 i think um\nso\nall right the presentation i'm actually\ngoing to be sharing about some of the\nother things that we've been doing uh\njust\nwe're doing a different session this\ntime we're not\nwe're not having any of the users it's\nbeen a while that we've not actually\ntalked about a bunch of\nupdates that we've been doing into the\nplatform and i just wanted to take this\ntime and share what we've been working\non and what's happening soon"
    },
    {
        "title": "Workflow Orchestration for Biocomputing ~ adopting Flyte @ LatchBio",
        "transcript": "so\nhey everyone thanks for having me uh\nreally excited to do this i i think you\nknow there'll be opportunity for more\ntechnical talks as more of our prs get\nmerged and and go through diving into\nlike deep decisions and implementation\ndetails we've made through\neither existing or future features in\nflight but i wanted to take this\nopportunity to really walk through like\nwhat we're doing at latch um how it\ntouches flight in many different ways\nand how workflow orchestration is even\nmotivated by needs and biology\num so yeah just diving ahead\nabout us where we build software and\ndata infrastructure for companies in\nbiotech we're at this point a 10 person\ncompany founded out of berkeley\nand this is the founding team and uh all\nof its glory\nso really quick wanted to dive into some\ntrends\nmotivating uh and also just keeping tabs\nhere it's like roughly 20 minutes right\nyeah i think you could go 20 25 minutes\nthat's fine just go for it great\nuh yeah so some trends in the space like\nthis is a\ncorollary to moore's law which everyone\nhas seen um as the constituent\ncomponents of computers get cheaper uh\nyou form these these classes uh these\ncomputing classes that also get cheaper\nand this is going to be analogous to\nsome things between biology but the\nscratches basically shows all these\nclasses of things their price goes down\nover time and their accessibility goes\ndown over time\nand then similarly the\nparticularly relevant to workflows is\nthe performance of file systems and uh\nio operations per second and throughput\nover time as well as capacity\nand this it goes by the canonical\npostulate of common\nsense um so the analogs in biology is\nthat the primitives are getting cheaper\nto read and write\neveryone loves throwing around this\ngraph that's the cost per genome the\ncost per base pair\nfor sequencing and\nyou can see it's rapidly exceeding the\nrate of more slot\nuh similarly writing dna so synthesis is\ngetting far cheap\nuh in fact the the rate at which we can\nwrite or synthesize the novel pieces of\ndna is exceeding that at which we can\nread it\nthis is significant because we can now\nlike write genetic code that did not\nhave an origin in nature we don't have\nto\nmolecular biology 1970s and 80s relied\non taking components of pre-existing\nlogic from bacteria\nfrom existing organisms and stitching\nthem together in new ways what dna\nsynthesis allows us to do is completely\nspecify new logic arbitrarily and see\nwhat the heck it's going to do in a\nliving\num so\nthese trends closely mirror what we saw\nin computing\nand the\nthe the fact that they're exceeding the\nrate at which computing progressed uh\npoints to some interesting things\nso\nlet's look at a couple more foundations\nfor latch\nthere's this field called synthetic\nbiology which was founded in coin in the\nearly 2000's out of tom knight's lab and\nmit and jewel 97 stanford and the idea\nis that we should reduce biology to\nconstituent building blocks and apply\nengineering principles to make de novo\nlife from scratch right so an early\ninitiative was called the biobricks\nproject where we can take genetic\ncomponents treat them as legos and\nrebuild them even rallying kids in high\nschool and college around competitions\nto build the best model organism it's\nkind of nutty but it's happening it has\nhappened it's continuing to happen so we\ncan translate uh logical genetic\ncomponents into\nlike things that exist in organisms\npromoters are you know\nconstitute how how you would start or\nstop the gene and the logic thereof you\nhave repressors which are\nequivalent of like an off switch you\nhave the things that the genes are\nexpressing you have the terminators\nwhich stop them you can imagine mixing\nand matching logical components to\nproduce behavior that you want in a\nliving system\nso this is the rise of synthetic biology\nand from this building philosophy we've\nhad many amazing things\none of which is the rise of metabolic\nengineering most of our\nbiological\nfood stuffs so as well as drugs are\nproduced from enormous fermentation\nsystems\nthis right here is amherst located in\nemeryville california\nand this is an example of how they've\ntaken this philosophy to production\nscale to produce things that we care\nabout and by\nand in addition to that there's really\ninteresting theoretical academic\napplications of this and genetic\ncircuitry so you can make really really\ncomplicated\nlogic from basic genetic constituents\nso that's kind of the philosophy of the\nthought shift in biology that is the\ntailwind at which last latches resting\nnow we have the tools\nso in biology recently there's been some\nlike really interesting ways to measure\nstuff\nthis is called flow cytometry and what\nthis means is you can basically take\ncells shoot them with high fidelity\nthrough a laser beam and sort them into\nbins based on properties of those cells\nwith the diffraction of that laser beam\ninteracting with some sort of sensing\nsystem\nthis means you can basically make\nchanges to millions of cells at a time\nand look how they're going to behave on\na single cell resolution\ncollect them and then\ngrow them and read them out downstream\nit's very cool\nand then this is an example of what we\ncall like a multiplex type throughput\nlibrary\nin biology a library is just a large\ncollection of genetic designs\nand so\nthat thing\nobviously is compounded by the\ndecrease in cost for dna synthesis that\nis also kind of compounded by the rise\nin gene editing technologies over the\npast decade or so i'm sure everyone's\nbeen following or at least heard of\njennifer doudna at berkeley and the\nwhole war between berkeley and mit and\nlicensing gene editing technologies but\nthey let you edit\nbase pairs or single nucleotides in a\ndna sequence with incredible fidelity\nand programmatic programmability\nand so the last thing here is the\nmarkets like who the heck are we selling\nto\nover the past 10 years spearheaded by\nall these these technologies in the rise\nof what are called cell and gene therapy\ncompanies now cell therapy is where you\ntake someone's own immune cells out of\ntheir body\nre-engineer them usually with a crispr\nmediated\nviral vectors to using viruses to inject\ncrispr into the cell\nre-engineer genetic code to an extent\nre-express\nsome sort of protein that's going to\nkill cancer\nusing this kind of modality companies\nare developing what's called\noff-the-shelf cell therapies to cure\npeople of cancer\nand then\nthe analog or not analog but closely\nadjacent is a gene therapy which is\nwhere you directly inject the patient\nwith a crispr loaded virus\nand then that crispr loaded virus\ntargets tissue with high specificity and\nis able to cure the disease\num\nso this is what we're selling to and\nthese guys are blowing up like crazy\nover the past ten years as this\ntechnology's becoming cheaper more\naccessible and it's being augmented by\ncomputation so let's get into the\ncomputation the last piece of the puzzle\nis the nature of biological data\nit's really big it's really\nheterogeneous and it's really hard to\ninterpret and wield\nthis is an example of a tiny chunk of a\ngenome and these genomes on average are\ngigabytes in size the human genome is\nthree point three gigabytes in size uh\nthis spruce genome is like 15 gigabytes\nin size this stuff is unwieldy and hard\nto wrangle\nin order to wrangle it you need to take\nthese short breeds from a sequencing\nmachine kind of align them\nand interpret\nyou know the consensus of a bunch of\nshort fragments this process takes days\nit takes dozens of cores it takes\nhundreds of gigabytes of memory and this\nis on a per sample resolution if you're\ndoing any of these experiments that we\ntalked about earlier we need to do this\ndozens hundreds of times right\nand from doing this kind of assembly\nthis is kind of facilitated by\nalgorithms you might all recognize\ndynamic programming and developing graph\nfacilitated reconstruction\nyou can generate you know interpretable\ninformation about how cells are\nstructured where they're grouped\nyou end up taking like\nvectors\nabout like what kinds of genes are being\nexpressed in different types of cells\nand you can understand things about how\ndiseases are progressing or how well\nyour treatment is working\nand additionally you can say things\nabout like how well you know christopher\nmight be editing yourself these are just\ntwo examples of how you might interpret\nwhat is otherwise completely\nuninterpretable data that would need\nsome sort of computation to make into\nsomething you can understand\nright so i thought you know i would just\nshow you all a quick video of what we're\nbuilding uh our platform is designed to\nenable this computation at scale with\nheterogeneous biological data\nfor the types of companies i showed you\nand the way we do that is relying on\nflights workflow execution engine to you\nknow have highly scalable kubernetes\nnative type safe workflows\nand we generate no code interfaces for\nthe biologists to use directly\ndynamically from a flight kit wrapper\nand by doing this we can expose a myriad\nof different bioinformatics tools to the\nend user and provide a really rich\nin-browser suite of visualizations and\nfile manipulation tools\nso this is our platform\nand then on top of the platform we're\nstarting to tap into\nthe bioinformatics computational\nbiologists and even software engineers\nat companies and giving them their own\ntoolkit to both write flight workflows\nand dynamically generate latch\ninterfaces\num on top of them\nso diving into a couple of technical\nbeats about the platform the first\ncomponent is a managed data store and we\nreally recreated a pretty much a fully\nfeatured mix flavored file system in the\nbrowser graphs arbitrary file stores at\nthe moment it's just s3 uh but the\nmoment uh you can move these around drag\nand drop them uh and visualize them see\nall that metadata you want in a much\nmore convenient way it's like the tip of\nthe iceberg the moment files with\nsemantic file extensions hit the system\nwhen you run workflows hooks on top of\nthem a great example is a dna file that\nwe recognize\nif you upload it to the platform we run\na flight workflow automatically to parse\nrun quality control and generate\nvisualizations on top of that file such\nthat when you double click on it you can\nthen see those visualizations right\nadditionally because we have our own\nfile system we can support our own\nnetwork protocol and our own absolute\npast system so you can pass\nthese object files\nobjects or files to workflows directly\nas semantic paths and the the workflow\nlogic and the flight character wrapper\nwill\nparse these and understand them making\nit much more easy to reason about where\nyour data is coming from and where it's\ngoing and how you can manipulate it\ndownstream if you found that i'm sure\nyou all have found that as well that s3\nand bucket source are incredibly\nunbuildly when working with large scale\nprojects it doesn't feel like a file\nsystem and you really need that\nto to scale things out\nso on top of the file system we have a\ncompiled type safe uis and what this\nmeans is we basically take the parameter\ninterface from flight from flight adl\nand we dynamically parse this and\ncompile react interfaces from typing\ninformation now from typing information\nwe can construct components with\nin-browser html native type validation\nthat makes sense as well as slap-on rule\nbased regex validation on top of html\nand data validation to add like rich\nlayering of browser side type checking\nbefore the workflow even hits kubernetes\nso\nthere's a lot that we've done here we\nhave a really talented designer and a\nreally talented product team uh that's\nreally like watching how biologists\ninteract with uh interfaces and and we\nwant to make sure that workflows don't\nfail\nbefore you know the programming logic\ngets executed in the container on the\ncluster\num and more to talk about here inside so\nthe last component here of the platform\nis the serverless scheduling find great\nscheduling and why we think flight is so\nimportant and why we think community is\nso important\nlet's just ground it into distinct\nbiological use cases\nso the the first reason why we need\nkubernetes and flight is we want current\ntasks control over how we schedule uh\nschedule tasks and this is because we\nmight have entirely different needs for\nan upstream or downstream task in both\ncompute environment and resources\na concrete example is we want to do one\nof these\ngenetic assembly operations on a highly\nthreaded machine and then we want to\nfeed the resulting assembly to a gpu\nenabled machine for inference on some\nsort of ml model maybe pick out\ncomponents of the code that are relevant\ntowards some experiment right it's a\nvery concrete example of why we need\nsomething like flight\nanother reason is obviously scalable\nscheduling\nso like if you need to do one of these\ngenomic assemblies on a human genome\nagain three gigabytes it'll take you two\ndays on a machine that costs a lot of\nmoney to run\num if you can leverage spot instancing\nand automatic reach tries through the\ncube scheduler and scale this out over\nkubernetes finding you know\nrelying on the schedule or like a cloud\nenabled scheduler to find capacity as it\ncomes up rather than implementing\nyourself on bare metal or something like\nthat\nyou can really do this thing at scale in\na way that other platforms can't\nyeah\nso this is the rough architecture of our\nsystem\nand how flights lets him um\nwe have a\na core kubernetes cluster uh which\nsupports a bunch of uh services\nit's not super interesting it's pretty\nstandard web stuff outside of the fact\nthat you know everything is kubernetes\nnative including\nnot just flight\nwe have a bunch of services named after\nbiological types that maintain a a set\nof local\nstate\nthat is tied to drop in flight\ndeployments with our own scaffolding\ncalled prion and private vpcs either in\ncustomer vpcs or in our own management\npcs what this lets us do is maintain a\nproxy file system and proxy workflows\nwithout actually ever touching the data\nor the workflows of customers but\npuppeting them around in remote systems\nusing a two-way sync service called\nribosome\nthis is really hard to do as you all can\nimagine and your team has spent\nconsiderable amount of effort like\ngetting this right\nthere's still things we need to work out\nto make this scale but um essentially\nit's lots of queuing and lots of uh you\nknow sns notifications from the aws side\nas well as um updates from a\nserverless system on the right some side\ni start stateless system on the right\nside\nthis basically cues on either side of\nany sort of file movement or workflow\nupdate propagation and that lets us like\nkeep a proxy of\nreally unwieldy objects in local state\nand keep consoles snappy\nthat's roughly how things slide in and\nagain the biologists interact with the\nplatform via the console and the\ncomputational biologist interacts\ndirectly with a server via the sdk\nand everything comes from the database\nin terms of like state hydration we're\nonly five percent or so the quests are\nsynchronous through the http server\num\nso jumping in really quick white flight\nagain so it's k it's native and so in\nconjunction with eks or some sort of\ncloud hosted solution that takes care of\nprovisioning instances for you we can\nprovision any computer we want which is\nincredibly important for what we want to\ndo uh there's language independent type\nsafety we wanted to support all the you\nknow\ntype primitives that exist in economic\nprogramming languages as well as\nexpanding biological types\nso we can support more fine-grained\nchecking of data things through\nworkflows we'll show you and we also\nabout the language independence is\nimportant because\ndevelopers of bioinformatics work with\nall sorts of languages although python\nis the most ubiquitous at the moment\nuh and then i would just explain tasks\nas independent deployment incredibly\nimportant for what we do and then we\nwanted a uh open source and\nwell-maintained code base that we can\nunderstand interact with work with the\nteam\nand uh push improvements to the general\ncommunity\nso so far we've made several\ncontributions and we have a lot more\nplan on the way we worked on some stuff\nin house that we want to spin out um but\nour our contribution velocity and the\nrate of which we're contributing is a\nreflection of like i really are\nconfidence in flight long term as the de\nfacto workflow orchestration engine um i\ncan't i'm not just saying this i really\nthink flight has got the model correct\nabsolutely correct with respect to how\nyou're thinking about architecting and\ndeploying more close as both code first\nand leveraging the committee scheduler\nto\nabstract away the scheduling on a per\ntask granularity i really think that's\nthe absolute right way to think about it\nand so you want to help support and grow\nthis project and make sure it sticks\naround for a really long time i i really\nthink it'll be around as long as as dr\nkubernetes we've contributed typing\nmetadata doctrine metadata and max is\nstill finishing up union types\nbut\nwe'll dive into docstring and typing\nmetadata in a second union types as you\nwould imagine optional sum between none\nand some other arbitrary type which is\nreally important for us\nto make sure that we can catch\nnull non-values before they hit the\nworkflow\nso\nreally quick why do we want parameter\nmetadata what is it\num\ni talked previously about adding\nserver or client-side rejects role based\nvalidation on types\nso if you want to add information to\nchange how the type is parsed on the\nfront end again considering that we\ncompletely generate our interfaces from\nthe function header alone uh you need to\nbe able to stuff arbitrary zone into the\ntype literal and the language at which\nyou're developing so in our case talk\nabout python we we want to annotate the\npython type with a class let's just like\nadd arbitrary information so we can\nparse that on the front end and tell the\nuser hey you know\nyour your string doesn't actually\nconform to the subset of strings that\nwe're allowing in this case nucleotides\ncan only be ace t c's and g's if you add\nan x it's probably incorrect here you\nwant to explain that information\nso by defining a type um exposing it to\nthe user either through the sdk and like\nkind of a predefined stuffed uh value\nallowing them to annotate the parameter\nand their function\nwe can then generate these parameter\ncomponents that\ndo exactly as i just described and are\nincredibly pluggable and let people do\nthis on their own if they want\nso this also extends to\nbiological types if you want to\nannotate like a file extension\nwith um you know to make sure that if\nyou know this workflow is only taking\ninside file um we also do the exact same\nthing with the same sort of logic and\nyou can find this supports almost\nanything you want\nreally quick we also have uh doctrine\nmetadata and this allows us to define\nkind of the layout ordering uh\ndisplay of a workflow interface\ncompletely in a function dock string\nborrowing from frameworks like sphinx\nand how documentation is parsed from\nrestructured text and those doc strings\nbut everything from ordering to the\ndisplay names to groupings different\nparameters to tool tips later um\nactually this week or next week i've\npaired groupings of parameters that must\nyou know have certain state in\naccordance with each other can be\nenforced in the doctrine of the workflow\nitself\nand this is really creating kind of a\nnew paradigm\nwhich we're calling like interfaces code\non top of flight\nwhich allows you to completely define\nfront end and like a couple of python\nfiles in version control which i think\nis very powerful\nand so lastly to wrap things off one of\nthe show examples of like an internal\nmodification we've made to the flight\nsystem uh you know i mentioned we have\nthe slash network scheme which allows us\nto\nresolve and understand file objects with\nrespect to the absolute path\nfor any given user's working directory\nin their account\nand we do this by proxying\nrequests\nfor files within flight kit to manage\nendpoints to authenticate and like then\nserve the pre-signed urls to the the\ntask code\nand\nwe actually met with the flight team to\nfigure out the best way to do this uh\nwe're using the execution name to store\nsecrets and it's a like a really cool\nand secure way to do something like this\num an example of like how we've leaned\non the flight team to\ndevelop something that worked for us\nwell\nand so\nlastly we want to talk about another big\nfeature we're working on in house and\nthey're excited to release and work with\nthe flight team to integrate into the\nthe master code base\nis server-side containerization um\nyou know constructing docker containers\nlocally like is a bad experience for a\nlot of people and it's it's expensive\nand can be improved drastically right\noftentimes you have to pull large base\nimages oftentimes people even know how\nto use docker which for some is\nsurprising but that's like really often\nthe case\nand then you have to pull large layers\nthey use lots of space they use your\nlocal file system throughput your cpu\nyou can't even watch like youtube while\nyou're building a docker container and\nyou have to push the containers back up\nit's like it's terrible\nso\nwe can create a specialized container\nbuild service we're calling centromere\nand we'll release this open source in\nthe coming months but by reusing very\nnaively if you just think you can reuse\na shared file amounts and share build\nenvironment to keep a warm cache of file\nerrors and prevent the network transfer\nof\nthe build contacts to do flight and just\ndo the registration on the server side\nthere's something more we can do in a\nsecond here but like this is the basic\nidea\nuh any sort of local state you need you\ncan parse from the docker file and just\nsend those individual files up to said\nservice and use that to build the docker\ncontext\nthe docker build contacts\none additional thing you can do if you\nwant to like roll out unilateral configs\nyou can recognize that a layer is really\njust file archive with the metadata and\nhash\nand so if you want to change something\nlike um you know the value in a config\nfile like your secret enough like config\nfile or something like that you can just\npull apart the archive and if you file\nslide back together make sure the hash\nis the exact same and as long as you\nlike understand that your change to the\nwhole\nthe aggregate file layer is safe\num you can then deploy this unilaterally\nto\nall the the containers that your users\nuse um so this is another interesting\nidea we're\nexploring and playing with and so this\navoids triggering rebuilds for really\nbig workflows\num\nyeah and so that's that's it's um happy\nto talk about i know this is like it was\nsuper deep and you know we're looking\nforward to doing maybe a deeper one\nlater on some of\nexisting or future contributions but\nthis is like what we do and how we're\nusing flight so um\nthat's it\nthank you kenny that was fantastic\nand asked there are a bunch of i think\nbiotech people here so a lot of\nquestions\nkenny i'm curious uh what you think\nabout handling like private data or phi\nor protected data on latch for uh like\npublic accession and\nuh like making sure that analysis is\nreproducible you know on large-scale\nprojects similar to like cromwell uh or\nother technologies\nyeah i mean so we uh like i explained\nthe way our architecture set up now\nwe'll just pull that up again is that um\nwe can really address\nall these concerns\nabout\nholding on to the actual files or\nholding onto actual workflow state by\nhaving flight deployments and completely\nclosed off epcs and puppeting their\nexecution and puppeting the movement of\nfiles without actually touching them\nso again what we do is we hold\nreferences to\nthe location of the objects and we hold\nreferences to the workflows themselves\nbut we never actually hold on to them\nwith an infrastructure that we own\nso that's one way we deal with this and\nthis is an incredibly hard thing to do\nbut it's something we've done for the\nexact reason that our customers are\npretty\nsketched out about this\nwe also are mostly pre-clinical so on\nthe discovery side pre-ind and we don't\ndeal with a lot of or we don't deal with\nany\napi\nor any clinical information at the\nmoment\nthanks\nabsolutely yeah also like\noh sorry go ahead\ni was just going to say it's like you\nguys are going to talk about if you have\nexperience in biology you want to talk\nabout that as well our team has\nexperienced there and i'm happy to talk\nabout that\nuh i i also have a related question to\nthe metadata you store on your side i\nsaid it's like five pointers and things\nlike that does it have\ndo you keep like uh\nworkflow names the spec of the workflow\nthe task names primitive inputs like\nthis kind of data do they live or do you\ncopy them in your\ncluster or do they stay\nuh\nexclusively in the user side\nyeah so um\nall we hold on to is like the typed\ninterface and that's what we need to\nboth generate the the front end and also\nto make any sort of actually create the\nexecution\nuh through the flight control\nuh service okay and then\nyeah i mean outside of that metadata\nabout the execution name when it starts\nand ends\nas well as references to\nthe location of the objects that if any\nfiles are being passed in\nthat's the extent of it\nas we have full flight deployments\nset up with\ni think your old customized system from\na year back and so you can imagine\neverything there is what lives in the\nnppc\ndeployment\ncool\ndo you guys use the event egress to\nactually in your ribosome do you guys\nuse event egress to sync the state from\nthe flight execution to your local\ncontrol plane i'm going to call the\nlatch uh\nyour own kubernetes deployments as your\ncontrol pane\nyeah we actually don't we um we have a\ntunneling instance that listens to the\npostgres deployment\nand then we have triggers set up\nyeah\nyou should you probably should look at\nthe event guys that might be interesting\nbut that's okay we'll definitely do\nyeah might be better that was the pain\nin the ass to set up\nyeah the tunneling is it's pretty hard\num\nyeah i had a question about this\nactually this sorry we are going a lot\ninto the security aspects but uh\nit seems that you know\nyou probably encode the nucleotide as\nyou said in a in a string\nuh and so if\nfrom our point of view if uh\nflight admin is the one that actually\nstores the derivative value which is\nincludes a string\ndoes that uh and this is for you know uh\ncraig and jay and whoever's here who who\nwere deploying their own right to\ninstances\nif the ui shows that value is that\ndesirable or is that not desirable\nbecause that\ni realize that there's a security\nvulnerability but there's also ux and\nand\nthe constant fight between them and what\nis the right experience\ndoes that question make sense or maybe i\ncan repeat it again so like strings\nintegers and so on that flight actually\nshows in the ui today as an input and\noutput of tasks and workflows and so on\nis that the security vulnerability at\nsome level and uh if not\nwhy not and if yes\nwhat should we do it should be like just\nin our background enough\nyeah i mean just quickly from our\nperspective we don't see it as a\nsecurity vulnerability our customers on\neither i think anything that's small\nenough to fit and like a reasonable\namount of json that you can send to a\nlike a browser\nis not\nsomething that people are like worried\nabout us holding on to um i think people\nwould be worried about us like having\naccess to their entire genomic corpus\nbut any sort of like input or output\nlittle string of nucleotides that you\ndisplay to someone to make them\nunderstand um you know how the workflow\ndid is not something people have\nexpressed concern over i do imagine that\nlike technically and maybe longer term\nwe would have to figure something out um\nboth with you and us\nyeah we do have some ideas around it\nwe'll definitely love to share with you\nguys also\nany other questions\nif not like i think\nthe latch team is just fantastic they\nare rock stars they they just go in\nsolve the problem get things done and i\nam just amazed at how how much they get\ndone in like just couple months so\nall power to you guys and uh thank you\nfor you know supporting us\nand thank you for sharing\nreally appreciate that yeah and like i\nsaid uh we really do believe in flight\nlike this this isn't there's no\nmarketing it's not here right it's\nsomething we just believe in us in\ntechnology so we'll uh\nwe'll continue to support and contribute\nand work with you all we've learned a\nlot from your team through the pr\nprocess and we're excited to keep on\ndoing that\nthank you thank you kenny thank you yeah\nit's been a long review but like\ni'm so excited about all these features\nso thank you for contributing uh and for\neverybody else like it's okay to\ncontribute it like i i've actually had\nsome people\num ask me questions that hey i have\nnever\nwritten code in this kind of an open\nsource project what should i do it's\nokay open up a pr\nbut then be open to like you know\ngetting comments and and we'll work with\nyou our i think the core maintainer\ncommunity is very very\nuh\nopen to receiving new code they\nunderstand that it's not always easy to\ncontribute but\nyou know we also have uh obliged to\nmaintain quality of the code for a large\ncommunity so\nuh but that doesn't mean you should not\nplease open up and we will work with you"
    },
    {
        "title": "⚡Life of a Workflow⚡ - Flyte OS Community Sync",
        "transcript": "um so hey everyone um if you don't know\nme i'm katrina i'm a software engineer\nat union a contributor to play open\nsource and today we'll be taking kind of\na peek at what happens at a flight\nworkflow\nafter you kind of finish writing it and\nwant to execute it on plate\num just so it's an overview we'll go\nover you know writing flight workflow\nregistering your flight workflow what\nexactly does that mean and then kind of\ndifferent models for executing a flight\nworkflow for different task type\nexamples\num so writing flight workflow hopefully\nyou're all familiar with what uh flight\nworkflow syntax looks like um here's a\nreally basic example we're using to kind\nof like walk through um so you have the\nsquare workflow which squares an input\nvalue\num and you're probably familiar with you\nknow writing workflow tests workflows\nrunning those locally testing those\nlocally\nbut after you're kind of like you know\nconvinced that your code works\nyou go through the registration process\nand what exactly happens there well\nlet's take a look\num so before we into like what happens\nwhen we register flight workflow let's\ntalk about a little why we register\num registering a workflow gives us uh\nallows us to do a compilation pass which\nensures uh which gives us kind of more\nlike static validation to ensure that\nyour workflow won't fail at execution\ntime\nallows us to version a workflow creates\num an artifact that we can store in our\ndatabase that's shareable reusable\nyou can distribute with your team and it\nalso gives us a concrete executable\ndefinition of flight workflow that\nallows us to visualize it um you know\nhas a dag where you can see how data\ndependencies kind of inform your your\nnode edges and define your workflow\nstructure\nand as part of registering flight\nworkflow you know first you write and\nyou test your python code like we\ndiscussed earlier then you'll use flight\nkit serialize which you may have seen\nbefore as part of your registration\nprocess which takes your kind of your\npython user facing code and converts it\ninto a protobuf which is a wire format\nand allows you to kind of have\nserializable representation of the\nworkflow definition\num as part of the registration process\nyou might also go ahead and build and\npush container images as mentioned\nearlier it can be sometimes a slow\nprocess but it allows us to essentially\ncapture the runtime environment that's\nused to run your python code definitions\nand your python tasks\nafter you serialize we'll go ahead and\nwe'll compile that pro that workflow\nprototype representation do that static\nanalysis and validation and then that\nproduces an executable artifact that\nwill persist in the database and which\nends up being used when you execute a\nworkflow or you do share that workflow\ndefinition or pull it perhaps saying\nlike a remote\nso let's take a look at what those\nindividual steps uh define or and entail\num so in this case like it's serialized\nwe talked a little bit about how it\ntakes that you know that original square\nuh task definition that's used in that\nuh workflow definition\nand converts it into this kind of serial\npro proto representation we see here\nthis has just been dumped to json uh\njust for ease of visualization\nthese representations are portable\nyou'll notice here that we don't\nactually have a project or domain filled\nin\nthat all gets substituted at\nregistration time but the core\ndefinition of the workflow is captured\nin this representation\nafter serialization we'll go ahead and\nregister that workflow we'll upload that\nto flight admin during api call fly\nadmin will take that kind of precursor\nworkflow definition go ahead and compile\nit which is where we do that analysis\nfigure out if there's any exceptions or\ninvalid workflow components\nand then if that\ncompilation step succeeds uh then we'll\ngo ahead and produce an executable\nartifact uh which is you know that that\ndefinition that you'll see when you\nlater flight ctl forget it or you know\nsay uh you know visualize it in the ui\nfor example that's that kind of compiled\nfinalized workflow definition\nit's saving database\nit's versioned it's immutable although\nyou can always create new versions\nand it's kind of the reference workflow\nentity\nafter you've completed the registration\nprocess\num and here's that meals as a weather\nforecasting example for a more fun\ncomplicated example workflow\nso after you registered uh persisted\nthat workflow definition you go ahead\nand call execute well what happens there\nso what we do is we take the the\ncompilation artifact we uh translate\nthat into kubernetes crt\num if you're interested in kubernetes\nit's a crds custom resource definition\nit's kubernetes extensions what we\nessentially use to store workload state\nduring the uh the duration of the\nworkflow execution allows us to capture\nindividual node statuses and overall\nworkflow status\nas far as actually executing the\nworkflow we'll so we have a dag that's\ndefined by these kind of data\ndependencies between nodes right and\nthat kind of informs you know how we can\nprogress in our workflow execution and\nthat we use that in order to traverse uh\nthe nodes in the workflow and figure out\nwhat we can begin executing now this\ncomes with you know a few benefits which\nallows us to kind of you know auto\nparallelize that execution when any node\nhas its input dependencies and that we\ncan immediately begin execution we don't\nhave to do this serially one node at a\ntime\num and additional performance benefits\ncome from say you know using discovery\ncaching\nwhen you run the same node with the same\nset of inputs for the same version we\ncan just go ahead and fetch that\npre-computed result without you\nnecessarily having to re-do that\nexpensive computation\nand propellers come in again is\nresponsible for kind of you know\ntraversing the workflow dag reporting on\nuh no changes as they occur\num it takes a look as uh you know your\nindividual tasks complete it'll report\non outputs phase changes and errors as\nthey do arise so you can visualize all\nthis in the ui and get real-time\nfeedback on the status of your workflow\nexecution\nso let's revisit our kind of square\nexample a little simple but it should be\nsufficient to get us to walk through\nwhat happens you know say for a python\ntask\num\nso in this case\num what we do is as we're you know\nprogressing through that square uh task\nuh flight propeller will create a\nkubernetes pod that uh runs that user\ncode uh that you'd find in square task\ndefinition inside of a container that it\nbrings up and that the container is the\none that we pre-built at serialization\nregistration time\num so flight propeller will keep\nperiodically monitoring the status of\nthat pod and reports as it progresses\nand changes phase after it produces an\noutput light propeller will send all\nthat data back to flight admin um and\ncapture that so that it can run any\ndownstream nodes that consume the\noutputs of that square test\nas their inputs\nso let's take a look at say another\nexample of tasks which is a you know a\nsql query uh example but this could be\nany kind of like remote service example\nyou see here that the structure of the\nthe task execution changes a little bit\nin this case we don't actually\nnecessarily need to bring up a pod\nwithin kubernetes we just make an\nexternal service call which allows us to\nkind of eliminate that you know\ncontainer runtime overhead of bringing\nup the pod in the container\nin this case flight propeller is simply\njust calling out to that external\nservice executing that query that got\ncaptured at serialization time and again\nyou know checking in with that whatever\nremote service might be um and uh\npulling to get outputs or an exception\nif it does occur and again feed that all\nback to flight admin uh person control\nplane and have a finite or have a final\nset of events\nand then let's take a look at one more\nexample this one's a little more fun\nwhat happens with a dynamic workflow\nso in this case we kind of see how you\nknow this registration or how like the\nworkflow\nsterilization registration process comes\ninto play at execution time\nso when you run when you have a dynamic\nnode a flight propeller will again kind\nof spin up a kubernetes pod it'll run\nyour user code in a container in this\ncase what the user code does is it\nproduces a brand new workflow spec\nand just like the process for you know\nrunning through a workflow traversing\nthe nodes within the dag flight\npropeller will repeat this for the\nworkflow that's produced in the dynamic\nnode uh proceed through the execution of\nthat newly generated workflow and once\nthat succeeds essentially mark that you\nknow enclosing parent node as uh\ncompleted and you know through the\nprocess of you know monitoring the\ndynamic node execution again propeller\nwill a report on execution events uh\nsend back output details and all this\nwill persist in the database so that you\ncan go ahead and use this\nto you know visualize and also\nsee individual inputs and outputs later\non\nonce that node completes um the\npropeller continues traversing the nodes\nwithin the tag and um yeah there you go\nthat is workflow execution\num so that's all for just this kind of\nquick lightning talk if you have any\nquestions happy to help answer\num and a few more resources if you're\ninterested in kind of learning more\nabout how dataflow kind of works between\ntasks or understanding the kind of\nbroader workflow state machine that\nlight propeller operates on\nthanks so much\nthat's great i think uh\nwe people don't have questions i want to\nadd one thing to it is\nwhen we are running the sql class that\nwe talked about that's called a back-end\nplug-in so if you have\nand those can be extended for yourself\nuh you can essentially invoke any\nservice\nand make state changes in that other\nservice outside of kubernetes and i\nthink one of the goals of flight was to\nconnect kubernetes with the outside\nworld\nin a very very simple way and and keep\nthe user experience the same\nso\nthere are already a bunch of\nconnections or back-end plug-ins\navailable uh but\nas you if you guys would want\nplease uh\nthere's docs on it if not you want to\nbring us up let us know we can help you\nright one i know there's somebody\nworking on a task plug-in there's a link\nplug-in that's in progress there are but\nthese are kubernetes specific plugins so\nthat allow you to orchestrate more\ncomplicated stuff on kubernetes but also\nthere is\nit can be query snowflake redshift\nexists atena exists but redshift is in\nprogress or something and\nso yeah so if you have ideas let us know"
    },
    {
        "title": "Flyte Community Update 008 - March 8 2022",
        "transcript": "morning good evening\nit's march 8th in pacific time\nand happy women's day everybody\nwe\nwill be uh i just quickly introduced the\nagenda for today and then we'll uh you\nknow do a couple of community roundup\nitems and then jump on to the encore\npresentations uh kenny and katrina both\nare adopted so fantastic\nso from a community highlight point of\nview we are\nthere's a hackathon in progress uh in\ncollaboration with uh mlabs community\nso\ni think the date for signing up has\nalready passed but there are about a\ncouple teams\nlots of\nfolks in each team\nand\nthey had the goal is to\nuh basically build end-to-end\napplications on on flight and\nmostly machine learning applications and\nwe will\nbe hopefully we'll be showcasing a bunch\nof those applications on april 5th or\nthe march 22nd so we're still deciding\nwhich one uh will be coming it's\nsupposed to be\ncompleted by a profile so it might be\nthe fifth\nand uh we after that on the conclusion\nof the\nhackathon we'll also be having a sync up\non the ml ops community channel so stay\ntuned we'll probably do a demo there\nso yeah\nlots of and thank you for everybody for\njoining i\ndon't know if all the folks who are part\nof the hackathon know how to join this\nthing\nuh it's on us to probably share better\non the other hand as usual the office\nhours i see a lot of people ask\nquestions and and some of these officers\ngo beyond 30 minutes\nthey go to like an hour hour and a half\nat times so thank you for joining in the\ncontinued interest\nwe'll be doing that again tomorrow\nwednesday\nmorning 7\na.m pacific time and 9 p.m pacific time\nand uh a quick look at what's going to\nhappen after this\nuh sink the next sink is march 22nd\nuh\nwe are doing\nuh i think there is one\ndemo that we are missing out but maybe\nwe are doing uh we will talk about intro\ncheck pointing and how you can use uh to\nrecover from failures for long-running\ntasks\nuh natively using flight\nand then we'll also be showing flight\ndecks uh and even a new\nfeature that probably will help a lot of\npeople\num so stay tuned and i think there is a\nuh one of the users also presenting i\njust i think we are still working on the\nyou know\nthe timeline for them versus drive works\nversus like spotify and so on\non april 5th uh it will be strike works\nuh talking about their chariot platform\nthat they've built on top of flight and\nthis they'll be doing a live demo with\njupiter notebooks and flight and\nall of that so stay tuned\nand yeah there is uh we'll also be\nannouncing the engineering labs\nhackathon winners\nso if you\na lot of people have asked questions how\nto join this meeting\nall of these events are available on the\nread me uh within the community section\nor on the flight.org\nhome page\non the community section uh it's an ad\nevent saying it should automatically put\nan event on your calendar it's great to\nactually put that event on your calendar\nyou don't have to join every time if you\ndon't want to but it's just easier to\nget things on\nanother way to stay updated is the\nyoutube channel please subscribe or\neven a newsletter that we publish mid of\nevery month so next one is later in next\nweek\nso please subscribe to any of those\nuh from a roadmap point of view it's\njust going to be a quick overview uh\n0.9.3 is coming up this week uh\nlots of ui improvement lots of like kid\nflight remote improvements\num\nand map task improvement so yeah\nit's a big\nbig uh\nrelease but it the one before\nthe mega release that we're calling 1.0\nand that's it i wanted to\nget this over with quickly so that we\ncan jump on to the presentations um and\ntoday we have kenny from latch uh we'll\nshare\nthat platform and like uh all the cool\nstuff they're doing\nand how they use like for biocomputing\nand then we'll have katrina from the\nunion ai team just talk about uh\nin general go from the you know the\nbeginning of how do you author to what\nhappens to\nand go through different modalities of"
    },
    {
        "title": "Merantix - Parameterizing and Executing Flyte Workflows With Hydra-Core",
        "transcript": "so my name is fabio i'm a\nmachine and an operations leader lead at\na company called neurotic labs based in\ngermany in germany\nand i will talk about the way that we\nintegrate flight with a library by\nfacebook\nai called hydra core which is used to\nparameterize applications\num\ni'll give you like a brief introduction\nof who we are and what we do and to\nwhich requirements is lead for a flight\nand then\ni have kind of two topics that i want to\ntouch the first one is that we wanted to\nbe able to very quickly reach continuous\ndelivery for machine learning\nand give our engineers even if they\ndon't know a whole lot about\ninfrastructure management and ability to\nset that up within a few minutes and\nthen just start using it\nwith slide and then i will talk about\nhow we\nintegrate hydra to manage workflows with\nflight to execute workflows\nand\nto do that we built two plugins one for\nflight kit to be able to handle the\nconfiguration objects by hydra and we\nbuilt one one plug-in for hydra to be\nable to launch flight workflows okay\num that's the plan so let's jump right\nin we are a\nmachine learning solutions provider you\ncould say in germany so we ideate and\nimplement machine learning systems for\nindustry clients in the german-speaking\nregion of europe\nso we have many different projects and\none requirement that comes out of that\nis that we need to be able to spin up\nthe infrastructure for flight very\nquickly and in many replicas for\ndifferent clients because they need to\nbe separated so we cannot have one\ncentral instance of flight and then run\neverything in there we need to be able\nto start the project and then within\nlike an hour or so have a replica of\ncomplete flight infrastructure the\ndedicated cloud project for that first\nrequirement\nsecond requirement we we strive to\nhave everything that happens on the\nproject go through git so starting a\nwork workflow goes through git the\nresults are tracked back and gives if\nyou want to deploy a machine learning\nmodel that goes through git\nso everything should be in version\ncontrol so we want to be able to do\ncontinuous delivery for machine learning\nvery quickly and configure it also like\nwith templates within a few minutes\num\nbefore we started looking at different\norchestration frameworks we already\nintegrate have integrated hydra every\neverywhere in our internal stacks so it\nhad to work with hydra\nthere was no way around that um\nand then\nyeah we want\nwe want the thing to be\nusable super easily for our engineers so\nthat's why we built our own launcher\nlauncher for flight that i will show you\nwhich lets you register\nand actually work with a single command\nso the way that flight currently handles\nit i counted it's like four or five or\nmaybe even six commands until you can\nexecute something\nthere's no way i could have convinced my\nengineers to do that um so we built the\nway for them that they can just do the\nsingle command\num and we have been in conversations\nabout this for a while and i also gave a\ntalk about it at the previous\ncommunities i think\num and this is kind of what came out of\nthe dots we have we have we used to have\nback then\nso\num\nmaybe let me right jump right in and\ngive a brief introduction about\ncontinuous delivery for machine learning\nfor those who haven't heard it\nthis plot here or the this graph here\ncomes from a really cool article\nfrom google from google cloud\narchitecture center um let me\ngive you the breathe like the the very\nsimple idea okay\nif you if you have a very\nor if you have a machine learning system\nthat's not automated at all you you\nprepare your data manually in one\nnotebook maybe then you train your model\nin another jupiter notebook\nyou validate that model and then you\nhave the model checkpoint you give it\nmaybe to some other team and then they\ndeploy or operationalize it for you okay\nbut that there are many many new steps\ninvolved and this article explains the\ndifferent levels of automation engine\nsuch a system and the most automated and\nmost mature one is what they call level\ntwo continuous delivery for an l\nand basically the idea is that\nlike data preparation data validation\nmodel training model evaluation model\nvalidation and model deployment all that\ngets orchestrated\nwhich for example by flight or some\nother framework so this orchestration of\nthese different tasks here is done by\nflight\nand that\nthis orchestrated workflow gets\ncontinuously delivered by some cicd\npipeline for us that's google cloud\nbuilt but it can be any other provider\nright so the idea is you make a change\nin your in your repository that defines\nthe workflow and cloud your icd provider\nruns tests and if the tests pass on your\nworkflow it will register it and run it\nfor you right so\nthe only thing you ever do is change\ncode in git\npush it and it will register and run the\nworkflow for you and if you set it up in\na way that you might even get like a\nsummary in github or something in the pr\nso that's the idea right you don't do\nyou don't manually run a jupyter\nnotebook and give some file to somebody\nelse you change code in github push it\nand it runs through until the deployed\nmodel that's kind of the idea of what\nyou see here\num\nand our goal is to be able to do that\nwithin like half an hour when a new\nproject starts so uh\ndo that super quickly do it for many\ndifferent projects at the same time\nand um\nthe what we basically did is that we\nbuilt a reference implementation of how\nflight infrastructure should look at our\ncompany so\nin the end that was a bunch of terraform\nfiles and some helm values filed to\ncustomize the\nflight helm helm\num\nand we have some\nwe have an internal templating mechanism\nthat is derived from ruby on rails\ngenerators where you have different\ncomponents and they can call each other\none can i'm sure one can achieve the\nsame with with cookie cutter but\nbasically what an engineer would do when\nthey start their project they would say\nmx the name of our company generate gcp\nsite infrastructure\nthe cli will ask them for a bunch of\nparameters like how the project called\nwhich gcp project do you want to use\nand then some other other things that\nare not too important and then this\ngenerator this gcp flight infrastructure\ngenerator will generate some terraform\nfiles that configure buckets for data\nerm roles so that we can use workload\nidentities with flight\nand it will generate some yaml files for\nflight and the engineer just goes here\nfrom applied control apply\nand\nthen within a few minutes you have you\ngo from zero to a gcp project that has\nkubernetes that has a dke cluster that\nruns flight basically\num\nand what they would then do is that they\nrun\nmx generate flight workflow they give it\na name and then there are some options\nthat are the workflows that we use can\nhave so you can say do you wanna do one\nspark or don't i want spark and some\nother things that we say okay this is\nsomething that we will use all the time\nand what this will give you is that it\nwill give you a minimum working example\nso maybe the interesting files here are\nworkflow.pipe that contains a minimal\nworking example of a flight workflow\nthere's a docker file that can be used\nto to package or to build a docker image\nthat contains this this workflow\nand\nthere is there are some other files that\ni will get\nthat i will talk about later for example\nthese these workflow bundler images\nbundle here\nand then this conflict folder with other\nyamaha files that is that is hydra stuff\nokay\nso what the engineer\ndoes to summarize is they call one cli\ncommand that will generate in the\ninfrastructure's code to create a cloud\nproject create a kubernetes cluster\num and then some kubernetes manifest and\nhelp chart values file to install soft\nlight in there that takes a few minutes\nand then they run a second command using\nthis templating mechanism that gives\nthem a workflow with a certain name and\nthen the option do i want spark they\nwant the tensorflow image\nand some other stuff and then\nthey\nthat takes like\nuntil the audit era from apply is done\nand everything is installed it takes\nlike 20 minutes i would say but then\nthey're able to run a new workflow on\ntheir own flight instance in the cloud\nthat didn't exist half an hour before\num\nthen\nthere's some other\ngenerator that will uh take the\ninformation that the user provided and\nthat was serialized to some state file\nlike i know there's a lot of code the\ndetails are not important here what\nyou're seeing here is basically what the\nuser entered as the cli arcs it's saved\nto some file and then later when you run\nanother generator it already knows what\nyou entered that's the only idea here\nand then the user could say let's\ngenerate uh\nbasically a cicd pipeline for feature\nbranches the command would be mx\ngenerate cloudable branch and with the\noption register flight workflows and\nthen you would\nbasically given the name of the workflow\nthat you registered and whether you\nactivate the flight or or high torch\njobs etc it knows what images you need\nto build it will build them for you and\nit will\nwill register the pipelines so the\nengineers themselves they never really\nneed to get into the details of the\ninfrastructure as code versus the icicid\npipeline\nthey're just given a command line tool\nthat where they can generate\ninfrastructures code they can generate a\nminimum working example of a workflow\nand they can generate\nthe the ic the cicd configuration to\ncontinuously deliver this workflow\nthat's the first idea\num\nnow\nthat brings us into a situation where we\ncan\nlike bootstrap a project very quickly um\nwhich is required because we have so\nmany of them that the single instance of\nflight is not enough but now we are in a\nposition where our engineers can say i\nneed to start a new project tomorrow no\nproblem will take half an hour and then\nwe have flight running in that project\nwhich is completely separated from from\nother projects\nand um\nnow comes the part where it's supposed\nto be very simple for them to use flight\nand also with our existing stack which\ninvolves hydra core okay\num\nhydra core is a framework developed by\nby normita ai in paris by a guy called\nomri\ni forgot the name really cool guido and\nthey call it the framework for elegantly\nconfiguring complex applications okay\nthe idea is that\nyou write yaml files that contain the\nconfiguration for your app so in the\nsimplest case\nthis could be a conslash conflict yaml\nfile that has\nsome database configuration\nbut you can have hierarchical\nconfigurations which means that this\nyaml file could reference another yaml\nfile and then when the config is loaded\nyou have like a tree of different\nconfigurations so the database it has\nchildren driver user paths but there\ncould also be another yaml file loaded\nin there so you can have a hierarchy of\ndifferent configurations which really\nlend themselves to to um\nconfiguring workflows because there\ncould be one config for the entire\nworkflow and then for every task there\nis a subtree in that in that\nin that config object and we use that\nbasically to\nto configure everything we do some\nin-house frameworks for computer vision\nnlp they all use hydra core to to\nparameterize the model training\nwhat's also nice about hydra\nto capture conflict with schema checks\nso\nsaid\ncomputer framework can say okay if you\nwant to configure a trainer task\nthis isn't this and this is what we\nexpect in terms of values and these are\nthe types and you are warned when you\nprovide values that\nbasically break things\num there are launchers for for\nframeworks like gray or redisque and our\nthinking was well there could be one for\nflight um that's what we built then and\nthere are nice nice features like\nmultiround which i will show you now\nyou can also see my\nmy my terminal right or can you only see\nokay\nmaybe let me go back one second so the\nidea is that you write the xiaomi file\nhere config yammer and then your main\nentry point function you wrap with the\ndecorator called hydra main and here you\nsay\nin which folder do i find my config file\nwhich would be this conf folder and\nwhat's the name of the yaml file which\nwould be config yammer okay and then\nwhen you execute your python script\nit will parse the config in this case\nit's a single file but it could be many\ndifferent files and it will schema check\nthem and type check them for you\nand then it will give this this dick\nconfig object to your main function here\nand this example here will just print it\nas a yaml file\num so when i execute that i'm just\ncalling python my app.pi\nit just paints basically this config\nobject that was parsed from\nit was parsed from the um\nfrom the yaml file but what i can also\ndo is here i can say\nmultirun and now i can for the database\nuser provide three different values\nwhich is my first name my middle name my\nlast name\nwhen i press enter it will start three\ndifferent executions of the of the the\nmodule where every time the user is a\ndifferent one\nand that's very nice for machine\nlearning when i say hey\nthis here runs my my my uh my training\nscript\nthis here\nruns my training script with a whole\nbunch of different learning grades and\nto just\nthat just spawns different processes and\nexecutes them for you and it would be\neven nicer if that would work in a\nflight cluster\num\nso this is what i meant with multirun\nwith hydra multirun\nso we did two things now the first thing\nis that\nwe needed to teach flight\nhow to pass these cons these stick\nconflict objects that you saw here\nwhich are given to the main entry point\ncfg here we needed to tell flight how\nthey could be passed around between\ntasks um\nthat's the first thing for that we built\na plug-in for flight kit\nand the second thing is that\nlike there are plugins for ray or rescue\nor some other frameworks we wanted one\nfor flight that takes care of building\nthe docker images registering the\nworkflow creating a launch plan\nregistering the launch plan and then\nexecuting the workflow in the cluster\nso we built these two things um\nand the first one let's talk first about\nthe cycle plug the flight cut plug-in\nturns out that's super simple to do um\nthis is i shortened it a little bit but\nit's in principle what you see in the\ngreen boxes here is the code we wrote\nfor that um which is really nice so we\nwrote the dick config transformer\nplug-in\nthat inherits from the type transformer\nand\nwhenever we use a dick config to\nlike as an input to a task or as a\nreturn value from a task\nthis\nclass here gets invoked to either\nconvert a dick config object to a\nliteral\nand in the end the only thing it will do\nis it will use the like omega conf which\nunderlies hydra to convert the the\nconfiguration object to some json\ncontainer\nto some simple json\nor\nload it from this serialized value back\nto a\nconflict object so that with that flight\nall of a sudden can\npass these dick config objects from\nhydra around\nwhich is super nice and our minimal\nworking example that you you get when\nyou render this template it might look\nlike this here so\nuh the pipeline it gets a conf like a\nconfig object\nthe evaluator task gets a conflict\nobject the train task gets a conflict\nobject so everything is conflict objects\nnow\nand\nas long as the conflict object doesn't\nchange the caching kicks in and the task\nis not run again but if one of the of\nthe values in these yaml files change\nall of a sudden you need to run the task\nagain\nthe way we do it is that typically we\nfirst have a create conflict task\nthat\ntakes this giant conflict object which\nhas like one subtree for the evaluator\nin this case and one for the trainer and\nwe split these out\nand then return them and pass them on to\nthe other tasks the reason we do it this\nway is that in the workflow here these\nthings that are returned they're\nactually promised objects so you\ncouldn't go config dot some sub uh some\nsubtree that wouldn't work but uh\nstarting a part for that is actually\nsuper simple which it works really well\nfor us it also uses caching so if this\ngiant config object didn't change the\nfirst task will realize\nthat um\nbecause we use the destruct type from\nprotobuf in the flight ui you can\nactually see the conflict object like\nlike a nested json here super nice you\ncan\nmanually uh load the launch plane here\nand then change the learning rate click\nlaunch and it will launch the the\nexecution with the change learning rate\nso it's very nice that we can also\nvisually inspect this conflict object\nand change it here so that's really cool\nnow now comes the fun part for us\nbecause when we when we looked at\ndifferent frameworks for orchestrating\num machine learning workflows we\nimmediately like flight the most from\nfrom all options like like q4 pipelines\nor perfect and all the other ones\nbecause it can\nit interacts really well with with spark\num but also the guys from work set\nand also because it integrates the\ndistributed training with with pytorch\nand tensorflow drops so nicely so that\nwas like perfect we can run spark and\nthen distribute the training in one\nworkflow we love it and then we said\nokay but we want to execute it with a\nsingle command so that uh it's very\nsimple to use for data scientists\nand\nwhat we indeed in the end settled on is\nsaying okay if i run python workflow.pi\ni want to execute it locally\nif i run python work for that pi minus m\nhydrologically equals to flight i want\nthe flight launcher to build the docker\nimages i wanted to register the tasks\nand they wanted to execute i want the\nlauncher to execute them in the cloud\nand if i run python works for that pi\nhydra launcher equals to flight and then\nspecify three values for the learning\nrate that don't make sense here that's\ndummy of course but if i specify three\nvalues for learning rate and two values\nfor some other parameter i want a thing\nto do the auto product six\nconfigurations i wanted to build the\ndocker images register the task and the\nworkflow and then start six executions\nin the cloud that's kind of what i want\nto do right\num\nand i will now give you a brief overview\nof how that works under the hood because\ni'm really happy with how that turned\nout\num and maybe there's also something some\nof these ideas can be taken also for to\nslide kit and\nmaybe there could be a way there too to\nautomatically build images and register\neverything\nso um\nthe main let's look at this hydra main\ndecorator here and the workflow\ndecorator so we have a pipeline that is\ndecorated with workflow and it gets a\nconflict object the conflict object so\nthis here is a flight pipeline right and\nit is configured using this this hydra\nconflict object\nand then there is if name is equals to\nmain\nand that here will call\njust\na function that is decorated with hydra\nmain\nand it calls the pipeline\nso when the user\nexecutes python workflow.pi first we go\ninto this if statement here because this\nis main so we call main\nand then the hydra decorator kicks in it\ngoes to this directory parses this\nworkflow bundle gamma file containing\nthe configuration\nit gives us a config object with this\npart first config\nand then it calls the pipeline and\npasses this this keyword argument here\nthat's when you would do it locally\nand when we say minus m hydra launcher\nflight\nthe same thing happens we first go to\nthe main\nit invokes the deck curator it parses\nthe conflict object but then it never\nreaches this line here\nso we never go we never call the\npipeline locally but we will go to the\nflight launcher class okay so that's the\ndifference when we don't specify\nwhen we don't specify this launcher here\nwe would call the pipeline here with the\nconflict object if we do\nthe decorator is still invoked here and\nstill parses the config but now all of a\nsudden we are in the launcher in the\nslide launcher this is the difference\nbetween\nthis file here this command here where\nwe don't specify the launcher we\nactually parse the config object and\ngive it to the pipeline locally and if\nwe specify the launcher we still parse\nas far as the conflict object but then\nwe go to the launcher code okay\nand what the launcher code will do\ni hope that you can see it and make it a\nlittle bit bigger\nit's actually very simple it's uh\nthere's some module called image builder\nand it builds a docker image and the\ninformation for how this docker image\nshould be built is in the config object\nwhich which here is called soft.config\nthat's basically the past conflict\nobject so it contains the information\nwhere the docker file lies it contains\nthe information where the docker context\nshould be in the repository it contains\nthe\ninformation how the image should be\ntagged it will push the image\nit figure out the version of the\npipeline which depends on whether you\nare in a feature branch or main\nit will check for you if there are\nuncommitted files in your repository and\nwill it will independent will append a\ndirty\nsuffix to your workflow version so that\nyou know that this wasn't the clean git\ncommit\num it will track in your configuration\nif there are any extra images and if\nthere are extra images like spark image\nor some tensorflow image it will build\nthem for those we'll build these for you\ntoo\nand then it will create a flight remote\nobject for you\nthat\nuses the image tag that we\nbuilt here and all these extra images\nthat we built\nit will open a temporary port forward\ninto the cluster so you don't even need\nto expose flight into the internet which\nis something we don't do typically when\nthe project lasts for four weeks and we\nneed to start very quickly so for us we\nkind of use the hack to start a\ntemporary port for work into the\ncluster the conflict object\ncontains the information which module\ncontains our workflow and we will\nextract all tasks and workflows from\nthere\nand then using this remote we just\nregistered them and we created the\nlaunch plan that drag registers the\nself.config adds the conflict in the\nworkflow\nand then we start an execution or if\nthere were like these over these\nmultiple overrides were passed as a\ncommand argument we will we will start n\nexecutions using different conflict\nobjects\nthat's kind of how it works so\nif you run it locally it parses the\nconfig from the yaml file and gives it\nto the pipeline if you specify the\nlauncher it will still\nexecute this hydra declarator and pass\nthe conflict object\nbut then it will never actually call the\npipeline locally but it will go into\nthis launch method of the launcher\nthere it will use the conflict object to\nfigure out how to build the main image\nof the workflow it will figure out\nwhether there\nare other images that need to be built\nfor your workflow it will open a\ntemporary port forward into the cluster\nand register the tasks and workflow that\nare\nbasically specified in this module with\nthese images and then it will start an\nexecution\nand now our engineers typically this\nwhen they're in development they go\npython workflow.pi\nuh locally and then when they're happy\nto say okay this can go into the cluster\nthey just specify this here\nthat by default registers everything in\nthe development domain and when the when\nthey do get at work for that pi get\ncommit\nthen they can specify in parentheses\ntrain\nand then the icd pipeline will do the\nexact same thing it will build the\nimages it will give them to to hydra\nand then if you if you specify training\nyour commit messages it will actually\nkick it off\nand then it will train\num so that's the way that we\nreach our goal of being able to have a\nsingle command and\nthat\nis the difference from running it\nlocally and running it in the cloud\num that's how we solve that um we are\nusing it in the first projects now so in\na few weeks we will have feedback for\nwhether people like it but at least i'm\npretty hyped about it\nso i think time is almost up don't want\nto take more of your time but let me\nquickly summarize what i talked about\nso\nthe first part\nwhich is kind of decoupled from the\nsecond part i talked about our engineers\nstart when they when they work on a new\nproject how they reach a flight like a\ncloud infrastructure with flight where\nthey can do continuous delivery for\nmachine learning within a few minutes so\ngo from git commit it automatically runs\nthe execution for you and uh\nsends you a summary to your pr\num and for that we basically have a\nreference setup that is templated using\nginger templates and we have some custom\ntemplating mechanism that allows people\nto render that but you could do the same\nwith copy cutter\nthen we have a standardized way of\nparameter parameterizing site workflows\nwhere the the workflow is given one\nconflict object from hydra and every\ntask in the workflow has its own subtree\nin that conflict object that is given to\nthat task\nand the template or the sorry the\ncaching mechanism will figure out\nwhether the entire thing changed or only\nlike parts have changed\nand there's a flight launcher that\nwhen triggered will build the images for\nus we'll register the workflow using\nthese images and we'll start an\nexecution with a single command so you\nwrite python work for that pi it runs\nlocally you write python word for the pi\nhydra or hydra launcher equals to flight\nand it will automatically transfer it\ninto the cloud for you\nbut still work in progress for us we're\ncurrently exploring\na way that we can\nput hyper parameter\nhyper parameter optimization into that\nso what we would ideally we want is that\nthere's some meta workflow that\nhas one part that starts executions with\ndifferent config objects of the actual\nworkflow and we are currently exploring\nhow we can prune away runs that we\nalready know are not going to create so\nwe're exploring how we how we will\nincorporate that into the stack\nand i think the interesting question for\nme is whether\nmaybe not not the integration with hydra\nbecause that's nothing custom to us and\nof course absolutely optional but i\nthink this way of specifying in a\nconflict file what the docker what the\nimage tag should be what the context\nshould be and what the name and fight\nshould be and then with one command\nsaying run i think i'm interested in\nhaving a discussion of how this could be\ndone by flight because i think this is\ninteresting for many people\nand really lowers the entry barrier\nif it's just a single command and in the\nconflict file the information is stored\nof how the images should be built and\nbasically the idea would be to refactor\naway this build and\nthis build and push bash script that is\ncurrently in the template in the flight\ncode template and replace it by the\nconfiguration file where it says okay\nthis is these are the images that these\nare the names the tags and the context\ndocker context\nslide hit run i don't know\nand it just does it for you\nthat would be the idea\ndo you have any questions\nthis is outstanding work this is amazing\nfabio and actually you've\nreally distilled the problem nicely so\nextremely grateful\num\nbut i'll ask any anybody has any\nquestions\ncomment i have a couple but wait\nyou have a quick question fabio\nis is the launcher a hydra\nconcept or is it custom to this setup no\nit's a hydra concept so there are\nlaunchers for ray for\naws batch and\nthese are all like custom plugins that\npeople wrote for hydra\nthere's\nan abstract base class where you you\nimplement the launch method\nuh the launch method is given the config\nobject that hydra parses for you and\nthen you take it from there\nand we figured that makes sense we use\nhydra we want to use flight let's build\nthe launcher\nwhere we put all this logic of how the\nimages are are built into the launch\nmethod and then use the flight remote to\nsay this is what we want to register and\nexecute\ngotcha\nand then and then invoking\nthe workflows script with that dash m\nhydro launcher flag\ndoes that that'll also actually execute\nthe thing on the cluster\nright so it'll do like pretty much\neverything\nplus\nactually running it\nyes so the way let me um\nso the there is in flight there is one\nor in the then the launcher the way\nwhere we built it there's a\num\na launch plan conflict object that's\njust some data class and there is a run\nattribute that is set to true by def by\ndefault\nand then you could say\npython work for that pi minus m launcher\nequals to flight and then you could\noverwrite and set it to false if you\nwant then it wouldn't execute and the\nway that the cicd pipelines are are\nbuilt\num\nis that\nactually whether we say true or false\nthere it depends on the commit message\nso if the commit message doesn't con\nlike the trainings we do typically they\nrun for hours so we really don't want\nevery commit message to trigger that\nso if if in parentheses or bracket\nstrain is in your commit message it will\nit will set that value within hydra\noverwrite to true and if it doesn't it\nwill set it to false\nso by default when you run it locally it\nwill always execute\num but you can with the overwrite and\nhydra overwrite set that to false and\nthen see i see it will depend on what\nyour commitment commit messages\nokay cool thanks it's pretty cool\nyeah actually in just in today's\nuh\ntwo sessions\nthere was one with bazel\nthe other one was with hydra so we see\nthat you know everybody wants to go from\nlike zero to 60 and completely get it\nyou should\num and\nwe are we we\nso what fabio do you have any plans\nenough streaming\nuh hydra\nlauncher into hydra core and as well as\nthe big config into flight kit would you\nsay\ndefinitely we definitely would do that\nyeah so if people say hey that we would\nlike to use that too then we're\nabsolutely happy to to contribute that\num\nyes definitely yeah i'm not sure if\npeople would want to have the temporary\nproxy the port forward if that that's\nsomething that should be in the default\nversion\ni feel like it's probably something that\nwe use internally but it's still a hack\nthat kind of feels wrong\num\nrather convenient if you don't want to\nexpose your cluster into the internet\nbut but still um\nyeah yeah i'm not sure if if that part\nmaybe we would take that part out\nbut in general\ni think we're very happy to contribute\nthat to hydra and if i could\noh yeah that that will be amazing um i\nthink jeev has a question\nis it possible to launch with hydra wire\nnotebooks\num\nyou could right you could convert and be\nthere\nso the assumption for for hydra is that\nthere is one function in in the module\nthat you call that you decorate with\nhydra main\nand\ni don't think that\nlet's say let's say it's not the idea to\nuse it in a notebook um maybe you can\nget it to work probably you can get it\nto work but the idea is that you have a\nmodule and there's like a main function\nand you decorate it with hydra main\nthat's kind of the idea of it\nyeah\njeev to answer your question so i'm\nworking on updating fred kit config in a\nway and psychic remote\nso\nyou should\nyou you can now call it func in a\nfunctional way so that means we can say\n5k dot register or we can add a new\nfunction called like kit.deploy\nthat fills the container and pushes but\ni think that deploy because of the very\nvariety of like hydra\nbut bazel and then like everybody can\nextend it potentially i think that\nbelongs really outside of uh flight kit\nremote but as add-ons right you can have\nplug-ins you can do pip install xyz like\nxy uh jupiter deploy kind of thing i of\nlike with jupiter deploy and that should\ngive you and the way i was thinking\nabout this geo i don't know if you've\nthought about it\nis that we could use fast register\nbut use pickle to register files and put\nit in the container and essentially\nuse a pickle resolver to resolve from\nthat loaded file to run\nat runtime and it's possible then to go\nfrom jupiter notebooks to\nto an execution\ni still don't know if you should do it\nbut i am i'm not going to be a person\nwho's going to be like hey you should\npedantic about this like\nit can't be\nthere are different setups and folks\nhave different preferences\nbut\ndo you have any ideas\nyeah\nyeah so we we've been sorry can you hear\nme\nyes\nokay yeah so there's a couple of things\nwe were thinking about uh you know i\nlove this hydra idea um some of these\nsome of the concepts are really amazing\nand i think they fit within the way that\nfreedom works uh but then some of it is\nalso you know is it is a change from how\nwe're working right now\num i think that\ni think that being able to launch\nworkflows from you know either scripts\nor jupyter notebooks would be like a\nfirst step because that gives us a lot\nof power about like you know actually\nbeing able to correctly specify inputs\nand things like that\nuh without having to you know users\nwrite either yaml files or somehow write\ntheir inputs into like you know cli\nstrings and stuff like that so so that's\nlike the first step the second step i\nthink that would be really nice um\nalthough not super necessary\nwould be to you know have users kind of\nlike work entirely in in notebooks and\njust write their tasks and workflows if\nyou specify like a base image and then\nhave those uh tasks and notebooks run\nwithin the base image so that's\nsomething that you know it's in our\nroadmap but we're not really like you\nknow pushing a lot of resources behind\nthat but i think the first the first one\nat least you know just being able to go\nfrom you know say you're working on a\nworkflow project so like just doing it\nin it\ncreating a project like writing some\nchanges to the workflows and then just\npointing to that directory and doing\nlike a run or a build right where where\nlike uh you know some some business\nlogic can come in and basically like do\nexactly what the launch of stuff is\ndoing that fabio presented which would\nyou know build and push either you know\nit's either doing it locally via the\ndocker or it's doing it on the cloud via\ngoogle cloud build or you know literally\nanything that you want k native or\nanything\num and and yeah like once we have that\nthen users don't have to like worry\nabout all this stuff right they don't\nhave to go and do the build they don't\nhave to go into the register all they're\ndoing is editing code and then basically\nin the directory like running uh you\nknow running running executing the\nworkflow\nso we want to get there i think i think\nthat's one of the big pieces that's uh\nslowing us down a little bit but i think\nuh this gives us this this presentation\ngives us some really good ideas to go\nforward\nthank you yeah i think i would love to\npartner on this i think what we're\nworking on in the core\nbasically exposing and making it\nabsolutely easy\nto build this on top and i think a\nlittle just like hydra plug-in we should\nprobably add\ngoogle cloud build plug-in we will have\nto do that right because again\nit's what i've realized already is that\nsomebody's using bazel somebody's google\ncloud build somebody's reading docker\nsomebody's using\nbuild gate\nso there is just no way to standardize\non this stuff and i get it right you\nknow\ndevops is like a big paying area and\npeople are changing all the time\num but we and we we don't want to\ndictate anything over that but we want\nto make it absolutely easy um and i\nthink the bazel project by uh woven\nplanet is also open source now under\nflight or is that right which uh\nmiguel you guys have not we have not\nmarketed it but\nbut is it going to be open sourced did\nwe lose them maybe they had to drop off\num yeah but it's supposed to be open\nsource and i think uh\nstripe is also using it so uh we can\ntalk more about like how do i how do we\ncreate a nice interface across all of\nthese things\ni would love to do that for you any any\nsuggestions from you like love what\nyou've done so thank you i think\ni mean i understand that the slides the\nhydra stuff is kind of\nan optional with the plug-in but it's\njust like it will never be it's\nabsolutely clear it will never be part\nof it i think what would be that this\none i think would be helpful\nis whether the the configuration of how\nthe images should be built meaning where\nis the docker file what's the tag that's\nsupposed to be used and what's the\ncontext\nit would be nice if there was a default\nway that this\nshould be serialized in the flight\nconfig\nand then\ngiven that flight kit should have a way\nif a docker demon is available to build\nthem for you\nand\nif you are in an environment where there\nisn't a docker daemon available i can\nsay icd like that sometimes sometimes\nthe case right or if it runs on the\nkubernetes cluster you need to use kane\nand stuff like that at least then people\ncan say okay\nwe use this and this and the cst\nprovider that you probably will never\nintegrate with which is totally like\nwhat you shouldn't like what you should\ndo but then people can say okay this is\nhow the images should be built we'll\nbuild them for you and give them give\nyou the resulting tags basically right\num\nand that will then fit in\nwhat we do with fl with with hydra and\nwhat the people at really works do with\nbazel\num\nby building the images outside and then\npassing them uh to flight i guess but if\nif the flyout config\ncontained information on how the images\nshould be built\nthen\nit could do it when there is a docker\ndocker daemon it could do it for you and\nif there isn't then at least the\ninformation is somewhere and you can you\ncan parse it and convert that into some\ncicd pipeline yourself and then give\ngive flight get the resulting image tags\nthat would be a thing super helpful\nyeah the way\nthe way i've been thinking about it is\nessentially the api says give me a\nconfig object which includes the image\nthat was built in the in the core this\nis the core api\nuh and it just does everything for you\nfrom that point on and then we create\none more layer up which is essentially\nwhere you\nthis is the default one let's say the\ndocker deploy which means it builds from\ndocker and deploys um and i think that\ncan be done i i think so\ni don't know\nwants to provide a service for building\nregistering\nuh containers and workflows\nand\nteam we're waiting for your contribution\nman\nno i just kind of like talking about\nmaybe we talked about this like a long\ntime ago where you can totally build\nimages in the cluster itself so\num yeah i mean it's still like it still\nyou know sounds pretty premium to me but\nwe'll get there\nthis is this is a community thing this\nbut cool um\nthis is this is great work and uh fabio\nif you need help in upstreaming and if\nyou can upstream you would be extremely\ngrateful on the upstream side uh the\ngojek team is upstreaming dbd plug-in um\nthat should be coming soon\nand\none other team is upstreaming something\nmissing sorry i i will bring it\nbut yeah lots of stuff happening so\nthank you for everybody you know for\ncontributing and making such a wonderful\ncommunity\nbut yeah we are well over time we should\nstop\nall right guys\nthank you again for all the presenters\nsee you guys again in two weeks\nthank you"
    },
    {
        "title": "Woven Planet's Autonomous Vehicle - Data Processing & MLOps at Scale With Flyte",
        "transcript": "hey guys um i'm varsha i'm here\nengineering manager at woven planner and\ntogether with my teammate miguel uh who\nis a flight\ncommitter reviewer our in-house flight\nexpert we are going to cover uh some of\nthe use cases\nthat we have\nat woven planet and talk about um\nlet's say some optimizations and add-ons\nthat we did on top of flight to suit our\nuse cases\num so yeah\nwho are we um\nwe are uh from the level five org of\nwoman planet and woman planet is a\nsoftware first subsidiary of toyota we\nwere initially with lyft and about uh\neight months back we were acquired by\ntoyota and um\nnow here we are\nand uh what do we do there our org and\nour team all of us are focused on\nbuilding a full self-driving system um\nso everything that goes around autonomy\nstack perception labeling um we are\nresponsible for all of those\nand uh specifically uh miguel and i we\nare part of the infrastructure arc who\nuh main responsibility you know is to\nprovide with computer resources\nuh injection pipelines providing data\norchestration frameworks\nmachine learning infrastructure you know\nuh to name some of the few um you know\nsystems that we are responsible for\nand uh yeah um today we're gonna talk in\ndetail about um\nwhy we choose flight\nuh you know um\nwhat is it that flight offers that you\nknow other uh other frameworks out there\nthat couldn't\num so yeah before i dive deep into um a\nquick overview um just like how\nyou know\nevery other uh use cases like it should\nhave a very interactable ui\nwe should be able to run let's say a sub\ngraph of the entire dag\nother features like you know support\ncaching these are all the common\nrequests that even our customers had but\napart from it some of the very uh\ntailored use cases for autonomous\nvehicle and you know um in this side of\nthe field are um we have a lot of c plus\nplus four and a good chunk of our code\nare still in a mono repo\na huge mono repo so the solution that\nwhen we were evaluating in 2019 where\nflight air flow and pachyderm so around\nthat time we were looking who is the you\nknow the best suited to uh help us with\nmono repo integration and the second\npart is that a lot of our data are in\nthe order of petabytes and uh a good\nchunk of them are still unstructured\nwith structured data it's easy to use\nand handle but when it comes to the\nunstructured part um we wanted to have a\nvery lightweight framework which could\nuh give us you know let's say like an uh\na sidecar task or a python kind of a\ntask to process them easily\nand another one that's very unique to us\nis that we are a research-based or so a\nlot of our workflows are going to be\nfirst run on an experimentation scale so\nthe overall let's say the skeleton of\nthe workflow code might not change but\nuh the internal logic let's say how we\ntrain our model or what are the\nhpo configurations that we would like to\nchange a lot of those would you know uh\nchange rapidly so having a framework\nwhich would provide us a good skeletal\nscore code support but at the same time\na quick iteration was something that we\nwere more focused on and adding on to\nthat is our uh you know local execution\nand debug support again because we were\nmore on we weren't bringing any revenue\nimmediately our cloud budget was very\nrestricted so we were trying to look for\num you know making sure to use only\ncloud resources when we know we have\nsomething in there so uh\nhaving a local execution and you know\nability to quick debug was something\nthat we were looking for so all of this\nin conjunction with uh the rest of the\nuh\nyou know other uh requests like you know\neasy api to interact um whenever\nsomething goes wrong how do we debug\nthem so we still had all of this but\nspecifically with autonomous use cases\nthese were something that we at level\nfive we're dealing with\nand adding more to that uh how we use\nflight here at level five is that uh we\nare an amazon shop so all of our systems\nare built on top of eks clusters and\nspecifically in our case uh\nall the workloads run only on spot\ninstances um and then you know we would\nswitch to on demand on unless and of\ncourse it's a really high priority\nworkflow that has to be run so our\nsupport with spot and you know uh when\nflight runs on spot instances that was\nuh super important for us and another\none is bezel because we were this huge c\nplus plus mono repo uh bezel was our\nbuild tool so the solution that we were\ngoing to choose uh has to be integrated\nwith bezel and um to end user to our\nactual\nlet's say ml engineers our data\nscientist to them when they uh interact\nwith the workflow\nit should be just another additional\nbezel command other than that you know\nthey shouldn't really uh worry much\nabout rest of the apis\nand because we deal with a lot of data\nanother solution that we have is data\nprocessing frameworks and we chose\napache spark was as one of our data\nprocessing frameworks so spark also had\nto work with the system that we choose\nso you know with all this in mind um we\nwent with flight and couldn't be happier\num and a few other things is that we use\nbuild kite so whenever a workflow\nyou know gets promoted to production we\nhave build kite pipelines which will\ntrigger these flight workflows and you\nknow go through the entire um\npipeline and you know upload our final\nmodels to artifact\nand yeah because we are very tight to\nkubernetes a good chunk of all of our\nflight deployment files are written in\nterraform and we have our internal\nsystem which would deploy a flight to us\nin uh almost all regions that we support\nour um you know our eks clusters in um\nokay so\nmaybe one more quick overview of some of\nthe things that i'm talking so in a very\nvery high level this is how you know\nsome of the um\nsome of the work that our uh engineers\ndo look like you know this they're gonna\nhave a lot of interaction with data then\nwe have our own um data set libraries\nthat we've written on top of flight we\nhave some of our machine learning\ntraining frameworks and orchestrator\nagain we use a combination of flight and\nother cloud providers there\nand the evaluation and deployment so in\na high level this is what uh you could\nsay the work we do\nand in here i'm gonna share some um\ninsights as to you know what we have\nhere uh at level five uh and this data\nis as of\nq4 2021 we have over 25 plus projects\nand up i guess close to in the last two\nyears we've run close to now 800k\nworkflows and\nmillions of task executions and you know\ndespite um python being the favorite one\nuh all of our engineers get excited\nabout spark tasks and uh um\nthe most anticipated tasks just like how\nkate and you were talking our engineers\nare looking forward to you know trying\nout map tasks and moving some of our uh\nspot to map tasks because i would say\njust like how soft sought after spark\ntask is also the most abused uh abuse\nsolution that we have in production so\nyeah in uh in a nutshell this is you\nknow where we are with flight\nand uh yeah i'm gonna dive into some of\nthe use cases and this is uh i really\ncouldn't fit everything in there but\nthis is a high level\num\na super high level birds view we have\nour inference pipeline running on flight\nour simulation on checkerboard pipeline\nis also on flight our model training\neverything around\nsupervised and unsupervised metrics so\neverything on that side now moving a bit\non the data side we have analytics\nnotification or etl pipelines um and the\nmost um i would say the complicated\npipeline that we have is our maps\nbuilding now building maps is not easy\nbut we have somehow managed to do that\non top of flight so that's i would say\nour sometimes our pipelines when we\nlaunch the entire map will takes about\n50 hours um and flight does support them\num\nand just for maps use case we have uh we\nwe don't use spot instances because it's\nhard to get an instance for 50 hours so\nwe've uh done some add-ons on top of\nflight just so that we could tweak in\nand add some\nnode selectors and ability to choose\non-demand uh instances for our map\nbuilds\num yeah i'm going to cover some\nexamples like how a data set curation\npipeline\nthat we have written on flight looks\nlike um our label store like i said\namazon shop so a lot of this is on s3 um\nand then you know\neverything from downloading a data to a\npod and then running our spark execution\num\nand then uh these code interact with a\nlot of other data sets and eventually we\nmove these data to a uh to our big query\nso\nthis looks super simple but you know\nwhen in production we run uh because\nagain we are a spot based company a lot\nof these uh run into majority of issues\nbut thankfully to flight we could debug\nand you know uh do quick iterations and\nsimilar to our curation we also have\ndata annotation where you know there's\nhuman annotators and then there's the\nactual data the raw unstructured data\nthat comes from the car and all of this\nhappens as soon as data from the car\ngets ingested by our data\ninfrastructures injection pipeline and\nthen um\ndata annotation is slightly complicated\nbecause now we need to make sure that\nafter we curate a new data we have to go\nthrough the cycle of deploying new\nmodels and then making sure that\nwhatever the human annotators looked at\nit also makes sense when we uh\nsemantically look at it and run our\nvalidation pipelines so i'm just\ncovering these two uh\nthese two sample pipelines but we have\nmuch more complicated version of um you\nknow some of our graphs when we look in\nflight ui looks super crazy i guess i\ndid have one in here this one here the\nthe first one the n8 nodes so the in\nintricacies between them the\ninterconnection that's uh\nthat's actually very hard on how the\nengineers have written them but\nthankfully you know it's it's easy for\nus to follow when something fails and\nwith retry we make sure that\nand a lot of these are also cached so\nwhen something doesn't go well we cache\nand then retry only that part of the\nnode\num so yeah uh probably i'll stop here\nand let miguel take over to uh share\nsome of our ml ops use cases on how\nwe've built our entire uh data up until\nthe deployment to car pipeline on top of\nflight and uh yeah um\nhey miguel do you want to continue\nyep\nuh thanks so\num i'll basically cover over a couple of\nour uh ml use cases and then go into\nsome of the\nmore um things we kind of built around\nflight uh to help us uh with our use\ncases and\nuh for our deployments\nso uh one tool that we actually bought\nuh that we actually built on top of\nflight was a model introspection\num so this tool is basically kind of\ngiving insight into the model itself so\nlike weights and biases can give you\num for those who don't know it's kind of\nlike a metric\naggregation um helps you track various\nmetrics from for model training\num so it'll help you with like basically\nhey is my loss curve going down\nprecision recall averages and stuff like\nthat give you a rough idea\nbut when you want to kind of go a little\nbit deeper uh it doesn't quite fill the\nbox\nso we built out this tool uh\nbasically called model introspection\nthat lets you kind of actually go into\ndetail over specific samples um so maybe\nlike you know trajectory of the car or\nspeed error acceleration maybe a\nspecific image type so like\nyou know left turns is a specific one\nthat it just affects the loss curve and\nso forth\nso basically uh on a validation step uh\nusers would define what are these\nsamples kind of look like and\nasynchronously this would trigger a\nflight workflow that basically\nkind of fetches all these\nvalidation samples distributes it and\nuploads it to weight devices as well as\nthere's another concept if you want to\ndo a commit job which the command is\nbasically kind of doing\nsome sort of statistical aggregation\nthat a user defines\nso that's all done that was a tool that\nwe just kind of built\naround flight leveraging container tasks\nuh dynamic tasks\nand even map tasks\nnext slide please\nso we also then kind of built out uh\nthis thing we call internally kit\num it is basically a\ncombination of a bunch of different\ntasks\nso\neach um basically you know you train\nyour model um and over time you want to\ntrack uh metrics you want to see\nregression so for example you know some\ncommit made it in and then all of a\nsudden uh you have a runtime regression\nso you can go back and historically look\nat what's going on and these basically\nmodels get trained um\nyou know nightly or on whatever schedule\nyou want to do so that constitutes a lot\nof just you know just\ndata preparation training\num validation so some are like you know\nevaluating on simulation depending on\nthe model you may also want to involve\nuh hardware in the loop so actually test\non kind of actual hardware\num\nand then you know send slack messages\nreport statuses uh so forth so basically\nwe built out a bunch of kind of blocks\nor they're all flight tasks and then\nusers can basically combine them\ntogether to define what is the ideal\nworkflow for their specific model so in\nthis case we actually have a specific um\nthis is for ml planner\nbasically a planning based system\nand here it kind of goes through the\nwhole process you launch an experiment\nit builds images it launches the\ntraining monitors\nmonitors the validation\nthen you know sends a slack message\nchecks for metric regressions if you\nwant it all will actually also go to uh\nthe validation workflow so basically\nalso launching simulations\num as well as running hardware in the\nloop tests which are basically just\ntesting the actual\nyou know artifact on hardware that\nsimulates the the actual car\nand so over time you can kind of like\nbasically track\nthe progress uh of the entire model\nitself and all that uh monitoring\norchestration is kind of handled by\nflight\nnext slide\nso\nthose are just some of the use cases on\ntop of that\n[Music]\nlike russia mentioned we are largely\na monorepo we have basically we support\nmonorail and multirepo but uh majority\nof our ml engineers and autonomy\nengineers work on the\nmonoreboot so\nwe actually then basically um to support\nmodern repo we use bazel bazel as a\nbuild system\nand originally\nwe basically recommended users to hey\nyou know if you want to define your your\nworkflow build it outside of the\nmonorepo but that led into kind of\nbasically a giant problem between trying\nto share artifacts across cross repos\nyou make a change in one repo then you\nhave to update the version and another\nrepo then launch it from there\nthe overall iteration cycle was pretty\nslow\nso we ended up building um\na basically bazel macro that kind of\ntakes care of the entire registration\nprocess for you\nso this is a image of the registration\nprocess that i took from the flight\ndocumentation um\nand just a as a reminder\nand so instead of users kind of you know\nuh check in the monorepo\nmaking a change\ndeploying some sort of artifact whether\nthat be a binary\nsimple source binary or something of the\ndebian package\nand then going to their workflow repo\nwhere they update the\nversion\nthen go through the registration step\nwe basically built out a macro that will\nhandle everything for you from building\nout the\nbinary or the artifact\nto building out the darker image to\nregistering with flight to then\nuploading the\nimage to\nartifactory we use our container\nregistry\nand finally even executing the uh\nthe flight workflow itself so most of it\nis is in dev so for basically for the\ndev workflow um russia if you want to go\nto the next\nslide so on iteration basically uh you\nwould just\ndefine your bazel macro this is kind of\nwhat it looks like just abs workflow and\nyou specify\na couple parameters\nname\nthe workflow uh python file\nuh whether you decide whether you um you\nknow need a spark or not\nuh workflow name packages uh the project\nit belongs to and then just the python\nbinary target that it goes to\nand um you know locally basically users\nwould just kind of iterate\nuh\nbuild out run the python binary itself\num with like your local development then\nonce they're ready to run at scale in\nthe dev environment this would\nautomatically just register in the dev\nand then launch the workflow\nand finally at the end when it gets\ncommitted to master\nthere's a separate pipeline to basically\npromote that dev to prod\nso we're not pushing\nevery uh work every commit in this mono\nrepo to\nproduction\nnext slide please\nso on top of that we also um there are\nsome caveats with our deployment for\nexample we don't use um\nproject and domain uh for namespaces we\nactually only have two single namespaces\nacross all the different projects based\noff the domain so a dev namespace and a\nprod namespace and all the projects kind\nof share\num that single namespace uh this helps\nfor operational\nyou know we overhead um\nbut\nthere's some tools that you just can't\nrely on like resource quota recorder\nworks based off namespace\nso we also built out some kind of custom\nweb hook\nwe largely run into these issues mostly\non gpu capacity there's obviously a\nlimited\navailability from aws\nand this is constant often cause a\ncouple issues where\none user will launch this very large gpu\njob and other user will just kind of\nwait and hang and then especially if\nthere's they're using spark\nyou basically\nuh one job does not get enough executors\nto fully complete the other one doesn't\neither so that's where like kind of\nstuff like gang scheduling um is\nbeneficial and so forth but we ended up\nbuilding out a gpu based quota so\num basically users uh get you know 50\nconcurrent gpus at a time\num or whatever their quota is specified\nand that we built also basically only\nfor flight workflows flight and spark\nworkflows\nwe also found a couple use cases where\nfor example like the\ndifferent cuda versions\non the actual aws cc2 machines\nso for example some machines were some\nuh users had\nwere able to run on like cuda eleven\nothers had to run on coup ten and\num having this kind of\nheterogeneous um\nno pool we wanted to basically say all\nright only this specific workflow and\nproject\nshould run on these subset of nodes\num otherwise they just kind of don't\nwork and basically\nwe built out these as part of the web\nhook we also built out custom rules that\nbasically just use a different node\nselector and node affinities to land\nspecific workflows and projects on\nspecific nodes so i saw in the chat\nthere's kind of talk about the pod\ntemplates so\nthat would also kind of allow us to\ndeprecate that uh the case webhook\nand then uh we also\nbasically have a scheduled workflow that\nhelps us with kind of reliability\nmetrics so we basically take kind of the\nflight database all that content offload\nit to our analytics pipeline and\nbuild out a bunch of dashboards so we\ncan actually see\num\nwhich projects have the most success\nrate which workflows actually tend to\nhave um more success or failures uh time\nduration so we can even uh be proactive\nwith our users and kind of say hey you\nknow 30 of your workflows are failing is\nsomething something off um or\nand sometimes we have actually found\nthat with one of the spark tasks they\nwere like\nintroducing a\na shuffle step um that was basically\nkind of complicating things with\nespecially with spot instances so that\nwould cause a\njust basically a lot of rework to be\ndone so we went in and fixed that\nas well as\nfinally uh wicker wicker's like an ml\ndataset storage and serving so basically\nyou can define a schema\num\nand\nit basically it'll essentially wicker\nwill basically just efficiently uh load\ndata in it's uh based off uh uses apache\nparquet or apache arrow on the back end\nor as a data storage format\nfor serving but then for actually\nwriting\nto the database it basically\nparallelizes a lot of that work with uh\nflight map tasks and spark\nso those are kind of the tools that we\ni guess added on to flight and we're\nbuilt on top of flight\nand i think that's uh about\nit\ni don't know if there's any questions or\nanything at all that we can kind of help\nanswer\nthat's\nthat's an awesome\npresentation thank you miguel and\nworship\nfolks any questions\ni had uh one thing uh miguel inverse i\ndon't know if you guys have seen intra\ntasks checkpointing we actually did not\nwrite about it a lot but\nthat's merged uh\nin january and and it's available so\nyour smart machines\nprobably you can\nbetter utilize them now because you can\nactually\nand and it's not only tied to model\ntraining you can do arbitrary\nuh\narbitrary checkpoints\nbut you of course your code has to be\nready to retrieve from the checkpoint or\nupdate or restart from the checkpoint\nrather that the retrieval part is\nhandled by flight but the restart your\ncode has to be on\nlet's see is that similar i guess how\ncash basically kind of skips the\nbecause we basically been using kind of\ncaching as the\ninter checkpoint for these large\nworkflows\nyeah for inter right but within let's\nsay you're running a training step or a\ndistributed training step and it's\npretty large and you want to now just\nrecover from a model checkpoint you can\nuh what it does is you you can decide\nwhat you want to checkpoint let's say in\nthe model training case it's classic\nbecause most of the training frameworks\nsupport checkpointing but let's say you\nare running an iterative loop on a\nmillion files\nyou can checkpoint every ten thousand\nfiles or thousand files or whatever and\nit\nand next time when you start you can\nskip to the last known checkpoint and\nthen continue processing\nand so it's very uh and it will handle\nthat it understands part and all that\nright\nit just gives you the right set of\nuh paths\nokay awesome\nyeah\ni'm also very interested in wicker\nbecause um\nhe has been working on the structured\ndata set\nuh which has a very interesting property\nuh\nflight schema was already\nparquet with arrow but there were some\nlimitations and actually\nworking with brian dobroy from your team\nhe had some requests for certain things\nwithin\nwithin schema and so i actually ended up\nwriting structured data set which is\ngoing to be the default way of doing\nschemas\num\nfrom 1.0\nand so that has some really cool\ncapabilities like streaming data and so\non so you should probably look at it\nand i think wicker also should be open\nsource pretty soon i guess they are in\nthe final step of approval or something\nso as soon as they do that we'll share\nthat with you\noh that's fantastic amazing\num\nany questions\nif not\nlet's hand over to fabio maybe\nall right good love all yours\nthank you versa thank you miguel that\nwas amazing"
    },
    {
        "title": "Flyte Community Update 007 - Feb 22 2022",
        "transcript": "it\nhello uh good evening good day good\nmorning wherever you guys are today is\nfebruary 22nd\nuh we have a\ngreat set of presentations today so stay\ntuned but i will quickly run through a\ncouple important announcements\nokay\nso as usual we'll do some community\nhighlights uh\none of the the important announcement is\nwe are in collaboration with\nml ops community we are running this\nthing called as engineering labs\nuh\nwe already have a\nlarge number of people registered we\nwould encourage a lot of\nanybody else to register i think today\nis the deadline\nfor that but once you register uh the\nidea here is to write some general\npurpose ml\num\napp using flight and\nunion ai is sponsoring the host of\nflight environment for most people here\nso yeah just definitely join in uh you\nknow learn have fun and also win prices\nmore details for this are on flight.org\num\nif you are going to take if you are\ngoing to participate in this definitely\njoin flight slack or develop slack\nand\nuh the goal is the the hackathon will\nrun for a month and you can participate\nin teams or individuals most people are\nworking in teams\nlots of other fun stuff happening\nwithin the flight ecosystem\nso latch\njust released an sdk for bio\num bioinformatics or bioengineering\nstyle workflows\num\nand\nlike we we got featured in a few places\nso thank you for the community you know\nhelping us evangelize the product\nwe want to continue doing this as we\nhave said that this year we want to make\nit really really grow faster\nuh\nthis office house has been getting more\nand more popular so but most people\ndon't know when office hours of how to\nreach uh there\nthe officers uh the best way to find\ninvite for the officer is to go to\nfight.our community\ntab and in the community tab you will\nsee\nfor\nthis event which is the oss sync\nyou can basically community sync you can\nadd a calendar invite from there or for\nthe office hours the office hours again\nas a reminder is on every wednesday\n30 minutes in the morning and 30 minutes\nin the evening pacific time\nyou can choose the one you\nwant to attend and you know the idea is\nwe don't do any kind of presentations or\nanything there it's more of a q a\ndesign discussions use case discussions\nuh proposals etc\nlike one or two of the uh\nat least some folks from union as well\nas some other teams are present over\nthere to answer any questions so please\njoin\nokay uh\nfollowing up to this i think uh next uh\ni think it's two weeks from now so march\n8th\nis uh we'll be diving deep into latch\nbio and how they use flight um\nessentially they've built up amazing\nplatform on top of site that allows uh\nbioinformatics workflows to be created\nfrom the browser and run from the\nbrowser so\nit's it's really\ncool stuff that\nthe latch team is doing and and they'll\nbe coming and sharing uh all of their\nlearnings uh they're\ndemoing their examples and so on\nwe'll also uh do a\ntype of a lightning talk katrina from\nunion will be presenting life of a\nworkflow uh the goal is to actually get\ncontributors as well as users to\nunderstand how does a workflow progress\nfrom writing the code to actual uh\nexecution and beyond\nagain calendar and right and i think\nsome folks asked\nare these things recorded yes they are\nrecorded there's a youtube channel\nplease subscribe um\nall the details for this are on\nflight.org community tab\nand and if you miss something then we do\nalso do a flight monthly newsletter\num sandra has been doing a fantastic job\nwith the newsletter please subscribe to\nthe newsletter\nall right a quick uh look into what's\ncoming up uh roadmap we are working on a\nbunch of things these are just some call\nouts um i think we've talked about them\nin the past but uh just as a reminder\na huge ask one of the most wanted\nfeatures you know map task overhaul um\nthis is uh to support retrieves within\nmap tasks caching the currency control\nsecrets etc all of that is slated for\nuh this month and dan has been driving a\nlot of work on this so thank you\naws batch task is uh it was always\nsupported for map like the array tasks\nbut now you can also do aws batch tasks\nfor regular tasks so you can still use\nkubernetes but you know offload things\nwhatever you want onto aws patch\num and and then\nwhat has been happening recently is\npeople have been modifying like plugins\nto\nsupport new default part features that\nyou want to add to a single container\nwe\nafter working with a bunch of folks we\nwe've uh\nmoving to a concept of pod templates uh\nkubernetes has spot templates so you\nbasically create a part template with\nall your defaults and it's like a web\nhook\nthat\nautomatically adds all of these defaults\ninto your launched container so flight\nwill handle uh merging them together and\nlaunching them\num on from the flight kit side a lot of\nfun stuff happening one uh most\nimportant one that we've been focusing\non expected remote uh ga\num it may it may slip a little bit into\nmarch\nbut\nout of that one of the interesting\nthings that's happening is config\noverhaul\nlike\nwe've been going through and as you've\nseen we've deleted some old code and\nwe're also completely overhauling the\nconfig\nthis might be a small breaking change we\nare trying to minimize any kind of\nbreaks as we always do but\num we actually did a poll internal poll\nwith the folks and we realized that\npeople are okay this doesn't break your\nruntime anything in runtime this only\nbreaks registration if at all\nuh if you're using certain you know\nunsupported config values but most of\nthe values most people should not see\nany kind of breaks\nand the goal with this is that you\nshould be able to register directly from\nflight kit remote\nand i think fabio will show more fun\nstuff around this\nbut i think overall both these\nwhat fabio's team has been doing and\nwe've been doing what kind of conversion\nand i think it would be a great\nexperience for most most of the\ncommunity\num\nalso uh offloaded uh data type caching\nthis is for like large data sets which\nuh so flight currently only\ncalculates the hash values for caching\nusing the references uh so let's say if\nit's a large data set like a tensor or a\nbig pocket file or\nwe only calculate based on the\non the reference of the file\nit can be very expensive to calculate a\nhash for a large data set so now this is\ncompletely in the user's\nhand so users can\nin the cases they want to let's say\nlet's say they generate a large data set\nthey can create a dynamic hash for it\nand then that will be used for automatic\ncache invalidation or cache creation\ndownstream so this is coming soon um\nanother feature which actually i've been\nwe've been working with a couple teams\non with is flight decks\nuh flight decks is a concept of\nlike to allow a visualization\nof various components within a task at\nthe end of a task education so here's an\nexample we integrate with\ngreat expectations you cannot see the\ngreat expectations uh\ntest results\ndirectly in the browser so with this you\nshould be able to see all that in the\nbrowser along with that\nit's extendable enough that you can\nactually show let's say if you want to\nshow the top 10 rows of\ndata of a pandas data frame or you want\nto show\nyou want to create your own plotly graph\nall of that is covered within flight\ntext and flight text is a is completely\nstatic rendered based or shareable\nwith every anybody within the community\nand so on so it's going to be cool the\nway we are progressing is initially\nthere will be a\ni don't want to call it a poc but an\nalpha release within flight kit alone\nwith our back end support because that\nallows us to complete you know hydrate\non it and then uh and pull back and\nsupport by probably the end of march\nuh\nand then of course lots of console\nimprovements i wanted to call out\ndynamic graph support we some people\njust in the community asked about this\nyes it's coming we're waiting on the\nflight\nnot like sorry react\ngraph component that we use the library\nthey are waiting on a release and so we\nare we are kind of behind them in the\nrelease pipeline uh but we might just\ntake their beta and\nrelease it out\nall right\ni will stop talking and we'll hand over\nto worship uh and miguel from\nfrom the woven planet team to talk about\nhow they use flight for\nuh\ntheir autonomous vehicle workflows\nand then\n[Music]\nfabio has some fun stuff about how to\nuse hydra core and\nand\nto actually just launch\nan end-to-end you know workflow for the\nusers\nso\ni'll stop there"
    },
    {
        "title": "Flyte's First Virtual Hackathon with MLOps - Engineering Labs #3",
        "transcript": "in this first engineering labs event of\n2022 we've put together a challenge to\nhelp you hone your skills in the\nmaturing field of machine learning\noperations or mlaps\nif you're a software engineer data\nscientist or machine learning\npractitioner who wants to experience\nwhat it's like to work with the\nproduction grade machine learning\nplatform\njoin us for a four week virtual\nhackathon to compete\nhave fun\nand of course\nwin some\nprizes so the challenge is this\nform a team and build an end-to-end\nmachine learning application using\nflight\nit can be anything from time series\nforecasting to computer vision to model\ninterpretability and explainability\nor literally any machine learning\nproblem you can think of\nyou can train a model from scratch or\nfine-tune a pre-trained model for your\nparticular use case\non top of figuring out what model you\nwant to use\nyou'll also need to build a ui to\ninteract with it in some way\nat the end of four weeks\nthe fly team will judge each\nparticipating team on creativity\nexecution and human friendliness\nall participants get flight swag but the\ntop three teams will earn cash prizes\nand get a chance to present their\nprojects to the amalops community and\nflight communities\nto learn more about the event head over\nto flight.org hackathon\nand we'll also be dropping the\nregistration link soon in the mlaps\ncommunity slack\nso be sure to join their engineering\nlabs channel to be the first one to sign\nup\nalright that's it for now stay tuned for\nmore updates and i look forward to\nseeing you there\nbye"
    },
    {
        "title": "Flyte's Structured Datasets - Feb 8 Flyte OSS Sync Up",
        "transcript": "cool so this is the talk on the new\nstructure data set type uh this is work\ndone by\nkevin and myself over the past several\nmonths probably took a lot longer than i\nshould have but here we are uh it's\ncalled structured data set it is the\nnext evolution of\nthe flight schema\ntype which\nencapsulates all tabular data\nso\ni wanted to go over why we wanted to do\na new type how to use it\nhow it's defined and then for those\ninterested in\ncustomizing or extending it how it works\ninternally\nso first off flight schema is the old\nand we're going to continue to support\nit which is the the initial data set uh\ndata frame encapsulating type that we\nintroduced uh there's a couple main\nissues with it\nin our opinion the the column types were\nwere primitives only so you couldn't\nsupport you couldn't specify things like\nlists and dictionaries and structures\num and even the primitives themselves\nwere redefined internally so it didn't\nwork well with other uh\nit was kind of redundant and didn't work\nwell with other types\nyou had to translate between them\num\nand\ni personally found extensibility kind of\nhard reading through the\nlike the spark code for instance um so\nhopefully this new new type will\nsimplify that\nno this isn't big enough but uh just\nstarting off with some examples the\nbasic usage remains the same so\nin general if you don't care to\ncustomize behavior if you just want to\ndeal with data frames and i'm using\npandos as an example\nyou can use\nit's\nanalogous to any other\npando's data frame type so dult\nspark or your own\nyou can take in and return just just\nshows returning but\ndata frames\njust by themselves and this will trigger\nthe appropriate transformer\nbasically if you want to customize the\nbehavior so in this case we are\nspecifying where that data frame is\ngoing to be stored\nthen you wrap it in this new structured\ndata set python object this is the user\nfacing python object this is not the um\nthis is not the idl object so there's\nadditional logic in the python in the\npython class and if you notice the top\nright we've changed the way you do\nyou specify columns and formats\npreviously we were doing the class get\nitem on the\non the type in itself which isn't really\nmy pie compliant um\ndisney one will parse through and pick\nup\nuh columns and formats format is just a\nstring so it detects more than one\nstring always an error\noh also the uh the way you return it is\nis different so current now you just you\ntake your data frame and you plug it\ninto a structured dataset object uh\nthere's no more\nof the open and right syntax\num\nagain showing the\nuh usage of\nannotated to specify the columns\nand here you have some\nuh\nmore complex\nmore complex types um\nif you're on\nif you're on blade i believe three seven\nwe have this code everywhere but\njust make sure to import it from\nuh uh typing extensions instead of just\ntyping if you're on older versions of\npython i think it's just three seven\nand this is not this is\nuh being read in but it's there's\nnothing that will show it quite yet but\nif you can also annotate your data frame\nor surface data set type with a\naero schema currently we just support\narrow and the idea is we serialize the\nschema\nand other tools down the road will be\nable to make use of that information\nbecause it's stored in the type as well\nthis is\na an example just to show kind of what\nhappens\num\nso\nthis here will trigger the download\nand you specify the type much in the\nsame way as you do flight schema today\nso\nwhen you take in a data set if you want\nto actually read it you have to tell\nbasically this tells the transformer\nwhat python data frame type you want to\ninterpret it as\nand from there it will select the right\ndecoder and then load it\nand notably this\nwill not re-upload because so this will\nbe interpreted as the same data frame so\nit won't\nrewrite the data in flight\nwe also support\ncolumn subsetting\nso if you have a task that say returns\num three that returns three columns and\nthe next task only takes in one column\nuh the the df in this second case will\nbe automatically slimmed down to only\ninclude that column\nso currently it's um\nnot\nnot very efficient currently it's just\ndownloading the entire data set\nand deleting uh the superfluous columns\nuh we're going to work on\nbetter transformers that will if the\nback end supports it like s3 maybe\njust query the relevant columns\nso that will be forthcoming work\ntype compile compatibility so\nthis\nformat was added to the type which we'll\ngo over in a second but uh the formats\nhave to be identical between the two\ntypes\nand\nthis bit here was\nadded\nbecause\nwe\nwe feel like this is going to be a\ncommon pattern like\ntask one here returns the data frame\nwithout specifying the types maybe\nthere's a lot of columns and the user\ndoesn't want to specify 30 columns\num but subsequent tasks maybe only need\nlike one or two of them it makes sense\nfor them to be compatible so\nif either one is empty then it's\nconsidered compatible\num\nand if you are looking for something\nthat is not\num if you doesn't have anything\nspecified and v does have columns\nspecified but they're not in the data\nframe then that results in a runtime\nerror\nand\nuh currently the\nwell we're going to patch propeller\nshortly to make that the case currently\nit's a little bit more restrictive\num\ni want to bring up the extension so i'm\nhoping this\nis more\njust at a glance\neasier to contribute to so this is the\nformer\nspark data frame code\nthe transformer code so you needed a\nreader you needed a writer and you\nneeded the\ndata frame transformer\nand there's some interaction between the\nschema engine and the type engine\nand in the new world\nyou write\nan encoder which an encoder which\nhandles\nuh taking\na structured data set\nthe python object and transforming it\ninto a literal and you write a decoder\nthat decodes the literal back to your\ndata frame\num\nand\nthat is all you need to do\nso\nhopefully that's a little bit cleaner\nand a little bit easier to understand\nthis is the new idl type\nso we've basically\nreplaced\nthe column type with just a nested\nliteral type\nthis is if you specify arrow this is\nwhere it will end up\nand format has been added\nin the\nso this is the literal this is the\npython class that users\nwill be interacting with if they want to\ncustomize the behavior\nbasically it's just a data frame uri and\nmetadata\nmetadata currently just racks the type\nso this is one of the and this is the\ncase in blank schema as well this is one\nof the cases where the literal\nincludes the type itself and the reason\nfor that is\nbecause\nin the case where\nyou\nyou can return\n[Music]\nthe task and return more specific\ninformation than what the type hint\nthat you can have no columns specified\nand type in and return\nadditional information so\nand\nuh so just briefly go over to how it\nworks on the back end\neverything is encapsulated in two\ninterfaces encoders which take python\nobjects and turn them into\nflight literals and then decoders which\ndo the opposite\nthey are registered only with the\nstructured dataset transformer engine\nthat in turn will internally take care\nof registering with the type engine\nand users basically have two options so\nyou can just use the data frame of their\nchoice or they can use the new\nstructured dataset\ntype class\nthe reason that you would use that is\nbecause\nwell let's get that to a second so when\nthe type engine when the transformer\nengine\nneeds to do one of these two\ntransactions it will pick the encoder\nand decoder based on\num\nbased on three things the python type\nthe format\nand the storage protocol so the storage\nprotocol is something like s3 or gcs or\nbigquery\nformat it's just um it's usually part k\nand python data frame is like the pandas\nclass or spark class or whatever\nthere is a fallback to the\nempty string format handler if one\nexists\nand the requested format does not exist\nand basically if you ever want to\ncustomize the behavior like lazy loading\nor whatever\nof structured data set\nor if you know that like if you were to\nhave a task that down took in and\nreturned\nfor some reason the same pandas data\nframe\nif you just use the panda's data frame\nas a type hint it would get\nre-uploaded if you use a structured data\nset it would not\nso\nevery bundled data frame type has\ndefault behavior that default behavior\nis determined because it has the default\nuh protocol and formats\nand from there the encoders and decoders\nare selected\nand yeah\num\ni think\neverything here is\ncovered already\nuh yeah register handler the encoders\nand decoders are called handlers in the\ncode so if you see that that's where\nwhere it is\nfine\nand we are supporting so with this\nchange um if you have a task that\nproduces a pandas data frame\npreviously\nit would get serialized into a flight\nschema literal\nnow with this new code it will get\nserialized into a\nstructured data set literal which means\nthere will be some compatibility issues\nwe we resolved that by basically adding\ncode to handle each other in the other\ntransformer\nthe spark plug-in has been updated to\nuse the new type\nand\nall the other plugins that\nrely on flight schema we're gonna update\nin\nin the coming months\nuh so there's some back-end changes that\nwill go in in line with deflected\nchanges\num so because of the requirements uh\naround\nthat requirements because of the\ncompatibility issues that may arise and\nbecause of the new type that the back\nend needs to be aware of\notherwise things won't compile properly\nwe are featuring this\num so if you want to use it you have to\nspecify the flight sdk use search data\nset equals true flag which will turn\neverything on\nif you have it off which is the default\ncurrently like your pandas data frames\nwill remain like schemas if you do\nchoose to turn it on make sure your\nflight backend is on 19.2 which is this\nrelease\nand these are the um\nthese are the\npropeller and admin versions that you'll\nneed it is unfortunately not yet\nsupported in flight console so if you\nwere to look at a\nuh\nan example you would still see this\nmessage\nbut that will be changing soon hopefully\nif you upgrade flight kit make sure that\nyou also upgrade all your plug-ins i\nthink that's pretty common\nand um\nyeah so we're this will be on by default\nat 1.0 which we're saving for the end of\nmarch early april\nuh design dock better decoders external\nyeah okay so\nin the future we're gonna\ntry to introduce better encoders and\ndecoders that take advantage of like\nuh just more efficient uh downloading\nand uploading those columns of setting\nbing\num\nand perhaps more data awareness just\num you can envision a world where like\nuh if it's not a huge computational\noverhead we can extract out column\ninformation when you on return or\nsomething\nyeah yeah anyways\nanything to ask you then\nyeah like i think one of the\nthings over here is uh streaming data\ntransforms for let's say model training\nor any of those sort of things which\nthere are some libraries now working on\nthat and i think the arrow community is\nmoving in that direction that's one of\nthe reasons why we're kind of keeping in\nstep with the aero community so that\nmost of the users don't really have to\nthink about efficiency while they are\nperforming\ntheir tasks the framework should support\nthat and that's the goal with flight\nand and we'll continue improving this\nuh also it's as in the\nvein of extended extensibility is\ncompletely acceptable so if you do come\nup with ideas if you do have\nrequirements please extend it and you\nknow where to reach out\nif you think that there are improvements\nto be made to the system\nthank you\nany other questions\noh yeah so if if somebody recompiles uh\nthey've got an old flight schema and\nthey go to the new persistence format\nthat will trigger cash flush on data\ncatalog yes or no\nuh if you change the type of the task\nyes it will absolutely trigger a new\ncache flash\nno but for our pandas data frame it will\nautomatically be changed right but if\nyou just\nif you just turn it on and you don't\nexplicitly change it will it still\nyes okay see okay so you should you\nshould be prepared when you turn the\nsucker on that\nit won't\nchange\nokay thank you\nagain i guess to clarify because he is\nlooking a little puzzled not if you're\nusing flight schema if you are using\npandas data frame it will turn on\nif you're using flight schema that is\nthat remains as a separate supported\ntype\nfor some more time we are not going to\nphase it out immediately it will be\nphased out slowly uh maybe over the next\nfew months\nquarters\neven at 1.0 we are not going to phase\nout flight schema support it's just that\nstructured data set will become the\ndefault handler for all pandas data\nframe moded data frame\nspark data frame all kinds of data frame\ntypes that we support um\nbut so that's the only thing we wanted\nto bring up and this was one of the\nthings that\nwe wanted to get in before 1.0\nyes\noh yeah um just a quick question about\nthe encoder decoder pattern is that\nsomething that we think is specific to\nstructured data sets or\num\nor are we thinking maybe that that's\nlike a better way of extending types in\ngeneral or no\num\nwe certainly approached it from the\nperspective of only force uh structured\ndata so data frame types yes\num\ntransformers the name itself probably\nshould have been encoders and because i\nthink it was just uh\nbad naming maybe martin and other folks\nwho have extended will agree it's just\nthat you know\nsometimes you have to live with the name\nchoices you make\nuh oh and i forgot to mention the it\ndoesn't have to be symmetric so you can\nhave\num\none encoder and then like you can only\nchoose to extend one side of it like\num when we when we do the column\nsubsetting thing like\num\nwe will only download\nthe\num like\nif you wanted to keep both you might\nintroduce a smarter decoder but there's\nno smarter encoder to to write\nyeah so the\nneeds in that sense yes you could\nactually\nlet's do it to explain this more clearly\nwhat that why what asymmetric hearing\nmeans is that a task may produce a\npandas data frame\nbut may consume it as a spark data which\nis already supported with like schema\nbecause the underlying representation\nbut here uh\nthis is what e means by asymmetric\nthat is actually true for all flight\ntypes to be honest so you probably are\nright um\nbut it becomes extremely complicated for\nusers to think about it that way and so\nthat's why we've made it like one\nencoder decoder us together but\nhopefully once people get used to\nunderstanding that translations\nimply more flexibility\nwe could bring it up\nand one more thing we\nwere partly inspired to do this because\nof the request to support bigquery as a\nback end\nfor pandas at least that has been that\nis bundled stock was just so we can\nwrite from a pandas data frame to a\nbigquery table\ncool"
    },
    {
        "title": "Large Scale Processing with Flyte: Blackshark.ai's Semantic 3D Planet - Feb 8 Flyte OSS Sync Up",
        "transcript": "i'm going to tell you guys a bit about\nwhat we do at black shark with with\nflight\nand how we're using it to create this\nmassive digital twin of the entire world\num\nso to get started a bit about us\nwe realized that not maybe everyone\nknows who we are\nbut we are a austrian startup\nwho has\nbasically created the 3d environment for\nmicrosoft flight simulator\nand from there on out we have started\nexpanding and trying to find\nnew ways to to implement\nwhat we do\nand one of the results is\nthese beautiful pictures\nuh of these 3d environments that we are\nmaking\nand we mainly focus on\nbuildings vegetation roads\nwater\n[Music]\nmountains whatever you can imagine\nto create the most realistic\n3d environment we we could think of\nand we're doing all this with ai models\nand satellite imagery and that's\nbasically our our core\num\nso what\nwe rely on is having this satellite\nimagery running our\nai models uh creating data from this and\nthen eventually providing this to to\ncustomers and\nwhere flight comes in for us is mostly\nin the ai detection and the content part\nwhere\nwe're using it to\nrun all of these\nmodels in production and also do the the\ntraining\num\nbut yeah you know why did we end up\nchoosing for flight\num the main reason is\nour data is is reasonably large i would\nsay so if you imagine that you have\nsatellite imagery for for the whole\nworld\nthat's about two and a half\npetabytes of data um and processing that\ntakes quite some some computation power\nand from there on out we produce about\none and a half billion\nbuilding footprints\nthat are run on hundreds of machines in\nparallel\nso we've been doing this on our own in\nthe past um except that at a certain\npoint you know your company grows from\nfrom 30 to 100 employees and there's a\nfew people\nthat have the knowledge of of how to\nwork with this platform and it's just\nnot scalable anymore\num another thing is that we have a\nturnaround time of a couple of days\nand in this time there's also new models\nbeing trained\npeople doing some explorations with\nnovel techniques\nand\ndoing all this in parallel kind of\nforced us to to find a new workflow\nengine other than what we've been uh\ndoing ourselves\num and so\nyeah along camp flight i guess\num\nthe reason we're we're mainly using it\nis\nit's cloud native capabilities we\ndo everything in the cloud we also don't\nwant to be limited to a single cloud\nprovider so\nhaving the ability to run everything\nthrough kubernetes is is amazing for us\nextendability is something that we are\nvery happy with so far\nof which i will elaborate uh\nquite a bit more later on\nso far it has been performant we have uh\nmanaged to to find some limitations\nevery now and then\nbut uh luckily the union ai team has\nbeen amazing in uh helping us find ways\nto to push the boundaries a bit more\nand uh yeah\nit's also pretty robust uh we haven't\nreally been able to to break it too much\nthough we will be trying a bit harder\nin the coming time so we'll see what\nhappens then\nto get a bit more into\nhow we use flight really\nso i mentioned this two and a half\npetabytes of data\nwe break it down into certain regions\nand\nwe scale them uh sorry we schedule them\nbased on specific regions because every\nregion relies on a neighboring region\nand we end up with about a million\nworkflow executions that we need to run\nsome way or the other\nand these are all scheduled into flight\num using the api\nand one of the main things we we end up\ndoing is we use these nested launch\nplans\num where a single launch plan launches\nmultiple other launch plans\nto ensure that every launch plan has\nenough workers to to\nhave the scheduling run smoothly\nand\nwe also really rely on on the caching\nbecause processing this data there's\nquite a few\nsteps that can be repeated\nand you know if you end up retrieving\ndata to train a new model\nit's nice if you can just pull\nsome region from cache and and continue\nfrom there on out\nso that's been really important for us\nas well\nand you can kind of see how the the\nworkflow is structured where\nthere's one workflow calling another one\nand uh\nit nests in itself there\nso to get more into\nthe actual implementation stuff we do\nuh one of the things we have tried to do\nquite a bit more is to\nmove\nfunctions that are used in a lot of\nplaces to low-level libraries\nso\nif machine learning engineer needs a\nfunction to retrieve data\nwe don't want them to need to import the\nwhole library we just kind of want them\nto use the flight reference which is\na nice feature\num so\nwhere we used to have a training\nworkflow that consisted of you know\ngetting your input data\npreparing it in some way or the other\ndoing your training and then saving the\nresult\nwe've now moved to\nhaving a common library with the the\nimportant functions\nand the machine learning engineer only\nhas to worry about\ndoing the training and calling the\nfunctions that we provide them\nso what that looks like in our case\nis we have a reference loader\nwhich uses the\nflight kit functions\nand adds a little bit of magic because\nwe wanted to to have typing and\neverything\nso that's why we have this second layer\nin there\nand this allows the\nmachine learning engineer or anyone else\nto just use these functions\nin their code without\nfeeling like it's any different\nand\nwe feel that it brings down the\nthe bulk for\na training quite a bit\nso what you can see in the the bold text\nis the functions that are being imported\nfrom\nfrom our common library\nand\nyou know in this case it pulls some\nregions of interest which contain\ntraining images validation images\nand\nhowever this is handled\nmachine learning engineer doesn't really\nhave to care about it\nthey run their training they focus on\ndoing their training and when they're\ndone they submit their checkpoint\nand if we ever decide to to change up\nthe structure for the\num\nfor the database or how we store the\ncheckpoints or what the relationship is\nto other checkpoints they don't have to\nknow\nit's just abstracted away which\nwhich is really nice in our opinion\nand\nanother thing we\nlike doing with flight is we create our\ncustom data types\nso\nthe advantage we think is in this is\nyou have a nice serializable object that\nyou can share between your different\ntasks\nit simplifies a lot of the\napi structure that you would otherwise\nhave to deal with and\nit's nice that\nit just works basically\nand an example of this would be we have\na data lake that contains a lot of our\nimages and in this example i grabbed\ntwo from madrid\nand\nif we would want to read\nour images from this data lake\nwe have a data lake scene we can just\nthrow that in there and in the\nbackground the data lake scene deals\nwith our http api which is used to to\nget data from this data lake\nit deals with transactions when writing\nto the data lake\nand you can query\nyou know you can query a specific scene\nname you can query a specific id\nif you want all images for specific\nreason uh region\nyou can just query a region and you can\nget images for there\nand then when you're done\nuh with processing in this case you know\nsay we want to get building footprints\nyou've run your inference you've gotten\nyou're building footprints\nyou just return the data set the data\nlake scene again\nand um\nyeah you know it shows up in the data\nlake you don't have to worry about it\nand i have to say this was an easy\nimplementation because it is basically\na one-on-one with the flight file with a\nfew extra bits\num\nbut being able to to incorporate\none of your own apis\nin i think it was about a day's work\nand having something that looks this\nnice is\num it's just really nice to be able to\ndo\nso from here on out\nwhere do we go um\nwe're currently\ntesting out different cloud providers we\nwant to\nfind out you know what breaks what works\nis there anything else obviously we're\nrunning on kubernetes so\nwe don't expect\nflight to have any issues with that\num but we'll see what happens same for\non-premise\ntrying to figure out how we'll deploy it\nthere um\nwhether we will or whether we'll just\nhave the\nthe final executions over there\ndo some more battle testing like i said\nwe're gonna try and hit it quite hard\nsee where we can break it\nuh figure out what limitations there are\nand then then work around those with\nwith new optimizations\nand\nthat is about it from my side\n[Music]\nso thank you\nwow\nthank you martin that was fantastic\nany questions from folks i have a couple\nbut i'll wait\nanybody i guess you have to see the hand\nor else you can just unmute and speak up\nprobably it's okay\nokay i'll kick it off it's always a\nwarm-up challenge um so a question for\nme\nto be honest maybe rather than a\nquestion a comment when i first heard\nthe 1 million egg pieces i thought it's\nnot going to happen\nto be honest in a short period of time\nso\nthank you for talking about it and thank\nyou about uh\nwe we know we will face some challenges\nin between can you hear us now thank you\nfor watching\nhello thank you\noh can you hear us hello\nyeah\nyes yeah sorry uh we're trying to fix\nthe mic\nbut yeah thank you martin for the\npresentation i uh i was curious about\nthe annotation the\nreference loader annotation you had\nthere\nthe\nwhat does it simplify or what does it\ntrap\num so so the main thing is there's the\nthe get reference task and get reference\nworkflow functions within flight kit\num\nobviously they they can't do typing\nbecause you're just grabbing a reference\nso\nthe only thing that it really\nencapsulates is having\nthe the typing in there\nbecause we really appreciate being able\nto type the function so that's\nwhat it adds the extra layer\nnice\nyeah awesome work\nthanks\nanybody else so\nsorry one question have you guys are you\nguys also thinking of doing synthetic\ndata generation with this\nor\nfor training\num i believe that there\nare some people working on\nthe synthetic data generation as well\nbut i have to admit that's more of the\nthe machine learning side\nmaybe someone from machine learning is\naround to answer that question but uh\nbut i believe that\nyeah most of the stuff we're doing at\nthe moment is circled around flight\num\nincluding\ndata generation\nuh hi martin i have a quick question\nabout the common library\num i'm just curious\nlike how\nyour team came upon that\npattern\num and how\nlike where specifically that that code\nlives is it like how what is the life\ncycle of that that code base versus like\ni don't know the more custom or\nresearchy\ntasks that you have\nright\num yeah so\nour main goal was to to try and reduce\nthe number of dependencies and the\nnumber of worries that a machine\nlearning engineer has\nand\na lot of our our main libraries they\nthey incorporate a lot of\nlarge geospatial packages and dependency\nresolution for those is generally not\nvery fun\nuh so trying to find a way to to kind of\npull that apart\nwas was the main goal\nuh in using these references\nand\nyeah in terms of life cycle these common\nlibraries are maintained by our\ninfrastructure team\nand\nwe have the the full control over what\nhappens in them\nand\nbasically we work from release to\nrelease and we\nmake sure to have\nfunctional production releases that can\nthen be used by the machine learning\nengineers\nso in comparison to to the machine\nlearning code\ni would say it's a lot more\nuh permanent and the machine learning\ncode\nyeah you know like\nit evolves a lot quicker and these are\nthe people that are really trying to\nfind the nitty-gritty and\nuh use new techniques so there's there's\nless control on that side as far as\nwe're concerned\ncool and and and besides besides the um\nthe typing thing that currently isn't in\num\nlike the flight kit code base\nis there any other\nare there any other features in the\nreference task\nreference workflow thing that um you\nfound maybe lacking\num i don't think so actually\num\n[Music]\ni don't believe so i think the only\nthing we're wrapping at the moment is\nthe the types\nnice cool\nyeah i actually have one thing\nthat i wanted to bring up for everybody\ni think what\nwhat reference stars and folks who don't\nknow essentially it's a corollary to it\nis essentially these are thick\napis\nthat are available let's say on a web\nservice of some sort and\num\nthe at reference loaders stuff that\nmartin showed is essentially\na way to extend those apis in the client\nside without actually bringing the\nimplementation\nwith it um and this is how you decouple\nlike you know in web services\nand this is how we\nwe propose to decouple you know\npipelines\nand i think\nspotify hopefully they'll come and talk\nabout it as well and lift and a few\nother companies are also using it to\nhave\nsystem tasks\nthat are maintained like a service and\nthen folks can use them in your in their\nown workflows but\nthank you for sharing the pattern of how\nyou use it this is fantastic\nwe would love to make sure that you know\nthe typing issue if there is anything\nmartin that we can actually just make it\nas part of the framework we'd love to\nbring that in so if you can find an\nissue\nthat'll be great\nsure\num i think we're able to share our\nimplementation\nmaybe\nas a guideline\nor\nas a hint\nso\nyeah that would be awesome\nokay if there are no other questions\nanybody else going one two three\nthank you black shark team that was a\nfantastic presentation we\nas i have said this openly you love the\nwork that you guys are doing\nthank you for choosing fight\nand helping with the community"
    },
    {
        "title": "Flyte Community Update 006 - Feb 8 2022",
        "transcript": "good morning welcome and today is the\n8th of february\nand\nwe have a packed agenda as i said so\nlet's get going\nas usual we'll do it in three parts\nwe'll quickly go over the community\nhighlights what happened in the last two\nweeks what's coming uh maybe in the next\nfew weeks\nand then we'll jump on to the demos\nwe won't take too much time we can\nreally highlight\nso\nwe just i think uh what we just released\non the version 19.2 and this is again i\njust said an incremental release\nas you guys know we have\nnone of these releases try to break\nanybody and we'll continue to do that\nwe are still targeting version 1.0 end\nof march\nuh which implies again no breaking\nchanges but just some legacy uh stuff\nwill be removed and there is one more\nchange that's happening and that we'll\ntalk about that today uh during year's\npresentation\nso\nuh a bunch of changes in the new one uh\nux i think one of the most requested\nfeature archive one archive workflow\nexecutions is in\nwith this\narchive or archive for all\nentities is going to be in in the next\nrelease or so\nwhich is essentially\nprojects\nfor clothes tasks execution everything\ncan be archived and again you remember\narchive is not deleting so you can\nalways recover it but it hides it from\nthe ui hides it from the cli unless you\nrequest it specifically um\nthere's uh you know extra node metadata\nthat has been so now\nwhen you see the execution view we did\nnot see\nthe nodes that are yet to be executed\nnow they are again this is all setting\nup for the timeline view which is almost\ni don't know if you guys have been\nfollowing the pr it's almost in\nso that you should be able to see the\nexecution timelines for various nodes\nand all this works well together and\nalso sets up for dynamic workflow\nvisualization as well as map task\nvirtualization\nin flight kit\nactually\nthis is an overall system change but\nspecifically happens reflected python is\na new structured data set type so we had\nif you've been using flight schema\num you\nsome of you have requested that there\nneeds to be a little more flexibility\nwithin flight schema for example support\nfor\ncomplex types as well as support for\narbitrary backends\nand so now structured data side\nsupports that and we'll be talking about\nit later after martin's\ndiscussion\nalso flight kit now supports intratask\ncheckpointing\nthat means if you have long running\ntasks if you schedule them on one of two\nthings if you schedule them on spot\nmachines um and you\n[Music]\nif you have a retry you in your retry\nyou want to resume from the last\nuh point you can use introduction\npointing you can\nuse it for anything this is not\nspecifically a machine learning check\npointing but of course in machine\nlearning you can use it for training\num and that's an example and so on in\nthe future\ncoming soon would be additional\npieces that will help it even\nuh make it even easier to use with like\nhigher level frameworks like pytarch and\nuh tensorflow keras etc\num and and one of the other biggest\nchanges probably in this reviews is that\nthere used to be an older flight kit api\nwhich probably most of the community is\nnot using there are some folks still\nusing\nit has been deleted the code has been\nreduced dramatically it was one of the\nbiggest\npr's probably 20 30 000 live or more um\nit's all gone\nso\nthe uh if you now check out flight kit\nyou should see a much more easier to\nunderstand code base potentially\nso please\ndefinitely check it out and let us know\nwhat you think\nfrom a system point of view lots of\nsecurity updates um\non on that note we are constantly\nrunning security updates we don't want\nfolks to end up in a situation that you\nknow with the lock4j or something so we\nare constantly updating um\nthe code base finding security\nvulnerabilities and fixing\nthat does still mean that there is still\na possibility\nthat we may not know about a security\nvulnerability so if you do find one\nplease report help us fix it\nlet's keep everybody in the community uh\nyou know who are using flight safe\nand yeah flight cpr also does support\nmultiple installations of flight now\nso you can just switch between multiple\ninstallations\nall right uh actually we don't have a\nlist of contributors today because\nsomething went wrong with the script but\nuh thank you to all the contributors we\nusually do put the pictures here\nbut we've we've had at least\n10 or 15 new contributors within this\nrelease so thank you everybody\nuh as usual we are actually solidifying\nthis\ni don't think everybody knows about this\nenough so again\nwe do office hours along with this\ncommunity thing this community sink\nhappens every tuesday every other\ntuesday\nand every wednesday about 30 minutes in\nthe morning and 30 minutes in the\nevening pacific time\nuh either hate them or i open up a\nzoo\nchannel and we just let anybody who\nwants to join ask questions\nuh come in and ask questions this is\nyour time it's absolutely you can ask\nany question\ndiscuss use it to\nshare certain information or discuss use\ncases\nand there are two ad event links please\nuse them if you guys want to get hold of\nus\num\nnext thing for community sync we have uh\nmerantics\nthey have been doing up some fantastic\nwork with flight kit and like making it\neven easier to launch\nworkflows uh\nalso easier to actually build their\ncontainers\nso uh definitely look forward to that\nanother one is by woven planet um\nand woven planet uh is essentially\ntoyota's arm of building autonomous\nvehicles\nuh and so they use flight exceptionally\nand they'll be talking about how they\nleverage flight for their\nperception stack or their\nslam stack etc\nand\nthere is a newsletter the best way to\nstay in touch with what's happening to\nthe newsletter please subscribe please\nshare it with your network really helps\nus on this youtube channel\nall right from a roadmap point of view\nas i said i briefly touched these things\nbut the timeline view dynamic workflows\nand map tasks we we weren't able to ship\nit this uh 19.2 release uh because of a\ndependent library but\nwe'll be shipping soon um\nfrom a system point of view we've been\nuh again another requested item is deep\nhashing of offloaded objects for\nimproved caching for example what that\nmeans is if you have a pandas data frame\ntoday flight only uses references to\ncache them\nnow if you can use\nyour own custom hashing techniques\nto\ndecide when the cache when to cache\ncertain things\ninstead of just using references\nand so for example because this is this\ncan\nbe expensive so this is optional but as\nyou opt in you can provide your own\nhashing method\nalso\nanother thing that we've been seeing is\ndifferent people have different setups\nwith kubernetes some people have dns\nrequirements some people have a\ndifferent sort of security policies\nso\nlots of changes have been going into\nflight plugins so we realized that we\nare going to solidify all of that into a\npot template and allow you to customize\nhow every container that fight launches\nwith the default set of parts\nparameters directly\nconfigured in the back end so\nalong with that map task retrieve and\nmap task max parallelism boundary\ncatalysm is all getting checked in\nuh this might come in we are not 100\nsure but failure nodes in workflows so\nevery workflow you can have like a try\ncat semantic\nand you can say in case of failures to\nxyz and it can be an entire workflow\n[Music]\nand and then a very\nfuturistic uh\nhelpful thing for folks who are going to\nuh you know\ngetting started with flight and are\ninitially working on a single binary\nflight all of life back into one binary\nso that you can get it started with very\nvery quickly\nuh\n[Music]\nflight kit remote we are going to do ga\nso there are a couple more small things\nto be fixed in there and we are working\non it this this month\nand then of course uh type annotation\nthe medium type which we've discussed in\nthe past\nall right that's uh that's what we are\nworking on in this next month and little\nmore than that potentially\nthere is also some work that's happening\naround uh\nvisualization of custom metrics within\nthe ui\nper task\num and so if you guys are interested\ndefinitely dm me we would love to get\nsome feedback uh\nthink about this like you can plot your\naccuracy\nor you can\nplot specific things if you are like\ndoing something as cool as uh you know\nbuilding a 3d planet and you may want to\nplot like number of buildings or some\nsort of other metrics then\nwe would love to support like there is a\nway that we are thinking of supporting\nthis as a general framework\nlove to chat with you"
    },
    {
        "title": "FlyteConsole UX/UI Townhall #2 - Jan 25 OSS Sync Up",
        "transcript": "all right\nso um my name is jason porter and i lead\nthe frontend team at union working on\nflight console and this is our second\ntown hall\num to get some context the purpose of\nthis uh\nbasically we want to make sure we're\nbuilding the right thing so we have our\nroad maps we have our assumptions but\none of the really cool things about\nworking open source is that we all have\nlike a vested interest in how this\nshapes itself\nso we wanted to give a platform for you\nguys to engage us and kind of give us\nyour feedback tell us what you like tell\nus what you don't like and kind of\ninform where you want this to go\num our last town hall was great tons of\nfeedback\nand i want to leave time for that so i'm\ngoing to kind of go quickly through the\nsome of the updates\num because i really do want to leave\ntime for conversation and feedback\num\n[Music]\nso let's start with\nuh\nsome things that we've been working on\nthese are things that have come out\nsince the last town hall\nand uh we're actually pretty excited\nabout a few of these features\nuh the first one is that we have\nimproved the the search ux\nbasically\nuh showing more information as you\nsearch and so you can see like the last\nexecutions you can see inputs and\noutputs things like this in a really\nclean usable format and you can click on\nany one of these executions that will go\ndirectly to\nthat execution\nwhy is there a lag here\ngeez okay so um another feature we have\nis along that same vein we've added this\nnotion of a bar chart for executions and\nit just gives you a quick read\nabout the state of an execution and how\nlong it took so you can see down here\nyou know for this particular workflow\nall these executions and the big ones\nstand out like these big tall\nyou know aborted ones or let's say you\nwant to kind of zoom in on which were\nthe longest failed workflows\nso you can kind of come in here and you\ncan say oh okay\nwhat's really cool is that when you\nclick these these become filters\nand that way you can kind of do a quick\ncomparison between executions and we\nthink this is going to help with like\ndebugging use cases\nanother thing from the last ux one of\nthe big feedbacks was we want to be able\nto archive\nand uh this isn't launched yet but it's\ncurrently in development will be\nlaunched soon\nand what we're going to do is per\nexecution we're going to allow you to\narchive them and archiving means we're\nnot going to delete something but we are\ngoing to hide it from the kind of the ux\nso when you click on one of these what\nwill happen is we want to kind of just\nprevent like mistakes so we'll have a\nconfirmation step but if you archive\nthis it basically becomes invisible to\nthe ui again not deleted you can always\nrestore\nand how you restore\nis in that same\nfilter ux we have a toggle and the\ntoggle will let you choose between do i\nwant to see like\ncurrently like active or do i want to go\nback in time maybe you made a mistake\nwant to bring something back or just\nlike for historical purposes you want to\ngo and examine some of these archived\nworkflows\neventually we're going to add this\nability to pretty much everything\nworkflows tasks executions all over the\nsite we're going to have this ability to\narchive things and that was one of the\nbig feedbacks we have for town hall so\nwe're really excited about this\nit should be launched within the next\nfew weeks\nanother feedback from the last town hall\nwas having more visibility into launch\nplans and so\nthe basic use case is when you're\nlooking at executions it's kind of hard\ncurrently in flight console to see what\ndid this launch with\nuh currently you have to kind of go to\nthe launch form\nrelaunch it and kind of see what it was\nlaunched with we're going to change that\nand for example on a per execution basis\nyou can click and see a launch plan\ndetail page that will tell you about\nwhich launch plan was used for that\nexecution again this is currently in dev\nbut again we're only a few weeks out\nfrom launching this and we're pretty\nexcited\nthe big one that we've heard tons of\nfeedback about\nwas people want to see a timeline they\nwant to see\nfor a workflow\nwhat was the duration of each task\nand\none really cool thing about this i'm\ngoing to show you guys\nuh we\nactually already have a demo version of\nthis running\nso i'm gonna open up just a flight\nsnacks\nand here you can see the the new ux\nwhich is awesome\nand so let's go to nested\nnested parent workflow\nso when we click this\nand you can even see these executions\nand we can like click and see how the\nfilter works\num\nso let's go into one of these\nand\nwe have a timeline view and so we can\nsee the duration that all these tasks\ntook\nand i'm even going to show you this in\nmotion\nso\nhere we go timeline\nwhat's going to happen is it's going to\nshow you all the tasks they're all going\nto start out gray and blue initially to\nkind of show the state running or not\nrun yet\nand as these executions update the graph\nis going to update with it\nso we'll wait\nfor\nthe whole thing but we're going to kind\nof wait to kind of see some of the state\nchanges so there you see those tasks\nalready completed\nand then\nwe'll give it one more update\nthere you go\nand uh of course you can do things like\nzooming in to different time durations\ndepending on the size of the workflow\nand there you go it's completed so we're\nsuper excited about this uh this is\nactually very close to release uh we're\ncurrently working on the pr right now\nso\nyeah that's what we've been working on\nbut as i was saying uh what i'm really\nexcited about is this next part so this\nis the part where\nyou guys can engage us so we open this\nup wide open\nanyone can chime in\nlet's start with\nwe are curious as we're developing all\nthese features\nagain making sure that we're focusing\nand building the right things\nwhat are some of our best features like\nif you have to say what is the most\nimportant feature of flight from a ux\nexperience\nuh what do you guys think\nspeak up people\ni would say\nto actually up on running a workflow\nwith predefined parameters to\npick one manually and override it for\ntesting stuff\nit might sound like not so much but um\nupon launching the workflow when i can\nmanually override some of the inputs\nso the launch plan ux the ability to\nkind of when you go to launch you can\njust update or relaunch\nmight not have the right terminology but\nyes i think that's what i mean\nyes so that i have both the ability to\nto\nconfine to define a pre\npredefined set of launch plans but also\nthat i have the ability to override it\nwhen i launch something\ni think that's that's makes it very\npowerful too\nto treat workflows as uh\nyeah well as as templates and then adapt\nthem to you to your needs\nawesome and then so\ndoes that launch\nthose screens you saw of the new launch\nplan workflows or being able to see\nkind of what was attached to an\nexecution is that something that\nthat you see is valuable\nit's super useful because yeah\nyou need to know then later on what\nworked and what didn't right when you're\nwhen you're sifting through\ntons and tons of executions\nand the new x makes it makes it way\nnicer to see like what\nsucceeded what didn't how long it took\nand then you can pick the ones and see\nwhat parameters they run with\ni think it's great\nawesome\nanyone else have any like core features\nthat they just they use all the time\nthat they just love\ni see jeev so i'm gonna just ask him\nhey\nokay well we can move on\nso\nuh this is this is actually the more\nimportant one to me\nuh as we're building all these things\nand you've kind of seen some of the\ndirections we're going\nuh what are we still missing i mean you\nguys can pretty much direct us any way\nthat you want what are some things that\nyou would just absolutely love if you\ncould do\nwhether it's your debugging workflows\nor just you know performance tweaking\nwhatever it is\nrole based access control\nokay so like\nin what sense like execution like to\nlaunch something or\nif i have anyone with flight ctl and\naccess to our\nflight deployment url they can register\narchive\nanywhere on the whole thing it would be\nnice to say\nthis person has access to this project\nand maybe only to this domain\nbecause otherwise i can have someone\nrandomly come in and\nprint secrets from production because\nthey can just register to production and\nexecute whatever\nbut that's like long term\n[Music]\nbut it's it's a thing that was mentioned\nlike we we went through a security audit\nwhen we started flight and this was one\nof the things where they were like yeah\nmaybe at some point\num\nyeah hey uh tim this is hi samir uh i i\ntotally echo that i think this is not\nthe first time we uh we heard about this\nand um\ni can can you so yeah uh uh limiting\nusers to projects want to ask a bit more\nabout what level of permissions you are\nlooking for\num\nprojects and domains but you\nare not thinking of you know you have\nonly have like cleared access on this\nproject and like right access to that or\nlike management rules or like what what\nkind of you know levels are you\nthinking about\nthat's\nfirst question the second question i was\nwatching your presentation it seems like\nyou have\nlimited people to\ncertain domains i'm curious how was that\nachieved\nessentially by having a separate cluster\nfor for broad flight\nwith only one domain\nand then not not giving out the client\nsecret\nonly giving it to the ci pipeline\ni guess that's one way to do it okay\nokay\nawesome\neffective yeah\nyeah yep uh okay all right um i had our\nfollow-up\nso i had a follow-up question on that so\nwould it be enough to actually have\nbasic always access control as a as a\nbeginning like just basically\nlike maybe handwritten somewhere with\npolicies or something\nwith groups and and essentially have\ngroups and that's good enough right and\nbasically just project\nnot like for workflow level and for\nexecution that peak that is where it\nbecomes very very complicated\nthe number of entities just keep on\ngrowing\nnow per project is a great start and\nshould be yeah\nawesome and uh are you using uh flight\nlike authentication\nis that or do you have your own layer on\ntop okay you're using the native stuff\ncool\num okay\nwhat about archives so yeah project\ndomain obviously do you think that we\nshould put i mean once we implement\nrole-based stuff would you want that on\nthings like archive\nsince it's reversible that's not so bad\nlike if it were delete then yes but\nright archive probably not\nbut but i guess if you only have a\nproject level access then you can only\narchive that project i guess it's still\nnot great but it's better than you know\nrandom like you cannot have any other\nanybody else's project like forecasting\nour archive pricing teams project which\nmight not be\npretty well received\num\ni had a\nmore in the ui ux front question\nbecause we didn't get this so we've been\nthinking about like two\ninteresting interactions and and things\nthat we're thinking of adding\nuh one of them is visualizing uh some\nruntime log\ninformation uh like actually three parts\nin there the one is just the basic\nlogging the actual login that you get\nright and today we only have links to\nsome external log provider we were\nactually thinking of getting those logs\nin line\nwhile things are running\nuh now and within the security perimeter\nright\nhas to be secure by definition yet uh we\nshould be able to pull it so that's one\nthing we were thinking but the second\nlogs by logs i meant is that let's say\nyou're training a model and you want to\nlook at the accuracy or the error\nof the um the loss\nuh\nwe could visualize that now that does\nneed some work but we could visualize\nthat with some external systems\nand then visualize them in the in flight\nconsole or we could give a link to let's\nsay wait some biases what what do you\nthink is the preferred ux and\neverybody like even sora ng all of you\nguys\nwhat do you what would be useful to you\nguys\ndoes it matter if it's a link basically\nmaybe make it even a binary question\ndoes it matter if it's a link to\nan external\nuh\nlike website like weights and biases or\nit doesn't\nyes or no it should be matters and it\ndoesn't it'd be very cool to to be able\nto see to see it directly in the file\nbecause if it's a link i can i can give\nit as an output\nto my workflow you know i'll give it the\noutputs and so they cannot like yeah\ntake it but yeah seeing it visually i\nthink would be very important because\nyeah we could integrate something else\nagain but that's\nthat's some work that you you need to do\nand\nand maybe i i don't know how complicated\nit is on every side but you know there's\na visualization sorry\nit's quite basic i would say\num\nmaybe it's a lot of good at least the\ndigitization is basic so yeah maybe\nsomething that should have the beginning\nyeah so the way we'll be i think i don't\nthink\nflight should completely own that part\nbecause there are many other open source\nalternatives that do a good job like\ndemo flow and so on what i what i meant\nwas like essentially somehow\nin allowing an integration portion to\nintegrate like ml flow experience\nfor the visualization or aim hub is\nanother one that we've been talking\nabout with\num directly in the ui\ngood to know i think we we've been\nthinking we'll continue to think of that\nand we'll definitely pick up some in our\nnext ui ux town hall or so\num\nthird one was\nthe outputs right like today you have\nlike let's say tabular information or\nsometimes unstructured data you might\nfly knows the format like if it's a\nmodel it knows\nit doesn't really know but it kind of\nlike has the meta information that it\ncould be a model\nwould it be useful to\nlike have custom plugins of some sort to\nessentially visualize that that data set\nso let's say if it's a pandas data frame\nshow the first 10 rows\nuh of that data frame\nnow that could be security problems but\nlike let's let's assume that's available\nwould that be useful uh another one is\nif you're using greater expectations i\ndon't know if you are but like you might\nwant to see the\nvalidation result of let's say\ngenerating the generated data would that\nbe useful\nso yes don't know if other people have\nan opinion but\ni would like it\nlike a plug-in system essentially\nyes sorry\nyeah i think\nsome kind of customization you should\nset customization i think\nif you're able to like\nhave a link or different kinds of links\nto\ni don't know see the data frame like\nobviously part of the data frame or\njust have a link where you can\nopen this up in the notebook which\nwith flight remote you know something\nlike this\njump\ndirectly to the results somewhere i\nthink this would be would be awesome\n[Music]\nokay\nthat means we need like a notebook\nhosting service uh\nthis\ncould just be a link to some\nbecause people use different\ntechnologies also\nfor for notebooks um\nbut just in general the ability to\num to make\nmore out of the the outputs\nthan just like seeing the uh the bucket\nurl you know\ni think that\nwould be helpful\nfor really helpful for certain use cases\nokay yeah that's\nboth of both of those things is we are\nactively\nthinking and exploring that's what i\nwanted to bring up and we are like\nreally we think\nthere is a differentiated experience\npossible here like essentially\nupleveling\nthe meta information that's like\ncaptures about your data right\nthe reason why we have a type system is\neventually to do these kind of things\nthat literally you know have a better\nvisual experience with things\nwhich other systems cannot do because\nthey do not have the type information\ndeeply embedded in the\nmeta information\nall right\nif no other questions i think that over\ntime\nall right thank you tim and stephen and\nthank you jason\num this is fantastic and thank you for\nanswering all the questions\nthanks guys\nthank you too\nthe ux looks amazing super excited\nhey thanks\nthank you\nall right bye guys"
    },
    {
        "title": "End-To-End ML With Flyte by Wolt - Jan 25 Flyte OSS Sync Up",
        "transcript": "so my name is stefan stephen whatever\nyou want to call it\nuh\nso yeah i work at vault uh so maybe a\nquick introduction for this p for the\npeople that are involved\nuh we are tech company with\na bit more than 5000 employees now and\nwe started as a\nas a food delivery company so kind of\nlike the oldest or a great so\nwhatever you want and won our delivery\ncompany so we shifted within that regard\nand we are in 23 countries more than 200\ncities more than 50 million customers\nand most importantly we have many data\ncenters\nuh and that's that's the most important\npart\nso to work with them\nwe are two machining engineers so we\nstarted and tim that is here as well and\nwe are\npart of a core machine learning and core\nmachining and data engineering team\nwhich\nis basically in charge of providing\nservices to all the other teams so two\ndata centers but two data analysts as\nwell and everything\nand so that they can they can work and\nthey they don't have to focus you know\non communities or anything related to\nthat\nthey can just\nwrite some code so yeah basically that's\nwhat we are uh and advanced we have so\nwe have ml uh we have the mlb is\neverywhere to be honest\num so we have supply and demand\nforecasting and then we have recommended\nsystems so on the app when you open the\napp you know you can\nyou get some different recommendations\nbased on your preferences so if you're\nvegetarian or not and all the things\nlike that and that is done with ml\nwe have one which is very important\nwhich is the derivative time estimation\nthat's\nwhen you order something uh then waste\nyou at the time that is gonna you know\ntake them to prepare the food take them\nto prepare the order\nthat every time drop off time all the\nthings is done with ml as well and that\nis a bit more real time than the other\nones and then you have for detection and\nticket ranking that is helping support\nwhen you contact them you know if you\nknow\nyour issue is like very important and\nneeds to be addressed\nwithin seconds or if it can wait like\none or two minutes for example and that\nalso is done with ml\nso yeah\nand so basically like that talk is on my\nside i'm just going to explain\nwhat's the idea uh of the ml infra and\nwhat we want\nand then tim will go a bit more into the\ndetails and our implementation\nand like what glues everything together\nbetween\nflight and the rest\nuh so yeah so basically what we want is\nwe want tracking and versioning for the\nmodels\nand then we also want you know machine\nlearning pipelines so that we can try\nmodel easily with tracking and\nversioning of different workflows\nuh model registry as well modern\ndeployment that is made easy and without\ndowntime\nbecause one of the problems that we had\nbefore was that deployment of models was\nvery hard\num and everyone was doing their own\nthing\nand then model monitoring so that means\nyou know like software monitoring like i\ndon't know latencies and everything but\nalso the feedback of your model so we\nwant to make sure that you know the\nmodel works as expected\nwe don't want to be blind\nbasically same\nso yeah\nand so how we're doing it\nis so we have flight\nas i think you guys know what it is\nuh so we use it uh we will use it to\ntrain the models and basically we're\nmoving away from airflow to flight for\neverything related to machine learning\nand then\nyou train your model then you're pretty\nhappy and then we save we make it\npossible to save everything in ammo flow\nso that you know you can check your\nexperiments track your models\num and well i mean\ni think everyone knows about flow here\nas well\nthen we have some services that are\nthere that we created that are basically\nmaking the link between\nuh segment core it's which is what we\nuse to deploy the models\nuh for the model standard for in real\ntime\nuh and basically flights so\nthe idea here is like you train your\nmodel in flight you push it to an old\nflow then you check if the metrics are\nbetter for example or you know if you\ncheck different things\nand then\nthat service is going to check it\neverything and then it's going to update\neverything incentive call without of\ncourse on time but also you know making\nsome checks like sending a notification\non slack\nuh saying hey uh that's what just\nexpected or hey there was a problem\nduring your deployment and were like\nworking on improving the service so that\nyou know once you push something you\nalso say hey yeah your model has been\ndeployed it's better\noh it's not better as the previous one\nso maybe you should check that and yeah\nbasically that's the idea is like flight\nwill be used for everything\nuh related to machine learning\nuh and then um\ni think\nfairly soon we should you also use\nflight for models but that are not real\ntime so seldom is here for real time but\nflight will be there for the models that\nare not real time\nand so yeah so that means i don't know\nlike the forecasting for example that i\ntalked about before\nand yeah and so that's basically what\nwe're building and so those are like\nonly the cool stuff you know the thing\nthat you you want to use\nuh but then behind the scene there's a\nlot more things um\nand so that's\nkind of what it looks like if i go a bit\nmore into details so we have\nwe use communities of course and then we\nhave a specific machine learning cluster\nwhere on there we have flight then we\nhave our cd\nand then we have\nour different python services\nbut then we also have the application\ncluster where you have all the\napplications on vault\nbut then where you also have certain\ncall deployed there because\nthe models are in the application\nclusters\nand then yeah basically\nlike you take the data for the data\nwarehouse on our site we use snowflake\nfor now but\nbut it's more like yeah how exactly do\nyou make it easy for data scientists to\nget the data from the from snowflake to\nflight for example\num or and then in the future for real\ntime and all the things will have a\nfeature tool wouldn't have it for now\nbut yeah that's that's basically the\nidea of\ni mean i guess it's what everyone is\nbuilding at the moment it's to have you\nknow very specific emma cluster and then\nhow do you make it talk to the to the\nrest\nso yeah basically that's that's it you\nhave the air pressure with flight\nand then argo cd which we use\nto deploy flight and\nand you know now i think\ni think\nmichael he's going to go for it now\nthank you steven\nhi everyone i'm tim\ncan you hear me by the way i don't say\nyes nice all right\nyeah i'm going to talk a little bit\nabout end-to-end integration\num but maybe not in the way\nthat people usually talk about it that\nis like\ntalking about\ngreat modern data stacks and the ml life\ncycle and painting a pretty picture i\nwant to talk about\nlike the nuts and bolts and the\nnitty-gritty details that you need to\nhave good good practices data security\netc while still\nenabling data scientists to focus on the\nstuff that really matters for them that\nis producing the best models\nso yeah we have this company internal\nmeme that people spam pictures of ford\nfocus around so i thought i'll echo this\nhere\nnext slide please\nso how do we want to enable them to\nfocus essentially by removing some of\nthe most common friction points in\ntheir\nin their workflows so we want them to\nenable to really quickly set up new ml\nrepos by using templates and\nshared resources wherever possible\nwe also want them to enable\nto access the data that they need for\ntraining their models easy while at the\nsame time being secure we don't want\neveryone to have admin access to our\nwarehouse that that would be easy but\nthat would not make us gdpr compliant\nand\non the note of gdpr we want this data\naccess needs to be auditable at the end\nof the day so i'm going to go into some\ndetails there in a moment\nand then we would like to keep\nbest practices best developer practices\nin terms of git branching and generally\nhave everyone\nsync their work and version control\ntheir work and make that match up with\nthe concept of flight domains so i'm\ngoing to go into detail all of those\nnext please\nso yes in terms of making it really easy\nfor people to set up new projects um in\nthe beginning we thought ah there's\ngoing to be one repository and a lot of\ndifferent teams have a lot of different\nworkflows in there but that would be\nhorrible in terms of code review and\nin terms of the peculiarities or special\nneeds and dependencies for every project\nbut if everyone is cooking their own\nrepositories anywhere\nthen we will have a hard time\nmaintaining common ground rules so what\nwe opted for is\nthat we have a git template flight\ntemplate repository\nand then we're using github's create a\nnew repo from template\nfeature\nto basically\num\ncookie cut without cookie cut\nuh new repositories that already share a\nwhole bunch of common\nuh tooling and configs so they will all\nshare the same ci pipelines just with\nlike the names of the project or\nwhatever automatically read from\nhowever they name their repository\num\nand these pipelines are then triggered\nand do things like creating the project\nin our flight cluster\nand then from there on they handle\neverything\nfrom registering workflows to\nsetting up configs and terraforming\npermissions for\naccessing\nsecrets within your specific\nrepositories so that\nideally\nevery\ndata science team has the maximum amount\nof autonomy in their repository they can\ndo their project there but everyone\nadheres to shared protocols\nuh next slide please\nand uh yeah one of the key things that\nwe need people to adhere to is uh\nwell data access rules so this is why\nthis is why we actually created a vault\nsecret manager plugin for flight because\nwe are heavy users of vault\nand one of the things that we want to\nenforce across\nevery\nmachine learning project that uses\nflight is that they follow the same role\nbased access control that we use\nelsewhere involved so\nand we want to scope this to\nbasically every project domain\ncombination so\nif i'm in my project in the development\ndomain\ni have an iam role i have a vault role\nand through this vault role i have\naccess to different secrets that i might\nneed for example dynamic database\nsecrets in snowflake where again i have\nmy corresponding snowflake roll which\ngives me access to specific tables i\nneed\nand this is a horrible mess to configure\nand for a little while we've been doing\nthis by hand\nand\nit was bad\nso now we're automating all of this so\nessentially\ndata scientists when they start\nnew machine learning projects they just\nhave a nice little config file in a\ndomain-specific language that we\ndeveloped at vault for\nbasically making stuff easier by\nremoving us one step further from\nfrom terraform\nhowever\nit does make things easier for them\nbecause essentially they can specify a\nlist of buckets of\nsecret locations of snowflake tables and\nschemas and then out of the box\nwithin their workflows with the vault\nsecret manager they will be able to\naccess the secrets that they need\nand we make sure that any access to the\nwarehouses is audited but also that no\none can like\nescalate from the development domain to\nprod and delete data there accidentally\nor\num that\nprojects\ncan\nread data from from another\nand vice versa\nso yes this is helping us a lot next\nslide please\nand lastly\nwe\nwere big fans of good git practices so\nwe thought we have this notion of the\ndomains and that\nthings are being promoted along the\ndomains in flight and we really like\nthat and we thought\nwe match that up with a branching policy\nand um i'll kind of\nset this a little bit in stone with how\nwe structure the ci pipelines\nso essentially\nwhen you're a data scientist and you're\nin your machine learning project\nrepository\non any feature branch\nlike you can do\nwithin the within the bounds of your\npermissions you can do whatever you want\nyou can experiment away you can fast\nregister you can\nstart workflows um\nbut you're isolated in the dev\nenvironment and scope to the permissions\nyou have in the development domain\nand then when you're ready for\nintegration testing you can promote this\nto staging by opening a pull request to\nstaging\nand have a classic code review and then\num you don't need to do anything then\nthe ci pipeline takes care of\nregistering your workflows to staging\ndomain\num and actually there we we safeguard it\nlike you cannot manually register there\nand you cannot manually fast register\nthere\nit has to go through a pull request\nso that people cannot like accidentally\nyeah skip a stage there\nand then lastly with the with the pr\ninto production\num after you've integrated integration\ntested everything to work together with\nmlflow and selden and the other services\nthen things can get properly trained and\ndeployed um and this seems like a lot of\nwork but because we\nwant to automate everything around we\nhope that for the data scientists\nthemselves\nit just feels like the normal everyday\ngit workflow because things like\nregistration stuff um\nrebuilding the images they just happen\nautomatically in the background\num\nso yeah this is the the ml funnel\nthat we\nliked\nthat we want to build with\nmatching the features to the domains\nnext slide please\nso to\nwrap this up i think this is our aim i\nthink everyone has seen this\npicture in one way or the other it's\nit's been talked a lot about but i think\nthere's truth in it\num there are all these components i\nhaven't really talked about these\ncomponents but i've what i wanted to\ntalk about is like the\nlet's say the threads that pull all\nthese things together behind the scenes\nthat's what we\ntry to configure in such a way that data\nscientists can really focus only in the\nml code and write that into neat flight\nworkflows\nthank you very much\nyeah any questions\nhi dude\nyes go ahead sure so when um\ni'm curious about the skill set of your\ndata scientists like did were they\nfamiliar with just using get in the\nbeginning do they write their own\nworkflows when they're typically trying\nto productionize something is that\nsomething they do with their own two\nhands are they working with an ml\nengineer\njust what's the persona look like for\nyour folks it sounds like it's a little\ndifferent from the folks that we work\nwith that live\nyou want to start stephen you know them\nlonger\nyeah so so all the data scientists are\nusually very curious and they\nthey are usually not afraid to get their\nhands dirty so that means it can mean\nlike everything so yeah\ngit check out\nwriting docker images\nuh right\nwriting fast api once needed so that's\nwhat we used to do before certain\nthey would wrote their own apis and\ntheir own routes\nand it's everything so so yeah they\nthey're usually very curious and i mean\nwe have a lot of different teams\ninvolved so the core teams\nwhere we provide services for others so\nyou know you might need to do a deep\ncheckout but then behind the scene you\nlike a lot of things are happening\nautomatically as well so we try to also\nreduce the complexity there but they're\nusually not afraid to\nto do things like that but just the\nwhole idea that if you do a pull request\nto the new branch you're going to get a\ndifferent set of permissions that you\ncan get that idea across to them and\nthey go oh that's a good idea\nyeah\nyes\ncool\num\nand then a little bit of just question\non how you got here did you guys sort of\nmake this big strategic decision to go\nkate's before you chose this particular\ntool set or did the tool set kind of\nlead you into kx\nokay it was a constant long before yeah\nokay so somebody decided like we're\ngonna try to make khp our core back in\ninfrastructure and that yeah i mean it\nhas been it's it's everywhere uh at all\nso\nlike our whole\na whole stack is on it\nso well and then i finally at least my\nlast question like which\nwhat uh\nwhat are you getting out of flight\nthat's that you were not getting out of\nairflow because you could have built a\nlot of this stuff\non top of airflow yourselves right you\nhave the skills to do that why did like\nwhy didn't you just extend their flow\nbecause apparent pattern we see\nyeah i mean\nthere are like there are things that we\ndon't like about her basically but let\nme think of it uh\nbut it's it's like also like you know\nthe different domains that you have in\nflight already in the first place which\nis which is good\num\nuh it's so that's the first thing\nuh and then\nyeah all the versioning all the caching\nalso that that you have\nyou know you don't really need to think\nabout it usually\nyou have it yourself so those are like\nkey decisions\nin the first place uh that we have yeah\nso so flight's a very opinionated system\nabout those things and you kind of\nagreed with those opinions is that uh\nyes but\ni mean\nyes maybe i'm sure you don't agree with\nall of them but at least those the big\nones\nyeah i think i think the benefits\nthere's so many benefits i don't even\nknow where to start just\nthat i mean airflow is not kubernetes\nnative like yeah you can you can force\nit to to work there but it's it wasn't\nmeant to work there and just the fact\nthat every task is a is a part\num\nthat it's that it says containerized\nseparately i mean\nand\nadds to the fact that you can\nparallelize within tasks adds to the\nfact that you can have dynamic tasks\nwhich i think is not possible in airflow\nlike you need to know the amount of\nsteps beforehand\num\nthis this just seemed next level like i\npersonally really love argo workflows\nthat to me is like the the king of\nworkflow executors but i think\nflight takes the best of that\nand\nmakes it usable from someone who prefers\nto write workflows in python in a really\nreally neat way so\nit was an obvious choice i think\nagain also it's it's language agnostic\nso you can you don't have to use python\nfor example and that's a big thing like\nit's the same for seldom\nyou don't have to use python you can if\nyou want to\nuh and that's a big thing because\nat the moment most of the things are\ndone in python but we probably will have\nsome some models in java and everything\nso\nso that's also something else yeah if\nthere was a if there was a the fourth\nlanguage you would recommend for an sdk\nis it\nwhat would that be i mean right now\nwe've got scala and\njava thanks to the spotify folks and\nthen the you know we did the python one\nanybody clamoring for you know r or\ntypescript or\nnow if i had to choose i would choose r\ni guess because we have seen some folks\nwho have been interested in it but\nyou know some other set of data\nscientists and and i think they don't\nget enough love\nfrom\n[Laughter]\nif there are any our folks listening to\nthe stock join our community we love you\nyeah\ni'd love to have somebody like wants to\nwants to pick the rsdk that'd be super\nfun but\nyeah yeah i think so and so we we\nactually went to a point of almost\ngenerating the our protobuf uh\ndefinition\nthe question over here is to do right\nlike our completely from scratch or use\nour pi kind of bindings or or use our go\nbindings or whatever\nuh but yeah we are open if you guys have\nideas if you have needs uh please join\nthe community and you know directly\nbeing me and we would love to see how to\nget this rolling\nuh i i had one question so\nit's it's great to see you know how the\ndomains are being used and stephen you\nmentioned things about domains so at\nlift we\nwe did have these domains and exactly\nsimilar sort of stuff where you cannot\naccess roles\nacross these domains like to our domain\nthe roles are tied and we actually you\ncan do it i don't know if you guys are\nplanning to do it at some point you can\nseparate the kubernetes clusters across\nthese domains if you want or whatever\nright um that way the blast radius is\nkind of like almost zero because the\nfabrics themselves are separated\nbut yeah we're about to do that\nyeah so uh the one problem\nthat that caused uh in the users uh\njourney was\nand this is where fast register was\nadded that\nevery time they created pr they had to\nbuild the containers\nright uh and that\ntakes some time specifically because you\nknow the caches and so on are not really\nlike local so it might\nand and we created fast register for\nthat but do you think\num\nhas that been a impediment uh are folks\nnot happy about it\nand you would love to understand how do\nyou really resolve this problem and\nstill keep the mlopsy way of doing\nthings we're kind of like trail building\nuh amalops here right we're doing\nsomething different\nbut\nthat causes pain and how do we really\nsolve that you would love to know if you\nguys have heard about it or thought\nabout it\nabout fast registering particular or\nyeah just in general maybe fast fixture\nis one way\nto achieve a solution or maybe you know\nyou build docker containers locally and\npush it like some people do that uh yeah\nthe free gnome team does that um and\nsome other like there are different\nthings that people are doing and what do\nyou think\nyou like or you don't like and what\nworks what doesn't work\nso\nmaybe i can answer\num when i tried out flight for the first\ntime a couple of months ago it took me\nlike i mean half an hour or so and then\ni was like\ndamn there must be an easier way i\ncannot like rebuild this all the time\nthere's no there's no like change to the\ndependencies\num\nso then we discovered fast register\nquite fast\nsetting it up was a bit hard because i\nthink we just\nstarted in the\nwhen you were doing introducing new\nchanges to to um pi flight and there was\nthis precursor to flight ctl\nwhat was it called flight cli\nand there was like a a stage where where\nthere were different ways to to do it\nand eventually or fairly quickly we\nfound one and we loved it\num so it's definitely\nit's definitely an integral part on the\ndevelopment domain\nfor for people to iterate really fast so\ni think one way to iterate really fast\nfor them is just to run the workflows\nlocally\nwhich is also great\nthat's a great benefit of light that at\nleast if you have python workflows you\ncan just run them like local python code\num\nand then the next one if they want to\ntry it on the cluster with more compute\npower fast registers is definitely super\nhelpful\nbut then\nas we move up into the funnel we we\nconsidered it safer to then use the ci\nbuilds\nand because we assume when you're in\nstaging and then in production there's\nnot so many code changes anymore you're\nbasically just executing the thing over\nand over again\ni agree i think\nprobably using fast register in\nproduction is not a good idea somebody\nmay just delete the piece of code\nsomewhere\nand yeah\nwith fast registered would you like i\nthink we've had this\nfor a bit and we've talked about this\nback and forth uh like\nwe are not a big fan of pickle as a\nmedium to transfer code um\nbut uh fast register essentially you\nknow is your entire code zipped up\nuh\nwould do you think it makes sense to\nactually add that kind of an api to\nflight itself like admin uh so that you\ncan just upload and proxy it to like a\ncode\nsandbox repository or not really a\nrepository but uh blob store bucket or\nsomething or do you think it's it's okay\nto have the users have the permissions\nto write to gcs or s3 i don't know do\nyou guys run it on google or\nyoutube we run it on s3\nand yeah no no that's that's fine\ni think it would be super helpful like\none thing that would be super helpful is\nif fast register figures out from from\nyour git state um\nwhich workflow files have actually\nchanged and only\nonly registers them\nare thinking\nyeah\nthat's a good idea yeah let's let's\nactually we'd love to you know pull your\nbrain on this and like we can again i\nthink there are ways uh to move ahead\nand go faster in this direction and make\nit cleaner\nbut no other questions for me anybody\nelse\nyeah i have a quick question uh sorry\njason\num\neating in here in your time here but a\nquick question just about um the process\nthat your data science teams have when\ncoming up with like a brand new model\nlike application\nis that\nis the whole like feasibility study\ndata exploration thing\ndone outside of this\ninfrastructure or within it\nit depends\nokay cool\ncurious because but\nit's mostly outside i'd say\num i'd say it's usually outside for now\nand and we'll see how it how it moves\nhow it evolves basically\nbecause a lot of things that were that\nwere\nthat were annoying for us was like the\nwhole access to snowflake and everything\nand now that we\nmade it easier we'll see we'll see what\nhappens basically\ncool thanks\nfantastic thank you guys you've almost\ncreated a reference architecture and\nit's interesting lift kojec you toyota\nall\nthe same architecture now\nit's awesome to see that\num\nreinventing the wheel\nexactly\nno\nno\nno that's great that that's that\nconfirms that we were onto something\nthank you\nyeah all right awesome\nyeah maybe there is a startup on top of\nthis"
    },
    {
        "title": "Flyte Community Update 005 - Jan 25 2022",
        "transcript": "all right\ngood morning everybody welcome um\nwe\nare in another edition of the\ncommunity thing\num\nso\nhopefully you are able to see my screen\nso the agenda as usual is we'll uh go\nthrough a couple\nuh like releases that we have done or\ncommunity highlights uh\nroadmap\nof the things that you should expect in\nthe next month um\nwe did a longer term map last time and\nplease take a look at the youtube video\nuh or follow our live road map\nand if you have any other features that\nyou're working on please bring them up\neither during this community saying or\non our slack channel or github\ndiscussions or github issues\nand\nfinally we have uh\nit's not demos but discussions uh one by\nthe world team and another one\nfrom the union team uh specifically on\nthe ui ux experience\nso uh community highlight\none of the biggest things we actually\nannounced i think most folks who have\nfollowed this channel know this but it\nwas officially announced just last week\nflight is a graduate top-level project\nin the next foundation\nit's all because of this amazing\ncommunity i am\nextremely extremely proud to be part of\nthis and thank you for everybody for\ntheir support\nwhat this means it really doesn't change\nmuch but i will reach out to some of the\nmost influential folks within this team\nwithin this community to actually create\na sharing committee\nwe want to create a steering committee\nthat is\nmake sure that the project is moving in\na direction that is that works for\nall of our top contributors as well as\nnew people that come in to the project\nuh besides that we do get some funding\nfrom the next foundation to actually run\ngicd and things like that so that really\nhelps the project um\nalso what it really means is that\nif you believe this project is here to\nstay it's kind of not going to just\nvanish overnight so\nif you have doubts about\nwhether or not you know you can trust an\nopen source project don't worry this\nproject is here to stay so\nplease\nif you have those kind of doubts uh let\nus know and we can help you in any other\nway\nand one of the cool things about this\nproject is one of the fastest projects i\nhave graduated from incubation phase\nin less than 10 months\nthank you\nand uh\nfor graduation actually it's important\nthat we uh we also are secure as a\nplatform and if you've already used\nflight you know that it kind of operates\non a zero truss security type of model\num so\nwe actually have a badge that talks\nabout the internal infrastructure\ncomponents also and how do we maintain\nvarious security practices within the\nsystem\nuh\nand again thanks a lot to all our\ncontributors again we try to keep this\nlist updated but it's not really\ncompletely updated because it\ndoesn't span all the repos that we have\nbut it's been a humongous effort by a\nlot of different people and so thank you\nthanks a lot and i think the community\nis growing\num\nall right so\nlast week we actually released uh 0.19.0\nand we quickly really 0.19.1 because we\nactually found a bug in case of like\nso one of our components that was\nupgraded uh uses a dependency on garm\nand\nin some cases when you do an upgrade the\nmigration script\nwould not correctly\nwork in the new version of karm and the\nreason was because it there seems to be\na bug in garm where it cannot resolve\nbig integer\nuh\nyou know version updates so we had to do\na bunch of changes internally are not\nmuch of changes within the migration\nsystem to essentially allow seamless\nmigration and that's why we released\n0.19.1 and we had to pull back 0.19.0\nand we are extremely embarrassed that\nactually this\nrelease\nwent out\nthrough our testing process\num and we have\nsince we've added more tests and we've\nactually changed the way we are\nreleasing now if you go to\nflight releases we are releasing a beta\nversions uh before our pre-releases\nbefore raising a release\nand we would love if folks would\nvolunteer to actually try out the beta\nversion in their test environments\nand help us\nhelp us make sure that the release is\nactually up to the standard\nthat being said our functional test\nenvironment and all those are still\nrunning\nthey are supposed to test before going\ninto pre-release\nbut there is definitely a possibility\nthat in some exotic scenario it failed\nso we would love if folks want to join\nin and help us\nmake sure that the beta release can be\nmerged into the final release\nso\ni'm looking\ntowards all of the different\nvarious organizations that are running\nflights here\nuh but along with that we had we did a\nlot of uh improvements within this\nrelease a lot of ui changes have gone in\nuh flight propeller port cluster now can\nbe ordered to be scaled you can run one\nper namespace or some\nin a round-robin sharding charted\nfashion\nand you can refer to the docs for all\nthe details of that and it's not really\na huge change from the user experience\npoint of view it's a single line\nchanging your deployment and that's it\nit goes from\na single\ninstance to multiple instances\nuh beyond that lots of changes in\nflightgate\nbasically support for 3.9\nsome edge cases that were fixed uh thank\nyou for all the contributions from\nopen source folks\nand lots of documentation updates\nactually if you've been following our\ndocs\nwe've been working a lot on talks and we\nstill continue to plan to do that\nwe uh in december we started the\nweekly uh\noffice hours and essentially haytham and\ni are currently manning the office hours\nuh eighth um\ndoes it\nseven to 7 30 a.m pacific time\non wednesdays and i do it nine to 9 30\np.m pacific time on wednesdays\num so the invite for those are\nuh\nyou available using ad event please uh\nif you have any questions or you just\nwant to talk or say hi absolutely\ncompletely\nopen office hours\nbut just be mindful that these are not\nprivate\nuh if so if you have if you do not want\nto disclose anything\nthese might not be the right forum\nbecause there might be other\nparticipants in it\nbut you know how to get in touch if you\nif you really want to discuss something\nthat is uh not\nnot open for you know disclosure\nall right uh another one we've not been\ndoing this we should be doing this going\nforward is we'll talk about that next\nweek's sync\nso for example uh there is an event\ncalendar please use it to add\nyourself to the events\num\nnext week\nblackshark.ai will be presenting how\nthey take a digital twin\nit's not next week it's two weeks from\nnow but uh because i think how they make\na digital twin of the world during\nflight\num and\npretty fantastic of what they are doing\ni hope uh i think it will be an amazing\ndiscussion\nuh we've been\nuh sandra has been really doing a great\njob releasing a newsletter every month\nfor the last three months\nand i think we have a very good format\nset up in the newsletter uh please\nsubscribe to the newsletter it really\nhelps us\ndisseminate information really quickly\ninstead of sending emails and bombarding\npeople on slack at here\nthis is more of a passive way of\nyou know\ninvolving yourself\nand\nwe also have a youtube channel so if you\ncannot\ncatch up with one of these processing\nall of them are recorded parts of them\nare also broken up and and\nrecorded\nand shared on the youtube channel\nall right what's coming up in\n0.19.2 um so 0.9.2 is slated for end of\ndecember mid uh sorry end of january mid\nfebruary\nuh it's a big release it's all the work\nthat we've been doing through december\nalmost uh many things in january\num one of them is structured data set\nsupport um this is like an upgrade an\noverhaul on flight schema\nif you have had issues with flight\nschema you'll you'll know what\nproblems are here are but if you don't\nknow we just we invite you to join next\noss thing where we'll have an end up in\nthe discussion and a demo of structure\nthey said\nuh\nwe have uh we've been working on you\nlike the latch bio team has been working\nunion type\ntype annotations both of them are slated\nto be\nmerged\nflight kit support for caching large and\noffloaded uh objects so let's say you\nhave a pandas data frame today flight\nonly does reference based\num caching that means if the pointer\nto the location does not change then we\nassume that the\ndata has not changed and we cache it\nthis works in most scenarios where\nflight is the owner of the data but if\nyou have data that's coming in from\ndifferent places or if you're generating\ndata it's impossible to know\nto know if it has changed or not unless\nyou completely perform uh the entire\nhash of the data set itself which can be\nextremely expensive so uh\nwhat flight kit\nand flight now support is optional\nuh or opt-in\nhashing for these data sets so in case\nyou know that you're generating kind of\ndata set data frame for example or a\nrandom image\nyou can\nconfigure flight or you can configure\nyour workflow it's not configuration at\nthe flight level it's only within the\nworkflow or a task\nyou can you can use action\nto\nimprove the\ncaching behavior\nthere is also uh platform level support\nfor intratastic pointing is uh it's\nmerged into flight gate um now\nthe example is getting merged and it\nshould be released in 0.19.2 what this\nmeans is if you have a\nreally long\ntraining job that's going on and you\nhave an infrastructure failure or you're\nusing smart instances or you know\ninterruptable machines instances in gcp\nand you want your\nyour training to continue\nbeyond those interruptions um\nright supports synchronous checkpointing\nat the moment and eventually you will\nalso support asynchronous checkpointing\nfor those tasks\num and the\nand it guarantees sandboxing of this\ncheckpoint retrieval of these\ncheckpoints and and restoring all these\ncheckpoints for subsequent ones\nso that's in now uh we would love your\nfeedback um this has been requested by\nsome team so this should really work\nlots of ui features and i think i'm not\ngoing to steal the show from jason he's\ngoing to talk more about this\nbut\nkeep your eyes open\nall right enough of me um\nwe have\ntwo interesting talks one of them is by\nstephen and tim who are here\nand another one of my christian so\nstephen all over to you"
    },
    {
        "title": "Adopting Flyte at Gojek - Jan 11 Flyte OSS Sync Up",
        "transcript": "all right uh thank you for uh this\nopportunity for sharing uh\nour\num experience in adopting flight at\ngojek uh i'm maria from data science\nplatform team uh gojek\num\nyeah so let's get started this is the\noutline of uh today's sharing session so\njust give you some introduction uh\nand then show you how we use flight\nproject what we love so far and then\nwhat we are planning to do in this year\nso yeah some of you\nmay have heard gojek uh some of you\nprobably don't so go jack is an\non-demand multi-service platform that\noffered mainly in the southeast asia\nactually our biggest market is in\nindonesia we provide uh\nmore than i think 20 products suffering\nfrom ride hailing food delivery service\npayments\nand other\nsmaller business such as uh\ndaily needs or ordering ticket\nfor movies and so on and currently uh in\n2021 uh the scale that project operates\nis the kojec ecosystem contribute to\naround 1.7 percent of indonesia's gdp\nand indonesia itself is in the top 20 um\nlargest gdp in the world so that's uh\nyeah\nto give you some\nidea about the gojek scale\nand\nat gojek we use a lot of machine\nlearning\nin some of our products for example we\nuse ml for uh optimizing for chara\nlocation\ngenerating pickup points for our right\nhandling business uh generate optimize\nprice and also performing matchmaking\nbetween our riders and our customer so\nwe have firsthand experience of a\nsuccessful ml project and the company\nitself is\ninvesting quite a lot in ml\nthe first thing that we invest on back\nin i think 2017 and 18 was uh\nbuilding ml pipeline\nat gojek\nback then we started with\nsome kind of a\nsmall\nabstraction on top of airflow where\nwe're strict user to only orion\ncontainer so back then we have like\nproblem about\ndependency management and so on\nand\ncreate like uh the dependency health\nsince uh every uh\nworkflow that we have you'll need to\nhave a uh same dependency so we separate\nthat uh using the container and then\nyeah we built this a small abstraction\nso far it has been successful and uh\ntoday we serve around thousands of uh\npipeline at gojek\nbut then there are a lot of pain points\nthat our users uh currently experience\nthe first one is that the main\npain point is the less than ideal\ndevelopment experience so\nthe strategic move of restricting only\nrunning container creates like problem\nin term of like testability and also a\nlearning curve\nsome of our data scientists uh is\nare not like uh expert in like building\ncontainers and so on so they have to\nlearn that part and also uh\nthey are not able to do a testing in\ntheir local machine so they have to do\nit in our shared staging environment\nwhich creates which prolong the uh\niteration cycle the next one is that uh\nin airflow\nor at least in our current airflow that\nwe are running\nwe have a scheduler scalability issue\nso\nuh as we have more and more uh pipeline\nin the platform we tend to see that\nat some\nbusy hour that the scheduler start to\nperform and start to degrade and then it\ncreates like a delay in the starting\ntime of our pipeline and of course\nthere's also reliability issue where we\ncan't have like a highly available\nscheduler to be deployed\ni think this is uh\nprobably fixed in the more recent\nversion of airflow but yeah this is what\nwe currently experience there's also a\nsmaller problem that quite frequently\nhappen is the missing dags so airflow\nhas this um\nairflow uh bag\nuh from the deck bag that uh to store\nall the a pipeline\nand we tend to see that sometimes there\nis a missing dag it's not getting picked\nup by the airflow itself so it's a small\nbut\nannoying problem that can be have\nand lastly it's uh there's\ninfrastructure overhead so currently we\nhave uh we run it on the vm so we have\nseveral tier of vm uh for running a\ndifferent kind of workload\nand since uh\nwe can't really do like a\ngranular resource request such as\nkubernetes uh it will create like a lot\nof um overhead in\nin\nfor running a workflow\nand\nthose are some few\npain points to currently experience and\nyeah we would like to uh what we get\nhow we get into this point probably\nwon't get us to the next several five\nyears so we will have to change this\nand what we want is that essentially\nfirst we have to make sure that\nwe provide the first class development\nexperience ideally there's a local\nexperience and\nminimize a boilerplate code and so on it\nhas to be data where\num this is\na problem that we recently experienced\nis that it's hard to track data\ndependency of data lineage and we would\nlike the platform to have a built-in\nability on that\nand especially for ml uh it involves a\nlot of data and yeah having the platform\nto be a data where is uh critical it has\nto be scalable at least we can handle uh\nan order or two order of magnitude\nfrom what we currently have and lastly\nwe want to be a kubernetes native\nso the team itself is building expertise\nin the uh operating or building a\nsoftware on top of kubernetes so you\nwant to leverage that into\noperating this new system as well\njust for\na brief\na\nfew uh this is the current development\nexperience so imagine this is a this box\nlarge book is a task that uh need to be\num implemented by our user in clockwork\nall of them has currently to be need to\nbe implemented and pay them and it\ncreates a lot of wealth update code for\nexample they need to transfer the input\nfrom some temporary storage which uh\nusually use gcs to input validation and\nthen the core of the task itself the\nbusiness logic is just a minor part of\nthe task\num\nand other than that there is\nthe platform doesn't know about what\nhappened within the task and uh if the\nplatform want to know about data\ndependency will have to build the tool\non top of it which is uh probably not\nthe best\nuse of our time\nso we ideally see that the the\ndevelopment experience should be like\nthis users should focus on their\nbusiness logic and the platform handler\nthe rest\nthe first one is that yeah\nit will\nmake sure that\nusual user is doing what they need to do\nand then at the same time you want to\nalso ensure that the fathom handle\ncritical aspects such as data dependency\nand\ndata lineage\nuh so yeah in the\nlast semester 2021 we started\nevaluations of several tools in the\nmarket that will satisfy our\nrequirements and eventually we just fly\nit on this and yeah we are\nhappy that uh\nwith our experience so far with flight\nso what we started is we built of course\nthe infrastructure how we\ndeploy flight in gcp there was a\nlearning curve along the way when we\ntried to deploy a\nour\nflight in our infrastructure but\neventually we we settled with the three\nseparate environment we have a\ndevelopment experiment where\nwe can use it for our internal tests\nwhere we uh probably\nuh do some uh upgrade of the to the\nlatest flight and or we build some\ncustomization there uh it's our for our\nteam internal use and then we have\nstaging environment where uh it is being\nused by our data scientists to pass\ntheir workflow before going into\nproduction\nthe next thing is that we want to ensure\nour platform is self-serve\nand the very first pain point that we\nwant to address is about uh\nhow the pipeline uh receive credential\nso our pipeline uh\nis usually interacting with uh many\ngoogle services\nsuch as bigquery gcs uh vertex ai for uh\ntraining and so on and you want to make\nsure that\nthere is no manual step when there is a\nnew project getting on board so what we\ndo is that uh\neach and every project that is getting\non board we make sure it will\nautomatically generate a credential in\ngcp it's called a google service account\nand then this google service account uh\nis created when we create the project at\nthe same time this google service\naccount is bound to the kubernetes\nservice account which is being used for\nrunning the task\nand\nwe give also the default potential such\nas accessing the bigquery\nor gcs the temporary storage or flat so\nthat\nin one step\nof creating project and then submitting\na simple workload it all just work for a\nnew project\nand other than that we also built some\nintegration with our project machine\nlearning platform\nso what we have been working so far is\nthat in terms of integration we built\nintegration with three of our core\nproducts the first one is merlin it's a\nmodel deployment platform so we have\nseveral tasks such as\ntasks for building a new model\nuh deploying it into some environment\nand then uh do some uh blue green\ndeployment\nwe built also some\na flight kit plug-in for interacting\nwith our feature store uh mainly to do\nuh offline to online injection and then\nto do uh historical data at level and\nlastly we built\na flat kit integration with our\nexperimentation platform other than that\nwe also\nthis is based on the\nrequests from our data scientist where\nthey work a lot with the dvd so we also\nbuilt a simple dvt plugin where it we\ncan\nexecute dbt project and also run the\ntesting this is just a simple snippet of\nour face plugin so\nrunning offline to online json is just a\nfew lines of code where the user can\njust\nspecify the configuration\nuh create the\ntask and then yeah of course create the\nworkflow which trigger\nthe\nthe the task itself\nuh and lastly what we do is to focus\nalso on the onboarding effort so on top\nof providing the infrastructure we also\ndesigned how can a user getting\nonboarded into the infrastructure and to\ndo that we also create a repository\ngenerator a cookie cutter essentially\nthat\nprovides a\ngeneric\nworkflow such as model deployment\nmodel of future engineering and then\nalso we provide them a\na simple ci cd pipeline that\nwill help them to deploy this into a\nstaging or a production environment i\ncan give you a brief\nsneak peek into this\nlet me\nshare\nyeah so\nthis is the\nsample generated project from the\ncookie cutter so essentially\num\nthe\nproject will generate two workflows the\nfirst one is feature engineering and in\nthis engineering we have two main uh\ntasks the first one is uh the future\nengineering which in this case it will\ntrigger um a dbt\nproject\nuh dpt run and then also do data\nvalidation using dbt tests\nthe\ndb2 project itself is located here so we\nhave a deputy project uh that a simple\ndvd project it's coming from their\nsample project\num and then also do some future\ninjection into feast\nnext thing is\nour common generic pipeline that we\nusually build is a model training and\ndeployment so in this case we\ncreate a sample project which\ndemonstrates how they can build a simple\ndeployment pipeline\nfor training and then uh building and\ndeployed test and model and then\nperforming some uh\nblue green deployment\nuh and all of them leverage the\nintegration that we built with our mlp\nand yeah and also we provide them with a\nsimple\naci so that they don't need to build\nthemselves\nin this case uh we help them to be able\nto\nhave workflow like\nthis uh so there's a testing stage where\nthey can test their workflow and so on\nand then there's also a stage where they\ncan publish the\npipeline to staging and then when there\nis a\ntag getting uh\ninto this repository they will be able\nto deploy it into a production as well\nso this will help them to\nat least\nhave pointer on to get started and uh\nyeah\nyou can focus uh more on the building\npipeline uh instead of like other aspect\nof a boilerplate kind of work\nthat's uh what we do also in terms of\noptimizing the onboarding process\nso so far uh\nin the last semester we onboarded one\npilot project uh for\ninternally and then we are planning to\nalso um\nyeah\nopen up to the rest of the organization\nthis year so\nuh yeah that's our plan this is\ncurrently what we love from flight\nthe first one is\nfrom flight kit itself it provides a\npowerful and expressive way of building\npipeline uh some of the things that we\nlove so far is the dynamic workflow and\nflow control it's quite powerful in\nbuilding the pipeline sometimes we\ntend to forget that the flight kit is a\ndsl and then yeah do some expect\nsomething like uh\nto use it like a normal python\nprogramming language uh that's a typical\npitfall that we currently experience but\nyeah so far it's uh it's been awesome\nuh the next one is of workflow\nfashioning this is quite important\nbecause um\ni think\nuh when it comes to productionizing a\npipeline there's only a few platforms\nthat provide this kind of versioning and\nto us it's critical when uh for use case\nwhere we want to roll back to certain\nuh workflow version in case there is a\nbug getting introduced into our uh\nproduction uh pipeline\nit's also extensible so\nwe can build a\nintegration\non top of what it's currently provide in\nthe open source\nuh yeah we have this mlp integration and\nalso we added some debit other\nintegration suggestivity it's also fast\nand scalable so during our evaluation\nstage we did some\nstress tests\nto understand whether flight can satisfy\nour\nrequirements and\nit was\nproviding us with a good result and\nlastly yeah it's been an awesome journey\nand uh\nwhen adopting flight\nit has a private vibrant and also\nhelpful community when uh and that\nhelped us getting onboard a new flight\nyeah so\nwhat we are going to do in this year is\nof course to uh build cost a closer\nintegration with the gojek mlp we want\nto make sure that uh\ncurrently it has to uh it's it has\nseparate dashboard from our internal\nmachine learning platform we want to\nmake sure that it's in the same place so\nthat uh\nit's\ndiscoverable by uh the rest of the\npeople and you want to have also some uh\nfull secret storage integration uh\nwithin this quarter probably we will try\nto integrate that and lastly yeah we\nwill open up with\nthis flight to the rest of the\norganization and\npush adoption\nall right that's all i have uh\nif there's any question i'll\ntry to answer them\nuh arya that was fantastic thank you so\nmuch for presenting um\nwe one of the most interesting things to\nme is like when we started at lyft\nthe workflow was like kind of similar to\nwhat you've actually ended up building\nand we i don't think we ever discussed\nabout like how we did um\nlike this is really ml ops right where\nfolks can build and it gets like nicely\nversioned and deployed and the\nindependent\ni am roles and so that people can't like\nyou know affect each other's floors and\nyou've kind of ended up with that\nclassical flow that we we started with\nas well\num so this is great like great to see\nthat\nyou were able to do that\nand you can get value from the platform\nyeah\none question over there so would you\nlike so we are working on certain things\nbut would you be\nand we should probably collaborate on\nthis but when you do the blue green\ndeployment would you want a manual uh\nintervention at that point to say\nwhether you know\nshould i go ahead or no\nor do you think it's like completely\nhandled in your system\nit's currently a completely handle in\nour system uh\nthe blue green deploy is usually uh\nhappening automatic uh so after of\ncourse a\ncomprehensive testing like we run load\ntesting before we switch to the new\nmodel we run also some functional tests\nto the model\nand and even after it's getting uh the\nproduction traffic we also track uh the\nimpact of the the new model uh to our\ncritical business metrics\nso yeah it's it's automatic\nwow that is amazing\nany questions from anybody else\nand where do i think you're mute\nor maybe your microphone isn't correct\ni still can't hear you\ni'll go well eduardo is fixing his thing\num do you have any\nthoughts on requirements when you said\num\nuh more like data awareness and data\nlineage do you have like um\nlike is that a well-defined ask\num\nit was not\nuh\nyeah it's not quite like materialized in\nterms of requirement but the first uh\nbecause we haven't\nfully like uh work on committing on that\nkind of work but the first thing the\nfirst step that we want to make sure is\nthat the platform itself will have to\nunderstand or at least handle taking\ncare of the data that's getting produced\nor\ngetting consumed by the task so that's a\ngreat start because by knowing those uh\nthat means the platform itself can\ncreate a feature where\nthey can expose this kind of\ninput output lineage into some\nsome kind of dependency graph and then\npresent this to user so that's what we\nthought initially of course we haven't\nlike fully fleshed out the idea of the\ndata\num\nlineage itself uh\nwhat we currently need\nbut we did i did some like a quick\noverview uh like research and what's\nhappening in in this uh area and we find\nthat there's a one project in\nuh lfi uh it's called open lineage which\nis the closest thing that i can say uh\nthe the to\nwhat's the ideal stage we want to\nachieve in that uh area\nwell yeah we're i'm gonna try to have a\nchat with them at some point soon so let\nme know if you're interested in that\noh yeah sure poorly surely yeah he has\nbeen really thinking about the data\nprevious stuff so yeah i think you and\nyou\ni would love to be involved as well\neduardo\nnice can hear me now\nyes cool um\nhi this is fantastic presentation this\nis great um i have a question about\num some of the integrations that you\nmentioned um\nparticularly dbt and fees like\nare you guys thinking of um open\nsourcing\nthat at any point or this is like too\ntight to your own link\nmight not be for for the dvd we are\nthinking like to actually open source it\nat some point uh so yeah we will have to\nclean it up a bit and then uh\ni'll coordinate with the rest of flight\nteams to how to open sources and for the\nfish\nplugin um i thought that the flight team\nis already\nstart working on that uh we we can't\nhelp on uh\nyeah uh pushing our version to the open\nsource but the problem is that we are\nrunning a bit older version of this uh\nit's a branch of\ni think today the fees version is\nalready\nzero point something so yeah that's the\ni think the blocker that we currently\nhave and open sourcing the our of this\nplugin\nyeah there have been some other teams\nalso fast for the dbt\nplug-in so that would be fantastic area\nand and just i also saw that you are\ninterested in what i think the world\nteam has but what\nvolt which is door dash is dms also\num i think they merged in one plug-in\nfor the back end\nso you should be able to we should be\neasy enough i i don't recommend the\ndocks but it should be\nand please let us know if you see any\nproblems\nsure\nall right any other questions\nall right\nif there are no other questions i think\nthis was our our core presentation today\nso\num\nwe are uh special edition today you know\nwe'll conclude it from next time i think\nthere might be two weeks from now it's\nalso get again maybe 30 minutes\nsuggested or something\nuh because volt team is going to present\ntheir usage um and i think\ncurrently at least we think\ntwo weeks after that\nuh\ni think the black shark team is going to\npresent if i'm not mistaken\num so\nlots of interesting presentations coming\nin\nmoreover\nnext time we'll also be doing a ux down\ntown hall come on a ui\ndeep dive into all the features that are\ngetting launched in january\num so yeah please join in i think it\nwill be\nvery interesting um to get feedback on\nthe ui parts that and jason from my team\nwho's been leading the work on the ui\nwill be presenting it so\nall right thank you guys for joining and\nuh thank you aria for all the\npresentation\nthanks for\nwatching no no please do and anytime you\nhave an update you want to present more\nwe would love to have you\nthank you yep\nthank you"
    },
    {
        "title": "Flyte Community Update 004 - Jan 11 2022",
        "transcript": "right um\nwelcome to uh\nto a new time\ntoday\nfor our uh biblical community thing\nfirstly happy new year\nwe this is actually four weeks after the\nlast one that we're meeting we took a\nwinter break\nuh hopefully everybody enjoyed their hot\ndays and\nand are back and ready to go\nuh we have a lot in store today so let's\nget started\nuh so the agenda is we will just you\nknow we wrapped up a year uh one what a\nyear so we quickly do a set of\nhighlights\nwe'll talk about uh\nwhat's coming in the roadmap and how\nwe're thinking about it uh at least\nwithin the\nunion.ti team\nand any of the core contributors who are\nhelping with the road map\nagain the road back road map is\ncompletely open so please feel free to\nchime in if there are things that you\nspecifically would like in the\nuh to be prioritized or would like to\ncontribute to you would love for\nfolks to you know\neither contribute on issues or create\nanalyses and so on\nand then we'll talk about what's coming\nin january uh\nit's already underway uh we\nwe just released 0.19 today\nand there will be 0.19.1 which is end of\njanuary early february\nand then the most important\nthing in the end of the day we have a\ndemo or slash presentation by\npradeep from gojek who's here\nall right so uh 2021 highlights we\nactually started the year um\nfairly like even though flight was open\nsourced\nin 2020 we really think that the open\nsource year was 2021.\nuh we grew the community at even in\nslack from 100 to 710 plus people and\nit's growing\nwe\nwe don't think it's like all the people\nthat who use flight it's just\npeople who have some questions who join\nin\nuh\nearlier in the year last year we new\nflight kit sdk\nwhich which is what everybody's using in\npython now\nwe did more than 20 integrations and\nlots of performance improvements scaling\nimprovements recoverably recoverability\nand lots of new ui features\nand i think going forward in 2022 this\nsets up a fantastic stage for us to do a\nlot more\nand and we have a full roadmap\nlots of companies uh joined in and and\nare using using plugin production\num we\nwe actually did lots of contributors\nmade their first commit last year\nuh and it's great to have all of uh all\nthe folks within this\namazing community\nuh for the union.ai team we actually led\na tax passion about bash\ntowards the end of last year\nand we\nwe also let development of functional\ntests\nand so if you\nif you have not seen it you can go to\nthe\nread me and we are actually trying to\nput it in the even much more uh\nyou know it's a better place but you can\nbasically click\nand see\nevery test that's run nightly\nand\nthe latest release version so 19 will\nget added over here\nand this is a test matrix that we are\nadding\nuh the eventual goal is every\nintegration\nuh also has a test uh this test\nsetup itself is open source\nso\nand\nif it's not open so we are actually\nworking on open sourcing it and anybody\ncould help us contribute to various\nother cloud platforms\num to run these tests these are\ncurrently run on aws and\ngcp but if folks have on-prem setups and\nthings like that we would love to hear\nabout that\nall right um\nand and i think the most important thing\nas i said i think this\ni'm really truly proud to be part of\nthis community mostly because everybody\nin this community is so helpful\num there have been people\nwho\nus like fighting an infrastructure tool\nand catering to\na lot of machine learning uh engineers\nit's it spans yeah\ninstead of\nthe users have a varying set of\nknowledge some people are like have used\nkubernetes in the past\nuh while some people have not even used\ndocker in the past and i've seen folks\nhelp everybody across the\nacross the gamut and i've seen\nas people join in they learn a lot more\nand\nthey have actually you know asked\nquestions and improved their workflows\nthemselves and this has been amazing i\nthink the community is great\nso thank you for everybody for being\npart of this community and let's keep up\nall right um just uh this is from the\nlfx insights um dashboard\nuh\nflight\nactually i forgot to add\none thing which is quietly mentioned\nover here flight is one of the fastest\nprojects in linux foundation to have\ngraduated from it from the incubation\nstate\nand this announcement has not happened\nthis is the first time we are actually\ntalking about it in the\nmeetup but announcement coming soon\nuh watch what graduation means it's a\ntop-level project that is supported by\nthe next foundation and also means that\nthere is\nuh funding and we think that the\nproduct uh the project is here to stay\nall right um\nas for commits we have about\nwe made about 11 000 commits\nuh i don't know how this metrics is\ncalculated but i i think when i did like\nthe math on myself it's more than six to\nseven thousand comments i think these\nare individual comments within those\ncomments uh and we have about 180 active\ncontributors with about 50\nuh\ncontributors drifted away this month and\nyou can see the graph on nfx insights\nand once again thank you to all the\ncontributors uh\nthis does not include every single\ncontributor\nso we are really sorry but\nthis is you know a combined list please\nadd your names we try to keep it up to\ndate as as someone possible so\nbut thank you for everybody who's\nwritten\nuh contributed in any way to the project\nall right so today as i said we released\n0.19 and going forward we're doing\nquarterly\nuh minor version bumps\nand we\nwe are also having a code name for every\nrelease so this release is the eagle\nrelease\nthe idea was to actually improve uh\nthe ui\nand essentially set the stage for\n2022\nand so\nin ui we've added a bunch of features\nand we'll go over them in a bit\num\nin flight kit itself\nwe had some back-end features which are\nnot enabled uh they are now enabled we\nsupport every uh all the all the\navailable versions of python\nincluding 3.10 just added recently\nuh and much more pythonic in terms of\nusage\num\nwe\nwe a lot of performance related stuff um\ngreat work by dan\nuh\nbasically makes it possible to\nhorizontally scale flight propeller now\nand it can be automated\num talks on that please refer to the\ntalks on that\nalso uh we added like we did a docs bash\nabout kill 45 plus stocks issues and you\nknow more\ncoming soon\nuh\nall the configs are documented every\nconfig knob existing in the system is\nnow documented every\nlike\nimportant features that we're missing in\ndocumentation like how to regress events\nand how do you actually react to those\nevents it's not documented for example\nuh we could regress events and create a\nlineage graph in an external system like\namundsen or\ndata hub etc and more on that also\ncoming soon\nuh\njust to go over a couple of the ui\nfeatures um the ui summary search\nexperience is now released so you can\nactually see\nfor a workflow what were the last 10\nexecutions were they successful or no\nand if you click on them it actually\ndirectly navigates to the uh\nto the execution itself the execution\ndetail page itself has improved on a lot\nmore coming in january stay tuned\n[Music]\nwe uh just like a super fast filter\nuh that's visual so if you we show in\nevery single workflow page we show the\nlast 100 executions and uh\nand they're\nthe height of them or their height\nindicates that the time it took to\nexecute that execution\nand so if you click on one of them it\ndirectly navigates to\nthat specific execution\num\none of the features as i talked about in\nflightgate is called cache serialize\nequals to true um\nand this is uh interesting and it was\nactually heavily requested feature this\nenables\nat least once computation of the cache\nkey so for example if multiple\nconcurrent executions begin\nand you\nwould like\nonly one of them to really compute\nthe\nthe data set for example could be a\nfeature set it could be a next very\nexpensive computation\num\ncurrent behavior is that if they all are\nconcurrent all of them will proceed\nbecause\nflight actually doesn't know which\nexecution\nmight succeed right it's possible that\nthere could be other failures and so um\nit tries to do optimistic concurrency\nover there and you know whoever wins\nactually writes the cache first\nnow in this case we have uh we have\nintroduced a sort of persimmonistic\nconcurrency or like spin locks\nuh if you enable cache serialize equals\nto true you\nwe\nflight will now wait uh if there are\nconcurrent executions that have a\nsimilar cash signature then it will wait\nfor\nuh the first one to complete or a\ntimeout to expire\nthat way we don't block forever because\nit's possible that\nin a fully distributed system things\ncould uh\nthings could just die and and\nuh it's very hard to actually do failure\ndetection so we we introduced a concept\nof a lose these in this case\nand this uh has is now released it was\nreleased as part of the previous\nplatform release but now it's part of\nlike gate so going forward you can just\nput cash serialize equals to true\nand let flight do the rest\nwe also started uh i guess in the last\nmonth we introduced office hours um\nhaytham and i uh manned the office hours\non wednesdays\nin the morning 7 to 7 30 a.m uh pacific\ntime\nand 9 to 9 30 p.m pacific time uh the\ncalendar invites for those are managed\nto add event so please\num\nadd these events to your calendar\nuh the idea is to\nit may not always be hitham and i but uh\nthe idea is that uh various maintainers\nfrom the project will\nhave these 30 minutes available for\nanybody to drop in ask questions\ndiscuss ideas\nor use cases\nit is an open forum so anybody\nwho has ideas or questions please join\nin\nwe we've we've done about three of those\nuh\nor four maybe uh i'm not 100 sure but we\nhave we've had at least one person\nuh joined in every single one of them\nand we have uh\ni would like to think that you know we\nwere able to answer a lot of questions\nso please utilize this time\nuh there have been times where some\npeople have presented their ideas as\nwell\nand we don't record these sessions\nbecause these are not meant to be\nbroadcasted but mostly\nuh just a pure face to facetime for\nfolks\nall right roadmap\nso our 2022 high level plan uh basically\nas i said we've 20 21 was\nthe layout we actually have laid out\nflight so that we can build uh amazing\nexperiences in 202\nuh so one of the goals that we're\nstarting with is to provide one of the\nbest mln data aware orchestration\nplatform experiences\num\nand in that journey we want to make\nflight accessible to more users data\nscientists and others through ecosystem\nbuild out so\nthat a lot of ecosystem things that\nwe're working on and working with\npartners and other open source projects\nand if you have any other ideas if you\nif you would like to contribute to ideas\nwe would love to\ncollaborate\nuh\nmoreover we've we've heard a lot of\nfeedback about the ui\nfeatures and we absolutely have ordered\nand\none promise we want to make is that\nwe're going to provide one of the most\ndelightful user experiences that\nanywhere\nany person who's using flight\ngets we also want to supercharge flight\ndata lineage that really help\npeople understand where the data is\ngetting created how it's getting used\nand so on\nuh and that may be through integration\nuh one of the other things that really\nworked well and we've realized and we've\ngot pretty good feedback on\nis um\nhow we can how flight is seen as an\nintegration platform for various of the\nopen source and mr technologies\nuh like for example distributed training\nto\nuh to\nyou know model\nserialization\nto many other things that we've worked\non feature engineering et cetera\nalso moreover we want to blow past\nyou know the scale boundaries that we\nhave had in the past\nwhich could be sites of the workflow and\nspeed of the execution itself uh and\nyou'll see some fantastic work being\ndone in this area specifically\nwe've have we have folks who are running\ntax of the size of a million nodes\nuh and there are some people who want to\nrun uh workflows that only last for a\nfew seconds\nand so we will be attacking both these\nproblems in the year\nall right but uh\nuh with the first\nmonth of the year we are we actually\nwanted to begin pretty strong\nand one of the first things we are doing\nis if you've gone to flightgate python\nyou might have seen oh why is there so\nmuch code\nthere's only\nwhile the used amount of code is\nprobably only\n30\nso\nflight kit actually has a legacy api\nthat was used by\nlyft since 2017\nor 2016 uh 2017. and\nit's really old uh because of which it\nwas made for python 2.7\nand\nwe are completely deprecating it\nthis month so all of that code will be\ndeleted\nit does not impact anybody\nany of our users anymore\nbut it's a good time to get rid of that\ncode and and i think the the\nlike 5k8 will be way more accessible to\ncontributors after this\nwe are also supporting a lot of ui\nfeatures like dynamic workflow graphs or\nvisualizing dynamic workflows in the ui\nmap tasks we have a brand new experience\nfor map tasks\nin the ui and stay tuned timeline view\nso uh how\ni think we have had a lot of questions\nabout how things have progressed and we\nyou folks wanted a quick way to dive\ndeep into a performance of attack and\nfigure out which stack took how much\ntime\nuh until we've been able to real-time\ntracking for performance in the ui\nitself\nuh and again this is going to be v0 and\nwe'll continue to write rate on this um\nalso uh this was a heavily requested\nitem you should be able to archive all\nentities within the flight\nui from projects to executions\nand also support for basic\npersonalization so you should be able to\nlike filter every single entity by\nwhat you have created\num along with that specifically for ml\nin fantastic pointing it's already in pr\nthis allows you to actually resume\nfor a retry any model training\nusing previous checkpoints and all of\nthis is automatically handled by flight\nthis makes\nusing spot or interruptable instance is\nextremely easy\nuh and actually efficient\nbecause you don't want really like one\nof the biggest complaints of using\ninterruptable instances is that you are\nnow running a model training cycle that\ntakes eight hours and it was seven hours\nin the instance was interrupted\nso this solves that problem\nfor our union type and optional type\nsupport um\nand\nfor type variable metadata this is\ndocumenting your you know variables and\ndatasets and so on and thank you for\nthank you large fire for both of these\nfeatures\nuh also\none of the one of big updates in\nhow we handle structured data sets is\ncoming in um\nthis this month\nuh the it should be available in the\nnext beta release\nthat allows essentially to\nhave very\nstrongly typed arbitrary shape\nstructured data sets\nand backed by pi arrow and you can\nactually serialize\nor persist the data sets into\nvariable data stores for example\nyou could use bigquery as the backing\nstore\nfor your data sets and then this is\nextremely useful where you're like doing\nsmall data set processing in bigquery\nand you want to interoperate between\nbigquery queries and\nkind of data frame and put it back to\nbigquery\nuh moreover um lots of updates in shell\ntasks itself uh thank you so much\nfor all the work um\nwe are also working on customizable\ncaching so this was another feature like\nif you use canada's data frame\nhow do you really cache a pandas data\nframe so we allow now client-side\ncomputation of the\nhash or the cache key\nmoreover thank you lyft for\nmulti-architecture support\nand all of flight snacks will be\nmigrating to the multi architecture\nsupport this allows\nrunning\num\nor having multi multi architecture\ndocker images and then targeting the\nright architecture\nin production\nso for example you can have a node pool\nthat's using the graviton or any of the\nother\ncool architectures that\nuse arm machines while in some cases\nit's probably preferable to use x86 and\nso you can have some other\ncomputations going to x86\nand lots of docs improvements"
    },
    {
        "title": "Improvements to the Flytekit Dataframe Handling",
        "transcript": "um okay\ncool so this is a quick\nuh lightning talk on\nthe improved\nuh data frame handling\nsemantics that we are proposing\nso let me jump straight through it um\nwe are going to talk about why we're\ndoing this\nuh what we are proposing and then\nuh questions from or suggestions and\nideas from the\naudience so if you have worked with the\nflight schema object at all\nyou may notice there are a couple\nlimitations\nor specifically one limitation around\nthe ability to use um non-primitive\ncolumn types\nand that's something that's hasn't been\na\nuh\na super\nmajor issue with their users but it is a\nkind of a strange limitation to have\nand it is um\nsomewhat indicative of\nlack of left maturity possibly so hoping\nto address that\nand then\num\nat least for me when i was writing\nextension code it seemed like there may\nbe a better api like a more\nuser-friendly api so\num\nwe are basically exploring whether\nwhether that is possible\num\nso the world that we want to get to is\nsomething that looks like this uh so the\ntop example\nyou can still\njust one thing bigger um you can still\ntake in um in this case but you can\nstill take in and return simple pandas\ndata frames or any other\ndata frames uh including custom\ncustom data frame libraries if you wrote\none\nand in the bottom example um if you are\nusing a data frame that doesn't have\ncolumn or schema behavior you can\nannotate it\neither by supplying\nuh fields which is kind of the the\nexisting um\nkeyword types framework uh notice there\nare more complex types you know\nor using possibly an aero schema or any\nother schema for which flight kit\nunderstands\nif you want\nnon-default behavior for your data frame\ntype you have to use the wrapper object\ncalled structure data set this is a new\nobject that we would introduce\nthat can wrap\nany data frame and allow the user to\nspecify additional behavior\nspecifically around the storage\nprotocol like s3 gcs which would be\nderived from the url\nand a format\nand we we're going to maintain the\n[Music]\nexisting syntax of giving\nthe the wrapper class\nthe uh the type the python type that the\nuser would like to see and then\num either an all or an editor depending\non whether you want to receive\neverything or if it's if the underlying\ntransformer\nsupports iteration\nso\nuh implementation in the idl this is the\nmain change that we're going to make\nfeel free to take a look at the pr if\nyou need but basically adding a new type\nwhere it there is a repeated set of\nuh basically name and sub literal type\ntuples\num adding a format\nwhich i believe is new and then\nuh\nan external schema this is where the if\nyou had used the arrow\nsyntax that's where this would go\nwith respect to\ncontributions and extensions um this is\nthe ux that we are proposing\nso instead of having to deal with the\nschema engine and the type engine\neverything is\nuh basically in will be written in two\ninterfaces two concrete classes an\nencoder and a decoder\nuh they are only registered with the\nvery long long named structured dataset\ntransformer engine\nuh and that engine will take\nwill be the one that actually registers\nwith those python types with the type\nengine itself\nuser signatures are just always the raw\npython type the data frame type or the\nnew structured dataset type\nand\nthe\nencoder and decoder that is used is\ndeterministically picked by the engine\nbased on three things uh the storage\nprotocol the format\nand the python data frame type\num let's see here\nuh uh so\nbasically if the user just returns the\ndata frame type then you get the default\nbehavior if you want to customize that\nbehavior or if you want like lazy\ndownloading or iteration then you have\nto use the new object\nand the reason we started this a while\nback the reason it took so long is\nbecause we we spent a long time trying\nto play around with whether or not it\nwas possible to add intermediate formats\ni.e make the\nthe conversion to and from\npython and flight uh\nlike two steps so that users\ncontributors might be able to just write\nsomething that\nserializes to like an era record match\nor something and then flight kit would\nhandle the rest of it but\nit it was too\num we couldn't\nfind a good contributor experience for\nthat so\nafter some thinking we just went back to\nthe original\nuh the original design\nuh there is a poc implementation\non flight kit at structured dataset\nproposal thank you kevin for writing\nthat\num\nand basically\ngoing through uh going through that that\npr you will see uh\nyou will see these guys so this these\nare what you would need to\nimplement to\ncontribute a new transformer for a new\num\nfor a new data frame type so obviously\nwe'll have spark and pandora and pandos\nand all the all the usual stuff\nbut you basically need to write an\nencoder which takes in\none of these new python objects and\nreturns one of the new literal objects\nand the decoding is the same thing uh\nnote here that you can return either the\ndata frame\nor a generator of the data frame so\ndepending on whether or not your\ndecoder supports iteration\nand the engine handles the rest of them\nso it handles registration with a type\nengine\nuh registration of handlers picking the\nencoder and decoder and then derivation\nof the flight\nliteral type so it will go through the\nannotations and\n[Music]\nfigure out what the columns are in the\nexternal schema if any\nuh we are going to leave flight scheme\nas is for now so users can continue to\nuse it i don't think i think we may\ntry to\neither update all the plugins or figure\nout a way to use the current\ntransformers with as handlers within the\nnew framework\nbut the flight schema support will\nwill remain for the time being\num\nand the poc isn't entirely done so\nassuming that there's no\nmajor changes uh major complaints from\ntoday or\nissues that are unforeseen as yet we are\nhoping to\nbe done with it by the end of this month\nand then we'll probably release it uh\njan next year and there's also back-end\nchanges that need to happen um\nuh in line with that\nuh the dock is still a work in progress\nonce it is cleaned up a little bit more\nwe'll\nmake an rfc link for it but there were\ntoo many comments\nto\nreasonably use hackmd for this one\nand i think that is it yeah\nquestions\nso that's\npretty cool and i think a pretty\nflexible api\num are we going to\nship in apologies if you have answered\nthat already are we what uh extensions\nare we going to ship with like out of\nthe box that uses this\nuh i think everything that like\ni i think i will we'll try to write\neverything that is there today so\num\nwe'll see how\ndifficult that is but at the very least\npandara spark pandas\num i think dolt is doable\num\nyeah\nnice"
    },
    {
        "title": "Caching Non-Flyte Offloaded Objects",
        "transcript": "my name is eduardo i work on the uh\nunion ai team and today\nwe're going to be talking about caching\nof offloaded objects\num\nquick agenda first we're gonna talk real\nquick about what caching even means in\nflight context um talk about caching\nsemantics how we represent some complex\nobjects\nflight and finally the feature that we\nare proposing called um\ncaching of offloaded objects\nopen up for q a and you know put out\nsome references because there's some\ninteresting work there so let's do it\nfirst of all what is caching um straight\nup from the docs\ncaching\nthis is the ability that flight provides\nto cache the output of tasks okay it's\nthe output of task executions it's not\ninputs it's not like actually executions\noverall it's like for specific tasks\nthat we all know and love in in flight\nthe rest is not so important um\nbut then you have to think about like\nokay so how do you actually map\num to those outputs you have to think\nabout like how how do you calculate the\ncache keys so again from the docs\ncache keys are composed of project\ndomain cache version tasks signature and\ninputs\nthe important thing here for for this\nfeature really is inputs because\nto think about it like flight allows you\nto pass in like\nfrom\nsimple you know primitives like integers\nand floats to\nthe most complex object that you can\ncome up with\nand um it's not\nsuper trivial to like come up with a\nrepresentation of objects that allow for\nlike good cache ability so\nso\nlet's talk about this\num\nreal quick just to you know refresh for\na\npeople who have been using flight but\nhaven't used cash tasks\nthis is how a cash test looks like this\nis like very very simple\nwe have the the test decorator and in it\nif you want to use\nthe cache\nyou should set the the cache view to\ntrue and provide a version this is like\nthe minimal thing that you have to do\num\nwhat this essentially means is that on\nthe back end now we know that for every\ncall\nto this square task\nwith you know\nper the this this single parameter that\nyou that you pass like the integer\num we're gonna cache the output so we're\ngonna catch this this\ninteger is the square of the whatever\ninput you're passing okay\nso we're gonna build on this foundation\nreaching the that feature of like how do\nyou\ngo and cache\ncrazy objects\nbut caching semantics isn't that that\njoke that there's only like three\nor two\nreally tough problems in cs caching and\num\nnaming and like half by one values so\nyeah\ncaching again from the docs\ntest caching is useful when you know\nthat you're gonna be\nexecuting um a task with the same inputs\nso\nlet's talk about what what is the this\nnotion of sameness\nthat um fly\nlets you work with so\nright off the bat one could ask hey are\nwe caching values by reference\nor by value what is the semantics that\nflight lets you um\nuse so it turns out that it depends\nfor\n99 of the objects really um you are all\ncaching by value like we do the simple\nthing you just like provide the\nrepresentation\nof the object that you know lets you\nassume that the caching by value is\nwhat's going to happen\nbut for some cases in here um i will\npick\nour loved pandas data frames\num\nas as an example of this\num\nit's not so easy to think about like how\ndo you even represent the data frame\nright\nlike\ndo you\nwhat if you have a data frame that's\nlike one petabyte of data\nhow does flight even know about that\nthat um that data frame\nso um\nfight has this great feature that\nat this point is a little bit\nunderutilized which is like we strive to\nbe\nmulti-language and in order to do this\nwe have to\nbasically map\nobjects in native languages to the\nflight type system\nokay\nso this conversion\nis what's\nwhat we're really we're talking about\nhere\nso again\nlook at the question how do we even\nrepresent data frames\nin the flight type system\ndon't get too scared this is just you\nknow some bit of code that will show\nwhy in the case of the the data frames\num\nthe reference\nthe caching really\ncan't really be by value by default um\nremember we we map native values to\nflight types and those flag types um are\nwhat we call literals and if you squint\njust forget about all this code but just\nlook at line 21. look at what we're\ndoing line 21\nwe're returning\nan object of type literal\nthat takes as inputs as\na schema and the schema has a remote\npath in some type\nso literally like\nwe are not when we we have a pandas data\nframe\nthat has to be mapped to a literal\nflight never really knows about the\ncontent of the data\nit just knows about this this this\nschema object this remote this reference\nto a remote path\nthat's how we represent data frames in\nthe back end\nthe two python values not super\nimportant it's just doing the reverse\nyou know like we have this concept of of\npipe transformers which i think we have\na great talk on\nthat we\nmight have done some time ago but\nanyways go look at the docs this idea\nthat we use of summers to\nmap to flight values and go back to\nnative values in this case it's python\nbut you know not super important but\num\noops we finally get to the feature that\nwe are proposing now like we are\ngoing to let users\nbasically override\nthe behavior of these\nobjects that are cached by reference by\nproviding\na hash\nand we will not be super prescriptive\nabout how you're going to go about\nhashing the\nthe objects so\nhere's how this how the syntax\nlooks like\nso let me walk\nthrough this example let's start from\nthe bottom\nwe have a workflow\nthat is composed of two tasks right we\nhave a task called foo\nthat\nproduces a data frame\nin a task called bar\nthat takes the data frame as an input\nand is cached remember we talked about\nhow cache tasks look like you have to\nset the\nthe those two fields\nlet's go\nback to the definition of foo\nin foo now\ntake a look at the output of um of the\nthe task for\nnow we're seeing that yes you are\nreturning a data frame\nbut we are saying that you're gonna be\nhashing that data frame using some\nfunction that you're gonna provide\nand and flight takes care of um\nbasically piping that\nthat hash value\nto the little so that flight can know\nabout hey now when you're caching this\nyou are\nactually going to um use\nthe hash to provide a cash by value\nsemantics\nto the caching system\nand\nyeah that is the feature we're working\non um\nthere's an rfc out the first pr in\nflight kit is also out with you know\nmost of what we talked about here\nthere's a lot of magic in the back end\nto provide a a better experience\nespecially about when we talk about like\nhow how can we let the users know that\nthey are misusing\num the cache in certain cases you know\nthink of the case of like\nsomeone provides\na task that takes a data frame as an\ninput and and that task is cached like\nif they do not override the hash it's\nvery likely\nthat this um\nthis\ndesk will not be cached at all\nbut again open for questions if you want\nto you know jump in the rfc to feel free\nit's rfc 1893 i put it in the references\neduardo so question\nyeah if you go\num back to the previous slide\nthis is uh\ni like the syntax a lot um\nhow does\nthis flight automatically analyze the\nfact that bar\nis taking the output of foo\nand so we'll\napply the type\nto the yeah for yes\nthat's a good question like for all\nintents and purposes the the the\ncompilation phase still um\nknows about the literal type schema\nand can know about like how to match the\nthe types just like a regular you know\nworkflow this is just like\nso that you can provide this extra bit\nof information about how to what is the\nhash of this object that you are\nproducing you know\nbut nothing else changes like it\nright in and in this example the bar df\nis anat annotated as just pd.data frame\nso is that\nhow does that task know\nto hash the data frame incoming data\nframe\nyeah good question so actually it's when\nwe\nwe go through\nwhen we transform the data frame into\nthe literal\nso that we can pipe into like to the\nnext you know um task it said\nduring that step\nthat we basically add of a value to\ndelete we'll say hey actually the hash\nof this letter is x which was calculated\nusing this method that you provided\nso it's not when the task is about\nexecute\nit's when the task that produces the\ndata that you want to override the hash\nthat that that step happens\nokay\nso essentially we do it once like\nimagine he had like several calls\nto bar\nyou know and imagine bar had like\nmultiple um\ninputs like\nwe only do it once\nokay cool thanks\ncool\nso remind us of the rfp number\neduardo\noh it's seen the references uh\nyes oh there we 1893\nso this is several people have been\nasking for this you know or\nhave been asking for some for the\nability to do something like this so\nthis is coming from uh\nyou know uh real life user pain yeah\nreal live user pain\nthey just want to control their own\ncaching semantics so this is this is a\na uh you know it's an extensibility\nmechanism it's you know relatively\nadvanced feature when you're really into\nthe cache semantics of your own objects\nand you want to take control over it\nbut if you so if you have a if you have\na dog in this fight if you care about\nthis um talk to eduardo directly or\npreferably comment on the rfp\ntake a look at the pr's if you want to\ncontribute we're open to ideas\num\nbut we're pretty excited about the\nability to let people\nlike drive this themselves because you\nknow ultimately\nyou know you know your data better than\nflight can know your data so\nexactly that's kind of like the key of\nthis feature yeah like yeah you can\ndefinitely explore an angle to your data\nthat we cannot know\nright so we i mean we can only survive\nthe semantics from what we can read but\nif you you actually understand the\nsemantics yourself you can you can take\ncontrol of that\nwith this hook so\npretty excited about that\num any other questions or feel free to\ntake those up\ndirectly with edwardo afterwards\noh yeah i have another one sorry um\ndoes this does this syntax support\ngrabbing like if you want to rely on the\ne tag of whatever blob store\nyou're using\ndoes this support that as well or is\nthat another\ntime yeah you notice that that is a\ngreat question and i know where you're\ncoming from um\nwe we actually talked about like using\ne-tags in the rfc so go read the docs\nbut\nthe the comment but essentially\num\nwe wanted to\num\ntake this\nlike first of all like you can e tags\nare associated with a specific resource\nokay you can't like reuse read tags\nbetween different resources but um\nthe second reason and i think it's it's\na more noble one is that\nwe really\nyou notice that we haven't really talked\nabout how we store\nhow we make better use of storage like\nthe blob storage\nto um\n[Music]\nimplement this feature like it's\ncompletely separate like we're just\ntalking about how um\nobjects that might be you know rep\nstored like multiple times they all\nhave the same same um representation for\nflights caching purposes\nthat's kind of it's good we've been\ntalking about it um it might be like a\nnext\nphase of like can we even\nnot\num pay this cost of like storing this\nthing multiple times you know\nthank you good question\nthank you eduardo"
    },
    {
        "title": "Flyte Community Update 003 - Dec 14 2021",
        "transcript": "morning everybody um welcome back it's\nuh our last uh community sink of the\nyear\nbefore the u.s holiday season kicks in\nwe've got a couple of talks today\nthat we want to\n[Music]\ngo over i also want to talk about\noffice hours that we've been um starting\nto take on\nin order for people to\njust come with any questions that you\nhave with the community and bring them\nhatham is going to be\ncoming on\nseven a.m uh 7 30 a.m pacific time and\nkathan is at nine to 9 30 pacific time\num on wednesdays so anybody that wants\nto bring any questions and this can be\nanything um that's when we'll be hanging\nout\nthe calendar invites are linked in the\npresentation\num\ni'm going to go straight down to\nhighlights in version 18.2\num these were presented in detail last\nweek but i'll put them here in summary\nform for anybody that hasn't\nwasn't able to get last week's talk\nand we'll skip right to thank you to the\n18.2 contributors um lots of external\nfolks chipped in this week which are\nthis version which is fantastic so i\nwant to thank everybody who\num who kicked in\nit's a lot of fun to see things show up\nthat um\nuh that come from uh folks just trying\nto make the whole thing better for the\ncommunity so thanks for upstreaming all\nyour changes\nuh today's talks are uh we have two\none from eduardo\nwho's going to be talking about\ncaching non-fl flight offloaded objects\nand the second talk is going to be from\nye about the data frame handling\nproposal that we'd like to get feedback\non\nand\nfinally we have to wish you all happy\nholidays\nthose of you in the\nus and\nthe americas and european countries\nwhich we're going into the christmas\nseason\nwhich is december 25th and then up until\nnew year's which is sort of a quiet time\nfor folk or family time for folks\nin this part of the world\nand with that i will\nturn it over to eduardo"
    },
    {
        "title": "Executing Flyte Tasks Using Raw Containers and Exotic Languages",
        "transcript": "my name is eduardo i work on the union\nteam and today we're going to talk about\nraw containers in um\nexotic languages\nyour mileage mini bearing you know what\npeople think of exotic might not be so\nexcited to others so let's do it um\nquick recap on what\nuh\nrock containers are how powerful they\nare how how we use it um talk a little\nbit about you know exotic languages show\na demo\nand\num so\nrock containers to me they look like\nthis they're like jet engines with all\nthe wires exposed you know you're it's\nlike a\nswiss army knife like you are on your\nown but it's very very very powerful\nlike you can run literally arbitrary\ncontainers this means like anything from\nalpine to like the most carefully\ncrafted you know image that uses like\nhaskell if you want yeah\num\nin the back end we do all the work to\nmap you know inputs and outputs so it's\nnot like a fire for that thing like you\ncan actually pass data to win from um\nraw containers and orchestrating them is\nsuper super easy they look like this\nthis is copied from the documentation\nhere just defining a rock container\nnamed square\nthat will\nread its inputs from this locally mapped\nin like from the data volume that flight\nmaps in the container when the container\nstarts\nfrom you know this directory slash fire\nslash inputs similarly we do the same\nthing to outputs and from the outside\nlike actually pass data in\nyou pass this dictionary here we have a\nsingle input of type int\nbut um\ndon't get to hang up on this this could\nbe like\ncomplex objects like protobufs or you\nknow dictionaries similarly to outputs\nsame idea have a dictionary of you know\nnamed um values where uh\nyou can refer to them afterwards\nin this particular case we're using the\nalpine image and here's the command\nthat's gonna run when this this task\nruns\nthis simple shell script that just you\nknow\nmultiplies the the to the input like the\nvalve\nthat you pass in it writes to this this\nlocation the slash bar slash outputs\nslash out it's like it's a file that\ntakes the the result of echo\nthe multiplication right um\nand from from the orchestration point of\nview like in flight with flight kit\nthis is how you use it like we have a\ncontainer task we name it square we have\num a task here it's called some\njust to complete the the picture\nliterally just like you know copy\nads to two integers but the interesting\nbit here is the workflow\nsee how we are um\nconsuming the square\num function or the square task and it\njust looks like any other task that you\ndefine in python\num\nnow let's uh talk a little bit about\nlanguages like last time i checked we\nhave\n10 000 languages\n10 000 programming language how crazy is\nthat\nso i picked a very small short sliver of\nthis\nand um just to show how to use\ndraw containers with these five you know\na shell script just like the example\nthat we saw it's just a little bit more\ncomplicated um python of all languages\nhow why why do i consider this as an\nexotic language for this demo like the\nthe feature that i want to emphasize\nhere is that you can you can run any\nversion of python that you want you know\nlet's say you have some code that only\nruns in python 2.\ngo for it it's fine um you want to use\num a different python implementation\nlike not see python like maybe it's pi\npi or some some other you know ad hoc\npython um\nruntime that that you want to run like\ngo for it\num r is\ni think he has a bigger presence in\nacademia but i i just want to show like\nhow hallelujahs are and pascal is very\ndear to my heart like in the previous\nlife i wrote a bunch of haskell i just\nwant to show how to use it like what are\nthe problems and how they compare to\nlike the other vr language that we have\nand julie is the new kid in the block a\nlot of people think that julia will be\nthe next python for data science slash\nmachine learning i just you know i just\ni just wanted to show how to use it\nso i picked a very simple problem that\nwould allow me to exercise you know some\nof\nthe the common problems that you have to\nsolve anyways if you want to use drug\ncontainers with with um\nany language so\nit's a\nyou\nprogram that just outputs the area of an\nellipse\nyou take you know a and b floats and it\nspits out um the another float which is\njust like the pi times uh a times b\nnot super important also some some\nmetadata like you know in our case we're\njust like speed we're gonna speed back\num a bunch of strings\njust to show that you can you can work\nwith you know more complex types\nso um\nreal quick let's jump to a\ndemo did you see my screen\noops\num do you see like\ntwo\nuh\nlike a code editor or no\nyes do you see the code editor and your\nflight deck\nthat is amazing so let's do this\ngreat um\ngreat um so here's the the complete demo\nhere i'm just like\nactually let me\nincrease the font\nthis is good is it okay\ncool i i try to be super um\nin it with this example i i wrote down i\ndid not you know generalize i just\nwanted to show how it would look like to\nrun a bunch of rock containers in\ndifferent languages\nusing like the same example right so\ncalculating the area of the ellipse so\num\nhere's how the\nthe code looks for\nhow to kick off a\num shell script\nrock container\nif you squint\nlike it is literally the same to run um\npython\nminers like the actual command that you\nrun but it they all look very similar\nlike you have you have a an executable\nthat you know points to file and then\nyou map the inputs and outputs so for r\nuh it is super similar to like you have\nour script points to file\nand and i say hey read the stuff from\nfrom slash bar age inputs similar for\noutputs and same for haskell same for\njulia\num\nlet me just show how the actual\ncode for\n[Music]\nthe shells oops\nand um\nthis is\nreading\nthis file called a\nfrom you know the input directory is\nsimilar to\nb it's just like calculating\npi times a times b uh\nshell javascript doesn't have a\npi so i had to do some math here this is\nfour times the arc tangent of one\nfine and it writes\num the result of that to this file\nwhich is\nslash bar slash output slash area\nand um\nhere i'm just outputting some some\nmetadata like hey i'm writing this\nscript or this string to this other file\nokay\nhere's how\nthe same code looks in python\noops very similar you know get the\ninputs\ncalculate the area write the outputs to\na bunch of files\nnow let's take a look at how the haskell\none looks like\nvery very similar to you know read the\ninputs\ncalculate the area write the file\n[Music]\nyeah julia has a bunch of i learned a\nlittle a little bit writing writing this\nexample so julia we can have like\nunicode variables so pi can actually be\nspelled out as pi it's crazy uh\nand in order to handle um command line\nargs you have this special you know\nkeyword called rx in julia but\nthe code is literally the same\nread inputs calculate the area and write\nsome some the output to\na bunch of files\nlet's go back to the uh workflow so the\nworkflow\nthat you know orchestrates this whole\nthing it just\nkicks off the\num\nthe draw containers for each one of the\nlanguages here again i did not\ngeneralize this like i just wanted to be\nlike super super like explicit hey\njust run these and at some point\ni'm just reporting so that we can see\nthe\num\nthat the containers ran and whatnot so\num\nlet's go now\nto the\nquestion\num\nthis is not important\nso\nhere's we have the\nthe ellipse metadata workflow let's just\nrun it with i don't know\n3.1 and 4.\num\ntakes about 20 seconds to run\njust one thing to know\nwe did not talk about how um\ndrop containers run\nthis is totally outside of the scope of\nbusiness talk if you were interested you\nknow to know about the design of\nco-pilot like there's a dock that kayton\nwrote some time ago and\nthere's a pr the issue\nyou can you know\nanyways um\nhere's we have\na very simple let's just start with the\ngraph\nhave all the\nthe raw containers running parallel they\nall report\nthe data\nto this this single task and in terms of\nlike how the text looks like they are\nvery much the same so like does the\nshell\num\nknow\nsee like how the python node looks like\nor\nlooks the same what it has to one or\nthe julia and just to show that they're\nnot lying\num\nthe logs\nso the report here this like these are\nthe five important log lines right this\nis just reporting the\nthe value returned by each each language\nand\nyou can\nsee this\nbut the precision it's not the same\nacross the languages\nlet's see it's curious but anyways um\nyeah that's pretty much it does anyone\nhave any questions\nagain i just wanted to show how raw\ncontainers could be used you know they\nall faced similar problems like you need\nto handle inputs and outputs\num for the shell case kayton\njust introduced a new type a new task\ntype for this release called shell task\nfor any you know\nanyone out there running complex shell\nscripts\nplease use shell task\nbut um otherwise you know go go this\nroute\nyou know wire the\nthe\nthe thing yourself it's it's all good\nand super powerful\nyeah the idea for raw containers was\nlike we cannot write sdks for every\nlanguage\nor every version of their language and\nso on so\nthank you for bringing that up eduardo\nand you don't have to write sdk for\nevery language because the other thing\nthat we realize is that most business\nlogic exists in one language or the\nother\nand then you always sprinkle in some\nother random languages for random things\nlike i i\npearl is still probably the best at\nhandling string so maybe i don't know if\nanybody uses it anymore but you could\nyeah\nor fortran or whatever you know like\npeople do\nmath stuff with fortran these days too\num\nso this was a bit rushed but does anyone\nhave any questions\nalright\nthank you everybody for joining\nit was a wonderful\nsession today hopefully we'll have more\nof these awesome sessions\nin the new year but there's one more\ndecember 14th uh\ni actually i know some folks cannot join\nthis session so i'm going to talk to\ngojek team they want to present like\ntheir usage of flight and so on but i\ndon't think they'll be able to attend so\nplease uh we'll keep you posted and\nthank you for joining today and look\nforward to seeing you again"
    },
    {
        "title": "FlyteConsole UX/UI Townhall - Nov 30 OSS Sync Up",
        "transcript": "so uh my name is jason porter uh and i\nlead the front end team for flight\nconsole development at union ai\nand today we want something kind of new\nuh we wanted to briefly share\nuh some of our design philosophies and\nassumptions that we bring into\nthe design process for flight console\nand then give you guys a chance uh as\nour community to speak into that process\num\nand\nyou know one of the cool things about\nopen source is if you want something you\ncan just build it but when we're doing\nthe ui because it touches so many\ndifferent kinds of users and different\nuse cases we need to make sure that it\nkind of works for everyone\nand so how we approach this problem is\nwe kind of think of different user\npersonas and we have\nthree of these personas in mind so i'm\njust going to kind of share with you\nkind of how we see our users and then at\nthe end\ndefinitely partake in this conversation\nuh tell us you know what we got right\nwhat we got wrong\nso our first user is a data scientist\nthis is kind of like the author of like\nthe tasks and the workflows and we think\nuh they want to run their code on remote\nmachines they want to test their code\nwith various inputs and they do this\nbecause they're trying to tune\nperformance\nwe think they use the ui to test and\nverify that their code works\nto debug errors\nand to access different versions and run\ndifferent inputs\num\nanother persona we have is the\nml\ninfra engineer these are really strong\ncoders\ndeep cs backgrounds\nand they're kind of concerned with the\nproduct of\nflight and so they want to ensure that\nworkflows are business ready\nthey want visibility into status and\nlogging and we think they use the ui\nmostly to manage and orchestrate\nworkflows as business tooling\nand they use the ui for debugging\nsometimes we think but again as they're\nreally strong coders we also think that\nsometimes they'll just log in directed\nthe box and debug that way\nour last persona\nis the business analyst this is kind of\nlike the end user of flight\nand it's because they want to use flight\nfor their business there's like a\nbusiness use case that they're using\nthis for\nand so they want to be able to run these\nworkflows and obviously view their\noutputs\nand they want control over the versions\nand inputs\nfor specific kind of pre-baked business\nuse cases\nso\nthat's kind of how we see the users\nwhenever we're going to build a feature\nwhenever we're thinking about yeah like\nall the new things that are coming down\nthe pipes\nthose are kind of the assumptions that\nguide us so this is the open part where\ni invite everyone to speak into this\nuh just go ahead and unmute\nand speak into this\nwhat did we miss in any of those\nassumptions\nanyone have any\ncomments\nof course i'd love to put g on this part\nfabio\na lot of other folks anybody\nthat makes sense to me\nokay cool\nwell the next question\nwhen you're using flight console for\nwhatever persona you are\nwhat are your current pain points\nand think about this like this will help\nus know what to build to make your lives\neasier\nah i couldn't talk about that sorry\nit's um\nyeah so there's like couple of things\nthat you know when you use the when you\nuse flight it's very nice but then if\nyou want to run a workflow or something\nthen you know you always need to click\non buttons\nand you can't really you know you you\nmight have a version you you see all the\nversions that you have in flight but\nthen you can't click on that version and\nbeing like okay click on there and\nlaunch it you always have to click on\nsome very specific buttons if you want\nto close\na workflow you have to click on the on\nthe cross\nyou can't click anywhere else in the\nwindow you know that's something that\nwe're kind of used to already and\nand yet it's also missing a bit of like\nflexibility\nbecause if i remember you have all your\nworkflows or your versions but then you\ncan't\nresize anything\nand at one point you know\nit's uh you're like sometimes i like\nokay i just want to see\ni don't care all about the 10 plus\nversions that worked correctly you know\ni just wanted to see one\nand then yeah\nthat's what i can think about for now\nthat's great that's awesome feedback\none and just so um oh go ahead please go\nahead oh please finish\noh i was just gonna say so just so i'm\nclear are you saying that maybe you\nwould like us to look into the ability\nto kind of resize\nsome of the different workspaces is that\nkind of what\ni meant i'm not going to teach you here\nnow but uh yeah when you have a workflow\nthen you can see you know in that\nworkflow you can see the task and then\nexecutions and i'd like to be able to\nresize the different things like\ndifferent\nspaces between those\nokay\namigo miguel plus ones that he's saying\nthat their users have also asked about\nthe same thing i tried every day every\nday i'm like damn it i can't\nokay okay good to know\nso maybe maybe i'll go next um yeah\nmaybe for\nfor reference we are currently in the\nphase of uh assessing slide and building\na reference implementation so we don't\nhave a whole lot of practical experience\nwith it\nand so maybe what i'm saying is already\nsolved but what happens very often for\nme is when i'm experimenting is that i'm\nlaunching a lot of workflows and some of\nthem crash because i'm still\nexperimenting\nand i can't delete them or i haven't\nfound out how\nhow to delete executions and workflow\nversions is that something that's not\npossible or have i missed how to do it\nis it intended\nso i guess i'll talk about this\nuh so we don't\num there is no delete in flight\nit's i don't think it's like\nspecifically by design but it is\nintentional\nuh what that means uh the reason why we\ncannot delete is because let's say if\nuh everything's immutable so deleting\nhere just means hiding\nessentially it doesn't really mean\nanything else so we do support archiving\num i the one of the problems that i\nfound is that there's no archive support\nfor every entity and i think we should\nwant a global archive support\nand the other\none other problem is that the ui doesn't\nsupport any of these mutations actually\nand\none of the reasons why the ui did not\nsupport for some time was because we\ndidn't have role-based access control\nso anybody can go and\narchive and delete you know\nwe are paranoid people we do not like\nthat even though we trust everybody we\nactually kind of operate in a zero trust\nenvironment right the entire design of\nthe system is almost zero trust so that\nit's like you know it's confused by\ndesign\nso um\nthat being said there is one api that's\navailable on the ui\nthat is\nit does not take the zero to to\ncompletely and that is like permanent\nyou can go to an execution and you can\nterminate um the only reason why we\nadded we realized if we didn't do that\npeople would just hate us\num so\nbut but that being said we would love to\nunderstand um\nwhere are the reasons uh what you would\nlike to archive it's extremely important\nand what archive really means what did\nit really mean does it mean delete all\nthe data behind it\ndoes it mean just delete it from the ui\ni don't i want to hide it\nor does it like we would love and i\nthink\nthese are\nsome of them are very trivial just you\nknow deleting from the database and\nhiding is super trivial deleting all the\ndata behind it\nis a little less trivial\nanother reason is what could happen is\nyou know because we allow referencing\nworkflows and so on if you delete the\nworks when somebody else is referencing\nit\nit's like you know the the famous thing\nin in p.m somebody deleted the left pad\nor whatever one of those packages and\nbooked the entire world with crashing so\nwe are kind of so you even if you see\nthe package registers they don't allow\nyou to do these stuff easily right and\nthat's also the same reasons why we have\ndesigned it so uh but archiving\nonce you archive it essentially it's\nhidden\nfrom most uh viewpoints\nbut if you're using it you can still\nkeep on get using the archive version\nand you will get the\ndedicated message can i archive through\nthe ui is that not possible\nonly from the cli today\nso i don't think i would want to delete\ni would just want to not see\nbecause what happens very often what\nexperimenting that i create i don't know\nhow many executions and the entire thing\nis cluttered with them\nyeah um\nand just saying okay look a filter for\nwhat i had like what i executed today\njust just hide it\nthat would be nice to\nkeep an overview\ni agree i think jason we probably should\nsupport archiving the ui because there\nis a filter on the admin apis that allow\nyou to say show me on archive things\nalso\nand so let's say you know this is not\nreally zero trust problem here it's our\ntime it's okay\nand then if you say show me all you\nshould be able to see and retrieve\nanything\nso i think and this is a huge ass so we\nshould support it for sure\nand i just want to say fabio that was a\nfantastic comment i mean as you can tell\nthis has clearly been something that\nwe've been thinking about too but\nexactly like this we want to understand\nexactly what it is like what is your use\ncase for wanting to archive and if i\nhear you correctly it's that there's a\nphase where you're just testing and\nyou're and you're seeing how it works\nand then when you transition from that\nto business ready you don't want to have\nlike other business owners see all this\nother stuff we\nwe're very much thinking about this\nproblem and your comment is extremely\nhelpful i assure you because that's\nsomething we've been thinking about so\ngreat\njob\nanyone else before i move on\nuh yeah i have\nanother one um some somewhat related\nmaybe because i have been\ni've done some experiments today\nas well and um\ni ran into an issue so i i'm i'm i have\na research project where some people\ncreated some\nbasically crazy\nuh algorithms with uh\nand python machines some machine\nlearning algorithms and i'm just\nbuilding\num\nflight workflows from from it\num\nbut i have i don't really know the the\nthe code and i don't really know the\nperformance characteristic and\nespecially um\nhow much memory these um these\nexecutions um\nneed so i'm just experimenting and what\ni what what happened today was that\ni ran in a lot of out of memory errors\nbasically so i\ndid\nexperimentation with adding the\ndifferent memory to to the task\nexecution\nand then i\nyeah what what happened was that i got a\nfailure of course when it was too low\nand but i just\nsee\nthere's a\nreturn value from the process\n137 or so which means the\nthe container has been killed\num\nand i have to check the logs um\nso\ni think this is just\njust something if the ui if this is a\nvery specific error but if the ui\nclearly says it\ntells me okay this is a memory error\nthat would be great but of course what\nwhat uh what would be even greater if is\nif we had some\num some ui support for actually\ni don't know\nprofiling or\nseeing uh in retrospective how much\nmemory um\nexecutions actually use you know so\nbecause that would be extremely helpful\nto\nto to tune these things and and so on so\nyeah yeah\ncan i add maybe it was a\nbig question so\nlet me explain so the the ui is driven\nby events from the engines like\nright\nour plugins and so on um and so\nout of memory should show up as an error\nsaying but it's in text\nit doesn't really show up as a special\nerror saying out of error and probably\nthat's something that we should think\nabout in the ui um because we do save\nthat as an error code\nso you know the ui can show something\nspecial um\nbut besides that\nand i would love to know if you didn't\nsee that\nin some cases\nsending that correctly\nbut uh\nso i had a lift in it to tie in a couple\nother places i think was there\nwhat we're doing is\nonce you start you know profiling at one\ntime it's an expensive endeavor\num\nespecially because we start building\ndata dog almost\nwhich is and we\nare like you know\nnot right now we don't think we have\nlike a bigger fish to fry but what we\ndid is\nif you use some of our\nopen source dashboards\nand then you know of course please\ncontribute to them uh\nthey should allow you to see in\nretrospect\nwhat was the container's memory usage by\nprometheus\nit's better to do it that way in my\nopinion even though i would love to put\nit like you know\nif you were to ask me yes i would love\nto build this one complete experience\nyeah but it's like building data and\nthat is\nyeah yeah yeah no i i totally agree i i\nactually didn't\ndidn't uh really\nuh\nthink much about it i just went into\nthis and i thought okay\nit would be cool if i if i could see how\nmuch memory this\nthis task actually takes so i can\nso it makes my life easier to tune\nthings\nbut yeah of course um i think you're\nyou're totally right that's that's\nprobably the\nthe the best way to go\nyeah so but what we have been thinking\nso i actually have been toying around\nwith an idea of\nof an event endpoint that actually gets\nemitted from the containers\nuh much like weights and biases right\nlike it's like but\nnot for weights and biases you can use\nsome biases for weights and basis right\num\nbut you can say whatever some\ninformation and then\nwe'll allow folks to essentially and we\nwant it to be not in the critical path\nso that\nlet's say this this server is down this\nthis endpoint is down doesn't mean\nyour workflow should stop because\nreliability is paramount and critical to\nus number one right um and so i would\nlove to sorry if you're interested love\nto share the document and then of course\nit will become an rfc eventually um and\nsee if anybody within the community\nwould be open to contributing this like\nit's completely doable\ni think the design exists for this stuff\nwithin like a\nflight and so on and i think eventually\nin the ui we can actually show all of\nthat explanation and so we could start\noff with something basic and just keep\non improving this\nso we think it's completely doable\nbecause we do i do want the gpu stats\nand so on to be in there as well\nawesome\ncool thanks\ngreat feedback anyone i'll go ahead i\nhave one one thing so sorry all right\nthanks for bringing that up actually\nthat was a uh that was an excellent\npoint and we're using essentially uh\nprometheus and grafana to track uh like\nour usage or our resource usage\nbut i think it would be nice to your\npoint to like you know be able to\nspecify\nuh arbitrary links to like attach to\ntasks and whatnot and so like you know\nhow we have view logs it'd be kind of\ninteresting to say like view stats and\nthen have that template to template out\nto like grafana or whatever you want to\ntemplate out to so you can run you know\nnot just grafana but like literally any\nother like datadog or something and then\nhave it point to like the datadog url uh\nthe the the reason for that is because\nlike you know nobody's gonna go to like\nwhatever monitoring dot something\nsomething they just wanna like go to the\nexecution and then like click on that\nlink uh to view their logs review their\nstats and i think that um that would be\na pretty good stop gap until we have you\nknow this thing that kate is kind of\nthinking about\num\nso i don't know if like that already\nexists but uh i think it should be\nfairly simple i think\nyeah can can somebody write a spec on\nthat that would be fantastic at least\nlike you know what you would want the\nrequirements because we've actually\nbelieve it or not like two three years\nago we talked about this and we realized\nthat you know everybody has these\ndifferent like platforms like\nthere's no one\nmonitoring platform almost right like\nprometheus almost seems like the one but\nfor me this is not a platform\nso uh\nso how would you want this and we would\nlove to hear feedback\nyeah i mean i can i can definitely write\nit up but you know very trivially would\njust be exactly like how the logs are\nset up right because logs don't care\nwhat the back end is and so the same way\nlike we don't want our metrics to care\nwhat the back end is um and i'm sure\nlike the metrics url path would have\nsome indication of like what the pod\nname and stuff that we can use for\ntemplating so we just set up a template\nlike an arbitrary log entry give it a\nkey set up a template and then let that\nlet propeller or admin fill out that\ntemplate for us so that we can uh click\non the link to get us the metrics page\nbut yeah totally i mean\nnot like i haven't spent a lot of time\nthinking about it just like oh it would\nbe nice but yeah\neven an issue would be fantastic like\njust okay yeah i'll do that you can all\ncollaborate would be fantastic i i don't\nthink it's too hard to so we should we\nshould get something basic it's just\nthat will it work for everybody i have\nno idea\nokay uh the other point i wanted to\nbring up was something that we've talked\nabout for a while which is the ability\nto have like arbitrary tags attached to\nyour executions\num and so like when you go to the\nexecutions page you know like you\nbasically get this like you know uh\npaged like all the executions that you\nhave that ever existed and you know like\nwe'd be able we would love to be able to\nlike say oh this was for uh this\nexperiment or like be able to tag it\nwith arbitrary tags and just kind of use\nthe ui to search and filter by tags to\nget the right executions that we care\nabout\num and you know that could be used for\nthings like oh uh show me the ones that\ni ran with like these like this this\nversion of the experiment i guess the\nversion like already exists but to be\nable to like make that a little bit more\nextensible uh where we can specify like\nour own key value pairs would be kind of\nnice um there is an issue for this i\njust like i think it basically died\nbecause we never followed up but um\nwe're happy to would be happy to pick\nthat back up\n[Music]\ni think lyft also is interested in this\nand i know i\ndon't know anybody else but\nmachine learning so the way we think\nabout this to be honest uh the admin api\nis also a model server\nright like it is a model repository so\nyou can retrieve models from there and\ntagging really just helps uh like you\ncould add some word style tagging on\nyour model\nmodel from it um so yeah we're extremely\ninterested in this also achieve issues\nand please\nplease\navoid them folks\none thing came to my mind when i heard\nthat um it would also be nice to be able\nto set a tag from from within the\nworkflow because what what i currently\nthink about is how we can integrate this\nwith an outflow we use it for yeah for\neverything that you use on helpful for\nit would be nice to get the link to the\nthe run and the tracking server from\nlflow\nin the ui of flight so that i can just\nsee oh that's my execution let's let's\nlook at how it performed and then i mean\nthen i can immediately open in a new tab\nto link to the tracking server so if\nfrom within within the run there would\nbe a way to say i want to tag this\nexecution with something and then i just\nadd the link there that would be very\nhelpful to quickly jump to\nbe it bit weight and biases or ml flow\nor\nany lineage tool you want to use\nabsolutely i think let's yeah so there\nare two good issues here\nchief when you talk it just makes\nuh better so please talk more\nokay anyone else this is great by the\nway but anyone anyone else before we\nmove on\nokay\num we're kind of light on time right now\nso i might skip this question unless\nanyone has strong feelings on this of\nwhich features you use most\nokay we'll skip this question\nand then finally i think we just kind of\nanswered this but is there anything else\nthat we haven't talked about like\ncompletely out of left field\nthat you would like to see built\nokay\ncan i suggest one thing like maybe let's\nyou know in every\nalternate week let's do this like maybe\nlet's spend five minutes on this you\nknow so that we can keep on collecting\nand people\nwhat you did is you prime their juices\nnow let's hopefully come back and\nall right awesome\ni love it all right well hey thank you\nguys so much and we will do this again\nand uh yeah i will pass this off"
    },
    {
        "title": "Flyte Community Update 002 - Nov 30 2021",
        "transcript": "yeah uh happy holidays everyone\nwe\nwe are only gonna have one more uh\nmeeting this year december 14th so\nplease bookmark that event um as usual\nthe calendar invite and the zoom link\nare part of the invite\nadd it to yourselves we\nwhat we realize is we are unable to\nreach the right number of people all the\ntime so please help us do that um please\nforward the event\ninvites you folks and uh if you want to\npresent here this is this is completely\nyour venue please present\nuh you can thank sandra samita me\nmarie and we can help you\nuh besides that we are also trying to uh\npublish uh monthly and so we already did\none also two actually um the second one\nwas in november um and the next one is\nrated for december 15th\nplease sign up for it on getreview.com\num\nand we try to\nalso you know completely give you a\nlittle bit of a worldwide tour on all\nthe things that are happening and that's\nprobably the best way to keep updated on\nwhat's happening in flight\nuh it's still early uh so we're actually\nimproving the content but we guarantee\nthat the content was just kind of\ngrowing and you can and the cool part\nabout review is that you can always go\nback and look at all the\nprevious newsletters\num\nanother thing that's coming up on\nthursday 6 am\nplease let me know if\nanybody interested in joining uh we are\nproposing flight for graduation in the\nnext foundation\nuh um\nin the chapter\nuh the deck for which is attached to\nthis uh\nto this like deck and the slide deck\nwill be shared\ntoday\nand so as part of the graduation\nessentially what that would mean is\ncurrently a flight is a incubating\nproject in underscore lf area and data\nand\nafter this it will be a graduated\nproject with a proper steering committee\nand and so on and we also get some\nfunding from the competition\noops\num next\nso\nwhat we realized is that this\ntiming doesn't really work for everybody\nit's 9 00 am pst and after the daylight\nsavings switch it's really really\nuh late for some folks or too early for\nsome folks\nso what we are trying to do is uh\nwe we actually had a survey long time\nago and probably that was not the right\ntime for the survey but\nwe we want to adjust\nthe timings of this meeting as\nfit for the community\nbut to start off instead of adjusting\nthe timings what we're going to do is\nhate them\nand i will try to man\ntwo office hours like 30 minutes\non wednesdays every week\nboth of those events are part of this\nslide deck so\nwe'll share that\nand\nwe'll be 7 to 7 30 a.m pacific time uh\nand then nine to 9 30 p.m pacific time\nand we are hopeful that this should\ncover almost all time zone let's make it\nconvenient for most folks to at least\nhop on\nyou know talk to\nhate mri and ask any questions and as we\ngo one of the principal things that\nwe're going to find out from here is\nwhat's the right time for most folks\nor we can have even two like we can just\nrepeat this meeting or alternate these\nmeetings in two different times and\nthat's absolutely okay\num\nso flight\n0.18.2 is around the corner um\nbut\nso\nit's it's happening but you know already\nthere have been a lot of amazing\ncontributions from\nuh\nfolks\num and\nhere are some of them hopefully i've not\nmissed out anybody but\nthank you for all your contributions i\nthink the number of contributions\nis what makes flights it's like\nsuccessful right so thank you\num\nanother thing that i don't know if you\nguys have seen this in slack please\nkeep yourself updated on the\nannouncements channel slack if not or in\nthe general channel\nbut what we\nwould like to know is\nwould you like to contribute to flight\nand if uh if yes are there any blockers\nplease\nadd that to the survey\ntell us how we can\nhow the union team can help you\ncontribute to flight better\nand also if there are outreach\nopportunities across that so that'll be\nfantastic\nall right\nso what the next bit is what's happening\nin in flight uh this month\nso\nas as i said 18.2 is going to be out\nagain these are not all the features\nthat are watching in 18.2 but these are\nsome of them uh\nmost importantly a lot of changes it's\nlike it\nso now you can get started with a flight\nkit repo using a cookie cutter template\nand this is an extensible module so you\ncan create your own cookie cutters and\nyou can host them in the same rookie\ncolor repo\nand there's a streamlined interface for\nactually getting started with that\none of the very highly requested items\nabout adding flight files like schema\nenums and data classes is uh in\nthere's a there's a new experimental\nsupport for adding shell tasks\num in flight kit itself uh so let's try\nto manage all the shell operations for\nyou you could uh write it more\nuh especially this is extremely useful\nfor bioinformatics by these cases\num\nand then a lot of improvements in\nflexible\nuh ui improvements continue to happen i\nsee jason here he can probably give a\nlittle bit more insight in here but\nlots of\nfiltering\nand searching improvements in the ui\nlaunch form improvements and lots of\nmore levels coming this month\nuh one of the bigger backend changes\nso flight propeller\n[Music]\nnow can be sharded\nand it can be scaled based on some\ndifferent short strategies uh it's it's\nstill beta it's not documented so we\nunderstand that you won't be able to do\nthe minute but that's the intention that\nwe want to get this out\num it it it doesn't break anybody so\neverything that anybody\nthat folks are running any of their\nconfigurations should just continue to\nwork\nuh and then it should be a small\nconfiguration change to enable a\nmulti-uh\npart propeller instance\nall right um\nmore on the road map and milestone and\nwe were we've been trying to brainstorm\non how we can make this more easily\naccessible to everybody within the\ncommunity\nso\nwe've tried different things in github\nnot been very successful but this github\nrecently released a beta for\nthe github projects the new projects\nview uh so we've adopted that and we've\nactually moved all our issues into the\ngithub projects view um and\nmaybe i can give a quick sneak peek on\nthis\nso\nif you go in here you can see all the\nissues in here but but you can start\nsplitting them by area\nuh and by milestone uh so they're done\nwhat being worked on on 0.2.2\nwhat's planned for 0.19 and so on and\nactually um for ui if you want\nspecifically\nor what's in progress within the\nmilestone then you can see who's\nassigned and if if you would like to add\ncertain other things to the milestone\nplease do that that really really helps\nus\num and\nuh\nwe are also planning a doc's patch so\nyou see\nall the things here\nright um\nso uh one of the other things that i had\nmentioned last time is that we are\nmoving to a a patch release for every\nmonth and a minor release every quarter\nuh and this we want to align with\nreleasing the 1.0 like early next year\nprobably end of march\nagain 1.0\nis not really a reliability or a\nstability change 1.0 just here means\nthat we are\nsignaling to everybody that there will\nbe some older deprecated api that we\nhave had maybe like more than three\nyears ago at least internally that we\nmight not be supporting\nbut besides that uh 1.0 essentially\nuh is stabilizing our api\nand more also signal that hey this is a\nstable platform\nuh\nall right um and we are also trying to\nuh use our marketing marketing terms for\nour minor releases so the\n0.19 release is called the eagle release\num\nall of the last two months of work goes\ninto this even release but just within\nthis month we are working on\num you know cash\nserialization that means allow two tasks\nif\nthey use cashing one of them can\nactually wait on the other task to\npopulate the cash this is\nhelps you in cost saving\nalso potentially reduces the pressure on\nthe downstream resources\num we are\nwe have heard a lot of feedback about\ndocs and we absolutely appreciate the\nfeedback thank you for still continuing\nto work with us and we want to improve\nthe stock so we are going to do a talks\npatch this\nuh month of december if any of you would\nlike to contribute to the patch\nuh\nawesome will be really helpful uh we\nhave about 50 odd docs items that we're\ntrying to tackle\nand the dates we will talk about the\ndates in the slack\num so i think it should be like the\nsecond last week of december\num that's a uh improvements uh\nspecifically you know uh ability to\nvisualize dynamic and arbitrary depth\ngraphs um in the ui uh this is a huge\nask and we think that it can actually\nmake\nvisualizing dynamic work was really much\nbetter\num\nanother thing is uh we've heard some\nfeedback about you know caching\nnon-flight objects like panda statistics\nor um\nor\nany other\nexotic objects that people add that do\nnot really\nlike where the cache is not really just\nthe hash of the\npath but you you may want to actually\nentire object itself and so we are\nworking on that um infra trust\ncheckpointing so the introduction check\npointing back end support is already in\nuh in the 0.18.2 but we'll be adding\nmore front-end support where you should\nbe able to use checkpointing for your\nmodels\nwhat this would mean is if you have\nfailures uh any kind of system failure\nthen you're gonna continue um\nlet's say you're training a model for\nmany many days\nuh\nmaybe weeks even\nand a machine is lost because uh you\nknow you have a recycle policy or\nwhatever or maybe there was a system\nfailure you can just resume the um\nthe training from that point on onto a\ndifferent machine this helps us open up\nspot instances\nto almost everybody which reduces costs\nby done\nalso community contributions uh thank\nyou for the latch team to contribute\nthis stuff\namazing work they have two rfcs in\nprogress and they have been merged\nthey're working on uh you know\ndocumentation extraction from flight\ndata itself from your tasks and\nworkflows which will eventually show up\nin the ui\nand support for union types which also\nhave some support options\nuh android to phoenix\nagain this is just a reminder slide it\nis not a breaking change we do not\nintend to break anybody we\nmay have a case where folks might want\nto have to re-register a workflow but\nthat might be still very limited\nbut\nlots of big features will land and lots\nof cleanup essentially of you know\nsome of the craft that we have\naccumulated across many many years or by\ntrading on this platform\num\nyeah and uh 2022 we wanted to also set\nup some goals for 2022 so our goals for\n2021 were like get started with flight\nin less than a minute\nand we\nwe actually were able to do that earlier\nin the year\nuh we still that doesn't mean you've\nstopped if we actually\nimprove that goal so accessibility will\nalways be our top focus\nbut in 2022 we actually want to bring in\na huge focus for our performance we want\nto support workflows that have run time\nfor a few seconds\nand that will need a huge amount of\nengineering effort but\nthat's our goal\num\nagain exciting work um by merantic labs\nwhich the talk today i'm really excited\nfor it thank you for fabio to actually\npresent uh the gojek team has been going\nawesome and i think their their\ncontributions have been fantastic\nthey're really proud to be working with\nthem\nand latchbio team has also been doing\nfantastic stuff so thank you for uh\nand besides that spotify always remember\npartner and they're working on the java\nsdk and many other pictures so thank you\nall right so that's it from my end um\nbut we'll talk about today's talk we\nhave about we have three talks i think\ni'll hand over to jason first who can\nhand over to eduardo\nand then we'll end up with fabio stock\nbefore we go on kitten there's a\nquestion in the chat okay\nlet me check that\nuh\nsingle proper\nwhat would allow different plugin\ntemplates for a project oh great\nquestion so uh jeep so basically the\nsharded propeller is essentially what\nwe're trying to do is set up the\ninfrastructure to manage\nuh all these sorts of interesting\nchanges in the future so first thing it\nwill support is three different types of\nchild strategies one is round robin\nso let's say um\nyou are\nyou are\nnot kubernetes bound that means you are\ndoing workloads that are outside of\nkubernetes and folks do that\num\nor else uh your kubernetes cluster can\nstay further than one profiler which we\nhave not seen\nuh\nif that is the case then you can use a\nround-robin strategy strategy and it\nalso allows you to reduce the total\nuh number of workers in one color and so\non and it kind of like you know produces\nthe\nmemory footprint required for\npropeller instance so if if you have\nthat sort of stuff then a round-robin\nstrategy would be useful\nbut the second we are also supporting\nfor names face sharding and per uh\nproject starting this is not dynamic at\nthe moment you have to statically\nconfigure if you want specific name\nspaces to go to a propeller and specific\nother name spaces to go to another\npropeller and it supports\nsome\nwhite cards and so on and same thing you\ncan also configure propeller to be\nautomatically\nhave a case where each domain can be\nhandled by a separate propeller instance\nand you can also do combinations of\nprojects\num so\nthat's the initial but if you can see\neach of them can have their own config\nand you can vary them each of them can\nhave access to their own\nsecrets and um\neach of them and that way i think\neventual goal would be that for certain\nprojects if you need certain secrets\nthen only those projects will be able to\naccess those secrets even even flight\npacket which is propeller\nso yes uh\nthis work will be done and we love\nfeedback and help in like thinking how\nuh you guys would like to use this chief\nuh\nyep you\nyeah but this you know the so the big\ndifference is you can always limit name\nspace and you can launch multiple\nproperties but you'll have to manage\nthose deployment returns so what we've\ncreated is a centralized\nand there is an rsc on this immigration\nlook at it but a centralized management\ncontroller that manages\nall flight propellers uh automatically\nand the goal here is eventually to also\nsupport auto scaling because we\nwe have a centralized controller that\ncan know the number of requests and you\ncan scale this\nany other questions awesome"
    },
    {
        "title": "Lowering the Entry Barrier to Flyte: How Could We Run Workflows With One Command Instead of 5?",
        "transcript": "maybe maybe let me let me introduce\nmyself first my name is fabio um i am\nthe machine learning operations lead at\nmarantis labs which is a company based\nin berlin in germany\ni\nbefore that i worked as a senior machine\nlearning engineer at merontics and\nbut i originally had a background in\ntheoretical astrophysics where i did my\nphd\non\nresearch of the saturnian system which\nis very different from machine learning\nbut also very cool\num\nso\nwe\na while back we evaluated different\ndifferent orchestration frameworks and\ndecided to build a reference\nimplementation using flight so this is\nwhy we're here\nand\nwhile we\nhave been building this reference\nimplementation we thought a lot about\nthe user experience of\npackaging the workflow registering it\nand then running it\nand the\nlike the title of this talk is\nlowering the entry barrier how can you\ncan we run a workflow with one command\ninstead of five so it's five not four\nthat's in the invite so it's actually\nfive\nso let's look at what these five\ncommands are um this is copied from the\nfrom the documentation so the first\nthing you need to need to do is you need\nto build a docker image that will be\nused by the pods\nso you can do that either either using\ndocker build and docker push that would\nmake it six commands but there is a\nthere is a shell script that that does\nthat for you so it builds and it pushes\nand after that you have the image you\ncopy the image tag and paste it into the\nnext command which is pi file package\nyou you you say here\nwhat your mod module is and you paste\nthe image tag and the output of this\ncommand\nis a\nsome archive\nthat you then paid that\ni mean you kind of have this in your\nbash history you don't need to paste it\nthe file name is kind of\nalways a standard one but then you use\nflight control to register\nthis packaged workflow\nand then if you want you can either\nexecute it using the ui or you use site\ncontrol to get a launch plan\nthe output here is a yaml file you can\nmodify the uh the values in the dml file\nand then you can use this yaml file to\ncreate an execution so that makes it\nfive commands\nand i totally see why in like in\nprojects that have i don't know like\ndozens or hundreds of tasks i get why\nwhy you need to do it that way because\nyou you might want to like only package\na specific task for example\nbut i think um\nfor a simple project that just has a\nbunch of tasks like download data\npreprocess data test data train model\ndeploy model\ni feel that it would be nice if there\nwas an easier way to do this\num\ni\nreferenced here\nmlf project run like ml for projects\nis a component of an outflow that\norchestrates tasks let's say and let's\nsee run up on kubernetes like it's way\nless powerful than slide but i'm\nreferencing it here because they have a\nsimilar problem that they need to build\na darker image containing the code to\nexecute it remotely and they do it under\nthe hood for you by by just building it\nfor you when you call them alpha\nprojects run\nand\nbecause we have been using email for\nprojects i was thinking maybe if pi\nflight or flight control could build a\ndocker image for you in one command so\nthat you don't have to do that\nso\nuh we have looked like thought of\nseveral iterations of how this could do\nlet me show you the first one that i\nhave abandoned\nin the meanwhile we thought why why\ndon't we just call a workflow.pi\nimagine this here is a workflow.pi it\nhas a task or more task it has a\nworkflow why not have a click\na click cli here\nthat you can pass a default image you\ncan pass it maybe a spark image\nor some tensorflow image you can pass it\nthe version\nand then\nin the then you have a function that is\ncalled package and register and if the\nname is equal to main then this is\ncalled\nand then if you don't specify the\ndefault image well then there's some\nsome module that builds the image for\nyou given the docker file and the docker\ncontext\nso either you pass it in you would do\nthat during ci or when you don't pass it\nin it will build it for you\nand the same it will do the same for\nlet's say a spark image or some other\nimages\nand then\nthis default image and the other images\nwould be given to the flight remote and\nthe flight remote would then register\nbasically in this case a train task and\nthe pipeline using that image and the\nversion that is that is given either by\nthe cli or the default could be to just\nget the current branch and the current\ncommit\nfrom from git from the git repository\nand maybe append a dirty if they're on\ntrack files\nso again you can either pass the version\nindirectly or it's obtained from the git\nrepository\nand then we could create an execution\nusing the remote\nif for example\nif if if we say run or we could say\nregister it to only register like can be\noptional\nand we thought that we want the workflow\nduring ci cd and during local\ndevelopment to be very similar so during\ndevelopment the user would just say\npython my app worked was example.pi\nwithout specifying anything and that\nwould tell the script okay i don't i\ndon't know what the image tag is i need\nto build it i don't know what the\nversion is so let's just get it from the\nenvironment\nand in cicd you typically don't have\naccess to the docker daemon so you might\nwant to build the images in the previous\nstep of your city pipeline and then tell\nthe script look i already have an image\nfor the default image i already have the\ntag with this one here don't build it\nand the version is\ncould be like the tag and on git for\nexample so use that don't get it from\nthe environment\nnow\nthat was the first iteration i have the\nbanner and that sims\nbecause that would lead to a lot of\ncomplications because that would mean\nthat you have this logic in every in\nevery word for that time right\num so\nwhat we currently are thinking of that's\nlike another idea i'm not saying it's\nthe right one we're looking of writing\nthe hydra launcher plugin for flight\nhard drives a configuration management\ntool by facebook ai that we use to\nconfigure basically all of our machine\nlearning systems\nand you can write launchers\nthat\nallow you to to launch stuff on they\nhave one for array they have one for for\nsome other systems so there could be one\nfor flight in principle right\nand the idea here would be to not\nspecify encode how to build your docker\nimages but have it in a structured\nwith because flight missouri hydra is\nall about structured conflicts conflict\nso there would be a workflow con\nconfig and in the workflow we would know\nthe project we would know the domain\nwe would know the module name and we\nwould also know all the images that are\nrequired so there's oh there would\nalways be a default image we would know\nthe path to the docker file we would\nknow the path to the docker context we\nwould know how to tag it and then there\ncould be a list of extra images\nand the hydra launcher could just say\nokay so\nthe image the default image is not\nexplicitly given as part of the cli call\nlet me build it because i have all the\ninformation i need to build it\nand there's a spark image and okay the\nspark image is not given\nlike as a cli argument so i'm going to\nbuild it using this information and the\nuser would just call python workflow.pi\nand then the hydra launcher would use\nthis information in the conflict file to\nknow how to package the workflow and\nbuild it build the images registered\nusing the fight you the flight flight\nremote api\nnow during ci cd you could with hydra\nyou can specify so-called overrides and\nthen the over right would be like don't\nbuild use this image for the default\nimage use this image for the spark image\nand then the flight the hydra launcher\nwould know okay like we don't want to\nbuild we have\num\nso that's another idea i don't think\nthat uh either of them is like what\nand maybe maybe that that's not that's a\nnice thing for us for experimentation\nmaybe we do that later but ultimately i\nthink the takeaway is that it would be\nnice to do it with a single command\nand one way to do it could be to have in\nthe flight config we could have this\ninformation what what are the images\nlike what's the default image what extra\nimages are required what are their tags\nwhat are the docker files that are that\nare used to build them what are the\ndocker contacts to build them\nand then\nallow you to say maybe maybe pi flight\nregister and drum package register and\nrun not sure how it would call and it\nwould then check okay does the user give\nme an explicit tag for the default image\nif not i will use the information i have\nin the config to build it\nso in the conflict it says there's a\nspark image am i like did the user give\nme one explicitly no the user didn't so\nlet's build it\nand then all of that could happen\nautomatically\nwhich would be very similar to what i'm\nalpha is doing because i'm outflow when\nyou say an alpha project run\nit generates a docker file for you and\nbuilds it i wouldn't do that here i\nwould have the docker file for example\nin the\nflight kit python template\nand then have in the in the flight\nconflict the information for how to\nbuild it\nso that's maybe the takeaway is there a\nway to come up with a command in pi\nflight\nand config and flight config that would\nsay build the image this way and then\nhave one command that does all of it so\nall right can you learn more um\ncurrently this has been\nonly in discussions between uh kitan and\nme and\nkevin and\ni forgot your name but one one other\nengineer from your site not good with\nnames we have been involved but if\nyou're interested in this or have any\nfeedback ping me on slack i'm on the\nslide flag\ni'm interested in any feedback\ni would love to actually start a slack\nchannel on this to just face\ni think\nwhat you propose is what we need to do\nwe need to get it down to one command\nwe actually had\none command for some things in the past\nbut the way was structured made it worse\nbut if you're using hydra or something\nelse uh i\ncompletely think it should be one\ncommand from\nstart to end for experimentation and\nstill keep the flexibility of actually\nof course now doing those things for\nlike folks who have like 400 work\neventually people just start having like\namassing workflows that's what happens\nwhen they get like access to this and\nthen you wouldn't want to do that\nprobably because you want to package one\ntask only\nyeah\nbut there should be an easier wait for\nit before you start it you have like two\ntasks right and then\nyeah absolutely i think i agree uh so\nare you okay with us slacking the\nchannel and we would love\nfeedback\nyou know i i don't think i am i don't\nknow anything about hydra\nso i please don't take my\nrecommendations of this just that\nthe only thing that else is i would love\nwas to have like one line\ni'm not saying hydra is the answer here\nthat's something we're trying now\nbecause we use hydra anyways for\nconfiguration management and there are\nlauncher launcher plugins for it so\nwe're trying whether that works and we\nhave a working prototype um\nit even allowed us to do like things\nlike\nuh with a with a flight a flight kit\nplugin for the custom type transformer\nfor hydra configs we can even now\nregister the task and then pass in\na hydra conflict object and it just\ngives it to the workflow which is super\nnice\nbut i'm not saying that hydra is the\nanswer here i think pipeflight could\nalso do that and having the information\nof how to build the docker images could\nbe in flight config so i'm not saying\nhydra is the answer\nso i would want uh i think that that the\nslack channel should be about discussing\nof how the user experience could look\nlike\num\nultimately i think that's something\nthat's valuable also on pipeline not\nonly in hydra\nawesome yeah does anyone have questions\nor feedback immediately that sounds to\nme like a like a great you know cookie\ncutter like kind of example that we\nshould add you know\nlike\nbut yeah let's totally discuss on this\nchannel like um\nmaybe you know\nusing hydra is the way to go i i don't\nknow let's talk\nuh fabio this is this is awesome uh i\nlove like where you're going with this\num we've we've actually been like at\nfreedom we've been thinking about this a\nlittle bit more to try and you know make\nit reduce the overhead of like launching\nflight workflows for our own users and\nso our our solution right now you know\nis a little bit is it's not as uh it's\nnot as neat it basically involves like\nusing make to like you know register and\nfast red shirt it's like a single\ncommand um right now because it just\nwraps like all these things in the\nbackground\num i would love to like you know follow\nup on this with you on slack uh\nand you know we've been thinking about\nlike ways of ways where we can just use\nyou know another tool that can that can\ntake care of these these kinds of things\nlike automatically that doesn't require\nus to write make files\nuh two things that i uh two points of\nconsideration here will be\ncan we you know using something like a\nlock file basically take away the need\nof\nuh you know determining if like a file\nif an image needs to be built or not\nright so like the only reason to not\nbuild an image it's like is if\neverything else remains constant but\nonly the code changes and if we can you\nknow if i can maintain some sort of a\nlog file to do that like then we don't\neven have to specify default image like\nlet flight manage all of that right\nthe second thing is that tags on this\nwould be\nin theory like what you need exactly\nlike you said is you just need the the\ndocker like the docker build context\nright to like build a whole image\nyou could run like fight code in theory\nhave a component that runs in the\ncontrol plane that just like receives uh\nyou know like a gcs pack like some\ndocker a build contacts and then\nactually build and register\nthe workflows like internally like when\nyou when you run the command\nand so you could run like build kit or\nsomething uh like you know alongside\nlike admin and propeller and stuff like\nthat so what that would mean in theory\nis that you just like run one command\nand like forget about it right and then\nflight will decide like oh do i need to\nbuild this image oh it's already cached\nonly the code has changed and then i'll\njust like you know use the existing\nimage and do a fast register all in the\nbackend while the user doesn't have to\nworry about anything\nso we've been like we've been kind of\nbrainstorming about ideas to do this\nlike outside of flight but like it makes\na lot of sense i think to do it\ninternally in flight and i would love to\nfollow up on how we can make this nicer\nyeah that sounds super cool now\nyou could use conical for that i guess\nrunning on the cluster it's kind of slow\nthat's exactly right yeah so there's a\nlot of options like chemical build up\nbuild kit right like there's just\nlike there's i think it's definitely\npossible because like all the tool kit\nalready exists it's just a matter of\nlike wrapping it nicely and i think i\nthink yeah i think if we all wanted to\nbrainstorm we could totally do it nicely\nyeah that sounds good how do we organize\nthat so that everybody everybody finds\nthis black channel\nyeah that'll be me so i'll create a\nslack channel and only the organizer\nhere so i'll create a slack channel i'll\ninvite a few folks and i'll put it in\ngeneral\nuh the name of this track channel so\njust join in uh we have to do something\nbetter about this discoverability but at\nleast for now i think uh eventually\nwith a thing like things right like you\nknow we'll need like basically focus\nareas uh focus groups focused on just\nthe user experience of getting started\nlike that and we\nwe care deeply about this so we uh i\nthink this can be the number one first\nthing that we work on\num so look forward to a\nfew folks getting added and then in the\ngeneral\nlecture\ncool that sounds good i'm looking\nforward to the discussions\nyeah we're actually already building our\ndocker images in\nin our backend so we we yeah it's kind\nof this this is the same thought we\nwe\nwrote a wrapper essentially so you don't\nhave to do it locally so\nyou you upload your docker file along\nwith your\nwith your with your workflows and it\nwill all be built in in the back end\nbasically\nand um\ni think\nthis configuration um\nbecause yeah this also we we think about\nhow what what if we have the different\ntasks uh that need different docker\nfiles for instance and and so on and i\nthink having a nice configuration here\nthat's integrated with flight would be\nreally awesome\ni'm definitely interested here as well\nyou're building all of this and not\ncontributing upstream sorry\nnow it's it's very crude so it's just a\nit's it's very crude so i i i love to\nto to contribute to to bring this to\nto to flight but it i think it needs to\nit it would need more work than how we\ndo it right now\nyeah they are on gcp uh\nso chief but but i think you know\nthis is another thing just from purely\ncontributions point of view what i've\nheard is that people feel that they you\nknow things that they're working on is\ncrude or\nnot\nbut usually it is better than what\nmany others are not thinking of so it's\nabsolutely okay and you know we can\ncreate a new sandbox report\nwhere you can you can experiment with\nthese ideas and find like-minded folks\num and that's why we are completely open\nsource under the you know flight talk\numbrella so\nuh\nthis is it's to collaborate essentially\nwith folks who are trying to solve cool\nproblems please don't think that you're\nsolving something that's crude or it's\nusually better than\nmany many other people and they'll\neventually arrive at the same\nconclusions so please do please\ncollaborate\nall right\nuh jeff g is asking for what's your\nfield environment so are you using cloud\nbuild\nyes\noh so um are you asking me or no you're\nnot talking to me right\nhere already i think\nthat's victorian\nwe actually have uh two\nyeah we we\nwe don't use cloud build even though we\nare on tcp\nbut we actually\nrun um\nwe have two ways we can we run either we\nrun uh docker and docker which is kind\nof well but but it works for everything\num\nin a cluster and but we also try\ntried uh\nkaneko so we have both\nboth ways\nbut kaneko\nsometime is is more fragile\nright now so it fails for something so\nbasically we still have docker docker\nand docker as a fallback but i think the\ni think the way to go is actually if you\nhave\nif you run in any cloud i think\nall of them basically have managed\ndocker build service right so i think\nthat\nthis is something we we actually want to\nswitch to\nokay awesome so jeeve is offering to\ndrive this conversation further us\nuh but i also want one point over here\nand i think maybe\nuh fabio would like to also\ni would love to listen to your thoughts\non this\ni think cloud build will probably slow\ndown the iteration cycle and i think one\nof the things i really wanted that's why\nwe have the fast register thing like you\nknow essentially building\navoid the beginning of docker containers\nand the reason is the docker container\nbuild is not slow stop and they'll build\nand push that cycle because but fabio\nwhat do you think\nso the the way maybe let me explain how\num hum alpha does it they you don't\nspecify what your docker file is\nand you don't specify the docker context\nthey just assume that the directory\nwhere you say ml alpha project run\nthat's where you where your context is\nand they generate a docker file for you\nand the only thing it does\nis it uses some base image that you\nspecify and then that does add dot dot\nokay\nand that is super fast that takes like a\nsecond to build and to push assuming\nthat you don't have like a giant data\nset in your folder where you execute\nthat\nand here i wouldn't generate the docker\nfile on the fly i would\ni would\ngenerate it using cookie cutter for\nexample but as long as copy dot dot is\nthe last line and you don't do the copy\ndot dot before you do some pip installs\nit's actually very fast um\ni don't think\nthat we will use the fast register\nmethod even because this takes like a\nsecond to build with what i currently do\nlike if my internet internet's bad it\ntakes like five seconds but it's very\nfast um\nso the only thing you need to take care\nof that your copy dot dot is kind of the\nlast thing before you do any heavy\ninstallation stuff\nyep i agree\nbut like you know what we have seen the\nreason why the fast register exists uh\nis because some people don't have push\npermissions to registries even though i\ndon't think that is like a\nlike there should be sandbox registries\nthat people can push to it's okay\num but yeah you know organizations are\ninteresting i'm not saying it's not not\na useful feature but i think you know\nthe way we will configure it we won't\nneed it\ni mean ultimately about but about cloud\nbuild or connect etc i think the default\nshould be the local docker daemon but\nyou can do it in a way that you specify\nsome other some other command to do it\nand um\ni think if you configure the docker file\nand the way that the copy dot dot is the\nfinal line then running it locally will\nbe way faster than using clutter\nbecause it has to retrieve the cache\nfrom from the container registry so that\nthat takes a while\nyeah\nlet's discuss this on the slide channel\nlooking forward\nawesome yeah thank you fabio for\nvolunteering"
    },
    {
        "title": "Extending Flyte Tasks and Workflows With Custom Python Decorators",
        "transcript": "okey-dokey\nthanks kitten um let me share my screen\nfirst\nmake sure i have the right thing\ncan everyone see this uh tapestry up top\nhere\nawesome okay\num yeah so\nuh\nhi everyone my name is niels ventilan\ni'm an animal engineer in the union ar\nteam\nand uh today i'll just\nquickly quickly demo\num this functionality that we sort of\nunearthed\nof chatting with the applied ai folks\nabout using\ndecorators to customize tasks and\nworkloads\num\nkind of the the subtitle of this is kind\nof a functional ish api for\nextending or modifying the behavior of\ntasks and workflows\nand i'll show you why it's it's ish and\nnot exactly\npure functional\num so just to\ngive you some context on what what the\ncurrent state is\nof customizing\nflight\nthere are basically kind of three ways\nof extending it so there's a\nflight kit\neither python or java i'm not sure what\nthe exact state of extension is in the\njava scala sdk but\num for now i'll just stay within python\nline\nso the like one of the ways to do this\nis uh defining custom types by the type\nengine and type transformer\napi so an example of this would be\npandera um\ni might jump into these\nexamples but just for the sake of time\ni'll just\nmove on\nthe second type of extension is via\ncustom tasks\nwith the task plugin api so an example\nof this would be the sql alchemy plugin\nand then finally backend plugins\nfor example kubeflow aws sagemaker\nthings like that\nand in the documentation i've pulled\nthis image out of like this decision\ntree of when when to\ndeploy what type of approach\num so\nfor example if if you want to use some\nkind of remote service\num\nand you're familiar with go\nthen writing a flight back-end plug-in\nmakes sense\nbut if you're in python and you just\nwant a quick\nuh if you're experimenting or\nif you're not using a remote service\nthen creating a custom type or a custom\ntask type would make sense\nwe have\num\ndemos of this like presentations in\nprevious oss in cups so um\ni'll i'll link this to you after this\nif you're interested in checking those\nout\nthat said there's there is a fourth way\num\nand this i think was like\nalmost already being supported but\nwe had to do just like a minor tweak\nof flight kit python to get this off the\nground but\nthis this will be this is kind of a\npreview\nand it will require a flight kit\n0.24 or greater\nand basically this is a this is\na locally built flight snacks i'll be\npushing this up\nlater today this week\nand um so really the use case for\ndecorating tasks\nand workflows is\ni'm like quickly experimenting or\nprototyping or even i just want to use\ndecorators like\nthis pattern that's quite useful in in\npython you're familiar with it\num\nbut i can use decorators to then\nmodify the behavior of tasks\nin the same way that i would decorate\nany old python function to modify its\nbehavior so\nif we just take a peek at this single\ndecorator example i'm defining this\nlogio\nfunction which is a decorator\nand i'm defining within it a wrapper\nfunction\nthat\nwill wrap this this input function here\nand the log io\nfunction here but i'm going to modify\nwhat happens before and after calling\nthis function\nso in this case i'm just going to log\nthe inputs and outputs of whatever\nyou know\nfunction i've passed into log i o\nand then also i'm going to log the\noutput\nof whatever this output by function\num\n[Music]\ngreat so this is this this is like\nregular python code this should work you\nknow in python but it also works in\nflight now\nand\nhere the invocation of the decorators\nmatters so the task decorator that we're\nall familiar with in flight kit should\nbe the outermost decorator\nand um here i'm just decorating t1 with\nlog io\nand\ni'll show you later in a sandbox\nenvironment you know this actually works\ni can also stack multiple decorators\nso here i'm defining a validate output\ndecorator\nsame pattern um oh sorry by the way\nstepping back\na little bit this at wraps um\nfunk tools\nkind of utility function\nmakes sure that this wrapper thing\nthat's being\nreturned\nas part of the output of this this\ndecorator\nthis makes sure that the function\nsignature the dock string and all that\nstuff is has bubbled up to the top level\nwrapper um so\nflight kit can analyze this wrapper\nfunction and see oh okay it's exactly\nthe same\nfunction signature as\nthe\ndecorated function\nokay\nso really this validate output decorator\nis pretty simple it just looks at the\noutput\nand checks whether it's a positive\nnumber so if it's\nless than or greater\ngreater than or\nif it's less than or equal to zero it's\ngoing to error out\num\nso now that we have these two decorators\ni'm going to first validate the output\nof t2 which is this new task i'm\ndefining here and then logging the\ninputs and outputs\num\nand we can you know string this together\nin a workflow and i'll show you a little\nbit later this should work um actually\ni'll show you now\nso here's the sandbox environment um\ni'll i won't execute\nthis thing but just to show you the logs\nof it\num\nyeah decorating tasks so our decorator\nlogger actually\nlogs the inputs\nand then the outputs of this thing\nso this was you know our decorator at\nwork\nso that's great um\ni guess one caveat here is\nthat\nmanipulating the function signature of\nthe inner function is not fully tested i\nhaven't quite thought through the the\nconsequences of this but really uh any\nany pattern that\nrelies on this this app wraps decorator\npattern\num will work in flight\nlike um\nnext i'll show you decorating workflows\nthis is um\nthis requires a little bit more work\nbut if you want to do something like uh\nestablish a set setup tear down patterns\nthat relies on an external service\nyou can\ninject\ntasks\ninto a workflow function body using\ndecorators which is pretty neat\nso in this example\ni'm just going to mock out an external\nservice so\nthis external service has some\ninitialize and some complete method\nfor people who are familiar with weights\nand biases or prml or similar types of\num\npackages\nyou can see like\nwhat weights and biases have and both\nboth of these libraries have this kind\nof\num\napi where you initialize something with\nsome id and some other metadata and then\nyou can complete it\nyou know mark it as failed or complete\num at some point during execution\nbut in this case i'm defining two tasks\nuh setup and\nand this doesn't rely on any\nstate or any uh inputs and outputs from\nmy main function body like the workflow\nbody\nwhich is where this works very well\nand then i define a workflow decorator\nand this is kind of similar to the\ndecorator i defined for tasks\nbut here i'm doing a little extra work\num we might refine this example to make\nit seem a little less\nlike we're kind of dissecting and doing\nall sorts of operations inside of a\nworkflow but\nbasically what's happening is i'm\ngetting the current context of\nthe\ncurrent flight context\ni'm creating a\nnode and this before argument is a task\nthis is a flight test so i'm creating a\nnode here\nto produce a before node\ni'm executing the workflow\nfunction body so this then makes the the\ncompilation state uh\nconsists of the before node and then the\nbunch of nodes that this function\nproduces\nand then the after node which is the\nsame thing and\nbasically this the compilation state\nof the workflow looks kind of like this\nsimplified version\num\nand then this this conditional just\nchecks you know whether\num\nthere is a compilation state when this\nthis wrapper is being executed\nand then makes sure that the before node\nexecutes before the first workflow node\nand then the after node\nexecutes after the last workflow node\num i'm doing a few more tricks here uh\nlike this partial thing\nin order to um make setup in teradome a\nparameterizable decorator so i'll show\nyou what that looks like\ni define two you know very simple tasks\njust to to show um\nto illustrate the point and here\ni decorate this workflow again the\ninvocation matters so workflow needs to\nbe at the outermost decorator\nand setup set up the setup teardown\ndecorator takes in the setup task and\nthe teardown task\num\nand does some magic so i'll show you\nuh this is the sandbox environment uh\nthis is a succeeded\nthing\nwe can see that setup executes\nhere our t1 and t2 executes and then\nthere's the teardown\nso our graph looks like this now\num\nso\nthis is pretty neat uh i know that we\nhave a few things like you know\nexceptions and error handling in\nworkflows but\num those things aren't uh you know\nstill working on those and um so this is\na really neat way of\nat least\nwriting\nuh the setup teardown pattern in\nworkflows\na caveat is if any of these error out\nuh this teardown i don't think will\nexecute\num\nbut there i guess there will will be\nways of doing that in the future\nand uh that is it for the demo um\nagain this will be released\nuh\nas part of the 0.24 release and um yeah\ni'm excited to see what kinds of\ninteresting things come out of\nexposing this new feature\nthat's awesome\nsee some new folks any questions from\nanybody you can just do a journal\nquestions or anybody wants to share\nanything\ni have a question about what we just saw\num\nthe\nthe logging example for the task that\ncurator got me thinking because we\ncurrently try to\nintegrate tasks with an airflow logging\nand for that you need to set\na single environment variable that tells\nthem elsa where the tracking server is\nrunning in the cluster and that could be\na neat way to to do that right because\nnot every task needs that but you could\njust decorate it\nwith an alpha task and then\nwe\nwe typically we set this environment\nvariable but we could also then just log\num the arguments that were passed to the\ntask in a ml for right so that could be\nautomated in the very clean way\nyep the mechanism that you would\nrecommend or is there another mechanism\nthat i should be aware of\ni would recommend this actually uh i\nthink niels what do you think\nyeah i\num\ni would definitely recommend it\nso there are two ways of doing it fabio\nso there's the functional way which is\nwhat needs to show which is much simpler\nand cleaner the other way is to actually\nwrite a meta task\nand the example over here is\npaper mail for notebook tasks\nuh what they allow you to do is they\nkind of hijack the function call do\nwhatever they want with that function\ncalled before proceeding to actually\ncall the underlying function\nand the way it is done is that you\ncompose tasks of each other so that\nmeans you don't lose the distributed by\ncharge and distributed spark and all of\nthat they continue to run but you still\nhijack that body\njust before the invocation\nbut if if you're just doing that this is\nmuch simpler actually i i might i we\nmight actually should we should look at\nif notebook tasks can be just rewritten\nusing this and made it like 100 times\nsimpler\nso will will the district will\ndistribute the tasks using um using the\nkubeflow pipeline uh\nthe cube for operators will work if we\nput the decorator if after the the task\ndecorator that that uses the partners\njob that will work\nit will work the only thing is uh so in\na distributed training environment what\nhappens is flight really doesn't know\nthat it's distributed right so from i\nmean the engine doesn't know so what\nit's doing is like launching and it's\nwaiting for a signal and so only one of\nthem can return if all of them return\nthat's like a weird scenario right\nbecause like let's say you're training\nit's only one model wins it's not all\nmodels so there is a case in which you\nraise an error called ignore outputs\nuh it's a meta error that error just\ncauses like\nsilent uh ignoring of the outputs by so\nout of your rank 0 to n if everybody\nelse can return\nthe ignore output except for rank zero\num and so the decorator should work\nbecause the after part will just get an\nexception and it should just bubble up\nthat exception and as long as bubbles of\nthe exception you should just continue\non\nyeah you get that right like so but but\nthe inputs will be invoked on every\nsingle one of them\nuh but that's by design right i mean\nthat we can we what we could we can\ncheck for for rank environment variable\nand then only\nyep\nwhen we do some stuff okay that makes\nsense yeah but the rank environment\nvariable is set by the outer task\ndecorator so so you should get that yep\nyou should you'll have to look into the\ncontext and say like oh this is rank\nzero all right\ncool\nthanks\nawesome\num\nall right anybody else\nso any questions about the release\nprocess any questions about what's going\ninto 18 the versioning plan to switch to\na major version of you know of 1 0 in\nmarch of next year\nif that's confusing anybody please reach\nout to caithin or me\nif anybody wants to present at this\nmeeting\nand show your work\num\nreach out to samitha sandra me captain\nwe're all on slack any of us\ncan get your stuff in we'd love to see\num contributions from the from the\ncommunity and and we can it's every two\nweeks\num so i you know obviously it's somewhat\ninformal so\nyeah so\nevery so this week uh what we are trying\nto do is that every\ntuesday we do this meeting and then the\nfollowing\ntwo weeks later the tuesday we do a\nworkshop and we've been doing that for\nthe last few months\nwhere the workshop actually dives into\nsome aspect of flight and just like you\nknow like maybe the back end or some\nvarious things and\nwe've been doing that for the last few\nmonths uh please let us know if that is\na good format or do you prefer just\ndoing a community sync every two weeks\nuh q a\nis done every two weeks q a should be\ndone every two weeks and this is the\ntime for the community\nuh we would love to know what is better\nuh how can we better utilize it please\nlet us know all yours for all of that\nand i encourage everybody to use neil's\nfantastic method of doing lightning\ntalks please do like me talks don't you\nknow this this is how\nwe will know what you guys are working\non and some some of you guys are doing\nfantastic stuff we would love to know\nthat and your use cases please talk\nabout your use cases that is even better\num and probably i'm gonna i'll have to\nreach out to folks but i would prefer if\nyou guys just come out and say like hey\nwe would love to share our use case\nfantastic\ncool\nand with that\nunless there are more questions or\nfeedback i think we are a wrap"
    },
    {
        "title": "Flyte Community Update 001 - Nov 2 2021",
        "transcript": "i'm george stelling i work with the\nunion team\nwelcome everybody this is uh the\nflight community sync we do this every\ntwo weeks\num\nat this time uh 9 am pacific on on\nsecond tuesday\nor the first and third tuesday of each\nmonth\nand i'm going to get rolling\nwe have some news just a community\nupdate who's been joining the community\nand what they've been up to uh samitha\ni'm hoping you're going to talk to that\nwe have an oktoberfest and community\nhighlights\num there are some community initiatives\nwe want to go through about\nwhat other folks in the community are\nare\ncontributing to the\nproject\nwe have a little time from caith and i\nthink he's going to talk about\nwhat the\nsort of core team is working on in the\nnext\nrelease and or two and uh and then we\nhave a demo from niels at the end\nuh\nsamitha are you ready to talk about how\npectorfest\nuh sure yep\n[Music]\nwent really well thanks to all the\ncontributors here i'm not sure if\neveryone is in the is on the call but\nthen yeah thank you everyone so here are\nthe top five contributions we have\nand we also have the other contributors\nas well\nthank you yuvraj for filing the most\nnumber of issues\nand helping us out with hacktoberfest\nwe\nuh also are gonna have hacktoberfest the\nnext year so we are expecting a lot more\ncontributions from your end\num\nyeah pretty much it\nthank you everybody\noh there you go top contributors\nfrederick tim anton\ntatiana and andreano\nuh\nappreciate that\nit's nice to see maybe i can i can step\nin over here i'm here on the phone sorry\nguys i'm a little locked out but\num so\nfrederick and tim uh recently started\ncontributing from world and they have\nbeen doing fantastic job\nawesome thank you continue on please\nuh anton and tatiana from intel uh again\nfantastic stuff they added the modern\nplugin\num and helping a lot with\nuh with schema\nuh and adriano from applied ai uh just\nthe security stuff that he's been\nhelping with and i've heard that they're\ngonna have upstream a bunch of other\nchanges so looking forward to that\num so\nyeah so i think we have not counted this\nas part of oktoberfest because we didn't\nlabel these at october fast issues but\nthey are\nas important or it's not more important\nso thank you so much\nnext maybe\ngeorge\nso yeah so\nwhat we basically have done is uh for\noktoberfest we had a blog and every all\nthe top contributors should be getting\nsome mugs t-shirts um various vouchers\nuh based on contribution levels\num and for we are actually we liked it\nso much and we saw that most folks were\nthe amount of participation was\noverwhelming and we really love it\nuh so we are going to continue with the\ngiveaway uh going forward it probably\nfor and we want to formalize the process\nso that's what we'll be doing but as we\nuh as we decide on a couple things we\nshould see a post also coming out but\nfor all contributors we want to start\nhaving a swag giveaway and that way\nwe know you should know when you join\nthe meeting next you can show your\nswagger off\nthank you\nanother thing is we are\nwe have we have been doing a lot of\nconference talks and so on\nthrough the union ai team and we\nencourage everybody else to definitely\ndo more conference talks\nif there are slight decks or you know\nsome sort of themes or something you\nwould like we actually made a theme for\nflight and we would love to share that\nwith you so please let us know\nuh on that note though uh on november\n16th uh through 18th is odsc west\num and\nas part of the flight what we're doing\nis we're doing a deep dive three and a\nhalf hour tutorial session\nuh on the tuesday november 16th\nat 9 30 a.m um and that's in san\nfrancisco\nso\nhatham and i will be there uh during the\nsession and\nwe\nif you go to the next slide on november\n15th\nwe've actually would love to\nuh get most of the folks\nor whoever are at least in san francisco\nor happen to be there during that period\nwould love to meet them please join us\nyou can\nmeet uh hit them sean and myself\nand there's another rsvp form here and i\nthink somebody shared that link so\nplease uh join in\nand that would be wonderful\nyep um\nin person not virtual real live people\nin the flesh if anybody's gonna be in\nsan francisco in the 15th um be sure to\nuh get together with uh the crew\nuh core team insights\nyeah just give me one minute i'm just\nswitching over to my laptop but\nimmediately sure\nthat about the modern plugin\num pandara is going to support modern\nschemas next release also so\nhappy coincidence\nperfect\ni guess i'll just continue here so uh\nwhat we wanted to do was\ngoing forward we were thinking\nuh\nput a little more structure around this\nmeeting and and essentially provide a\nset of highlights of an upcoming release\nas well as roadmap for the next release\nand\nand we'll expand that to actually\ninclude a roadmap to the next uh major\nrelease\nor at least the next uh\nmine release and if you if you've seen\nthe rfc we are trying to come up with a\ndifferent versioning system for every\nmonth instead of releasing\na new miner\nwe are going to release a patch\nversion\nand every three months we release a\nminor\nand as the\nand we'll have a name a code name\nassociated with the minor so that it\nbecomes easier to remember\nand\nwhat we're trying to do is uh around\nmarch and next year again this is\ntentative maybe slightly might change a\nlittle bit but\nuh we plan to\nrelease uh version 1.0.0\nagain this is\na semantic version it really there's no\nbreaking changes planned for it for\nanybody except for some legacy stuff\nthat we had at lyft\num but other than that there is no\nbreaking changes what it really means is\nthat the\nuh that that\nthe api\nwill continue to be stable and we have\nkind of like you know solidified the api\nbut usually when you do this major\nversion update people feel that we are\ngoing to break folks that's not the case\nhere we are just wanting to signify that\nwe are a stable\nuh platform uh we have actually met our\nreliability goals and that's what uh we\nwill be signaling with that one point\nzero release um and well a couple things\nin there are\nyou know having reliable testing\nuh automated you know end-to-end testing\nand so on and we'll talk about it in a\nlittle bit\nso uh as part of this uh we are going to\nrelease the 0.18.1 uh version this week\nuh hopefully other folks within the\ncommunity can also start helping us with\nthe release process but\nat the moment the union team\nis more than happy to carry it on\nand some of the major features\nwhich have been highly requested by some\nfolks\ni think one of the biggest things and we\nhear you loud and clear documentation\nneeds updates and we keep on working on\nthem\nand so more coming\nsoon are already in in some cases\nand\nuh another major updates have been in\nthe flight kit world where\nnow you can use any\narbitrary python type\nbetween tasks\nand when flightgate does not have a\nregistered transformer for that type\ninstead of\nreturning an error it will say it will\nprovide a warning but use pico as a\ntransfer now pico should be used with\nits own caveats\nuh it's definitely possible to work with\nit in 99 of cases because of flights uh\naccelerated model but it's still\npossible to use arbitrary tasks within\nflight and so\npiccolo is all python only\nand also very version specific and\nenvironment specific\nbut uh\nfor 99 of use cases it should just work\nalong with that\nof course modern plugin the mpi plug-in\nthat we've been working on for a while\nwhich used hardware\nuh support uh support for\nuh decorators which is what niels is\ngonna talk about in his demo\nuh we've also launched official support\nfor python 3.9 so now it's available\nacross 3.7 3.8 3.9\nthere were some performance improvements\nfor dynamic workflows and a flight kit\nremote has been a major area where we\nare improving many things there have\nbeen some enhancements but more coming\ndown\nin the month of november\nthere was a huge performance uh upgrade\nin the back end for flight propeller uh\nthis is for\nworkflows that have a large fan out\nso the cases in which folks launch\nthousands of\nparallel nodes and not using map tasks\nso map we recommend using map tasks\nwhenever you're running a large number\nof\ntasks in parallel but if that's not\npossible there are there are cases in\nwhich it's not possible so you can use\nbut um just arbitrary dag structure\nwhere uh the fan out is\nvery high\nand this is this represents the the\npoison pill for flight where if the fan\nout goes extremely large the performance\ngets impacted but we've actually\nmade it and we've been testing with\nabout 5 000 to 10 000\nfan out nodes and\nwe are we are pretty\nuh we're not extremely happy but we have\nwe are happy with our performance uh\ngoals at the moment but we will be\nimproving it further\nuh please let please try it out let us\nknow we know some users have been trying\nwith like 100 000 odd uh\nnodes and so we would love to hear what\nwhat you see\nflight console has been i don't know if\nyou guys have been noticing but fight\nconsole has been going through\nquite a few changes\na few of them include bar chart\nimprovements\nso\nso if you go to the versions page and or\nany of the executions page you'll see\nbar charts now that bar chart is\ninteractable it's actually a visual\nfilter so that you can filter down the\nlist very quickly\nalso a major requested item was workflow\nand task versions\nand in case of extremely large\nexecutions we can use pagination\nwe also have a back-end plug-in for mpi\noperators um and\na bunch of security fixes\nthank you for the work by worked on that\nnext slide\num so\nwith that we also uh are embarking on\n0.18.2 uh stated for end of november\nuh a lot of work again on flight kit um\nbasically a cookie cutter uh so we're\nworking on a cookie cutter integration\nso that you can just safe right gate\npipe or pipeline in it and you should be\nable to initialize um some of the basic\nthings required by flight\nuh we are also working on data class\nsupport for complex types so you can\nnest flight files by schemas inside the\ndata class and flight will handle them\nseamlessly\nit's essentially mostly the work on\nflight kit side\nwe are actually working on a major\noverhaul on the\nschema uh that being said again in a\nnon-breaking way it might\nmake you update your code a little bit\nof not not the code just upgrade the\nlibrary but we are making sure that we\nwon't break you\nunless absolutely necessary but the\nschema updates are so that we can\nsupport multi-dimensional schemas like\ntensors and so on\nas well as multiple formats underneath\nincluding like\narrow feather\nor parquet or\nother\nother interesting row wise formats\nand we are also working on some type\nsystem updates\nand and fretlet remote remains our focus\nuh on the console side we are working on\nsupport for dynamic workflow graphs\ngraphs and map tasks today you cannot\nreally visualize that them in the graph\nview\nuh and probably somebody actually filed\na bug\nthis week that when you click sometimes\non the dynamic\nnode it breaks the graph uh we are\nworking on that fix as well\nuh we\nthis was also a request from some of the\nusers that are having a graphql endpoint\nfor uh doing the n plus one type of\nqueries\nuh and we also know that this happens\nfor visualization and so we are working\non uh graphql server poc\nwe should have more on that probably\nin four weeks from now\nand many more improvements to the visual\nfilters\nso a big feature in the back end that\nwe're working on is like propeller\nsharding\nso it's in review at the moment i hope\nyou guys had seen the rfc\nif not please definitely take a look but\nonce you see the rfc the the code is\ncoming\nhopefully it gets checked in this week\nand that allows you to scale out flight\npropeller by multiple different sharding\nstrategies so you can say one per day in\nspace or you can say\njust arbitrary round robin\nshutting\nand and\nthis gives you higher throughput\npotentially um because you get multiple\nkubernetes api clients\nuh and we'll be benchmarking all of this\nand letting you guys know\nuh also\nuh kate's array job actually supports uh\nqueuing\nand and this queueing is done through\nredis uh resource manager we just don't\nhave documentation on that we're working\non that and we're sharing more\ninformation about it\nuh on the platform side we're working on\nintra task check pointing essentially\nuseful for\ngetting uh machine learning uh workload\nlecture pointed so that you can recover\nin a retry and restart from a point that\nyou previously saved\nuh this is besides using workflow as an\nactual checkpoint because workflow is\nmore heavyweight and this is more likely\nthat an individual task at checkpoint\nitself partially\nuh we're working on logical types and\ncustom uh you know a complex type custom\ncaching and this is a case in which you\nmay want to\ncache or change the cache key for\ncertain types like let's take an example\nof pandas data frame\nflight really does not know how to hash\nthe pandas data frame but\nusers probably can provide a better\nmethod\nand one of the biggest things that we\nare working on for the 18.2 is stability\ninitiative actually we've been working\nfor it for the last two months we will\nbe unveiling it\nthis month and that is a full functional\ntesting environment that and it's a it's\na joint effort across union lift spotify\nand couple other places and what we're\ngoing to do is essentially this becomes\nthe de facto source of release\nartifacts that you should be using\nuh our aim is to have one single page\nwhere you can go and check the matrix of\nall versions that are working\nsuccessfully all the plugins uh uh and\nacross which cloud platforms so you\nshould get a matrix for aws another for\ngcp another for on-prem and so on that's\nthe eventual goal\nuh and\nand the intention over here is that\nthe users can basically\nrely and reliably use a version um as a\nas a stable\nbackground and and if they are\ninterested they can go to the bleeding\nedge uh as and when they see fit\nall right next slide\nand last one the plan is that we are\nplanning uh\nversion 19 in the new release kit and so\n0.19.0\nto the end of december uh it's the new\nyear's release\nand then version\n1.0.0\nat the end of march\nagain just to remind this does not mean\nwe're going to break people this just\nmeans\nwe are we are making sure that the uh we\ncan delete some legacy artifacts from\nour\nuh across our repos uh which actually\nmakes the repo the code even cleaner\nand the software a little more\nmaintainable\nthis also gives us to a chance to\noptimize some things underneath\nand\nnext slide\nuh from community work point of view so\nthe community has been hard at work uh\nthank you woven planet and um\nan unnamed community partner who are\nworking on basal rules for flight\nuh lyft is working on multi architecture\nsupport so you can have tasks that run\non arm as well as x86\nuh this has a huge cost implication um\nand so we are extremely excited to\npartner with lyft on that\nspotify is working on bigquery and flink\nplugins bigquery backend plugins already\nin\nflink plugin is open source under\nspotify so you guys can please check it\nout\nor there you can always use that\nbut uh the python plugins are coming\nsoon\nshould be in this week or so\nspotify is also working on java sdk um\nand and\nworking on streaming some of the\noperational dashboards\nworld has been working on upstreaming\ntheir work on vault integration and i'm\nextremely excited about this because\nyou know some things just work when\npeople use them in real world and\nwon't have been helping us write this so\nthank you tim and frederick and stephen\napplied here is working on integration\nwith vml and this is what\ncaused the work that anita will be\ndemoing today\nuh latch is actually working on\nupstreaming union and optional types\nsupport within flight\nand spotify intel and union are helping\nus with the schema\nsystem"
    },
    {
        "title": "K8s Backend Plugins for Flyte -  Oct 19 2021 OSS Meetup",
        "transcript": "all right so uh just a quick recap we uh\nwe have been doing these workshops i\nthink for\nthree weeks now uh so you just wanted to\ngo over what we covered so far um as\ngeorge said we talked so far about\nflight kit\nplugins uh there were three of those\nthree workshops uh you can hopefully go\nback in history and watch these i think\nthey're also posted in our docs\nthe first was the task template these\nplugins and we gave an example of\nsnowflake these are plugins you can\nwrite completely in flight kits\nyou can customize\nwhat container gets run when\nwhen you know when when we encounter one\nof these\ntasks in our field\num and you are given the task template\nwhich is you know the the flight uh\nrepresentation of the task or the the\nserialized representation of the task\nand your implementation of the plugin is\nresponsible for\nhow to execute it\nin the example of snowflake takes the\nlast complete and just extracts the sql\nquery from that and then and some\nconnection\ninformation and use that to\ntalk to snowflake back end and\nauthenticate and run the sql query and\nwrite the output\num\nand then the second one was the\nmaybe core task plugins in flight kits\nif for the lack of better terms\nbetter name\nand we give an example i think of a\nspark task if i recall correctly those\ntasks\nyou get\ncomplete control over how they get\nserialized\nthey uh and you have sort of two options\nyou can serialize you know you can uh\ntry to map\nuser code into one of the existing\ntask types so maybe you want to\nintroduce a new syntax for writing a\nspark task or you know\nand so on\nand you uh\nyou can handle the user code and then\nserialize into an existing you know\nspark task structure or you can be\ncompletely different completely new uh\nif it's a completely new task you know\ntemplate\nuh format or or data you will likely\nwant to implement an equivalent back-end\nplug-in those are the ones we will talk\nabout today\nand then lastly we talked about slide\ntransformers i think there was an\nexample of panel data frame if i'm not\nmistaken\nthese allow you to again completely\nliving in flight kit to transform\na sort of a python type or a type that\nexists in python or in the java case\ntype that existing java into\na flight uh understanded uh understood\nuh type and those types you know once\ntransformed can can then be passed\naround to other task types other\nlanguages they're fully interrupting the\nsystem they can be you know checked in\nthe compiler and all that\num and you're also responsible for the\nyou know the converge and other way\naround so once a task is called uh with\nyou know uh the the transform type in\nyou know flight literal you're also\nresponsible for transforming it back\ninto the python equivalent type\nright and today we will talk about\nuh kx plugins flight case plugins\nuh so we'll start off with what are\nthose it's a way for you for you know a\nplug-in author to\nextend flight to\nseamlessly integrate into existing\nkubernetes\nobjects\nand\noffer sort of a full interrupt\nsystem between these uh you know\nkubernetes objects and the rest of the\nflight tasks you have in your workflow\nuh and once they are in once you get all\nof this done you get all of the goodness\nof you know the rest of the flight tasks\nuh you know things like passing inputs\noutputs to these\nuh objects right so flight will take\ncare of uh\nsandboxing the inputs and parsing out\nthe outputs and all of that\num you get to be able to orchestrate\nbetween different\nyou know object types inconvenience some\nare completely not non kubernetes\nobjects as well\nso maybe you run a query and then you\nrun the kubernetes object and then the\nthird one is you know a task template\nbased object right so you get full\ninterrupt this way uh even if you know\nthese systems don't\nreally understand each other\nyou make them\ntalk flight right\nyou get life cycle management and\nretries and a bunch of other things\num\ni wanted to talk about why would you go\nwith a\nwith implementing one of these back-end\nplugins as opposed to maybe you can put\nall of your logic into a flight kit\nplug-in right because you can you can\nwhen the time when when your\ntask plug-in is called to execute you\ncan decide to then\ncreate the kubernetes object uh you know\ncreate a cube\nclient create the object monitor the\nobject and do all of that in your\nflight kit code and you certainly can\nand maybe even you should uh just to\nvalidate that the the idea you have for\nuh writing this plugin uh is you know\nusable and people want it uh but once\nthis is ready there are some advantages\nfor writing a back-end plug-in that uh\nas of now\nare not easily done\nuh when you just write a flight kit\nplug-in uh these are some of them so one\nyou get lower runtime overheads\nand what i mean by that is\nonce once your\ntask once the user code is serialized\ninto a task template\nthere's no\nmore containers you need to run\nto start this kubernetes plug-in right\nyou the code will go to the the task\ntemplate code will go to the back end\nthe back-end will decide to invoke your\nback-end plug-in\nand then you are free to do whatever\nyour whether that's you know creating\ncommands objects or some other uh\nback-end plug-in type that maybe just\ncalls uh you know back-end service to\nrun a sql query\nuh there's there could be no containers\ninvolved there's no\nuh you know look at image download time\nand container you know start time like\nall of that is completely eliminated uh\nwhen needed obviously\nuh skill and this is sort of related and\nbecause of that because you don't have\nto start the container for every task\ninstance of this plug-in type you uh you\ncan scale to millions uh with like very\nlittle overhead\non the system\nanother benefit is uh again as of today\nyou can you get to customize the links\nthat show up in the ui uh so when you\nyou know click on a a note in the ui you\nget on the side panel you have links for\nyou know logs and whatnot\nuh if you write any back-end plug-in you\nget to customize those add new links and\nso on\nwe're always looking for ways to expose\nsome of these features to again the\nflight kit plug-in system so it's\nbecause we think it's an easier way for\npeople to build plug-ins and it's\ncompletely insulated it's a very good\nenvironment to experiment and try like\nit's a low barrier to entry environment\nfor people to start extending the system\nbut as of now this is one of the\nfunctionalities that are only available\nin back-end plugins\nyou also get uh resource or capacity\nmanagement\nsort of for free\nfrom the the from propeller from the\nback-end plug-in system\nyou can uh decide you know i\nsome projects some flight projects only\nget to launch\nyou know 10 spark applications at any\ngiven point\nright or you can like distribute the\ncapacity however we want maybe you have\nsql queries uh and you don't want to\noverwhelm the\nthe external system because everybody's\ncalling the same you know endpoint to\nsend queries to when you want to\nthrottle them or you want to just impose\nlimits on every project or every\nworkflow so you get a lot of flexibility\num and\nsort of for free it was just\ndeclaratively\nmanaging the capacity\nthe system can impose these limits for\nyou\nyou also get and this is you know\nspecific to k-8 plugins uh the system\nwill manage will fully manage the\nresource the gates uh resource life\ncycle uh and your plug-in like the code\nyou will have to write uh is very\nlimited to the pieces of code that we\ncan generically implement and we'll talk\nabout that\num in a second uh but uh before you do\njust a quick primer for uh you know\nabout kate's objects life cycle in\ngeneral um there\nand these objects really offer the you\nknow basic premise is they give you uh a\ngeneric uh crude you know api interface\ncan create uh\nyou can read update and delete these\nobjects right and it's a very generic\nstandard interface you can interact the\nsame way with any governance object\nwhether they are like you know natively\ncomes natively with kubernetes like\nconfig maps and pods and whatnot or you\nuh you know import them into the system\nthrough you know custom resource\ndefinitions\nthey're all they all have the same\ninterface but in sort of practice there\nare two sides of this there are people\nwho uh you know create these objects and\nand are interested to\nutilize them like calling them consumers\nuh and then there are operators the\noperators are the the you know processes\nthat uh are responsible for actually\nexecuting uh or you know making some\nchange based on the contents of these\nobjects\nuh from consumers 10 points you are\nusually\nyou usually go through this you know\nsort of cycle of uh creating an object\nor maybe modifying it for some objects\nthat do support modification after\ncreation\nyou are monitoring it so you want to see\nyou know did it finish executing is it\nfailing what's happening and so on so\nwe're going to this uh monitor\ncheck frequently\num and when it's uh when it's time to\nyou know delete the object when you're\ndone with the object you can finalize it\nagain this is this is an optional step\nthe allowing uh the system and the\noperator to know that you know are no\nlonger interested in this object right i\nam done with it\num and you can also explicitly call\ndelete right you can\nuh\nif you want you know if it failed maybe\nif you want to clean up yourself or\nwhatever you can explicitly delete these\nobjects\nfrom an operator side of things so the\nthing that's executing this object you\nget uh you usually go through this\nreconcile loop and it's basically your\nmonitoring\nchanges to objects and new objects that\nshow up um and you want to you know do\nsome change in the case of uh the pod\noperator for example you will go uh\nmaybe launch the pod in uh\nour sign assign the part to a node and\nlaunch it in a cubelet or if it's a\ndeployment you create a\nreplica set or whatever right or in the\ncase of flight workflows our operator is\npropeller and it goes through the\nworkflow to execute the steps right so\nyou do some work and then you mutate the\nstatus of the object to to reflect\nwhat you did\nuh did i you know successfully create uh\na pod or i successfully created the\nreplica set and you know what's\nhappening uh maybe the deployment and\nyou say the user said i want you know\nfive instances of this\nuh service\nthe the status the operator deployment\noperator might say well i deployed three\nof them and two are remaining you know\nso you\nput in in the status of the object what\nuh what's the current state of the of\nthings\nuh and you go over and over again right\nso it's sort of a state machine and you\nmanage your own state machine this way\nuntil you try to\nget to the desired state of the object\num and in the case again of uh uh there\nis a cleanup needed you also do this\nfinalize step where you maybe delete the\nexternal resources you created or\nyou know maybe put in the final status\nor do something right with the object um\nand then you finalize it and now it's\nyou know\nfor the system is free to delete it you\nno longer\nuh can do anything about it\nuh right what we are trying to do with\nthese kubernetes with the flight gates\nplugins is um simplify the left side\nhere so simplify being a\nkx\nobject consumer\nuh allowing you to\nyou know create modification or finalize\ndelete um in a very safe uh\nyou know environment or framework\nuh and as i was saying earlier\nintegrated interested into the rest of\nthe flight ecosystem\nuh typically this is where you start\nright uh the user you have to decide\nfirst what you want the users to write\nuh i have two examples here on the left\nside of a spark plug-in on the top right\nso this is how you write\ntoday how you uh declare that the the\nuser declares that they want a spark\ntask they write just a regular what\nlooks like a regular python function\nright with\ninputs outputs uh and the annotation the\nad task annotation\nuh and the only difference you see here\nis this task config equals spark\nand this is sort of the entry point to\nusing your plugin so now\nthey will look at this code in a bit\nnow the system will know that the this\nis not a regular\npython task this is a task that wants to\nuse the spark plug-in it will invoke\nthat it will pass all the information\nthe user you know declared here in this\ncase the spark config with memory and\ninstances\nuh and then the\nthen the plug-in will take this part and\nconvert it to the right side\nuh this is the output of the task\ntemplate right and your plugin gets to\ndecide what goes in here uh the i i you\nknow i deleted a bunch of things from\nthe task template but you can see the\ntop three\nfields here the type config and custom\nare probably the things you will\nyou know deal with the most\num the the type is what tells the rest\nof the system that this\ntask is a spark task it's not python\ntask it's not an array job right it's a\nspark task this was generated by\nsome plug-in on the you know sdk side\nand this name decides\nwhat backend plugin can\npick it up\nthe config and custom are two fields\nthat are given to you they're\ndifferences the config is just a key\nvalue map and custom is a full json\nobject uh where you can fully pass or\ncustomize what to pass to your backend\nplugin right so your sdk plugin can say\nwhatever\nwe can set all the fields uh needed so\nthat the back-end plug-in can do the\nwork uh you know you want or the user\nwants it to do\num\ni also gave two examples here of of\nplugins of user codes uh the site the\nuser side of things the top was the\nspark the bottom left here is an athena\ntask just to show that\nwe don't care as much about how you\nexpose\nthe task to your users\nit can be a regular python function in\nthe case of athena it's just a query\nwrapped with some some other fields\nit can be maybe you have your own dsl\nthat you expose to users\nthat right doesn't matter at the end of\nthe day\nyou get to decide what goes into the\ntask template and that's what this is\nthis should have all the information you\nneed for your back-end plug-in to fully\nexecute\nthis task\ncontinuing on so once you have the task\nindex right and then the user let's say\nwrote the full workflow using this\nuh\nwe create a workflow crd every time you\nlaunch a workflow\num\nuh so right uh and then propeller is the\noperator in this case and picks up the\nworkflow and starts executing these\nnodes\nuh in this case let's say that n1 was\nthe task\nthat we wrote earlier\num and this is where propeller starts\ninvoking you know or try to associate\nwhich plug-in should handle this task\nright so it will look at the type you\nsay well this is a spark task let's say\nso to invoke the plug-in\nregistered for this\num and we have a sort of a base\nimplementation we call it flight kits\nthis is the base implementation that\nhandles\nflights\ncreates plugins right so as i was saying\nwe we try to automate\nuh as much as we can from\nyou know the operations you would need\nto do as a consumer of okay it's object\num and so all the operations that don't\nthat you don't need to customize in your\nplugin are handled by this base object\num\nthe propeller can call you know handle\nuh abort and finalize and we'll talk a\nlittle bit about the\nsort of the state machine here when does\nit call what\nuh\nand the this base takes care of the\ninteraction with api server uh whenever\nwe need to create an object or detail\nobject and so on right\nthe part that it will need to customize\nthough is uh or doesn't know how to\nhandle is given this task template that\nwe looked at earlier like spark for\nexample how do we convert that into a\nkubernetes object right it doesn't know\nwhich coins object doesn't know how to\ntranslate field x to field y in the\nobject and so on this is where your\nimplementation of a plugin is you\nimplement these\nmain three functions uh build the\nidentity resource this just returns an\nempty object so like in the case of\nspark it returns an empty spark\napplication object\nand we'll talk about how we use these in\na bit\nuh then there is a build resource which\ntakes care of translating the task\ntemplate into its object to the full\nobject um\nand then the plug the base the flight\ngate space takes care of persisting this\nobject in in\nkubernetes and retrieving it and\nmonitoring it and so on\nuh and then the last one is get task\nphase this is how\nwe ask you the plugin the author to\ntranslate from a kubernetes object\nstate to a flight phase so this is us\nasking you to translate back\nyou know did the\noperator of this object decide that it\nsucceeded it finished is it like blocked\nwaiting for resources or what's going on\nwith it\nand you should know that and look at\nthat look by looking at the governance\nobject and tell us back well this\nstate here means\nthat in flight land or flight lingo\nright so these are the really only three\nfunctions you need to implement uh\nin your comments object and it just\nplugs into the system\nand you get the full life cycle\nmanagement and all of that\num\nwe uh\nthe way our\nbase implementation knows about you know\nwhat state things are is uh is that we\nstore estates right so it was saying we\nwe fully manage the lifecycle of\nkubernetes object and to do that we need\nto know\ndid we launch the task yet is it you\nknow running and waiting for\nuh for it to finish or waiting for\nresources or it's not scheduled yet or\nis it failing like we need to know that\nright and we store this in\ninto the crd\nand every time we\npropeller wants to\ncheck status for you know a node let's\nsay n1 that's a spark task\nit will pass this part of the state to\nthe plug-in to say well this is the last\ntime the last state i know about the\ntask\nand and then we take this and try to\nunderstand if things change maybe it's\nstill the same when we launch the plot\nand still waiting still running there's\nno state change\num that's okay we'll come back and say\nwell nothing changed and we'll probably\nkeep\nuh pinging the plugin every once in a\nwhile there are rules there's like time\nbased and event-based uh triggers for\nthis but uh but at the end of the day it\nwill\na\ncertain points uh during the time of\nexecution overflow it will call the\nplugin again to say well did it finish\nyet give me the new status and so on\num\noh yeah and this is where the\nbase implementation we'll call your\nplugin to get the latest\nupdates and then update the state update\nthe state in the crd and keep doing this\nover and over again\nuh we uh yeah we\ni wanted to talk about when do we call\nthese functions and the the this is the\nsort of the state machine that the base\nimplementation\num handles for you\num and we store the phase and you know\nhandle the the state machine\nstate uh\ninto the workflow crd for persistence\nit's durable uh and it's\nuh\nguaranteed to only progress forward\nthere are some guarantees that you get\nfor free\nby just leveraging the state we exposed\nto your plugin\nwe start off with we haven't started yet\nright it's the first time we see the\nnode\nuh the base implementation will then\ncall your plugin to say well build me\nthe resource well here's the task\ntemplate that you know the sdk\nserialized uh build it into some command\nsubjects that you want me to persist\nuh and then the base implementation is\nthe one that will you know call okay\nit's create so it's the one that will\nmake the api cube api call to persist\nthis object in hcd and then you know the\ncorresponding operator will pick it up\nand so on\num\nonce it's created uh we go into this and\nthis is usually where we spend most of\nthe time um you know the sort of\nintermediate states uh you know maybe\nit's waiting for resources maybe it's\nrunning and you know things can take\ntime to run and so on um\nand we'll keep calling right to keep\nstaying in this sort of parts of the\nstate machine for a while um every time\nwe come here the base implementation\nwill call you and say build an identity\nresource and related resources as i was\nsaying it's just an empty shell it's\njust a go type um\nthat corresponds to your\nk it's object\nuh we take care of them filling in\nthe standard stuff that kubernetes\nexpose like the name of the object the\nnamespace the annotations and labels and\nwhatnot and we retrieve the full object\nfrom kubernetes whether that's from the\ncache that again we take care of\nautomatically refreshing\nor if it's not yet in the cache we go\nmake the cube api call to retrieve it\nand cache it\nso\nwe we you know bring back the full\nobject uh the latest object we know of\nuh that from kubernetes and then we call\nyou again you say okay so here's the\nfull object give me now the state that\nyou know the flight system can\nunderstand uh because we cannot like all\nof the kubernetes objects you know store\ntheir phases in slightly different\nplaces\nthere's no standard way of doing that is\nthere's no standard way of parsing this\nout or understanding what phase that\nthey succeed or not\nuh kind of thing\nso it's your plug-in implementation\nagain that will take care of this\num\nthere are a couple of operations that\ncan happen sort of out of band um\nthrough flights so\none of them is aborts\nlet's say\nthe user in the ui decided to abort the\nentire workflow\nso flight will take a will will take\ncare of making sure that um if there is\nanything that's currently running in the\nworkflow that we make sure they are\ncleaned properly\nand we called your plugin to again\nbuild this identity source and we make\nsure we delete the object from\nkubernetes so that we don't leave it you\nknow running and consuming resources\nwhile the workflow is aborted and the\nuser is no longer interested in the\noutput of the workflow anyway\num finalize a little bit trickier but uh\nuh and and the kx delete here is also\ncustomizable we might there are some\nsituations where we don't delete\nuh we can decide not to delete\nuh but for the purposes of this\ndiscussion let's say yeah we uh again\nfrom the plug-in implementation side we\njust call the build identity source and\nthen we take care of the base\nimplementation takes care of doing the\nright thing\num so with just these three implement\nfunctions uh you get the you know fully\nsort of integrated into the flight back\nin system you\ndon't have to worry about\nyou know the objects leaking\nor creating duplicate objects um or you\nknow redoing the work over and over\nagain you get\nretries sort of for free like the\nimplementation for making sure that you\nget sandboxed inputs and uh\nand only like sort of one instance of\nthis\ntask will be running uh in a given\nworkflow or the quantity try of this\ntask would be running at any given point\nuh so all of these guarantees just\nhappen for you you don't have to think\nabout them you have to code for them uh\nthey they you just get them for free\num\nthis is sort of the overview i guess\nabout all of the like the the\nmagic behind uh running these plugins i\nwanted to go over a couple of examples\npulling code now uh the\nlook at flight kit the athena and spark\nso these are the two examples we looked\nat for the user code and\nhow are they different from flight kit\nside\nuh and then we'll spend a little bit\nmore time looking at\nimplementations for gates pod\nand the spark plug-ins which are the\nbackend implementations and what do you\nreally need to do\nto implement one and get one like loaded\nand integrate in the system\nso let me and all the links i will show\nnow\nare included here so you have to\nremember them uh\ndid this work did it switch to the code\nit thumbs up down\nyes okay right so i wasn't sure if it uh\nif it will switch with me all right so\nthis is the first uh what i want to look\nat uh looking at what is exposed in your\nin-flight kits\nuh this is the this task uh sort of\nabstract\nclass of sorts uh intellectual that you\nget when you implement a plugin and a\ncore uh task plugin in flight kit these\nare the two things i was mentioning so\nyou get to override these two functions\nto return whatever additional\ninformation you want to include in the\ntask template\nso you just either return a\nessentially a json object in custom or a\nkey value uh you know dictionary\nfor config right and it's up to you what\nyou need to do\nuh or what you need to expose\nor serialize in the tasks template for\nyour backend plugin to execute you know\ncorrectly\num\ni wanted to go over the couple of\nplugins i talked about um just to show\nwhere this is so this is the root of\nflight kit we have a directory here of\nall the flight kit plugins all of these\nget compiled and packaged and shipped as\nseparate pipeline packages\nyou can download one or none uh you have\nto download the one you want to use\nobviously but it's just to say that you\ncan implement\nuh\nlike a flight kit plug-in completely\nsort of out of repo um\nso when you are experimenting maybe you\ndon't have to write modify flight kit or\nchange how people install slightly this\nremains the same and your plug-in is\njust an additional package they install\nand they can start using your\nlike the new\nplug-in a new functionality\num this is the uh the plugin the pod\nplugin um\nand you can see the only\nsome customizations here for how the\ncontainer is serialized and whatnot uh\nbut the one i wanted to show is like it\nonly needed to didn't need to fit in the\nthe custom parts it just needed one\nconfig to be passed uh this is primary\ncontainer name\nuh just a string to string right so this\nis the one thing that will get our own\nadditional thing that the\nback-end plug-in will get from this\nuh flight kit plug-in\nthe the spark one is a little bit more\ncomplex um so again it inherits from\nyou know again eventually the base task\nand it uh it returns\nit customizes the custom field um it\nstores a full\nuh pro to above object in this case you\ndon't have to again you just it's but it\nis\nit gives you a full json object you can\nstore it however you want uh we chose to\nstore a protobuf object here\nit's a spark job you know uh proto with\na bunch of fields uh\nthat you know spark needs to\nbring up a spark cluster\num\nin your case maybe simpler even more\ncomplex\nbut you will pretty much just want\nto you know start with one of these\nfiles\nand then customize your way until you\nhave all the information your backend\nplugin needs to execute correctly\n[Music]\non the back end side uh the these\nplugins as of today again they are all\nstatically compiled into flight\npropeller binary uh and they need to be\nloaded uh using these silent import\nstatements so every second plugin you\nadd that you want to load in propeller\nyou need to add one more import here\nwe have uh in we've seen other places\nwhere they sometimes implement a private\nplug-in right it's a back-end plug-in\nthat only lives in that company\nthey\nyou can replace this\nmain file with your own main file\nand unload your plugins this way\nnot the easiest most friendly way to\nimport plugins but this is the currently\nsupported way of doing so and yeah there\nare some production systems that are\nalready doing this\nuh and then i wanted to look at a couple\nof plugins uh so this is now i'm looking\nnow at propeller codebase i will switch\nto the plugins codebase uh this is the\ncontainer plugin\nuh and i want to talk about a couple of\nthings here the first is this init\nfunction uh so when when this\npackage is imported in propeller um it's\nnot enough to just import the package\nright in the package when it's imported\nneeds to\ndeclare itself as a plugin\nand this is how you do so you\nregister\na kx plugin or a code plugin or web\nplugin those are other plugin types we\nwill talk about in future workshops in\nthis case we are registering a k8 plugin\nand it has some information about um the\nname of the plugin uh what task types\nthat it can handle this goes back to\nthat string\ni showed earlier in the presentation the\ntask template that says you know the\ntask\ntype spark\nthis is the thing that ties these two\npieces together buys the sdk part with\nthe back end plugin\nand a few other information here\nthen there are the the three functions\nwe talked about here the build a limited\nresource just returns an empty pod\nbuild resource\nuh takes the\nuh\nthe task template from here and converts\nit into a pod and it turns the pod right\nyou don't create it inconvenience\nyourself you just return the pod\nuh and then the get task phase so this\ngets\na kubernetes object in this case an\nactual pod\nthis is sort of the latest version of\nthe pod that the system knows about um\nand you are asked your plugin is asked\nto look into the card and\nfigure out\ndid it succeed did it fail is it still\nwaiting for resources and things like\nthat\nthe one thing i wanted to point out or\nthe additional thing i wanted to point\nout here uh i talked about customizing\nlinks in in the ui uh this is also where\nyou do that um you can look at the you\nknow the pod the object and decide\nwhat additional links log links you want\nto expose to users\nuh in the case of\nthe pod plugin it's simple which\nit invokes\nthe log plugins and we will also talk\nabout log plugins in a later\nworkshop\nuh to expose you know all the links for\nuh logs for this pod\num\nwe're switching to the spark one is a\nlittle bit more complex because in a\nspark case you stand up an entire\ncluster\nthat has you know a driver pod and\nmultiple executor pods it has its own ui\nthere's a history server running\ncentrally that tracks your work all\nright so there are a bunch of more logs\nthat we expose\nuh in uh maybe the tens of distributed\ntensorflow distributed bytes operators\nuh convince objects uh it's also similar\nthere are like a notion of you know some\ndriver and executor pods um and uh and\nyou will also want to expose you know\nthe links for all of them\nso that's exactly where you can do that\nthe spark task also plug-in also you\nknow registers itself\nsame way and then\nhas a gain implementation of task phase\nthis is where it gets a little bit more\ncomplex uh ex trying to figure out the\nbright links given\na spark application object\nand then just the normal stuff right\nagain looks at the object state and\nfigure out what does that mean in flight\nlingo and return that\nuh yeah this\nis\nwhat i was talking about when you say\nmore complex uh so all of this\nimplementation is just to get to compute\nthe right load links for in this\nparticular case um we can like if\nsomebody's interested to go deeper into\nhow spark does it we we can talk about\nit but it's just i just wanted to\nmention that this is where your the hook\nyou have to do that\nuh the id builder source is still simple\nright just builds the an empty shell\nobject\nuh and then the\nthe more the other build the resource is\nalso complex\nthis is all of it right so this takes\nthe task template that is passed in and\nits responsibility is to build you a\nfull um spark object to return\nspark application there's a bunch of\nthings uh one of them is in this case is\nuh parse dac the custom fields that we\nlooked at earlier into the proto and it\nuses that to figure out all the\nproperties it needs to set on the spark\napplication uh what you know\nthe driver spec can execute respect are\nand some substitutions for the command\nlines\na bunch of things uh resource you know\nrequirements that you specified all of\nthat and then at the end it just builds\nyou a full you know the\nkubernetes object uh and return that\nand they right from this point on these\nplugins will behave exactly the same\nright from the simple container one to\nthe spark one they just need to\ntell us how to convert you know class\ntemplates to objects and objects back to\nthe state uh that we understand and then\nthe the base implementation the floccid\nspace take care of uh\nyou know all the life cycle management\nbecause the all combinations objects\nessentially behave the same way\nwhen it comes to that\num\ni think that is it i would be happy to\ntake questions um and i was saying yeah\nthese things we looked at are all here\nso you know point as to if you want to\nget started\nand we'll be happy to take your\nquestions now"
    },
    {
        "title": "Lightning Talk - OpenAI Codex",
        "transcript": "i'm niels bentilan i'm a ml engineer in\nthe union ai team\num today i just wanted to show off this\nfun little thing that i discovered\nthat openai codex can do\nwhich is generate\nsemi-valid flight kit boilerplate code\nso\num just to give everyone some more\ncontacts and background\nuh the openai\nteam recently\ndeveloped and released\nthis model called openai codex which\ngithub since there's like a connection\nbetween openai microsoft and github you\ncan see the\nconnection there\nthat github copilot is a thing that is\nbacked or um that is driven by open ai\ncodex but\nyeah this is it's kind of you know\nmarketed as this ai pair programmer\nthing that you know you write some code\nand then\ngithub copilot will generate\na bunch of code that it thinks would be\nvalid sort of conditioned on the\nprevious code\nso to just\ngo over some terminology\nthere's a prompt section\nand then based on the prompt the model\nwill generate code that it thinks\nis somehow\nsort of coherent with the prompt\nso openai beta is a thing that's um\ncurrently only private so you have to\nkind of wait uh get on the wait list\nand the same goes with github co-pilot\nactually you have to sign up\nbut i just wanted to\ngive everyone a sense of how this works\nso\nopen ai codex is\nkind of\nan\nuh\na specialization of a model called gpt3\nand what gpt3 does\nis write\nis complete\ntext it's like basically a text\ncompletion language model where\ngiven the previous tokens generate a\nbunch more tokens\nuh tokens being\nroughly like letters you can think of it\nso i can say write an essay on world war\nii\nand it'll try to do something okay\ncool so you can see on the right hand\nside here there's a bunch of little\ntoggles so i can increase the response\nlength\num\none known limitation of this model is\nthe longer the\nthe length the response length the the\nmore it will kind of break down\nand become incoherent eventually but you\ncan see here that you know there's\nthere is some language that kind of\nmakes grammatical sense\num i don't want to read this out to you\nright now but so that this is this is\nlike\ntrained on basically the entire internet\nor a big part of the internet\num you can see the british spelling here\nso\nyou know it's\nlike some people consider this as like a\nstochastic parrot so it's like\nsomething that's seen a bunch of texts\nand as basically kind of regurgitating\nwhat um it has been trained on\nthe more optimistic folks will say like\nit's actually learned how to like\nabstract concepts and is able to like\nsynthesize it in new ways\nso you know\ni think\nthe jury is out on that but\nto switch over to\nthe davinci codex here which is the\nopenai codex model so this\nhas basically been trained on all of\ngithub so any public repo\nand i'm not sure of private repo i don't\nknow probably just public repos\nyou can be\nfairly confident that this model has\nbeen trained on so it's seen this code\nand presumably it since flight kit is\nopen source and you know the flight\nrepos are open source it's seen that\ncode and you know has been trained on it\nto some degree\nso i played around with this a little\nbit and i'll just show you like a happy\npath\nof this so i'm gonna prompt it by saying\nfrom flykit import task and workflow\nand then in a python code comment\ni'll say this function is a task\nthat\ntakes\ntwo floats and outputs\ntheir sun\nso sometimes i found that this outputs\nlegacy flight kit\nuh syntax\nso yeah so to today if the demo gods are\nnot smiling on me\nit's um\nso let me just tweak this a little bit\nuh this function test it\ni don't know i'm just\nthere's a whole field of like prompt\nengineering now of like how do you\ndo these prompts ah okay great\nso\nas you can see it's done all the type\nannotation here as is\nwhat flight requires\nand then i'll say this task\n[Music]\ntakes a float\nas input and computes this square\nand let's say squares\nsquares it\noh okay perfect\nand then finally\nthis workflow calls\nadd two numbers this is maybe a little\ncheating but\nand\nsquare a number\nin sequence\nokay so it just\ndid the wrong thing so it squares\nthe inputs first\nbefore\nokay well i mean in sequence is sequence\nso yeah maybe that's why i messed up but\nlet's just keep that\nyeah so i mean i didn't really specify\nin one sequence so technically this is\nstill\nokay\num it even does this register thing\nwhich is interesting um\ni don't think this is actually valid\nsyntax but yeah for now let's let's get\nrid of this so so what what what did i\ndo i basically wrote\ncomments\nand\num open ai codex\nwas able to generate\nyou know\ncorrect flight code obviously\num\nthis probably won't work for anything\nmore than like trivial examples\nbut\ni was still blown away that\nit was it was able to give me the right\ncode so like you can you can imagine\nright so\nthis without the the fly kit prompt\nthis is still valid python code i mean\nthe simplicity of flight kit makes it\nsuch that all it has to be aware of is\nthis ad task and ad portfolio decorator\num but in my explorations of this this\nwas generating like the legacy\nlike syntax with\ni think like the out at input and add\noutput stuff\nso\nyeah that's uh all i wanted to share um\nif anyone is curious or\nuh wants to play around with this i'd\nrecommend you know getting on the\nwaitlist and just seeing how this this\ncan break um this is\njust finally i'll show you this one\none example i did uh played around with\nbefore with involving data data frames\nso like adding new columns to data frame\num\net cetera et cetera it even knew about\ntask resources so i think i i asked for\na flight kit task that requires certain\nresources this is wrong\num\nbut you know you get the idea it it's\nwrong but\nsurprisingly good\nyeah\nthis is amazing\nand just just to clear these functions\nare not in any of our examples right\ni don't i don't think so so this data\nframe one is not so this is like it's\nmashing together what it's seen in data\nframe pandas airframe code with the\nflight\nsyntax so arguably it's actually it is\nactually learning something useful about\nthe structure of code um and by the way\nthis this applies to\na bunch of different\nlanguages as well\nso you know my the parting thought that\ni have for you is like\nyou know if we have all if we all have\nneural links\nthen we have a thought to text model\nand then we have you can you know so you\nhave like a brain to text\nthat generates this stuff\nand then this open ai codex so you have\nlike multiple models like kind of in a\npipeline\num so you can like think\nand then generate code which would be\nyeah that's sci-fi dystopia for you\nnice"
    },
    {
        "title": "Flyte's Native Scheduler - OSS Meet Up Oct 5 2021",
        "transcript": "we've got some\npretty big news today\ni don't know if how many people know\nthis but flight has not until now\nincluded its own native scheduler we've\nalways relied on an external scheduling\nservice from whatever\nhost provider we were on\nbut profile is going to be showing some\nwork he's done over the last couple of\nmonths to fix that and include a\nbaked in\nfully performance scalable scheduler\nthat will become just part of the flight\nstandard distribution that\ncan be relied upon without\nexternal dependency so we've shed a\ndependency which reduces complexity in\nthe system and helps\nsandboxing\nand sort of everything else for the for\nit to feel\num\nlike it's a completely integrated piece\nof the system um and people have been\nasking for this for a long long long\ntime and uh propel stepped it up and\nbuilt it so i'm gonna let him take it\naway\ni think you're muted my friend\nhey thanks george um\nfor the introduction yeah uh so\nuh so my name is professor so i've been\nworking on the flight native native\nscheduler i think the requirement is\npretty clear for everyone we have been\nsupporting this\nthrough aws and\nmost of the examples that we currently\nhave\ncannot be run because most of the folks\nwhen they first try out flight they need\na sandbox environment and in the sandbox\nenvironment they cannot try this\nparticular feature and uh so it's kind\nof a\nmain feature that people want to try out\nuh before accepting uh or ongoing with\nflight\nso instead of deploying an awful setup\nright now we natively support\na scheduler that comes up in the sandbox\ndeployment and can be used\nuh later on uh can be replaced uh in gcp\nor any other cloud provider\nthat\nthat flight is being deployed on\nuh with that being said let me just\nshare my screen and just go over\nuh some of the aspects of it\nyou guys can see my screen\ncool uh\nall right um yeah\nso\nwhat are schedule execution so any ml or\nany workflow engine you\nuh definitely need a ways for executing\nsome of your tasks or workflows in on a\ncertain cadence or a schedule like uh\nyou have a monthly job or a daily job or\nsomething you want to run every few\nminutes or sec\nevery few minutes or uh at a clock time\num and these these can be database jobs\nor any any of the\nml pipeline jobs uh that um that are\nneeded\nso\nso\nthese were the two types of schedules\nthat\nwe supported so there are crown base\nschedules like you say that you want to\nrun something 10 o'clock every day\nor\nsomething like a fixed-rate schedule\nwhich uh every 10 minutes or\nuh or an hour\nor\nper day sometime so all uh these were\nthe two types of uh schedules that we\nsupported and the reason being that uh\ncron uh cannot be used for a fixed\nschedule because\nuh uh the way it divides the time so\nit's always like uh if you're having\nlike uh if you want to run every 25\nseconds you can't just say in the chron\nlike star 25 and uh the entire\nexpression because uh it will execute on\n25 then the next would be 50 and again\nit will\nuh run after a minute and at 25. so\nthere would be a difference of uh\nlike\nthe next the third one would be executed\nafter 35 units which is incorrect so uh\nso for that reason like most of the\nschedulers support like fixed-rate\nschedules\nand\nso that that's that that's the one we\nall we also support in our\nand\nso currently how you create those\nschedules are\nyou you have every\nuh workflow has a launch plan associated\nwith and then you can create\nuh which is a default launch plan which\nyou can uh you can create an execution\nuh through a console or flight ctl or\nanything but if you want to run it on a\nparticular cadence uh then you have to\ncreate a new launch plan for it and this\nis how you actually create uh in flight\nkit so if\nuh he would say that\nsome workflow that you have already\ndefined i'll go through an example of uh\nfrom the existing docs that we have\nthis is a simple example just\nthat okay\nyeah this is a\nsimple example that currently uh in the\nuh we support cron schedules on uh just\nto demonstrate\nthat uh so this is just a task which\nformats a date time and then you have a\nworkflow which\ncalls that formatter and prints it and\nuh what the\nuh the cron scheduler does over here is\nin\nin um\nwhen you're creating a launch plan for\nit you would\nuh instead of creating a regular plan\nyou would say that i'm creating a\nscheduled launch plan and here you'll\nspecify the schedule for it\nand uh\nwhat kind of schedule it is so this is\nbasically a ground expression and\nand currently what we have is uh every\nworkflow that\nuh\nwe you want to launch in a scheduled\nmanner it can accept one additional\nargument which is basically kickoff time\nthat that is basically the time uh the\ntick at which it gets scheduled\nand this could be useful uh for many\nreasons and uh we have included that as\none of the parameters uh that the the\nschedule function can use\num and similarly as i mentioned like you\nhave fixed rate and double schedules\nwhich are also\nuh which you can create and\nuh those will be having a fixed rate as\na\nuh as a function and you can specify\nhow often you want to run it\nthis remains the same how we will be\ndoing it in the native scheduler from\nthe api perspective nothing changes\nother than basically now we have\nthe kron schedule expression now changes\nfrom\nthe aws specific format which was the\ncase earlier because we only supported\naws as a platform where we could run\nthis and they had some specific changes\nor there are some customizations that\nthey have in their schedule so uh we\nhave gone with uh a standard format\ncrown scheduler and\nyeah that's the the main difference in\nthe api and how you actually use it\num\nand okay going back\nto this so after you've created\nthe\nthe crown schedule is basically you will\nactivate the launch plan and activating\nthe launch plan is going to\nuh tell admin that uh it has to create a\nschedule i mean uh it has to create a\nschedule for it so the current mechanism\nof how this is done is basically\nuh we have it's a it's supported through\ncloud watch rules so whenever you\nactivate a launch plan it will add a new\ncloud watch rule saying that\nthis particular launch plan needs to be\nlaunched every few seconds or at this\nparticular time and when that happens\nthere is a particular sqs target which\ngets invoked so there is a payload that\ngets sent from\nuh from the cloudwatch rules so whenever\nthat cloudwatch rule gets triggered uh\nuh\nthe sqsq is populated with a payload and\nflight admin actually listens to that\nsqs target for um running those\nexecutions so that's how we currently uh\nare supporting schedule workflows\num\nwith the native scheduler now we have\neliminated that requirement and the way\nit\nuh does it just an overview of how uh so\nthis i've already mentioned it's uh\nit doesn't support aws specific we have\na standard\nron expression that we support uh it's a\nseparate binary that gets built and it\ncomes out as a independent board so we\nhave a flexibility of\nscaling it independently\nit's a very small footprint uh it's uh\nalthough binary\nuh it's a go binary certain\nuh go\nand uh\nall these schedules that are run are\npretty lightweight and these are run\nthrough go routine so\nevery time you activate a particular\nlaunch plan and the scheduler picks it\nup it will uh\non on the schedule it will create a new\nroutine for it and it will launch the\nuh the function to execute that\nparticular launch pack it will send an\nexecution to the admin at that\nparticular time so\nuh it sends the execution schedule time\nas well as i mentioned that\nit's it's one parameter that you can\nsend to the launch plan uh which could\nbe\npassed in uh to the execution\num another thing uh if we wanted it\nsince we relied on a very uh highly\navailable and fault tolerant systems\nlike aws we also wanted to have uh those\nuh properties in our system so we built\nit for that\nand\nalso like i mentioned like people want\nto try it in the sandbox environment\nfirst before trying it in their\nproduction so\nthat would now be supported in the\nsandbox environment\nokay\nso this is just a overview of some of\nthe components that we have\nin the scheduler\nso you have the schedule management\npiece which is basically going to create\nthe schedules for you and\nso as i mentioned earlier the earlier\nway of creating these schedules was\nyou activate the launch plan that you\nhad created and\nit will go and create a cloud watch rule\nfor it\num\nor if you want to deactivate it it will\ngo and delete that rule from cloudwatch\nnow in the new system this is basically\nmanaged um through db records so we\nwrite the information about what this\nover the\nnew schedule is and\nuh what the expression or the interval\nis and we save it\nfor that particular launch plan so every\nlaunch plan that you activate the\nschedule management system takes care of\nuh creating or\ndeactivating a record based on what\noperation you do if you want to\ndeactivate the schedule uh you do\nactivate uh the launch plan so the\nlaunch plan and the schedule are tied\ntogether so whenever you deactivate the\nlaunch plan the schedule gets\ndeactivated and vice versa\nand\nagain like\nflight supports like multiple versions\nof the launch plan so the schedule\nmanagement system also make sure that\nit supports version management so uh\nschedule also has multiple versions of\nit but at a time you won't be having\nmore than one active schedule because uh\nyou don't want multiple versions of this\ngame schedule being running so\nwe take care within the system that only\none\nuh schedule is activated when you\nactivate the launch line\nand\nyeah so\nso this is the management portion the\nthe main scheduler portion is uh\nhas multiple subsystems within it and it\nuses the flights control link which is\nthe flight admin for\nuh sending those execution for creating\nthose\nexecutions um\nso let's just go over uh the core pieces\nof it so\nuh the core piece is basically the the\ngo con wrapper so this is\nuh an open source cron library that we\nare currently incorporates written in\nthough and\nand it supports uh both the requirements\nthat we have for supporting fixed rate\nand standard cron schedules um\nso\nthis particular one uh\nis uh open source and we have modified\nit a little to make sure that uh uh to\nthe callable functions which can\nactually pass the we want to pass in the\ntrigger timestamp for basically the\nscheduled timestamp\nwhich none of the existing schedulers\nhave\nso\nwe have modified the library so that\nthe function accepts uh the current\nschedule time so let's say that you have\na\nschedule that you want to send to the\nadmin because it has to be uh it has its\nschedule time is ten o'clock it might\nhappen that\nyou send the execution maybe after a few\nseconds or a few minutes because of some\ndelay but your downstream systems need\nto know what time this particular\nuh execution was created so that's how\nlike uh we want this uh crucial time uh\nof what uh\nwhat was the scheduled time that this\nparticular execution was sent and that's\nwhy we pass in that\nthat schedule time uh to the function\nand\nso for that we have a modified on this\nparticular\ncode piece of the scheduler\nand the executor is the one which\nactually sends the executions to the\ncontrol plane\nso what it does is whenever um the kokon\nwrapper is actually going and scheduling\nnew go routines based on the the\nschedule that it has read um so\nuh\nit basically uh\nso it has read a set of executions from\nthe schedule management system and now\nuh it wants it it creates the in-memory\nuh routines so that it can execute those\nso\nuh so it\nit calls the executive\nuh the executor subsystem to actually uh\nexecute those uh functions uh those\nparticular schedules to in flight admin\nso uh\nyou have every ten minutes so\nthat go routine gets created every 10\nminutes and the executor's job is to\nmake sure that it creates a unique\nidentifier for that execution based on\nthe\nuh the timestamp and the\nthe name of the schedule and it sends it\nto admin admin immediately goes and\nexecutes that\nthat particular schedule and\nso\nand\nit can run multiple so we have uh you\ncan have multiple schedules being\nrunning simultaneously and each of them\nwill be uh sent to the executor as\nseparate go routine functions and uh\nthey will be independently sent to uh\nthe admin\nso it becomes like\neasily scalable and uh\nindependent in how each schedule behaves\nin the system\num\nuh\nanother thing we also wanted to uh\nis\nwe didn't want to\nmiss any any of the the schedules uh so\nfor some reason if\nuh if let's say that the scheduler goes\ndown um\nthen we don't have the information about\nlike during the time it is down uh what\nall things we missed so\noops sorry\nuh so for that reason we have two\nsubsystems over here we have the\nsnapshotting system and the catch-up\nsystem\nso the snapshotting system's job is to\nrun as a separate routine and what it\ndoes is just stores the information of\nwhat the\nuh what is the current state of the\nschedule so if it's running every minute\nit will\nit will just update a timestamp saying\nthat i executed at this time\nand it will update an in-memory map\nsaying that this particular schedule\nexecuted at this particular timestamp\nand this map of\nthe schedule and timestamps is then\nsaved periodically into the dvd so that\nlet's say that\nthe scheduler goes down for a few hours\nyou can still come back and read the\nsnapshot and try to catch up on those\nschedules so\nuh because it knows when it executed uh\nin the last time using the snapshotter\nand\nthe catch-up system helps in that way\nthat it will it will read that snapshot\nand schedule all the missed schedules\nduring that downtime so uh it will\nexecute\nuh from the last snapshot of time to the\ncurrent time and only after that will\nthe go contractor will schedule the next\nnext set of execution from the time.now\num\nand\nanother thing was like uh in order to\nminimize the footprint of the snapshot\nof this\nuh footprint of what gets saved uh\nthrough the snapshot which is basically\na map\nof the schedule and the time stamps so\nuh we don't we uh we rely on other\nbinary format of storing it so that uh\nit's uh it's compact uh instead of\nstoring it as a json or\nany other format so um\nso\nin the database we uh store one record\nsaying that this\nuh this particular\nuh\nat this so we execute it every uh\na\nfew minutes or like every 30 seconds and\nthe snapshot is taken and the\nsnapshotter is\nexplicitly running as a separate go\nroutine and\nit updates\nthe\nupdates of the tv with all the schedules\ntimestamps\num and\num yeah and the catch-up system is the\nas i mentioned like okay\nwhen the scheduler comes up it\nthe first thing that it does is it runs\nthe catch-up system so\ngoing through all the schedules it needs\nto understand um\nuh is is there anything i need to catch\nup and\nuntil those all schedules are caught up\num it won't start new schedules through\nthe gold coin wrapper so so the catch of\nall system is the one which uh takes\ncare of doing that so it utilizes\nwhatever information has been saved by\nthe snapshotter and additionally uh\nwhatever is saved by the schedule\nmanagement system and\nit decides uh which are the schedules\nthat it needs to uh send uh for\nexecution\nuh so\nyeah so\nthose are the main components that i had\nover here i'll just go over like\nhey profile i had a question about the\ncatch-up system so the catch-up system\nstarts in\nparallel with the actual executor right\nbecause otherwise\nyou might delay executions\nnew new executions that should actually\nhappen let's say at the point\nyou know you are down for a little bit\nbut when you come back up you will\nprioritize the new executions right\nright right so so the executor\nis\nnot a separate routine it relies it's\nused as a library\nso\nso\nexecutor does\nit is available when the catch-up system\nis running\nokay yeah\nyeah\ni guess because when you said that you\nknow we don't run any new execution\nstill we don't catch it that's probably\nconfusing we run new execution between\nin parallel catch up the older execution\nand this relies on the property that all\nexecutions can\ngo in parallel in the worst case\nyes\nokay um\nso\nsome of the failure scenarios that i\nwant to just go over just browse through\nso if the scheduled time execution fails\nlike let's say the admin is down it\ncannot accept any executions\nor it\nif\nit fires an execution but\nsomehow it fails so what we have is uh\nretry mechanisms built into this\nevery uh every execution that you send\nto admin uh so there are certain certain\nconfigured number of retries that it\ndoes through\na back off and\nif if it fails then again like it tries\nto attempt to retry until exhaustion\nbut it is possible in a scenario where\nyou have like\na piled up uh schedule where you have\ntwo time stamps t1 and t2 so t1 fails\nand you have\nand it's trying to attempt to send an\nexecution to the admin and you got\nanother scheduled time t2 for this game\nschedule uh but that succeeds uh because\nuh for some reason due to some race and\nso the order in admin could be uh t2 and\nt1 executions uh can come out of order\nbut uh the schedule time still remains\nintact uh in admin so admin does know\nthe\nuh the actual time systems of the uh\nof the executions\nuh another thing is like if the\nscheduler goes down uh this actually we\nwent through that so we have the\ncatch-up system which uh goes through\nall the schedules with the last snapshot\nat time and until time dot now it will\ngo through all of those um\nand\nanother one is if the snapshot of fails\nto\nrecord a timestamp uh record some\nsnapshot\nand it has an older snapshot\nstill the system will behave correctly\nwhat it will do is\nuh from the last snapshot of time if\nthere were certain attempts made during\nthe last uh\nexecution uh or the last system uh\nuptime uh it will try to send those\nexecutions again uh but we rely on the\nidentity aspect of the execution because\nthe uh the execution itself is uniquely\nidentified by the scheduled time and the\nidentifier of things\nof the schedule uh so it includes the\nschedule name the project domain\neverything regarding all the context uh\nfor that schedule uh is there in this\nidentifier and along with the scheduled\ntime so if admin knows about it that i\nhave got this execution already\nbased on that uniqueness criteria it\nwill uh it will send a failure back uh\nto the scheduler and currently the\nbehavior of the scheduler is to ignore\nthis error and do not retry so that uh\nit's aware that the schedule already is\nthere in admin so\nuh that's how it uh\nit will be able to um\nrun through any of the missed executions\nas well uh and not worry about any\nduplicates being sent\num\nnexus like you have scheduler being down\nagain like if you have something that\nduring the downtime something gets\ndeactivated or activated\nwe just go by the last state and if it's\ndeactivated we'll remove the schedule\nfrom the scheduler and once it's up and\nif it's activated we will just use the\nthe new timestamp from which it will\nstart executing\nuh sending new executions\nit won't send um\nit won't try to attempt any uh\nany of the uh sending any executions\nbefore that time stamp before that\nupdated timestamps\nuh another one is multiple instances\nthis is already covered over here where\nuh we rely on the item bodies itself so\nyou can have multiple instances of the\nscheduler being up but uh\nthe execution identifier itself uh will\nthrow out the execution and um\nif uh if it exists in that one\nand\nyeah this is this\num\nsystem downstream is similar to that so\nuh only the scheduler being done we rely\non the system\nflight admin being down rely on the\nretries on the go routines to make sure\nthat the execution goes and\num is reattempted uh assuming that admin\nis not going to be down for like a long\ntime but we can make it configurable so\nthat uh\nright now it has a fixed number of free\ntries uh that it goes through which are\nenough uh according to what we studied\nbut if uh yeah something like what\nhappened yesterday with facebook if\nsomething like that happens and you\nprobably can expose uh like\nuh\nuh\na configuration which can increase the\nnumber of retries or uh\nuh in the future i'll also explain that\nwe are planning to have\na dead letter q which can be used for\nuh attempting those failed uh executions\nuh schedule executions\num we also support both the scheduler\nand flight admin down uh this also\nuh is supported and\nthe catch-up system will take care of it\nand the future enhancements is basically\nuh more intelligent free choice\ncurrently we just have\nuh we try on all attempts so that's the\none we want to uh target and\nand\nand uh the other one is basically as i\nmentioned about the deadline to queue in\ncase of\nuh retrieves being exhausted and we have\nsome certain mixed schedules we want to\nuh create records which can then be\nretried later maybe manually or some\nthrough\nuh through some trigger job\nwhich can read this uh information and\nthen send those missed schedules to\nthe admin again\num\nanother one is\nseparating uh\nthe dd or the schema currently it's\nco-hosted uh i mean it's\nuh it's a separate board but it's still\ncurrently using the same database us\nand the schema\nbut\nwe can plan to move to another schema or\ndb uh if\nas like a micro service architecture\nwhere you have your own db\nso that can be a separate feature\nyeah that's\nit from\ni think from me uh\ni was facing some issues i wanted to\nshow a demo\nuh\nbut\nlet me see if i can get something\nyeah i think this might take a while so\nuh\ngetting while\nyou're talking like you have\nany of the\nthings that we\nwant to add for this\nnot from my end we should open up for\nquestions if anybody has\nyeah i think i'll add one thing so uh\nwhen we started flight we actually\ntalked a lot about writing our own\nscheduler uh and we constantly\ndecided not to do it\nbecause it's not a simple and\nstraightforward problem that\nmany folks make out to be because\nwe do not want to ever have missed\nschedules and and that's a very tough\nguarantee to provide\num and so it's it on its own is a full\ndistributed system that exists and runs\nuh and making it fall tolerant and\nreliable was a pay exercise and\nthank you for the contribution\nthank you um\nso you can see that flight scheduler\ncomes up as a separate board uh we can\nrely on just one instance\nand it waits for the admin to be running\nso only after admin is up\nuh it doesn't run\nokay so\neverything is up so i've already have\nserialized examples so this is the lp\nseries examples that's there in the\ndocument so i've just modified it to\njust uh\nlaunch it uh every minute uh i'm just\ngonna i've already have a serialized\nversion of this\nwhich i'm gonna\nregister\nthis is\nokay so\nlaunched once\nso this is the the crown schedule that\nwe've created which is going to execute\nevery minute\nso i mentioned that we have to\nactivate that launch plan\nand that should\nstart executing it\nso this is how you do it from flight ctl\num\nso\nit takes a minute for this one to\nschedule a new\nexecution so right now there are none\nuh\nuh yeah another thing mentioned\nover here is yeah we also publish\nmetrics from this board\nso they can be\nmonitored and visualized\nokay so you can see over here that\nsuccessfully added a schedule and\nit also fired a new schedule\nso you should see a new execution here\nokay started\nrunning uh\nit has a scheduled time\nand\nso this is going to run every minute\nthen execution would be 1635\num\nyeah i think i that is the the minimum\ngranularity that we have we don't have\nper second granularity uh\nit's currently\nyeah so you can see the next execution\ngot triggered at 35.\nnow another thing i wanted to show like\nwe can bring down all the bots uh not\nall the parts basically the admin board\nand\num\nthe scheduler board so i'm gonna just\ndelete those\nwhat\nit's very brave for a demo\nand you're deleting them constantly\nyeah\nso\nin a while loop\nwow\nyou have tried this\ni hope that works\nokay i'm deleting the scheduler as well\nso\nso right now\noops\nso\nbecause it's unhealthy i can't get any\nexecutions uh or get any status from the\nadmin because uh both the scheduler and\nthe admin around so this relies on admin\nbut we don't have any data from that so\num\nwe're\ntrying to actually stop this so that\nstop the\nmadness yeah\nscheduler is coming up\nokay\nso you can see the\n35 after the 35 yeah we got\nfrom the downtime 36 37 38\nuh those got carried every minute we\ndidn't miss anything\nuh\nand also i think\nthe new ones will be\nwhat\nso this is where the\nthe catch-up system run\ngadget caught up with the schedule and\nthis is the new one which got fired\nafter the uh the catch-up system ran\nso\nyou now see it\nthis 38 i think was the new one\nyeah this was the one which was after\nthings were caught up\nyeah i think that said from my side i\nthink uh\nawesome\nthank you for the demo actually that\nhopefully helped a lot of people\ni i want to add one more thing so the\ngojek team actually helped they were\nvery brave when they took the\ncode from the\npre-checked-in\n[Music]\nbranch and they ran a big load test on\nit\nand it worked\npretty well under thousands of workflows\nconstantly being run through the\nscheduler so\nit's almost ready for prime time\nthis will be available in the current\nrelease\nand\nit will be available in the flight\nsandbox and can be used in any other\ncloud provider even the existing uh\nuh aws one and see we we plan to uh use\nuh nato scheduler if\nthings go good\nnot only cloud provider you can use it\neven on-prem\nyeah\nso obviously this is new and it's a\nyou know tricky hard problem so\nplay with it cautiously at the beginning\nbefore you you know really rely on\nproduction deployments for it but\nwe believe it works and we've got some\ndata that that uh\nthe gojek has helped us sort of validate\nfrom their point of view that it works\nunder a under a heavy load test i'm sure\nwe must something in our testing and\nthere will be some small problems\nlater on so don't\nyou know\nbe cautious but we are optimistic that\nthis it's you know essentially\ntheoretically correct and there aren't\nany problems with it that we can't fix\nif we find them we know about it so we\nbelieve it's ready but it's new code\num\nuh so thanks protocol that's\nthat's awesome"
    },
    {
        "title": "How to give Lightning Talks at Flyte OSS Sync Up Meetings",
        "transcript": "next on the agenda is nils has proposed\ndoing um\nuh lightning talks for this uh for this\nmeeting going forward and he wanted to\ngive us a meta lightning talk about\ngiving lightning talks\nthanks george um can i see my screen\nexcellent uh hi everyone my name is\nniels i'm a machine learning engineer\nat the union ai team and i also\ncontribute\nthings here and there to the flight kit\npython sdk\num\nincluding examples and tutorials and\nother things in the core code base\num\nbut today i want to talk to you about\nhow to give a lightning talk and\nit's in the lightning talk format\nso before i go on i've taken great pains\nto avoid\ninfinite recursion here because at some\npoint i was like\nas you'll see there there's a certain\nstructure of lightning talks and so you\ncan easily\ntalk about\nhow to do it and then keep going down\nthat rabbit hole\num so what's electing talk\nit's usually a three to five minute talk\nand the purpose of it is to expose\npeople to an idea you think is worth\nsharing\nit's very high level\ndirective there but\ni found it actually very useful and even\nfor longer form presentations i use the\nthings that i've learned\ngiving these things and i applied to\nthem and sort of tweaked it because\nobviously\nlonger form presentations allow you to\ngo into more detail\nso the general structure of lightning\ntalk um this is according to one of the\nreferences i'll link to\nis you want to start with who and this\nis important because\nif you just kind of show up as a talking\nhead\nand people don't know who you are where\nyou came from what you do\num\nthere's not\nor i'll say it another way doing so\nintroducing yourself provides everyone\nwith cot more context of like okay wow\nhere's this topic hopefully the the\ntitle of the talk is informative here's\nthis person now delivering these ideas\nto me\nget into the what so that was the\nprevious slide so i talked about what a\nlightning talk was\num\nand this could be you know something you\nbuilt something you learned from a blog\npost or a book or an article\nand then the why and this is important\nbecause it provides\nlike in combination with the who it\ngives you\nadditional\ncontext on why\nthe topic is important and why you\nshould care\nyou as the audience\nthen go into how so if this is say like\na technical talk this might go into\nhow you did\nwhat you did\nand the last two are important so the\nthe penultimate\npoint here is so what can i do with this\ninformation\nand so this gives\nthe the speaker like the chance to\nsort of articulate why other people\nshould care about this topic\ni'll get into why lightning talks are\nimportant\nand so what you can do about it and\nand finally the last point is is where\ncan i learn more and so again to kind of\ntie this structure together\num\na lightning talk doesn't give you enough\ntime to go into the details into the\nmeat of something but it does\ngive you a chance to like hook\nuh get you know introduce a little like\nmind worm into someone's brain to like\nsort of\nyou know poke them into\nlearning more um and really\nthe whole\nthe whole structure of this is to\neffectively be able to you know tell\nsomeone why\nyou'd want to learn more about some\ntopic\nso the first i guess the zeroth level of\nwhy\num we're doing this is we want to\nprovide a lightweight venue for the\nflight community to share ideas and\nprojects\nand we have the oss sync\nthis seems like a perfect venue to do\nthis and\nso\njust to\num\njust to say\nmyself and some of the other teams i'll\nbe i'll be bugging them\nto do at least one lightning talk\nper\num oss sync up that's not a workshop\nbecause i think workshops take the\nentire time\nbut um\nif no one's gonna sign up i'll just be\ntalking about something\nsomething or other you know packagex\nwith like package while i was like so\nyou know if you get tired of\nlistening to me\ndo that then i would encourage you to\npropose something and i'll show you how\nto do that\nkind of the second or i guess the first\nlevel of why um for you\nis as domain experts or engineers\ntechnical folk um\npeople who build things um or study\nthings analyze things we tend to\nlike to put\nour head in the sand like this ostrich\nhere um i might be projecting but this\nthis is just me maybe but\num i think\nthere's a common mentality of like if i\nbuild it they will come because it's so\nawesome right\nand the reality is someone needs to\nbroadcast the fact that something exists\num hence why advertising and billboards\nand whatnot is such a big industry\num\nand enlightening talks are a way of\ndoing this\nfor complex ideas and you know simple\nideas as well\nso why you should\nconsider giving a liking talk is it\nin this venue in particular is it it\nprovides a low stakes way of you for you\nto practice communicate communicating\nideas in a quick and simple way\nit'll force you to distill complex ideas\ninto their core principles\nand expose others to things that you\ncare about\nhow this is fairly simple i mean you\nhave some kind of slide deck software or\nsomething\num\nthese are kind of just rough guidelines\nthat i use but one idea per slide is\ngenerally good\nuse images and gifs that uh says we've\nseen i use a lot here\nand the fewer the words the better\num again these are rules of thumb and\nand in longer form presentations you'll\nwant a lot of detail and you'll want to\ndwell on a single slide\nso what can i do with this information\nit might seem scary i know public\nspeaking is not\ni think is like the number one fear of a\nlot of people i i don't i forget where\ni'm pulling this out of but it and i\nmean it is scary but i think this uh we\nwant to give a venue for you to like\npractice and\nto be able to put these things together\num the sign up form is here\nso if i click into this\njust a google sheet\nit takes you to two paths so if you want\nto be a speaker great um you can go next\nand\nyou know provide some information about\nthe talk title uh when you want to give\nthe talk\nif you just want to propose an idea you\ncan go through this branch and just give\na kind of high level topic and\ndetails on\nwhat\nthe topic would be about\num\nand then my last pitch to you is if\nyou're a procrastinator like me toxic\nfor you so i put this together in 20\nminutes right before oss sync up\num\nagain this you know this doesn't have to\nbe super polished super refined you know\nit doesn't have to be like this huge\ninsight or anything it could just\nliterally be\nyou know i did x with flight or\nhere's what i learned from a blog post\nabout distributed data processing or\nsomething you know around the topics\nthat we care about here in the community\nor here's a python package uh that would\nbe cool to integrate integrate\num\nthis is a an idea i had for some talk\nthat's like\nhere's like julia like this language\nthat doesn't have an sdk here's how you\nrun something on the raw container\num and then there's another wacky idea i\nhad on of like building a distributed\ndata frame on top of flight probably\nprobably not\nyou know would be a cool exercise in\nlike understanding how these things\nwould be built from the ground up but so\nthese are like\npretty wide and broad and you can talk\nabout pretty much anything\nyou know at least within the scope of\ndistributed data processing\nwhere can i learn more here's the self\nreference to this slide deck\num i'll i guess i'll share this in the\noss\nnotes\nand then a few other things that i i\nkind of i've known before slash googled\num when i'm making these slides just\nabout\nother people's guidelines about how to\ngive a liking talk\nand with that um\nthanks\nhopefully\nthis will get you to\nput something in that google form"
    },
    {
        "title": "Training Parallelized, GPU-accelerated Pytorch Models on Flyte - Sept 21 Flyte OSS Sync Up",
        "transcript": "um today one item on the agenda um so\nwelcome to new people\nuh niels is going to be showing us off\nsome work that he's been doing to\nillustrate um training uh using gpus\non flight and so\nwith that\ni will turn it over to niels\nthanks george um hi everyone\ni'm niels and i'm a machine learning\nengineer in the union ai team\nso today i wanted to talk about\nusing flight to train pi torch models on\ngpus\nso before i get\nstarted just want to make sure people\ncan see this full screen right here\nexcellent\num\nalso feel free\nuh to\nask questions or interrupt if you if\nanything\ncomes up\nuh so yeah so\ni'll talk about training parallelized\ngpu accelerated high torque models on\nflight\num this is the flight oss sync september\n21 2021\nthe asterisk here is that this is for\nsingle node training so\nthis is not multi-node training\nso we're going to be training some\nmodels on\nmachines with multiple gpus\nin the future\nwe're going to support multi-node\ntraining that involves a little bit more\nwork i think there's an mpi operator\npretty much done and in the works\nso\nlet's get started\nthe outline for this workshop\nis\nuh i'll get into i just wanted to\nmotivate this\nlike y scale up and accelerate your\ntraining in the first place\num\nbesides just like having a train faster\nwhich is\nthe obvious motivation but i didn't want\nto go a little bit into the model\ndevelopment life cycle\nand then\nfrom there go from go into\ngeneral ideas about scaling models\nand and then kind of\ngoing into a deeper dive into pytorch\nparallelism\nthe kind of the main concepts there\nand then\ngive you a little demo of\nsuch a like a task and workflow in\nflight that uses high torch and gpus\nfinally some takeaways for you and next\nsteps\nso\nlet's get into it\nso in this context\ni'm referring to model development life\ncycle in a kind of a narrow scope so\ni mean the process of training a model\nin the lab\nsetting aside data set collection you\nknow labeling an annotation\nand then also ignoring for now the\nproduction deployment\nand um also like the maintenance\ninvolved with retraining and stuff like\nthat so really here i'm i'm assuming\nthat you have a fairly high quality data\nset\num so for example in the supervised\nsetting you have a pretty good you're\npretty confident that your labels\nappropriately capture the concepts\nyou're trying to model\num\nand so this is really about the\nrnd kind of iteration process before\neven\ndetermining that your model is good\nenough to deploy\nso the question really like one of the\nquestions here is like how do you get\nthere\nand so just to review the bias variance\ntrade-off\num you can think of bias as how close a\nmodel's predictions are to the ground\ntruth so if you have some training set\nreally how how good are the predictions\num and so in a regression setting you're\ntrying to predict some real value you\nknow continuous variable\num in the graphs below this is kind of a\ncartoon that's often depicted\nwhen people try to explain\nbias and variance\nso on the left side you can see an\nunderfit model so you have on the x-axis\nthe variable you're using to predict the\noutput variable which is on the y-axis\nand you can see you can kind of eyeball\na little kind of curve there where as\npredictor as a predictor increases in\nvalue\nthere's an uh\ncorrelated like increase in the output\nbut then after after a point it kind of\ndrops off\nso\nif you just draw like a straight line\nto try to fit this curve you can see\num also can you see my cursor just want\nto make sure okay excellent so i can use\nthis you can see like the difference if\nyou sum up or if you average all the\ndistances like it's not a great fit so\nthat's that's a high bias model\num on the right all the way on the right\nyou can see\nthe you know you've just done a connect\ndots type of model so maybe you've\nintroduced you know high high degree\npolynomial terms in your model and now\nyour model is just\ntracing through all the dots in your\ntraining data\nso this is a low bias model and that it\nfits the training data perfectly\nbut it's a high variance model which is\na bad thing\nand variances um\nyou can there's a lot of ways to look at\nit but one way to explain it is it's how\nstable a model's predictions are in the\nface of perturbations in the training\ndata so imagine if i like\nadd a little dot up here\nthis model you can imagine this dotted\nline is gonna gonna change like crazy\nor if i add another dot\ntraining point over here\nagain the model is going to change like\ncrazy whereas if i add a dot here\nwith a very simple model just a straight\nline that line is not going to change\nthat much it's just sort of kind of\nstay fairly stable\nand so it's robust to\nto you know random noise or\nperturbations in the data\num and obviously in the middle is like\nthis goldilocks model where it kind of\nfits the data pretty well but it's not\nlike if i introduce a data point up here\nyou can manage\nyou can imagine it's not going to change\nthat much\nso another way people think like to\nthink about this is in terms of this\nplot where the x-axis is some\nsome kind of model complexity so you can\nthink of this also as like how long\nyou've been training like the deep\nneural net or any model of interest\nbut in this case model complexity can\nmean how many terms how many features\nare in your\ntraining data\num\nhow many interaction terms you're\nintroducing to the data things like that\nnow the y-axis is error\nand\ngenerally speaking\nthis is obviously an idealized\num plot but generally speaking the the\nmore complex your model is the more\ncapacity it has to\nfit the training data\nbut then it's you're also trading off\nvariants in that if it's really complex\nit can just kind of memorize the\ntraining data\nand then once you have something out of\ndistribution in deployment it's it's not\ngoing to know what's what um what to do\nso like the predictions for those data\npoints that are out of the training set\ndistribution will throw throw the model\noff and so as you increase model\ncomplexity\nyou're you're better and better fitting\nthe training data but then you're also\nlikely going to\noverfit the training data and so this\nthis um\ni'll show you one more plot to kind of\nhammer this home\nso in the real world this is what\nerror curves loss curves\ngenerally look like\nso this is probably a batch wise plot\nactually not these are epochs never mind\nso i just took this from stack overflow\nin blue is the training set lost so\nlower the lower the better\nand the test set loss is in orange\nand so you can see after around epoch\n100\num\njust as a reminder epoc is like how many\ntimes you've gone through your entire\ndata set\nso around the epoch 100 you're you're\nkind of seeing the sweet spot go away\nand the gap between the blue and the\norange line you can see as a measure of\noverfitting\nso right so so here in practice\nwe assess overfitting by splitting our\ndata into a training set and test set\nthe model doesn't see the test set um\never to make predictions and to update\nitself\num you iteratively update the model and\nthen you plot\nyou plot the different losses\nokay so\ni hope that gives you some intuition\nsome sense of\nof the the kind of science behind\nmachine learning it's very much\nempirical you have to look at the data\nyou have to look at the model\nthere are kind of general\nrules of thumb of how to debug models\nin this way\nbut\nyeah in my experience\nlost curves don't\ntypically look like this they're a lot\nmore bumpy and\num especially if you have very complex\nproblems and\nthings you're trying to model\nokay so with that i'll get into\nthe\nthe model development life cycle sort of\nas i see it this is not uh canonical in\nany way but\nso\nit's good to start small\num small model small data so you have\nyou say you have a million data points\num and then\nagain i'll i'll preface this with the\ncontext that\nwe're using a model family that's\num\nwell actually this actually sort of\napplies to various other model families\nbut\nhere since we're using pi torch let's\nsay you're using deep learning or neural\nnets and and all the bag of tricks that\ncome with\nmodern neural nets\nso\nsay you have like a non-trivial problem\na non-uh kind of benchmark data set and\nyou're trying to you have like some\nproprietary data in your company and\nyou're trying to model it you don't know\nthere's no literature on any of it you\nmight like look for some papers that\nsort of fit the problem\nso okay i have some kind of architecture\nsay i'm using like cognates or something\nor transformers\nso first i want to see if i can actually\noverfit on just a small subset of data\nlike 20 data points or 10 data points\nand that makes sure that my the model\nimplementation is correct\nyou know even with tools like pytorch\nyou don't have to code back prop by hand\nanymore which is awesome\num but still\ngradients might not be flowing through\nall the parameters of your model you\nwant to you know make sure of that\num\nso there are a few other debugging steps\nhere that i'm kind of glossing over or\nskipping\num but\nthe main question is can i just overfit\ncan i just memorize it\nthe data\nso if no then there's something wrong\nwith your implementation there's a bunch\nof things you can try here that i won't\ngo through but there's something there's\nsomething wrong with the model\nimplementation if you have to go back\nif yes then you can go to phase two\num and if you can answer yes to this\nquestion\nyou kind of you know these are\nrelative terms uh very hand wavy but you\ngo to a medium model and a full data set\nso then\nnow the question you're asking is can my\nmodel fit the training set\nlike all of my available\ntraining data without overfitting on the\ntest\num\nso even if you're using like a smaller\nmedium model and if you're overfitting\nuh then try a small smaller smaller\nmodel or try regularization or something\nthat kind of bounds or constrains the\ncomplexity of the model in some way\nand if you're seeing like you know test\nloss and and training loss of just keep\ngoing down keep going down and you know\nthere's\nyou've been training for thousands of\nepochs or maybe even less but\nif you see this pattern where\ntest loss just keeps going down\nthen you can go to phase three\nwhich is\nyou know a large model and here a large\nmodel can be\nsomething that generally doesn't won't\nfit on your laptop you know another like\na modern laptop so you're no longer\nyou're like struggling maybe you're like\neach batch is a batch size of one or two\nand you it's just taking forever\nand just so now now and and see you're\nkind of well justified your\nyour test and training loss is going\ndown\num\nand now you're fairly confident that\nscaling up your model size\num\nis is going to get you more performance\nthis is what this workshop is about um\nhere in the teal\nuh\nother so there are other ways of\nthinking about this so you don't\nnecessarily need a large model maybe\nyou're trying to tune hyper parameters\non on a fairly simple model that trains\nfairly quickly but the hyperparameter\nspace\nis huge and as a reminder\nhyperparameters are non-learnable\nparameters so these are things like what\nis the architecture what is the learning\nrate so these are things that are not um\ntunable by whatever uh\nwhat whatever the learning algorithm is\nthat you're using\num\nso i'll kind of this is a topic for\nanother workshop perhaps um for now i'm\njust going to\ntalk about you know scaling up your\nmodel\nand the things that are associated with\nthat\nso these are the yeah maybe i should\nhave clicked through these but yes so in\nbold is is is what the focus of this\nworkshop is\ncool\nso we start small as like an ant and\nthen become a mountain\noh\nwell this is not working let me\nrefresh\nah okay\nso\nthere are two two types of parallelism\nthere are probably more but uh two big\nbuckets are data parallelism and model\nparallelism\nso in data parallelism what you do is\nyou\nyou have some\ncanonical model you replicate it across\nyour devices in this case we have four\ngpus\nso we just copy over the weights\nacross all the gpus\nand now we can handle larger batch sizes\nso say you've scaled up your model\nand you can\nyou can only\nprocess you know a batch size of 10 or\nyou know yeah 10\nper device\nso now you can throw\nmore devices at the problem\num it's it's not\nit's in a sense it's wasteful because\nyou're copying model weights over but\nthen you know what can you do um\nso instead of batch size of 10 we have\nan effective batch size of 40.\nand um\nthis again glosses over a lot of details\nbut\nyeah so each each device handles that\nthe batch and computes gradients for the\nmodel and then\nwhatever framework you're using will\ngenerally handle the combining of those\ngradients in order to update your your\noriginal like the canonical model\num\nso if you think of this as one update\nthen this this updated model is then\nall the weights are propagated across\nall the\ndevices for the next for the next round\nwith model parallelism\nit's it's tougher because it's uh it's\nsort of\nyou kind of have to get into the guts of\nyour model\nand what you're doing here is your a\nsingle model can't even fit in a single\ndevice so this is when you'd want to use\nmodel parallelism you can obviously\ncombine the two which then\nincreases the complexity of your code\nbut here what you're doing is you're\ndistributing the weights across\ndifferent devices\nyou're handling whatever batch size you\ncan handle\nand it's kind of\npassing the tensor from the from the\ndata batch through to all the different\ndevices\num you get the gradients at the end and\nthen you'll update your model\nso these are the two\ntype of um\nframeworks for parallelizing a model\nfor your you know your your training\nalgorithm\nand so getting into pi torch parallelism\num\ni'll only get into data parallelism for\nfor now\num\nit's a little easier to think about and\num demonstrate although it i think it\nwould be worth another talk or workshop\nto to show model parallelism\num\nso here's the the the general anatomy of\na pie torch\ni tried to do it in 10 steps but uh alas\ncould not so it's 11 11 steps\num so first you want to define a model\nhere\num then load your data\ninitialize the model\nspecify some kind of optimizer so sgd or\natom or whatever else\nand then for some number of epochs\niterate through your data\ncompute the loss\nback propagate the errors\nuse a gradient update\nand then collect metrics\num here i'm kind of glossing over the\ntests the test set\nstuff but you can imagine it looks very\nmuch like this without the back prop\nand then you save your model at the end\nand this is the artifact that you're\ninterested in persisting and maybe\ndeploying\nso there are two ways of\ndoing data parallelism in pi torch the\nfirst is this torch.nn.dataparallel\nit's quite magical in that it's a\none-liner so if we have four gpus\nwe just wrap our model\ninside the\ndata parallel class\nthis is i just did this to be explicit\nyou don't even have to do this it'll\nautomatically just\nuse all of the available devices\nin the runtime environment\nbut in this case um i'm just specifying\nyou know device id gpu0123\nand then every everything pretty much\neverything else in your code is the same\nexcept for when you save it you have to\naccess this dot module attribute\nin data parallel\nso that's amazing um what's happening\nroughly under the hood is that data\nparallels with the batch plus specified\ngpus\nperforms the forward pass on each gpu\nconcatenates the outputs\nand then the gradients from each device\nare summed into the original module\nthis is all happening under the hood and\nit's it's\ngreat because you don't really have to\nthink about it\nwith um the other type of\nuh data parallelism in pi torch you have\nto do a little bit more so\ni want to say that this is all you have\nto do but there's more with using\ndistributed data parallel\nbasically you need to spawn the n number\nof processes for each gpu\nfrom rank zero to n minus one\nthen you need to invoke this emit\nprocess group\nfunction\nbefore running your training routine\nand then you have to write code\ncarefully being that aware that rank\nzero is a host process so there are\nthings you generally want to do in the\nhost process that you don't want to do\nin the other processes\nso i'll just kind of breeze through\nthese uh so you have to spawn some\nnumber of processes using this multiple\nprocessing dot spawn\num\nfunction\nand uh here i'm specifying this trainer\nwhich is the training routine that it's\na function that encapsulates all the\nmodel so what's trainer\num trainer\nis a function that does basically all\nthe stuff i showed in the anatomy of a\npytorch script except you have to you\nknow there's like a little ceremony you\nhave to do\nbefore you start training which is\nyou set these environment variables\nand call the init process group\nfunction\nwith some some various other settings\nthen you define your distributed data\nparallel\nclass with the which wraps the model\nand then you pretty much do all the same\nstuff\nso there there are some gotchas\nobviously um\nso when writing code carefully here for\num\nusing this type of data parallelism\nthings that you want to think about is\nyou kind of want to\nsometimes you want process specific\nmetrics like you know gpu utilization\nbut generally\nif you're just reporting loss and\naccuracy and other kind of model level\nmetrics uh you wanna you wanna use do it\nin the main process so\nhere i'm just i'm logging the training\nloss and making sure that\nthe rank is zero which is the host\nprocess\nand also the same thing with um saving\nthe model weights\nokay so it looks like i have\nmaybe 20 minutes left so\nto summarize the kind of conceptual\nstuff\ndata parallel\nis great because it's a one-liner change\nbut there is a performance overhead\nbecause under the hood it is\nmulti-threading so it's subject to the\npython global interpreter lock which i\nwon't get into but uh safe to say that\nthe kind of the\nbetter way would be to use distributed\ndata parallel\num\nbecause even though\nit has some additional setup code it\nuses multi-processing in the background\nso each gpu has its own process\nwhich implies better performance\num\nso some things things to think about in\nwhen you're\nuh training a distributed high torch\nmodel is on the modeling side\nif you're using dropout\nthat actually\nthat implies that you're you have like\nthis binary mask over your\nweights and so that's going to consume\nmore memory so\none thing to think about\num\nthe precision the float position of the\nair bottle will also consume more or\nless memory\nand you know when you're performing up\nlike custom operations only on the whole\npost process you need to invoke\ndistributed dot barrier to block all the\nother processes from continuing\nexecution\non the data side\nif you're downloading data\num\nlocally so if it's like some flat file\nyou're downloading\ndo that first in rank zero to avoid race\nconditions\nand um\nyou'll\nfor almost every setting you'll have to\nthink about what batch size fits into\nmemory so generally i sort of\ndo this empirically\nbut you can of course do back of the\nenvelope calculations to get yourself in\nthe right\nballpark\nand then as i said metrics\nlike unless you're you're collecting\nprocess specific metrics you want to\ncheck that you're in rank zero before\nyou you know collect collect them\nokay\nsweet so uh i'm gonna demo really\nquickly the um\nthe flight snacks example so this is all\nthe stuff i'll show now is on flight\nsites for you to try out um and read\num so it's a you know the classic\nendless data set it looks like this\ndigits hand written digits from zero to\nnine\nand we'll be using a conf net to\ntake the pixel\nimages and then predict the digit\nso\nbefore i show any code\ni did want to\nokay cool\ni didn't want to just run uh like a\nquick experiment so\ni'm just going to kick off on the signal\nsingle node\nsingle gpu\nflight workflow\num\ni am going to\njust test out different batch sizes so\ni'm going to run this job for let's say\njust\nfive epochs\non a batch size of 64.\nand i'll run or i'll run three of these\njust to show you at the end what um\nwhat generally you kind of kind of want\nto look for\nin when you're kind of profiling\nyour model training runs\ni'm gonna double this\nto 128\nmake sure we're\ndoing five epochs\nand then just one more for a good\nmeasure\num five epochs\nmatch size of\nsix\nhuh no let me\neven more\nso 512.\nso while these run\nlet me show\nthe code that does this\nactually it might be better to just go\nto the documentation\nso the again as i said this is in the\ntutorial section of the flight\ndocumentation\num\n[Music]\nit goes through with exposition and code\nat the same time\nall the steps you need to implement this\nso i won't i won't go through step by\nstep\num\n[Music]\nbut the the big\nthe big parts are\nas i said in the anatomy of the pytorch\nscript you're defining the model here\num this is just pie torch code\nyou're defining the data loader so\npytorch comes with\na pre-built mnist\nloading function\nand this kind of might look familiar\nthis is the training function for\niterating through batches of your\ntraining data\nyou're updating the model here\nand then you're collecting metrics\ndown here\nand what this looks like\nsince we're using weights and biases i\nguess a quick plug on the weights and\nbiases uh\ni don't know if it's technically an\nintegration but you can use it with\nflight very easily\nso that that would be worth another\ntalk perhaps but so noble glade 62 is\nthe thing we just started running\nthese are some model metrics so as the\nepochs go from one to five i think this\njob is wrapping up soon you can see the\ntraining loss here per batch it goes\ndown\nyay\num\nso this is\nthe bias decreasing\nspeaking in in terms that i\nintroduced earlier in the workshop\nand then we also see that the test loss\nis decreasing so that's this is a good\nsign\nand accuracy you can think of as the\nopposite of loss it's like how well your\nmodel is doing\num\nand then here\nunder the system metrics\nyou can see\ncertain\nmetrics\nthat weights and biases is collecting\nabout\ngpu cpu utilization\num so\nwe're not using as much memory as we can\nfor this particular machine i don't know\nwhich instance type this is or what the\nspecs are but\num and also utilization is fairly low so\nso this we're not you know this this\ntells me that we're not using like the\nbest\nuse of this\nof the resources allocated to this\nflight workflow\num\nso this is the experiment that i ran\nwith the three workflows is probably\ngonna run in sequence since i'm not sure\nhow many gpus we have available\nlet's just\nlook back here\ncool\nso these are the three\nones\ni'm not sure it should\ni wonder if these are it oh yeah okay\nso if you squint\ni guess as we increase batch size\nmemory allocated and gpu\nutilization\nyeah i don't know i haven't haven't\nlooked at these data but\nthis is this is kind of like the\nexercise you go through when you're\nexperimenting with different models\ntrying different batch sizes\ntrying different model sizes\ni would love a tool that would\nhelp me think about this in a more\nsystematic way\num\nand then finally uh just to wrap up here\non this demo side this is the multi-gpu\ntraining example\nyou can see here that we're just\nimporting a bunch of the stuff from the\nsingle gpu example so we're reusing the\nmodel definition\nwe're using the our hyperparameter\ndefinition\nand\ncertain other functions for\nlogging\nthe main difference\nas i showed you with the extra ceremony\nyou have to do with this\nis um\nthis distribution this setup function so\nthis initializes the process group that\nhelps\npytorch coordinate\nall the different you know batching and\ngradient updates across all the\ndifferent processes\nand then you might notice here that i'm\nonly doing the weights and biases set up\nand logging and all that on the main\nprocess\num\nthere are some other things here that\nyou know if anyone's curious we can chat\nabout it maybe offline\nbut this is this is pretty much it\nwhen you define your task\nhere\nyou just have to make sure uh we did a\nlittle bit of extra stuff here for um\ndistinguishing between sandbox and like\nan actual\num like back end\nbut\nreally all you have to do is specify you\nknow your gpu\nnumber of gpus you want\nmemory and storage all the regular\nthings\nand\nyeah i think we wanted to do maybe\nanother talk on like how to configure\nyour back end to support gpus but\num\nconditioned on that\nyou should be off to the races uh\ntraining models with flight on gpus\num\nlet me hop back onto the slides just to\nsummarize some stuff\nso\nmain takeaways is to start small to\nensure model your model is implemented\ncorrectly\num you can characterize like convergence\nand overfitting dynamics uh in this\nsmall\nthing small data set and small model\nthat you can do\niterate locally\nyou know scaling up\nbasically scaling up reduces iteration\ntime\nso iteration time for getting insights\nonto your analysis or insights about\nyour model\nand then before scaling up know why\nyou're scaling up so you know\nyou don't want to prematurely do that\nnext steps\nwould be\nmulti-node training with the mpi\noperator\nand then\nimproving cost effectiveness with\ninterruptable instances\nand before i end\nuh we just also wanted to announce\na new thing we're trying which is\nlightning talks\nso\nthese are very short kind of five minute\ntalks that you don't even need to\nprepare a slide deck for it's like\nif you just have an idea about flight or\nexcited about a new package that you\nfound that has some like relevance to\nwhat we're doing\nwithin the flight team flight ecosystem\nwe would very much encourage\npeople to go check out this form\num just a google form\nyou can either give a talk or propose a\ntalk idea if you don't want to give a\ntalk\nand\ni'll be making a point to like push the\nflight team to make give lightning talks\num i'll also be probably giving these\nsince they're very lightweight and um\nthey're designed to be lightweight and\nyou don't have to like you can literally\ndo these last minute\nif you\nwant and with that um thanks for your\nattention and i'll\ngive it back to you george\nthanks niels\nthat was awesome\nuh caitlyn do you have anything you want\nto add before we\ni have uh yeah a couple of things that\nneil showed um and part of the lightning\ntalks that people should propose and\nprobably do\nso we do have a distributed training\nexample for sagemaker uh if you're using\nsagemaker\nthe real difference that you need to\nunderstand when you're doing distributed\ntraining is that from flights point of\nview it's running one unit of work\nso it has to have one output being\nreturned\num and that's what you know niels was\njust showing that if you are process\nzero\nuh within your mpicom world you say that\nthis is the guy who's the leader who's\ngonna output all the outputs\nflights support that too um\nyou can do that in the code as well so\nwhat neil showed is like a fully\njust using the basic flight constructs\nto construct a deep learning job uh we\ncould go beyond this like basically you\nknow if you're using data loaders or\nsomething you could write a transformer\nfor it and get the data loaders directly\nloading from an input data that's passed\nwe haven't done that so you know if\nanybody is interested in contributing\nthat code that that will be amazing\nbut for the distributed training case we\ndo support uh something called as ignore\noutput so\nif you're non process zero if you just\nraise the error colliding or output\nthen flight will assume that it's okay\nfor you to have\nnot produced outputs\num and then it correctly shows up in the\nui so definitely try out\nthe sagemaker example as well and then\nas neil said we are working on a better\nexample with mpi and horowitz and\nuh even you could use glue or so to do\ndistributed training\nthat should be coming soon\nbut yeah we would love to hear any\nthoughts and ideas and suggestions on on\nthis front\ncan you increase your volume a little\nbit soren\nsorry\nsorry better now\nuh i just wanted to ask about uh hyper\nparameter tuning and i've seen\nthat steven uh\nbasically asked the same question in the\nchat i guess\num so\nyeah so so what's\nwhat's your recommendation to to\nactually um\ncombine that with with flight or such a\ntraining job\ndo you already have some\nexperience\nyeah that's i guess that's the\nmotivation behind the whole flight kit\nlearn rfc\num\nto make that easier but in theory you\ncould you could do\num\nthe search\nyou know in it within a task\ni'd have to think a little bit about how\nthat would work with distributed data\nparallel\nif you're like tuning\nthese you know multiple very large\nmodels which i've never done in practice\nbecause it just it'll take a week\nto like train one model\num\nin my experience so\nyeah you just you kind of have to do\nsome other\nyou have to be like kind of satisfied\nwith a single model if you're training\nlike fine tuning like a uh\nmodel or something like that but if you\nif you have if you're using\ni i would say if you're using just\nvanilla pie torch yeah you could\nyou know\num\ni'm not familiar with optuna but with\nthe other tuning libraries that i'm\nfamiliar with like they really just\nhandle like the search\nover random settings or you know\ndistributions and things like that\num so in theory you could do that within\na task\nflight task\num the whole thing with flykit learn\nis that\nit's a layer that then\nkind of orchestrates this at the\nworkflow level\nso\nyou specify a grid\nit pre-samples the grid\num or the distributions and then\nspins up a bunch of map tasks\nwith your training code so so that's\nthat's the convenience there which i'm\nexcited to help build out\nyeah that's cool i was just thinking uh\nwhen when you when you showed the\nyou started the different uh\nruns with different parameters manually\ni thought okay if we can now automate\nthis in a workflow perfect\nyeah cool\nyeah\ni like to say that so people did that\nfor example at lyft but they did it\nmanually we did not have a specific api\nlayer for it and then i think there was\none smaller api layer for single node so\nagain what neil showed is one node from\nflight point of view that's one task you\ncan do whatever you feel like it has\neight gpus i think on aws there's one\nnode that has like 16 gpus or something\nit's pretty\npretty much for you know most uh\nbase 90 of the applications i would say\nunless you start writing a completely\nnew model from scratch and you know that\ntakes for a while\nuh but\nthat being said so again i i'm not\nplugging sagemaker i don't want to but\nstage maker does have a hyper parameter\ntuning thing and we've we've kind of\nadded that just to showcase how\nthe expressiveness of such a system\ncould look like and then right kit learn\nessentially takes that to being native\nin flight kit and this is this is\nnothing to do\nwith the back end potentially it's\npurely\nhow we can write this and expressively\nand and and also you know get right\nmetrics from the system because\nquite a bit of it is visibility right if\nyou don't have visibility i don't think\nyou know it would be that is that useful\nso yeah i'm excited i was like really\nexcited to see the rfc and i'd love to\nwork with any of you guys who would love\nto collaborate on that\nstephen\nwhere's your mlflow\nexample that you had right\nuh it's in progress at the moment we're\ndeploying flights so then we'll we'll\nhave the integration\nonce we have flight running awesome\nokay i guess i oh sorry go ahead\ni was just gonna ask for any other\nquestions of course\nyeah i should say there's a lot of\ngray areas in between so you could\nyou could make training more efficient\nlike say are you using bird\nyou could have a thin layer on top\nthat's learnable and then freeze the\nweights of bird so that you're not\ncollecting gradients\non this gigantic model underneath what\nyou care about\nso i can imagine in that setting you\ncould do high perimeter\ntuning on just the architecture of that\nthin layer on top\num\nso\nyeah there's just it's like lego bricks\num that's how i see deep learning these\ndays\nno more questions uh thanks very much\nniels\nthat was enlightening\nuh\nand we will see everybody in a couple of\nweeks\nso sandra will have this up on video uh\nlater today sometime if you want to pass\nit on to your colleagues\num\nand we will try to integrate it with\ndocs that's our newest trick\nall right it's already documented\ngeorge\nno integrate the video\nthat's our new trick\nawesome\nall right\nthanks everybody everyone\nthanks"
    },
    {
        "title": "The New FlyteConsole Graph UX - Sept 7 OSS Sync Up",
        "transcript": "so uh my name is jason porter and i'm\nkind of new to the team so if you don't\nknow i lead the front end team here\nand today we are talking about the new\ngraph ux\num\nand you know what is the purpose of this\nwork\nuh provide users with a graph that is\nboth beautiful and useful and that's as\nsimple as it is broad but that really is\nthe balance that we kind of have to\nstrike here we want to be beautiful and\nengaging and intuitive but also useful\nand actually help users when they're\ntrying to\nview or debug their workflows\num\nso\none question i think is important to ask\nis why a graph at all and it's it's true\nthat graphs provide like this intuitive\nway to understand\ncomplex data and our brains are like\nabsolutely wired to process visual\ninformation like i was reading one thing\nthat said like 65 of your brain is\nrelated to processing and mapping in\nyour mind visual things so we kind of\nalready have this uh feature in our\nbiology let's let's use that so\num\nat first we just had a few things we\nwanted to change\none we wanted to show the actual path\nthese workflows by looking by being able\nto look inside of nested nodes\num we also want to leverage color\nto simplify uh the state of the workflow\nand then you know just small things like\nhuman friendly naming\nand so\neven with those small things you can\nstart seeing a pretty substantial\ndifference so above we have our previous\ngraph experience and this is the same\nthese represent the exact same workflow\num although\ni shouldn't forget which workflow it is\nbut these are the same workflow\num and they both failed\nbut on the top one it's really pretty\nunclear why it failed whereas on the\nbottom one our new experience uh you can\nwe have this legend you can quickly see\noh this one was aborted so the workflow\nkind of paused there and so already\nwe're kind of equipping our users with\nthese subtle cues that give them tons of\ninformation really quickly\num and it also helps with the debugging\nexperience\num you know this is a\nexample\nof a failed workflow and again we don't\nreally see much about what happened we\ndon't understand why it failed how it\nfailed but on the new experience\nwe get\nthe exact same workflow the exact same\nexecution we get tons more information\nand so we can see for example\nthis red box around\nthis nested workflow these boxes\nrepresent nested workflows that we can\nnow look inside\num or sub workflows actually\nand\nnow we can also tell that this second\nnode uh is purple and it was aborted and\nwe can kind of put it together in our\nheads like oh i see t1 failed in this\nworkflow which caused this node to fail\nand so now it all kind of it very\nquickly makes sense exactly what\nhappened so we think this is really\ngoing to help users with the debugging\nuse case just like okay i see exactly\nwhat this is failing\nand you know we do show all this\ninformation other places in the site but\nthis is kind of a more intuitive\nfeel for that same data\num and so\nmoving right along to keep time i have a\nfew simple examples\ni'm just going to load up one of our\ndemo sites but you know if you pull down\nthe latest this is included so here's a\nreally simple workflow\num\nyou know looks like it's exceeded great\nand so we can go here and it just looks\nbeautiful it's like okay yeah and now we\nhave human readable names we can tell\nexactly which\nuh what these nodes represent um\nif we go\nback\nwe can see another nested case which is\nbranches so in this branch case we see\nthat there's actually you know branches\nthat occurred\nand in this particular case uh there is\na nested nested branch\num on this current build we only show\nnesting down to one depth but we're\ngoing to touch on that more later we\nplan to go fully deep in the near future\num and then finally\nuh\nthis is like a good question how do we\nhandle monsters you know like really\nreally big graphs\nand uh it turns out that it handles just\nfine so this this workflow is pretty uh\nlots of things going on here\nand when we graph this\nwe do see\nuh a lot all at once and so these boxes\nonce again represent sub workflows that\nwe can now look inside\num and this you know this graph has\nfeatures like zooming\npanning\nthings that kind of help you\nif you want to like\nsee\nwith the higher fidelity kind of what's\nhappening\num\nyeah and so\ni'm going to go back to this\nuh this the cool thing about this is\nit's just a start this is like v\nnot even one v zero point one uh we have\na lot more things in the pipes\num\n[Music]\nup next we're going to start doing fully\nsupported nested graphs we have\nuh we have a path forward of how we're\ngoing to kind of handle the idea of\nbeing able to kind of infinitely zoom\ninto a graph that might be nested upon\nnested upon nested\num\nwe're also exploring ideas of how to uh\ndisplay more useful data like one\nexample idea i'm not committing to this\nbut one example idea is showing on the\nedges like what is the data that we sent\nwe can have like a hover perhaps\num\nand then one really cool thing and this\nis out throughout the entire site uh\nwe're doing kind of an overhaul\nand we're gonna start adding these\ngraphs everywhere uh to kind of give\nusers that quick read they'll be static\ngraphs but like for example imagine\nuh when you go to the workflow page uh\nseeing all your versions and instantly\nyou see a static graph to kind of know\nthe differences between workflow\nversions and kind of instantly kind of\nget that read of what these different\ngraphs look like\nso um it's actually pretty exciting i\nthink this is going to be\na really really cool feature to use now\nand especially in the future um\nso that's that's all i got\nany questions\nthat's really awesome i think the graphs\nlook so much more better and usable i\nthink\nso thank you jason\nuh we we probably should do a ux deep\ndive because i think\nwhat jason's really as he said he's\nimplemented like v0.1 it has a huge ux\naround entire um\nfight console\num and our team is uh and so our team\nwhen i say like the union ai team which\nis back in flight is committed to\nbuilding the most beautiful user\nexperience in open source that you can\nget\nin\nthe pipelining system"
    },
    {
        "title": "Great Expectations-Flyte Plugin",
        "transcript": "um so i'm just showing great\nexpectations dot io which is the home\npage um great expectations is an open\nsource project that does data quality\ndata validation data monitoring um we\nhave a fairly specific take on how that\ncan be best done uh and i'll walk you\nthrough the main pieces so the the core\nabstraction and great expectations is\ncalled an expectation it's an assertion\nabout what data should look like at any\ngiven stage in a pipeline and when we\nsay pipeline that could be\na table in a data warehouse that could\nbe\ntraining data going into machine\nlearning pipeline\nit could be\nalmost anything just as data flows\nthrough systems you're going to have\nlots of places where you want to be able\nto assert here's what it should look\nlike at that point\nand we do everything from kind of schema\nchecking no values regexes\nto more kind of anomaly detection type\nthings like um checking column medians\nor not not shown here correlations\num\n[Music]\nchecking to make sure the distributions\nof values uh match previous\ndistributions uh within some range of\ntolerance so it's it's declarative it's\nvery flexible it's also intended to be\nvery extensible\naside from just having these assertions\nwe've also set it up so that you can\ntranslate back and forth between tests\nand documentation\nso\nall of your tests can be converted into\nwhat we call\num prescriptive documentation which just\nsays here is what the data should look\nlike\nyou can also generate descriptive\ndocumentation which is here's what the\ndata actually did look like at a given\nmoment\nand then what we call diagnostic you can\nsay okay\nwhat it was supposed to look like and\nwhat it looked like we're different then\nyou can also generate uh docs for that\nand we take a pretty expansive view of\nwhat a doc is\nthat can be static html or markdown\ncould be a slack message\nwe've got some cool stuff where people\nactually auto generate notebooks that\nmake things editable and the notebook\nitself is just another type of\ndocumentation of rendered\nformat for the expectations themselves\num last and this is a piece that's still\nvery much in progress partly just\nbecause there are a lot of moving parts\nto it and getting the apis right making\nit work fluidly in a lot of\ncircumstances is a legitimately deep\ntechnical question\nis automated data profiling so being\nable to take existing data or in some\ncases metadata about that data so like\nschemas from sql for example or logs\nand based on that put together candidate\nexpectations\num and our philosophy and the kind of\nfeedback that we've gotten from the\ncommunity is you shouldn't imagine that\nyou can automatically profile data and\nknow everything you need to know about\nit there's always going to be a role for\ndomain expertise kind of fleshing out\nthose rules\nbut um\nit's a very good starting point uh and\nit can be really helpful for getting\nyour test deployed quickly\n[Music]\nso last two pieces um we have a kind of\na whole stack to help people set up data\nvalidation and to make that work in a\nrecurring kind of ongoing monitoring\ntype way i won't dig into details here\nand then last it's open source which\nmeans that everything is very pluggable\nvery extensible and our philosophy is\nvery much the developer needs to stay in\ncomplete control of the whole end-to-end\nexperience and that means any way we\nwant to extend or add the code you can\nso that's great expectations in a\nnutshell um we've been working with the\nflight team for a while to integrate\nthese tools more closely because they\njust fit hand in glove um so i'll turn\nthe time back to\nuh actually who is doing the demo i'm\nnot sure\nwe've got it i'll be doing the demo okay\num yeah let me turn it over to you\nyeah thanks that was amazing yeah thanks\nso much\nyeah\nnow let me just give a brief overview of\nwhat the great expectations integration\ndoes\nand how one has to use the integration\nso let me just share my screen first\nyep\nso in flight we already have a native\ntype system that validates the type of\ndata\nbut what if we want to validate the data\nall ends up and we want to make sure\nthat our data follows a specific\ncriteria in such a case this plugin\ncomes in handy with the great\nexpectations and flight integration we\ncan make the flight pipelines more\nrobust and resilient we can ensure that\nnew data isn't out of line with the\nexisting data we can prevent data\nquality issues eliminate bad data etc\nnow how do we define this integration\nso this plugin comes in two modes one is\ntask and the other is type so what's a\ntask task is a simple flight task so\nyou can just assume that this is a\npredefined flight task that you can call\nfrom within a task or a workflow now\nwhat's the type a type is a custom type\nthat you could attach to your data in\nour case it's it's the great\nexpectations type now both task and type\ncan accept a string a flight file or a\nflight schema a string refers to a file\nname or a database query a flight file\nis useful when you have a remote data\nset like say you want to validate the\ndata that's present in a remote csv file\nin such a case you can use flight file\nflight file automatically downloads the\nremote data set and the plugin then\nvalidates that file\nnow flight schema is useful when you\nhave a data frame like a pandas data\nframe or a spark data frame in such a\ncase you can use flight schema so flight\nschema automatically creates a parquet\nfile out of the data frame and that is\nwhat the plugin validates so this is\nbasically the inner working of the tool\nnow let's just look at a couple of\nexamples to understand these concepts\nbetter\nfirstly i'll be showing the great\nexpectations task in that the flight\nfile example if you see this particular\ncode snippet\nuh we have a workflow here\nand in that we have the data set\nparameter now this data set parameter\nhas the csv file type now csv file is\nlike a varied representation of flight\nfile it's just like a combination of\nflight file and the csv extension now\nthis data set is a remote csv file if\nyou see the url in here now i'm passing\nthis data set to the task now in the\ntask i'm again calling the great\nexpectations task which has a couple of\nvalues in it name is the name of the\ntask\ndata source name is the great\nexpectations\ndata source\ninput is the data set and its type is\ncsv file\nwe also have the expectation suite name\ndata connector name local file path and\ncontext root directory so context root\ndirectory is the directory in which you\nhave your great expectations main\nconfiguration file so if you see this\nconfiguration file this is the great\nexpectations configuration file and this\nis present in this particular directory\nand that is what i am mentioning in here\nlocal file path\nis the path to which your file is\ndownloaded so this has to be given in\ncase of flight file and flight schema\nyou also have to make sure that local\nfile paths directory and base directory\nin great expectations have to be the\nsame we have plans to remove this\nparticular value because we want to\nautomate this part but for now you will\nhave to input this value\nnow when i run this workflow the\nvalidation happens automatically and if\nat all the validation fails this is\ngoing to throw a validation error and\nit's not going to proceed to the next\nline only if it passes will it proceed\nto the next line\nlet me also show you a great\nexpectations type examples now this is a\nflight schema\nin flight schema again have a workflow\ni'm calling a task within the workflow\nand i'm passing a data frame now i'm\nfetching this data frame from a\ndifferent workflow so if you see the\nstars i have the data frame now the type\nof this data frame is great expectations\ntype and in great expectations type i'm\npassing two values one is the data type\nthis data type is flight schema you can\nalso give a flight file or str meaning\nstring and the second value is the great\nexpectations config this configuration\nis similar to the config that i've shown\nyou before\nwe also have local file path in here\nwhich basically means that this is the\npath to which the parquet file will be\ncopied\nnow there's also runtime batch request\nso runtime batch request is the great\nexpectations runtime backed request that\ncan wrap either an in-memory data frame\nfile path or sql query now the\ndifference between the runtime batch\nrequest configuration and the other\nconfigurations is that the data\nconnector that's being used in here is\ndifferent now if you see this data\nconnector\nlet's let me just\nyeah so if you see this this has a class\nname set to runtime data connector this\nis what is the\nthis is what differentiates this from\nthe other configurations so you'll have\nto make sure that the class name is\nruntime data connector then the plugin\nautomatically converts your request to a\nruntime batch request you need not\nspecify any additional parameters to\ntell that\nand yeah pretty much it\nlet me also show you how the validation\nerror looks like on flight console so if\nyou see this the validation has failed\nin this case and i can see all the\ncolumn names against the expectation\nnames we also have plans to integrate\ngreat expectations data docs with the\nflight ui but for now this is how you\ncan debug your data\num yeah so that is it\nyou can find all the code examples in\nthe flight documentation\nif at all you have any questions\nrelating to the great expectations\nconfiguration just refer to their\ndocumentation let us know if you have\nany suggestions or questions thank you\nthank you samitha\num\ni had a question for kyle and eve maybe\nuh\nso\nwhere do you\nsee like how do you see most of the\nfolks in your community using\num great expectations like like do they\nuse it as a\nas a way to you know to codify\nthe expectations of the data or as a\nactually as a validation framework where\nthey are actually running things you\nknow and the compute in there\nit's some of both to be honest and what\ni'm hearing is an echo of conversation\nthat we've had a few times on is this\nlike monitoring or is this typing and i\nthink the answer is yes like it's both\nwhat i would say is i think most teams\nthat come and start adding data quality\nchecks to their pipelines tend to think\nof it more on the monitoring side but\nthen as time goes on you realize that\nsurfacing of that as metadata earlier in\nthe process using that to align on\nexpectations becomes really helpful i\nmean if you think from kind of a\nnon-engineering standpoint like what are\nthese expectations you can think of them\nas tests which kind of puts it at the\nend of you know the life cycle of\ndevelopment but you can also think of\nthem as requirements which puts it right\nat the beginning of the life cycle of\nthe development for a data product and i\nmean as far as we're concerned yes\nthey're both i i get that people tend to\nwant to put tests in after they burn\ntheir fingers and something's broken so\nthey tend to put it in at the end but i\nthink the way the data world is moving i\ni think i mean\nyou guys on flight i think have some\nreally good ideas on how data typing uh\nwill will flow forward and inform future\ndata development i i think those things\nare going to end up being largely the\nsame thing um give it a few years\nokay i don't know if that's the question\nyou're asking yeah yeah\nthe\nyes i think we we kind of communicated\nwith perfectly saying the same terms\nthat's amazing um but uh no thank you\nyeah i think\nwhat i like you know when i\nhad a conversation with a couple of the\nusers they\nthey tend to think about it from the the\nmonitoring on the end\nand i'm like what about we bring it\nearlier but the same thing that you did\nin the end what if it automatically\npervasively goes throughout your system\nand that's our point of view because\nthat actually empowers them to slowly\nimprove the quality of the source\nuh while well of course keeping their\nworkflow as is which is to start at the\nyou know once you have the data don't\nburn your fingers right early on as you\nsaid but let's\nget things controlling and then slowly\nimprove progressively but thank you for\nthat\ni'll say i mean if there's anybody who\nwants to kind of wax philosophical about\nor or more practical but just like where\nthis is going to go\ni mean this is a technical thesis that\ni've had for some time i think tighter\nnotions of typing\num just really haven't existed in\noutside of schemas and sql which i think\nare a very loose type of typing\nand um\n[Music]\ni think there's been a lot of notion\naround how software excuse me how the\ndata world needs to learn from the\nsoftware world and i think testing is\none of those concepts that's kind of\nbeen brought pretty fully in which is\none of the things that's driving a lot\nof the enthusiasm you know\nwhy great expectations has become such a\nbig project\nbut i think there's a comparably large\nset of ideas around okay how does how\ncan typing really help the data\necosystem and i just i haven't yet seen\nthat flowering of ideas and it feels\nlike we're probably due pretty soon\nso if there's anybody wants to chat\nabout that or go deeper\nwe'd love to just riff on that come up\nwith content together because i think\nthere's quite a bit you could do there\nyeah\njust for folks to know that even i have\nbeen talking about like doing like some\nsort of a series essentially to talk\nabout typing and\npotential powers of typing in data would\nthis uh i just add this is refreshing to\nhear more people talking about given how\nmuch i was uh\nbanging my head against the wall uh in a\nprior role trying to get other people to\nthink about these uh\nlike how early can we introduce and\nenforce types in our pipelines whether\nit be batch or\nyou know\nindustry yeah\nyeah totally with you and and with so\nmany people working in python in the\ndata world where types are you know very\noptional i think it's gonna take a while\nbut i think there's a lot of benefit\nthere and and partly what i'm searching\nfor what i'm hoping people will come up\nwith um maybe even this kind of\nintersection communities is if we can\ncome up with a few hero experiences it's\njust really powerful things that happen\nbecause you had types earlier i think\nthat'll start to open people's eyes\nbecause not that many people are willing\nto change the way they code just because\nit's philosophically better but if you\ncan say oh this bad thing will stop\nhappening or this good thing will start\nhappening then\nyeah things can move fast for them so if\nyou have ideas like that if you\nencounter those kind of hero experiences\nplease speak up and we'll figure out how\nto boost them\nproof right\ndo you want to add something i thought\nyou were gonna\nah no nothing specific but um yeah feel\nfeel free to also jump into our our\nslack community um that's i'll put in a\na link over there if you need any help\nwith uh great expectations\num always always happy to help people\ncoming in\nand thank you for joining our meeting\ntoday so\nuh this really helps\nyeah thanks for having us and thanks for\ndoing so much of the heavy lifting on\nthe integration it's really cool to see\nit come together\nyeah thanks samir that was awesome"
    },
    {
        "title": "Local Caching - Sept 7 Flyte OSS Sync Up",
        "transcript": "um my name is eduardo probably mario i\nwork on union i joined\ni don't know five weeks ago um\nin here\ntoday we're going to talk about this\nthis feature that i added the um local\nexecution workflow not to you know\noverload\nthe term but um\nthe ability to cache the result of tasks\nin local executions\nso\nyou know we all love flight but we know\nhow complicated it is and um\nin the very first like\ncycle of of um operations that you do\nlike you write your business logic you\ntest things locally test remote\norchestrate blah blah\ntoday we're gonna zoom in on just like\nthe missing one of the missing features\nthat that um\nare missing from from the local\nexecution um like\npipeline let's say\ntest caching what is this uh it's this\nability that flight has had like the\nremote original flight has had for i\ndon't know ages now since i've i've i've\nseen flight of the very first time like\nfour years ago this is already a feature\nin\num it's this ability to catch the output\nof a task right\num\na few common use cases like you're\niterating on my workflow you're like\nexecuting it multiple times you don't\nwant to spend time like um\nwaiting for the result of a test that\nyou know\nis either item potent or like it can be\ncashed\num\nanother common one is like you have this\nthis test that is shared across you know\na huge number of workflows or\nyou're simply you know in the iteration\nphase you're debugging a workflow\nyou want to cache a portion of that\nyou can with test caching and how does\nit do this\nthis is like the prototypical um\nway of of caching\num\na task in flight kit\nnotice the um the arguments cache and\ncache version to the task\ndecorator\nwe're saying\nthat this function called square\num is cachable\nand it has this particular version like\n1.0 but um it is an arbitrary string\njust put it 1.0 because the definition\nof square probably won't change in 1000\nyears um but how does it do it remember\nwe're caching the output of a task\nand the output can be\nlike an arbitrary complex data type like\nflight files data classes you know\nyou you pick\nit's not restricted to like just um\nprimitives like\nscalar types\num but it it is a cache so we have to\nthink about like how\nhow are the cache keys of this\nthis spoke cache determined\num it really boils down to\nwhat mode of flight you're running\neither remote or local and as i said\nbefore in this presentation we're gonna\nfocus on the local um on the details\naround like the local execution so\nif you're interested in how the remote\num case works\ngo check the docs it's all described\nthere um\nthe criteria for creating or calculating\nthe the cash key in the case of of the\nlocal cash on the local execution\nwe only take into account um the cash\nversion which is that attribute that we\nwe saw in the previous slide the test\nsignature you know inputs outputs the\nname of the task and um\nthe actual inputs to the task like we\ndon't want to catch the the result of a\ntask that takes like\ninputs a and b\nwhere a is like an integer b is another\ninteger but like you if you pass like\ndifferent inputs to the skull\num\nyou don't want to cache the wrong thing\nbasically so like the inputs are also\npart of the cache key\num\nlocal executions\nwe have like this very\num\ninteresting feature that is not present\nin in remote which is like there is no\nclear way to invalidate caches or cache\nkeys in um remote flight\nyou're you have to resort to either you\nknow change the version\nor modify the signature of your function\nbut in in the local cache\num in the case of the local cache we\nactually provide you with another um\ncommand\nin the pi flight cli that lets you clear\nthe cache so you just you run locally by\nflight local cache clear\ndamn the clash is empty\nnow i'm gonna jump to do a quick demo of\nhow this thing actually works so\nbear with me because i'm not sure if um\nzoom will let me do this you see i i\ncan't see any of you but um please\ncan you see like two screens now\nor just still like the google docs thing\nyes eduardo\num\nit's working\nit's working yeah yeah we see two we see\nwe see your two separate windows side by\nside yeah that's that's amazing and\nthat's all i wanted to hear thanks\ngeorge um\nlet's go through this this first demo\nthis is a very very very simple workflow\nyou know it doesn't do much um it\nbasically calls the uh\nthe\nthis stats called t1 which as you can\nsee here it's set to be cached it has a\nversion called 0.1\nhere um just like pretend that this is a\nvery very long computation but here\nwe're just like simulating that with a\nsleep of two seconds and it just returns\nthe um\nthe sum of the integers\nso how does this look like when you run\num locally\npush\nthe cache just to be sure\nand let's run that workflow locally\nsee that you know it is a very long\ncomputation and it um\nit ran once\nnow let's run the same um workflow again\nand it pretty much immediately just\nreturns because you know\nthings cache you can see from the\num from the logs that\nwe hit the cache\nfor that particular you know execution\nlet's go back to this again\nit's just like you know calling wf1\npassing a as one and bs2\nso let's\nlet's uh change the version this is like\npoint two now\num\n[Music]\nyeah it's no longer cached\nyou had to wait there was no cash key\nlet's just approve the cash is working\nthe workflow immediately returns\num\nyeah let's go to the\nsecond\nso here\num we are just sharing this task\ni'm calling here expensive that's shared\nbetween two workflows\nbetween like you know two workflows so\nwe have like workflow one\nand workflow two and what we're going to\nsee\nis that indicate we\nwe execute you know workflow one\nand that takes you know two seconds of\nreturns like um three\nand when workflow p2 is about to execute\nit pretty much returns immediately\nbecause we're we're um hitting the cache\nagain oops\nso\nthis is a little quick but um here's the\nexecution of workflow one and\nwhen workflow two runs we\nsee uh a cache hit for those two inputs\na1 and d2\nsimilarly to the um\nthe uh the other case if we change the\nversion change the\nthe signature\nit will all you know invalidate the\ncache\num\nnow\nfor the\nfinal\num example so as i said\nbefore the\noutput of\nof tasks\nor inputs can be um\narbitrary data types like we cache\nany any value that can be serialized by\nright so in this particular case we have\na very simple workflow\nexcuse me\nthat\nit\ncreates a flight file writes some string\nto it and returns it right in the past\nlike the actual workflow like it runs\nthen we just check that hey this thing\nactually\num returned the\nstring that we wrote like to the five\nfive right again i'm i'm pretending here\nthat there's a long and expensive\ncomputation happening\nbut um\nit's the proof that the the cache\nactually can load\nthe um\nthe flight file that we we returned from\nthis\ntask\nso um let's run this\ni think i have the stage right\nand it's feature here\niphone zero three\nnow we're reading three seconds but you\nknow\ni think ran it did not fail like\nremember the the workflow actually\ndoesn't assert so if we could not run or\nload\nthe\nthe string hello world the new line this\nthing would have failed miserably\nnow um\nfrom this again\nit pretty much returns\nimmediately because we hit the cache\nwe actually see a cache hit\nand um i don't take\na lot of time but let's dive\nquickly\ninto like how the actual um local cache\ndoes this\nwe use um\nthis cache as uh\nas the backing store for the cache but\nif you\nyou know\npop the hood of this cache how it does\nit it's pretty simple\nit's just like a sqlite database and\nhere um\nwe're just showing\nthe contents like the two cables that\nthat um\ncache uses to um\nlike as the banking store right so\nand just\nshowing hey i i have a table called cash\nand people call settings the cash table\nis the interesting one so\nhow does it look like here's the schema\nof the cache table it's pretty simple it\nhas um row id the the actual cache key\nlike the raw\num value that you have to try to store\nwhich so that's not super interesting in\nhere\nas like the very last column you have\nthe the value right\nso um\nhow about we\ndo a little inspection just for fun just\nto see what is happening under the cover\nso\nin this case we just you know cleaning\nthe cache and running um workflow\nnumber one that we run earlier today\nand that thing\ntakes like two seconds to run here the\nresults read into um\nthe uh\nstandard output\nlet's take a look at the cache\nbecause you know doing a select star\nfrom cache\nand here's what it looks like\nas row id1 here's the cache key so it's\nthe name of the the package the the name\nof the task that was uh\nwhich is part of the cache key and the\ninputs right\nthat's not super important\num\nyeah i'm just showing if we wanted to\nlike see\nthe\ncontents of the cache in\na more\nfriendly um\nformat it's just like outputting it in\njson we can see row id the the actual\ncatch key here is the very\nlong thing and the value\nand um\njust for kicks let's run all three\nworkflows that we have and check the\nvalues\nof the of the\ncache\nuh\nthis should run okay\nhere's the contents of the on the cache\nwe have like\nthree values so row id one ready two or\neighty three which is like the three\ntests that we\nwe defined in each one of those\nworkflows\num yeah this is more a curiosity again\nword of caution do not rely on this like\nwe currently use um\nthis cash as our backing store but like\nthere's no guarantee that you know five\nversions from now will not change this\num all the interactions through\nwith the cache should be through pi\nflight local cache command\nwhich right now we only offer one sub\ncommand that lets you empty the cache\nbut um\nif the community thinks that it might be\nuseful\nto provide ways to inspect the cache in\na more friendly manner\nthis is like the extension point that we\nprovide\nand that's\npretty much\nit does anyone have\nany questions\neduardo i don't think you showed that\nbut it of course follows through that\nyou can change the inputs\nand the cache will not be used correct\nyeah as as\nwe discussed real quickly i didn't i did\nnot show but the the inputs are part of\nthe cache so\nor of the cache key so if you change the\ninputs if you add one remove one like\nthis will um basically invalidate the\ncache you'll create a different cache\ncase it'll be like a different entry in\nthe cache but\ncorrect\nit is very similar to how the remote\num cache works\nalso so it's very very similar semantics\nyeah i guess that's that's the most\nimportant thing that we are trying to\nbasically get the remote like the\ndistributed system semantics onto your\nlocal\nexecution environment so that you can\nget the benefits of caching even if you\nare right\nwriting a simple\nuh\nworkflow that runs locally or some sort\nof training algorithm where you have\nebox that you want to just store into\nthe cache you\ncan cool does anyone have\nuh questions\nuh i i do have one uh so you know there\nare some tasks that uh don't that we\ndon't yet like support running locally\nyou know maybe some of the query engine\nones that run that has to run like\nremotely\nuh we there is a way to like mock these\nright these executions\nuh locally can the mock\nlike function also leverage the cache\noh that's a great question if the\nthe object\nthat contains the results of the query\ncan be serialized then the answer is yes\nmeaning if you can return that from a\ntask\neven if it's a mocked object doesn't\nreally matter like we we serialize the\nobject\nokay cool\nyeah i think\ngreat point to that actually\none of the ways uh for example a\ntypical user\nusually writes the workflow is they they\nrun some query that results\nin a data set which they want to let's\nsay train a model on you would want to\ncatch the the result of the computation\nof the generation of the data set or\nmaybe even the cleaning set steps and so\non\num and when you're running locally just\niteratively work on your model\nthe problem with the query is that you\nhave to run it remote\nand\nthere is no local execution unless you\nuse flight kit remote so if you could\ncombine marry those two\nit would be a fantastic experience i\nthink\nnice\nyou know features that's still it yeah\nthat's pretty cool\nvery good question\nuh\nthanks eduardo for the presentation i've\nbeen waiting for this uh\nfeature myself\num is is the series serialization format\nflight idl\nor like protobuf or something else\n[Music]\nah\nit\nthat is a very sneaky um way that this\ncache does this but it basically\nit pickles what it can\nso it's not like it\nit's not\nwe're taking off of um flight idea just\nlike because of the python objects\ndoes that answer your question yes\nyep that makes sense probably\ni mean since it's pi we're executing\nstuff in python it\nwould i feel like it'd be a lot more\nwork to\nuse flight idle versus just pickle\nyeah but that's um\nyeah\nwe we should probably talk about that\nyou know because like how how do we\nwhere we make this accessible\njust not only for like flight ideal\nobjects you know so like remember people\ncan define like data classes and like\nreturn any\ncustomly defined um data types too you\nknow in their in their local or own\nworkflows but yeah\nuh\nif people you know keep using this and\nlike we can make the local cache more\nmore performant or i don't know\nmore expectable\nwhoa\nthanks very much eduardo so can i have a\nquestion is this in the this is in the\ncurrent release right\nyeah\nalthough there's like a minor bar but\nlike it should it it should be in this\ncurrent release yes 0.17\ncool um who's next\nthank you very much"
    },
    {
        "title": "Workshop: Developing Flytekit Type Extensions - Aug 24 Flyte OSS Sync Up",
        "transcript": "today i think we have uh\nour own fabulous yutung is going to lead\na workshop on\num how to create flight kit plugins\num\nand then i think we'll have some time\nfor a little q a after that\num and maybe we'll talk for a little bit\nof talking captain about what's coming\nup in the next month\nand with that i will turn it over to you\nsure is this coming through for everyone\ni don't assume it is\nuh okay cool so\nuh\nthis is i guess the start of a\nmaybe not no\nnot the first uh the first formal i\nguess flight workshop where we try to go\nbasically more of like a hands-on\ntutorial type um\ndemo\nor rather than a demo um so as you know\nflight works on a type system\num in the typing engine in python is\npluggable so\nthat is what this talk is about\nby the end of this talk you should be\nable to understand\nuh what the type engine in flight kit is\nand how its transformers work and be\nable to use them to convert between\nflight types and values and your own\ncustom\npython types classes and values\nwhy would you want to do this\nyou would want to do this because you\nwant to maybe you have\nmore complex data structures that help\ndefine your business domain or you have\nlike pre-existing\ntypes and objects\nand if you implement the transformation\nthen you get the rest of flight for free\nso\nas you know the flight\nthe flight system in general is defined\nin the idl repo the types in particular\nare in types.proto\nso specifically\na type is defined by this one message\nliteral type\nso\na a type in flight is one of these\nprotobuf messages\nuh yep\nthat is that\nthe flight kit type engine\nis responsible for converting between\na python value and a flight value and\nvice versa so for instance\nif you have a python integer 42 that you\nget by like\nx equals 42 how do you convert that to a\nvalue in flight\nthat is the responsibility of the type\nengine this conversion\nuh by the way happens\nfor every single task for run at the\nbeginning and at the end\nbasically at the beginning of the task\nwe uh the engine will convert from the\nflight value to the python value your\npython code will run and then the output\ngets converted back into the play system\nand pushed down to down the line\nthe type engine operates on registered\ntransformers\nand it provides the following interface\nthere is a function called to python\nvalue that's what you call if you're\ngiven a literal\nand you want a flight literal and you\nwant to get a python value out of it\nyou also have to pass in\nuh the\nthe python type\nso\nthis type engine relies on the\nannotations still being there\nthere is a function to literal which\ndoes the opposite so given a python\nvalue a python literal\nand the python type\nand the expected flight type you get\nback the flight literal value\nand for a given python type also that\nthe transformer should the type engine\nwill tell you what the flight site is\nand there's some additional functions in\nthere but these are the main ones\nand how does it do that it uses it does\nthat from\nby\nthrough this type transformer system a\ntype transformer is\na generic base class where t is the\npython type that you want to build your\ntransformer around\nso when if you want to use this you\nshould first subclass that base class\nand then implement the following\nuh get literal type this is just\nreturning the the flight type for your\ntype\ntwo literal so given the python value\nhow you get the\nhow you construct the\npipe the flight literal and then device\nthe opposite\nand if possible you should also\nimplement guess python type\nbasically given the flight type\nthis function by default just raises an\nexception\nif you\nsee you're the type that you're\ninterested in and you believe\nyou can accurately return it\nthen you can do so what i mean by that\nis for instance schemas\na flight schema can come from any number\nof things so it's not really possible to\nknow\nwhat the original python type was\nbut\nsimpler types or perhaps you're a custom\ntype you can do this so\nand this helps back out\nthis helps back out uh the the python\ninterface from the flight interface and\nthat's useful in some of the the flight\nremote cases\num\nonce you've written a transformer you\nhave to register it so\nuh there's a bunch all\nall the\num\nall the basic types integers strings\nfloats time deltas whatever are also\nbased on transformers those are\nthe default ones that come bundled with\nthe flight kit and they are registered\nautomatically\num and then some others have to be\ntriggered manually which is why so\nschema comes bundled with flight kit but\njust in case you don't use it\nwe we import all these in the init file\nand then also there's a new\nplugin\nfeature i guess so for flight kit\nplugins you can add your plugin to\na\nsetup.pi entry point that flight kit\nwill then automatically load\nso if it's installed\nin that virtual environment it will get\nautomatically loaded\nuh if\nflight kit is triggered as leave\nso demo there's actually a\nfully featured\nexample for this already that we'll be\nwalking through this is in the flight\nuser guide also known as flight snacks\ncookbook\nuh so pretend that you have a\ndata class um for sorry pretend you have\nthe class my data set which is a custom\ntype\nthis one represents is constructed to\nrepresent just a bundling of files into\na folder and it comes with a couple\nproperties and this new\nfile figure a new file function\nbut this can be\nany object in python\nso if you want to convert between\nthis and flight you can write one of\nthese transformers\nwhich subclasses type transformer\nand\nyou have to first decide basically what\ndeflect type will be so in this case\nwe've picked\nblob\nblob types are so named because\ntypically they are stored on a blob\nstore like s3 or gcs\nand they have\na string called format that is part of\nthe type not the literal and a\ndimensionality which can be either a\nsingle or a multi-part since we're\nworking with\nuh files folders multiple files we\ndecided to go with multi-part\nyou give your transformer a name\nand then you implement these two\nfunctions to literal into python value\nin this case because we're working with\nfiles basically we're\nuploading\nand downloading\nfrom to and from the blob store\nto and from your local local file system\nsince the a since the python literal\nvalue basically represents a series of\nfiles it makes\nyou have to upload and download that\ndata\nregistration happens like so so you call\ntype engine.register\nand then\nyou can\nuse this\num just as any you would any other type\nin a workflow or in a task\nand\nassuming the transformer was implemented\ncorrectly\num\nlocal workflow execution and local task\nexecution should still work\nuh and i ran this\non a local\nflight sandbox and you can see\nthe\nthe first task generates and the second\ntask consumes so\nand\nbasically we're just\nuh writing a line to each font but\nthis is the output of the first task and\nif you were to look in the browser you\nwould see\nthese files if you were to download them\nyou get to you get the thing\nand\nyeah that would be it actually\nactually\nyes so there's some\nuh existing transformers that are a\nlittle bit more complicated that you can\nlook at so\nschema is the most complicated probably\namong these just because\nthe transformer itself\nhas an engine aspect to it\nso that it can convert from\na variety of schema types including\nspark into the\nintuitive flight schema\nokay then anything to add\num\nyeah so uh the\ni think the most important thing over\nhere is that\ntransformers were added for higher level\nlike really complex data types mostly\nfor example\na lidar\ndata type or maybe\nyou have a way to represent blood\nsamples\num and and traditional\ncolumnar structure like a schema is not\nthe right\ndata structure for it maybe you know\nthere's some better\num representation like for example from\na triceps there could be a a csr style\nrepresentation or\num some other you know like for maps\nthere's like a way to compress uh\nbig lines into like polylines\nso you could really write polylines as a\ntype\nthat's available to your users and and\nconvert the polyline to an underlying\nbinary format or a file or however you\nwant to represent it in the backend\nand\nthis way the goal of flight is to make\nsure that the users can code in the\nin the business domains that they really\nreally\nlike\nand which for\nwhile not worrying about how the\ntransport is being handled and then the\ntransport has to be done once right you\nconvert it to the flight known entities\nand then flight will take care of the\ntransport\nuh\nthis being said we also heard that uh in\nrecently that folks would like that if i\nhave a normal python\nobject and i don't want to annotate it\nwith the data class i should be able to\ntransport it between two python\nfunctions python tasks\nand so\nwe are soon going to support\npickle based transform but pickle base\ntransforms are\nare very weak because they are shallow\nthey like if you have let's say a\nreference to a file and you want the\nfile to be transported to the next task\nthat's not going to automatically happen\nbecause pickle really doesn't know that\nthis is a file and i need to take the\nfile and push it to the back end\nuh so it will be limiting but it's it's\nreally useful if you both the tasks are\nin python and you can just move the data\nbetween them so uh\nso we are going to support that but\nuh it's\npickle is also not as efficient as\nactually transforming it into a very\nknown\nquantity in flight because flight then\ncan optimize can compress can store\nuh like for example the columnar schema\nare stored as a parquet file it's a much\nbetter uh optimized format than as\ncompared to just a regular\nspeaker\npickle of a numpy or a\narrow object\nso that's how\ntransformers started and i think\nuh we've seen\ngreat use of it uh one of the cool\nexamples of spandex and great\nexpectations i would really\nuh advise folks to go and look at it\nuh it allows us to assert\nin quality\nat the point of in like usage which is\nwhich is really where the assertion\nshould happen\nuh it's like a\ngrpc service with validations on it\nthat's how we think about it\nso\nyeah but nothing else from my end uh we\nwould love suggestions ideas more\ncontributions here\nlike new data for data frame formats can\nbe easily added\ni think there is a couple issues one\ntalks about mode and another one talks\nabout wakes another\none talks about mars\nso yeah we would love contributions in\nthis this is a different domain which\nallows you to specialize flight\nfor\nfor different scenarios\ni guess one last thing that i forgot to\nadd um if you implement one of these\ntransformers you also can flight will\nautomatically handle conversion of lists\nand dictionaries\nof set points\ncontaining those clips\nyeah because they are like a composite\ntypes automatically lists and\ndictionaries are containers and and they\ncan be containers of any arbitrary known\ntransformers\nso\nthat's a great point\nyeah that's it uh\ni think uh uh one other thing we wanted\nto add was like what what are we working\non and\nthe core team here\nmostly all of us\nwho've been contributing\nso\nuh\ni think i'm really excited for this\nrelease coming up end of this month uh\nwe have a native scheduler in flight so\nthis is cron and fixed rate scheduler\nscheduling no need to configure\nanything in one of the cloud providers\njust use flight and it comes even on\nsandbox you should be able to schedule\nstuff\nuh\nwe are working on improvements to the\nremote\nthe remote api that we just released\nrecently\nlots of improvements thank you for all\nthe feedback it really helps\nwe are also working on uh some\nenhancements in the total back-end\nsystem we are working on adding\nmpi and snowflake plug-ins\nuh snowflake allows you to allow you to\nquery snowflake tables and use them in\nflight functions\nand mpi is to run distributed training\njobs\nnot only receiving training any\ndistributed mpi jobs\nand we want to make it really easy for\nmost users to not think about how to\nstart the containers how to bring them\nin containers together and so on so\nthat's all will be handled by flight\nand\ni'm sure there are a few other things\nthat i'm missing so i think oh there's a\nbig update in the documentation uh\nnot\na docs but like how users write\ndocumentation for their workflows tasks\nobjects and so on how that gets\nrepresented within flight um\nit's a big story it's ongoing we're\nconstantly working on it um\ni think the back end will be in place\nbut\nthe front end will take some more time\nfor it\nyeah and uh we are really looking for\nfeedback on on\nwhat aspects should we prioritize and uh\nwe have a huge backlog you can look at\nour issues and\nuh stories and there are lots of them\nand every day many are created all the\ntime\nbut prioritizing means you know uh with\nopen source is really hard like i find\nfour things really cool and i can go and\nimplement them but it's better to know\nif if something's really blocking you\nthen\nbetter to bring it up and we will be\nhappy to help\nanything anybody else wants to add that\ni've missed out hate them do you know\nsomething that we are working on that i\nmissed\nwe merged i guess recovery mode a little\nwhile ago that didn't make it yet to the\nuh release so that's also coming up end\nof this month and\nthat i think i don't think we deal with\nthat but um\ni think that's\nwe have\nheard people uh ask for something\nsimilar and i think it will be pretty\ncool to try it out\num and it does\nset up set the stage for a few new cool\nfeatures to add afterwards\ni think we should schedule a few demos\nnext time maybe a quick\nyou know few minutes\nto demo like these features coming in\nthis release\nin the ui as well\nthat's going to show up um i think we've\nsolved the problem of when you have a\nyou're trying to input\nuh json we now have a automatic form\ngeneration for json inputs for people\nthat are doing you know sort of manual\nhyper parameter inputting when they're\nre-running their workflows so some we've\nbeefed up the ui team a fair amount so\nthat you'll start to see some changes\nfrom those guys coming in soon\nyeah i think it's shipping up to be a\nbig\nrelease this yeah and great expectations\nis also part of this release uh yeah so\nand lots of flight kit enhancements uh\nhere and there so yeah\nas promised every month we release\nthis releases\nend of august\nnext one will be end of september and i\nthink uh\nwe are in a stage where we are rapidly\ngoing to\nexpand our integration\nwith the systems that are around because\ni think the core has been uh\nheavily worked on already and we think\nwe are in the peripheral integration\nphase\nthat doesn't mean we would stop working\non the core it just means that we get\nmore bandwidth to work on a lot more\nimplications\nthat's it\nthank you everybody for coming we will\nuh see in a couple weeks and uh thank\nyou yi for that that was super clear and\nit's super nice to see the docks like\nshipped when the features shipped that\nwas great\nthe improvement\nall right\nall right week everybody bye bye"
    },
    {
        "title": "FlyteRemote - Interacting with a Flyte Backend Made Easy - Aug 10 Flyte OSS Sync Up",
        "transcript": "uh welcome everybody uh niels is gonna\nuh show off some of the work that he's\nbeen doing on\nrefactoring the\nflight kick to sort of more cleanly\nuh\nseparate the remote interfaces from the\nrest of the interfaces so i'm going to\ntake that\nand take it away\nwell thanks george\nmorning everyone\nmy name is niels i'm um\ni guess fairly recent addition to the\nteam\num but maybe not so much anymore i don't\nknow\nuh but yeah uh can everyone see my\nscreen just\ncool\njust making sure\nand if i do this does that mess anything\nup that's just full screen okay cool\ngreat so um yeah i've been doing some\nwork on the flight code base and\num\nthe thing i'll present on today is the\nnew remote package um\nand like the main thing as you can see\nhere is it's called like remote and it's\nsort of\nyou can think of it as a client to any\nkind of flight back end\nand it does\nit does use the synchronous client\nthat\nsome of you might be familiar with but\nit it kind of package repackages it a\nlittle bit more nicely\num\nso right interacting with the flight\nback end made it easy\nso for anyone who's curious that the\nslides for this i'm using hack and b\num which is\nwe're just kind of\nexperimenting with not using\npowerpoint or google slides and um\njust making presentations with markdown\nwhich is pretty cool\num so the outline for this presentation\nis i'll give you a little bit of\nmotivation behind this work give you a\nlittle before and after snapshot of how\nthings used to look and how they look\nmoving forward\nthe design of the remote package\ncertain decisions we made and we'll\ncontinue to make as we\niterate on this which actually flight\nremote is kind of in beta so that's a\nlittle warning there\num\ni'll show you a quick demo of the new\nfunctionality\nand potential next steps for improving\nthings\nso i like to frame things in terms of\nuser stories\nso this one is\nas a workflow author i want to fetch\nregister execute and inspect\num\nflight entities in\nan interactive environment\nalthough not exclusively an interactive\nenvironment you could potentially use\nthis sort of in a script to orchestrate\ndifferent things but\num the primary use case is if you're in\na rebel or a jupiter notebook or\nsomething you know and you want to\nimprove your ability to debug tasks or\nclosing launch plans\nthis new\npackaging\nmight suit you better\nand as i said we'll\nimprove this\nas we go\nso just as a reminder\ncurrently there are two ways of\ninteracting with a flight back end\nbesides the console obviously but um\nit's a programmatic interaction i\nsuppose so there's there are\na bunch of command line interfaces\nflight ctl being the\nnewest shiniest one shiny shiny one\num and uh in the past\ni guess these kind of currently exist so\npi flight and flight cli also offers\nsome\nways of interacting like registering\ntasks and workflows and stuff\nand the second way\nis with the python sdk\nie flight kit\nand um\ni'll show this a little bit later but\nthere has been we we have supported this\nfunctionality for some time\ni think but um as i said\nthis is sort of like a repackaging of\nthings to make things a little bit more\nmentally ergonomic\nso if you consider just one use case\nthis is probably kind of like the\nmajority use case although i have no\nnumbers to back this up but i'm guessing\nthis this kind of workflow fetching\nthis kind of task oh gosh everything is\noverloaded this guy if i want to do\nsomething right and uh like fetching a\nworkflow and executing it\num\ni want to be able to\nsync the state of the workflow execution\nwith whatever is in\nthe\nremote back end and i want to wait until\nit's complete and look look at the\noutput so look at the errors that\nresulted\nso before you would do something like\nthis in flight kit\nor the obligatory imports\nyou would fetch you would use this sdk\nworkflow\nclass to fetch the workflow um pointing\nto whatever project domain name and\nversion of the workflow you want\nyou can create a launch plan out of it\nif you want to execute that workflow\ni'm\nyou know i mean i think maybe or someone\ncan correct me if i'm wrong but i think\nthis was the way to\nexecute said workflow before um with the\nold sdk star stuff\nso\nyou would then launch an execution with\nthis launch with literals method\nand um you'd have to provide a literal\nmap\nof\ninputs\nto feed the x uh to feed the the launch\nplan and then you would call this wait\nfor a completion method\num and then you can you would be able to\nprint the outputs at that point\nso this was pre\n[Music]\n0.21.2\n21.2\num\nso now after\nthe code doesn't look super different\nbut i'll kind of highlight\nthe main differences in\na second but you can see here that i'm\ni'm doing one import of this flight\nremote class and i'm instantiating this\nobject from configuration so this\ninitializes a remote object\num with a default project and the\ndefault domain and there's a bunch of\nstuff that happens underneath to pull in\nthe config files and environment\nvariables\nand that populates\na bunch of um\nthings under the hood that are necessary\nto like actually make remote remote\ncalls\num\nincluded in this would be like the url\nof the backend\nthe port and stuff\nso once you have a remote object\nyou can fetch it\nyou can fetch a workflow by naming it\nand you could provide a version but if\nyou don't provide a version it'll just\ngrab whatever the latest version is\nand you can then execute this workflow\nthis is now a flight workflow object\nand it it represents\nthe state\nof\nwhatever you've registered in your\neither sandbox or your remote deployment\nand you can give the give it python\nvalues instead of like having to import\nliteral this uh the whole literal\npackage thing back in the um\nbefore\npicture\nand you can see that i'm adding this\nweight equals true\nargument and x the execute method and\nthat will wait uh synchronously for the\nexecution to complete before\nreturning the workflow execution\nand then you can print the outputs\nso\nthis is the before after snapshot\ni'll get into the design of why we\ndecided to\ncentralize the\nthe object or the class that makes all\nthe network calls then basically all the\nother kind of flight entities as i'll\ncall them so tasks work those launch\nplans\nand others\nof those\num\nare basically data containers that you\ncan access the inputs and outputs and\nlike various other things um so they\nthemselves don't make any\ncalls over the network\nso the goal for the design is to just\nstreamline\nwhat it's like to interact with the\nflight back-end\nand\nso before\nwith these sdk\nstar classes\nbasically each flight entity implements\nits own methods as i said before that\nmakes network calls where each class has\nsome overlapping and some distinct\nbehavior right so\num like workflows you would fetch it you\ncould register serialize it and create\nlaunch plans and then the resulting\nlaunch plan is the thing you can execute\nand for tasks you can do a similar thing\nbut kind of like the method names were\nslightly different\nof course we could have cleaned this up\nin another way we could have like\ncreated\nuh kind of maybe a parent class that\nimplements\nthese kind of\nuh methods in a more consistent way but\nthe direction we went with is\nto centralize things\num so that we're kind of separating\nconcerns\nin the sense that the remote object\nnow handles all network calls\nand any flight entity you\nget from it or\nany flight entity you get from it or\nfeed into one of these methods\nare basically just data containers\nso\nas you can see here\nthere are basically\nfive method types in flight remote\nthere's this fetch\nmethod for task workflow and launch plan\nyou can call remote.register to register\nlocal flight task workflows and launch\nplans execute them\nthis will return an execution object\nthat you can then sync\num so you can call sync to just get\nwhatever the\nthe most up-to-date state is in the in\nthe remote backend\nand you can call wait to wait for the\nexecution to complete uh synchronously\nso one one other thing just i this is\nnot like super important but\num just to call out that the\nthese flight classes take advantage\nadvantage of single dish dispatch method\nso\nwe can have the same name and apply\ndifferent\nlogic or implement different logic for\nthe various different flight entities\nso in this case it's a single method\nthat can take in you know\nflight tasks workflows or launch plans\nand it'll know what to do with them\nand finally\nas i mentioned before\nyou could you can inspect these objects\nmore easily like the properties are\nfairly intuitive i want to say um so you\ncan print out and i'll show you this in\nthe demo but\nworkflow.interface will give you the you\nknow the interface\ninputs and outputs of the workflow\nyou can do the same for tasks and then\nfor executions they have this inputs and\noutputs property\nokay so hopefully this doesn't\nfail miserably um as demos sometimes do\ncool so i have a jupyter notebook here\nthere are some like background stuff so\ni have a flight sandbox running\nlocally\n[Music]\nthis environment has access to some\nenvironment variables that i've set\nbeforehand don't worry about it too much\nlike for example\nthat it points to localhost\n381\nso we're just making sure\nthis remote can actually connect to my\nlocal sandbox\nso this remote object\nwe can look into it if anyone has any\nquestions but um\ni'll just run through this demo\nin order so\nthe first use case here for fetching and\nexecuting a workflow\num\ni'm calling fetch workflow to do that\nand um\ni'll just remind you that this is you\nknow one of our\nbeloved flight snacks examples a really\nsimple one\nthat looks kind of like\nthis oops that's the wrong\none yeah\nso this is\nthis is basic workflow.pi\nit has my workflow which uses two tasks\nand they're fairly simple\nyou can kind of just read them here\nbut when i fetch it\nthis\nthis is now a flight workflow\ntype\nand it exposes a bunch of stuff that um\nokay\nyeah so it gives you you can inspect it\nand see what the id is what the\ninterface is\num so as i mentioned before we can call\nthat interface\nhere\nand let's go ahead and execute it now so\ni have my\nlet's move my screen around i also have\nthe console here available to us so let\nme just find\nthe basic workflow\nand i'm gonna run\ni'm gonna execute it and wait for it to\ncomplete i'm giving\ntam and fubar as inputs\nso now you can see it's running\num\njupiter notebook is still waiting for\nthe output should be\ndone in a few seconds\num but i've pre-run this notebook\nand uh\none thing you can do with this is also\niterate through the node executions so\nyou have access to all the different\nnode executions of your workflow so this\nis\nthe node execution for task\ntask 0.\nyeah this is lagging a little bit okay\nyeah so you have the node execution for\ntask zero you can print the inputs and\noutputs of that specific one and then\nsame goes for\ntask number one\nyou can do the same you can actually\nexecute local workflows with a big\ncaveat that\nthis is more of like a convenience thing\nwhere if i have if i can import\nmy workflow here so\ni've added\nthe flights next cookbook to my python\npath and so now i can\nimport it here\ni can actually\nexecute the workflow\nfunction directly\nwith the caveat that i actually i can't\ni have to know the version that i want\nto execute beforehand so this isn't\nactually doing\num anything anything super fancy it's\njust um\nmore for convenience if you if you\nactually have your\nworkflow um\nyour app workflow decorated function in\nhandy um your environment\nso this will do the same thing i know\ni'm here i'm just printing the inputs\nand\noutputs um\nyou can basically i'll kind of breeze\nthrough these but\nthis is the same thing for tasks i can\nfetch a task\nlook at its interface\nand execute it as a single task\nexecution\nso\nit should be here and you see that it's\nit's running\ni can there's actually a terminate\nmethod here as well so i can terminate\nit from\num my\nmy remote like my interactive\nenvironment\nuh\nyeah i have to kind of time this well\nbecause\nwait a few seconds and then terminate it\nit's like running\nand i terminate it\nand then this should\ngo into the board state at some point\nthere you go\num you can do the same thing with local\ntasks so if i have the local task at\ntest decorated function\ni can\num\nexecute that\nthis is this is something\nwe haven't discussed too much but it\nwould be nice to be able to actually\nregister\ntasks from this\num\nusing the remote object but um\nnot sure yet\ni'll skip this launch plans thing\nbecause it's very similar to the\nworkflows so you can do the same exact\nthing with fetching launch plans\nexecuting them\nthe last bit i did want to get to before\ngoing back to these slides is\nthe whole limitation around not being\nable able to register and execute local\ntasks and workflows or local tasks\ndoesn't apply to task template tasks\num\nso you can for example\ndefine a sql like\nthree task here as an example of a task\ntemplate task\num and so here i'm using this\none\nsql sqlite table\nand doing like a super simple query on\nit\nso let's define\nthis\ninteractive sql task\nand\nyou can register it\njust to make sure i'll give it a\nname here so\nmy\nnew\nversion\nand if we hop on over to the console\nwe can see that there's this basic\nquerying\ntask\nwith\nmy new version here\nthat's the latest version\nand let's go ahead and execute that with\na limit of 10 so this will give us the\nfirst 10 rows of this table\nand this time i'll give weight equals\nfalse just to prove to you that you\ncan't it'll complain\nso here i'm calling sync so i'm getting\nthe latest\nstate of this execution\neventually it'll\ngive me the\noutput\nwe can also prove to ourselves that this\nis running oh i guess it succeeded\nyes and so it's completed with this\noutput\num another way you could do it like so i\nthis is\nnot sure how often this case will come\nup but you could\nexecute a task\nuh and not wait for it synchronously do\nsome stuff and then call down the remote\ndock wait on it\nto\nsynchronously wait for the execution to\ncomplete so here i'll do another\nworkflow execution\nthat happened really quickly\nand then\ni'm just unpacking it here to prove to\nyou that\nwe have a data frame of results in this\nquery\nokay\num\nso in summary\nflight remote provides a centralized api\nfor interacting with the remote back end\nthis is sort of\nso on the top here are the methods the\nmain methods of flight remote\nand um\neach of the different flight entities\nthat\nit can operate on\nand you can kind of support for it so\ncheck means it's supported and tested\nuh the the questioning emoji is\nwhether we\nwant to support\nthe method for the entire corresponding\nentity at all um\nand then these gray circles are kind of\npartially supported\nflight entities and method combinations\nso for example for python tasks only\ntask templates\ncan be registered and executed um\nfor the rest of the local flight\nentities you kind of just\nit's just a pointer to whatever the\nremote backend\nworkflow or task or launch plan is\nand so you know for potential next steps\nit would be pretty cool to\nbe able to fast register local tasks and\nworkflows um\ni'm not sure if this is\npossible so it may not be\num and then\nright now you can't really re-register a\nremote flight entity under the new\nproject and domain so that might be a\nuseful thing to have\nand um as always yeah if people you\nstart using this and have ideas so if\nyou welcome any of those\noh\ni have a question news\nthe\nthe very first line that like sort of\ninitializes the remote object\ndoes that\nensure like\nconnectivity uh and the thing that says\nyou know flight from config\nuh\nyeah remote equals from quickly does it\nlike actually validate the connection or\nlike that it can connect or or do you\nhave to do like an operation to do that\nand second of all second question sorry\nyou can answer both uh does it\nwhat happens if you have authentication\nenabled on your like server\nso\nfor the first one it doesn't actually\ncheck that would be\nprobably a good thing to do\num\nand for the second question i think\nthere's some\nlike so\nyeah\nsorry can you can you repeat it\nsorry yeah uh so if you have uh\nauthentication uh on your like flight\nback-end server\num and you try to do any of these\noperations will it like you know go\nthrough\nthe like authentication flow and\nuh\nyou know maybe prompt you with the\nbrowser to like authenticate how does\nthat does that\nwork fine with this interaction or does\nit\nrequire any extra setup\nfrom you know like the client side\num also\ni guess for jupiter i'm not sure how\ndoes that\nwork if it tries to like pop up a\nbrowser to authenticate\nnot sure if any of that is tested that's\nokay too it just yeah\nyeah i'll i'll i'll have to get back on\nyou in that because i i remember testing\nthis on demo.new client at some point\nbut i'm not sure if that\nrequired authentication it does right\nyou did yeah yeah yeah\nokay\nyeah\nthat's good um\ncool uh that's pretty cool uh the the\none part i think i i\ni think you clarified later on like if\nyou have if you import a python like\nworkflow\nuh and you use that in execute the only\ninformation it like extracts from that\nis like the id\nright yeah basically\ni had one question so\nwhen you execute a workflow isn't\nfor most users once they go through the\ndocs it might be a little counter\ncounterintuitive that\nwe actually execute launch plans and not\nworkflows\nso\nhow\nlike what's the mental\nthought process behind that\nbehind directly executing workflows yeah\nlike because what what's actually if i\nhave to guess what's happening is\nactually it's executing the default\nlaunch plan\nfor the yes\nbut uh\nbut because we show a workflow as an\nexample maybe\nit might confuse some folks\nis that right\nand then how do you how do you execute\nuh launch plans uh they're named launch\nplans\nyou can\ni guess you can you can register launch\nplans via the other kind of way\nthe longer or i guess the canonical way\nof doing it with like pipeline and all\nthat then you can fetch launch plans\nwith the remote um\nhere\nso yeah in this case i'm just fetching\nthe default launch plan which is exactly\nthe name of the workflow\nbut if you had registered another launch\nplan here you could\nyou could\nprovide that name and it would return\nthis\nthe corresponding launch one\nyeah i think i missed the method name\nhere it's french launchpad with\nsomewhere else it was fetch workflow at\nleast that's what i thought\nuh oh maybe the left hand side you\ncalled workflow\nin the other example in your slide\noh yeah maybe there was a mistake there\num but yeah i mean i think we we\nour thinking behind this was that um\nand by we i mean ye and myself\nis that like you can you can\nit would be it's sort of a convenience\nto be able to directly\nto mentally not have to think about\nlaunch plans when you want to execute a\nworkflow\nbut under the hood it is fetching the\ndefault launcher\nyeah\nyeah i think we should add a method to\ncreate a launchpad to because launchpad\ncreation is just like register maybe\nit's very lightweight operation\nyeah yeah agreed\num yeah this is awesome like i think it\nlooks extremely clean i i actually think\nit's possible to do registration for all\num\ntask types through jupiter notebook\nbut yeah in a future iteration into a\ndemo one day\nand and this is all in the\nlike already released news\nyes that's right\num i need to write some examples on\nflight snacks but\nthere should be documentation on the\nflight kit\napi reference\num\none question um\nis it possible to\nlike search or just list\nworkflows and tasks\nsimilar to like the flight flight ctl\nget command i think it is\ndoes does it does remote allow it\nsomething like this or do you have to\nknow the\nworkflow id in advance\nno that's a great idea um\nyeah i mean like remote\ni don't know why my jupiter thing is\nright so like\nit's yeah it's a matter of like exposing\na method that just you know makes it\nsuper easy but yes that that would be i\nthink that's a good idea\nuh but it current it currently does not\nokay good\nyou know we figured you so we expose\nthis this client thing which is the\nsynchronous flight client\num for any enterprising users who\nwant access to that but um yeah i think\nbeing able to list it directly\nwould be great\nand\nanother question i think is more for for\ncaitlyn maybe because you just said\nit might be possible to actually\nregister all kinds of tasks\nso\nhow how is it possible if you if you\nneed to build a docker image\nso i think you're muted or is it just me\nyeah i know it's uh well as youtube um\nyeah so the the i think\nthe reality is the reason why you cannot\nregister is to build the docker\ncontainer\nbut fast registration goes around it\nright\nokay okay\nand so the other reason why we cannot\nreally execute is because in jupiter\nnotebooks are not like\npython modules where you can load them\nand just like\nyou know load that one function though\nit's really possible with magics\nuh you know in jupiter notebooks so if\nyou if you would do some sort of stuff\nyou should be able to essentially load a\ntask\num\nand run it and use fast registration to\nkind of serialize the the notebook\nitself and send it over to the server\nbut\nthis is theoretical in my head\nright now\nokay cool interesting\nyeah but but after that point like and\nthen this circumvents\nthe fact that you have to use paper mail\nor you have to convert the notebook to a\npython module\nor use pico either\nall three of them are a little\nnot\nuh great that way they have various\nproblems in different scenarios so\ni think it's possible but we'll have to\nwrite it won't be natively possible in\njupiter notebook i think we'll have to\nadd some like matching\nelse\ni think this was uh thank you needs for\ndoing this because this was one of the\nmost requested features the last\ntwo months almost everybody's like oh\nwhere is that way like we had this whole\nthing how do i do this and so thank you\nuh neil's really\nyou know\ndid an awesome fantastic job"
    },
    {
        "title": "Flyte Event Triggers and Recover Mode Demo - July 27 Flyte OSS Sync Up",
        "transcript": "actually start with recover mode because\ni think that's kind of a shorter demo\num but yeah let me introduce recovery\nmode so uh we kind of started off with a\nquestion of like let's say you know\nyou're writing workloads and you have\nsome sort of like broad system failure\nuh where maybe you say like a\nthird-party like plug-in connection\nfails or\nyou know your flight deployment has\nissues and for whatever reason\nthat kind of results in like the mass\nlike result of like system\nerrors that end up terminating your\nworkflows um\nand recovering those is pretty annoying\nespecially if you don't have discovery\ncaching enabled\num because you're gonna have to go there\nand like recompute a lot of work if you\nwant to retrigger all those executions\nuh so recovery mode be introduced in\norder to uh basically help with that\nscenario\nuh and the way this works is it'll allow\nyou to kind of say recover an individual\nexecution\nand it will copy all the successful node\nexecutions that i ran previously\nand then start essentially from running\nany of the failed nodes\num so you don't have to kind of redo all\nof your work even if you didn't have\ndiscovery enabled\num and that's probably best visualized\nwith the demo so let me share my screen\n[Music]\nsorry like that's a little grumpy um so\nlet me know if you can see my screen\num um and so so we can take a look here\nwe have like this example workflow\nexecution in this case i just supported\nit we didn't have\nsystem failure necessarily but in either\ncase it didn't succeed\nuh pretty simple workflow execution just\na couple of nodes one succeeded one\nwas terminated when i ran the abort um\nthe workflow execution is awarded\noverall\num so let's say i want to recover this\nexecution uh\ngo to our handy dandy flight ctl uh\nthanks you brought in april for that\ndemo again\num and we can just create an execution\nand you'll notice in this case we have\nthis recover flag that we passed and\nthat is just\nexecution id within the same project\ndomain sg4 bkn\nsj4 began etc so we can go ahead and\ntrigger any workflow execution um we'll\nget this new execution identifier\ntake a look at it in the console here\nand you can see that this n0 is\nrecovered immediately\num so there was just like no like you\nknow like overhead or anything we didn't\nhave to start a pod again or fetch you\nknow discovery values\nand now we're immediately running n1\nwhich was the failed node execution\npreviously\num take a look here you can see that the\ninputs got passed\num and um yeah and it basically\nstarted off where the last execution\nfailed um so niels is going to go over\ninto kind of more like you know the\nremote like programmatic uh flight\nexecutions\nuh stuff in the next meeting um you can\nimagine you know you can like\ncreate like a script basically run\nthrough like a series of you know failed\nexecutions over time duration\ngo ahead and trigger recovery for all of\nthem and you can basically go\nand recoup kind of like your system\nstate um previous to kind of a broad\nlike system failure\nsee now execution succeeded um and we\nare all good to go\nawesome um yeah so that's pretty much it\nfor recovery mode\num i don't know if there's any questions\non that um\ni have a question yeah um is there gonna\nbe console support for this this seems\nreally nice to like just kind of trigger\nfrom the console for sure yeah yeah so\nthat's forthcoming as well too like\nwe'll probably add some kind of like\nrecover button in the console as well\num yeah i just yeah ui support is always\na\nstruggle because yeah\nbut that'll happen eventually cool\nthanks this is really neat\num yeah any other questions\ncool okay if not we can go into uh react\nto workflow events\num so let me go ahead and\npresent this\num and i think i'm using the same theme\nas purple so great theme um\nso reactor workflow events um\nso before we go into what reactive\nworkflow events are let's talk a little\nbit about the state of how to trigger\nworkflow executions today\num so traditionally you know you can go\ninto the console launch an execution you\nlaunch one from flight ctl\nor even you know a flight cli before\num but they always kind of require like\na human in the loop right and so you\nknow there are ways to kind of like\nprogrammatically trigger executions but\nyou have to go ahead and kind of like\nmanually like initiate that create\nexecution request\nwe do have scheduled workflows which is\nkind of like a departure from that model\nright\nwhere you have these kind of events that\ntrigger um workflow executions on a\nregular schedule but we don't kind of\nlike really extrapolate that model to\nanything else\nso for the most part you kind of have to\ngo and manually trigger an execution\nso this is where reactive workloads come\nin um and a lot of scenarios especially\nin kind of like you know etl like data\nlike uh\ntraining or data like processing jobs or\nmachine learning like data\ntraining jobs um you kind of want to\nlike trigger an execution once something\nlike you know upstream has happened\num some event and that you're listening\nto and then you go ahead and use the\noutputs of that event in order to launch\nan execution\num so some examples of that are say you\nhave a data partition that lands up\nstream and then you want to go\nahead and trigger some kind of\nprocessing job uh maybe a different\nflight workflow execution\nuh you know terminates successfully and\nin that case you want to go ahead and\nuse the outputs of that execution\nto you know feed into the input to a\nseparate execution um\nyou kind of continue on the data\nprediction theme we could just have just\nindividual file uploads\nsome kind of marker that some process\nhas terminated and we can go ahead and\nstart a different execution\nand you know there's tons more examples\nof this you know different sorts of like\npop sub queues on different cloud\nproviders um you know see even like web\nhook uh web applications and\ncustom applications like slack or you\nknow a stripe or github or even more\nif some event happens you know in one of\nthese applications you want to trigger\nflight workflow execution\nand what these all kind of have in\ncommon is that you know you're like\nwaiting and pulling on an external event\nto trigger execution as opposed to\nhaving a human kind of sitting there and\nclicking like\nexecute so\num in this talk i'll kind of go over\nlike the user facing proposal for like\nhow we want to expose reactive workflow\nevents\num in like say python like sdk type\nenvironment not so much\nimplementation but i'll kind of go over\nthat a little bit here we're just kind\nof gathering feedback right now\non like how to um like expose this and\nmake this usable\num so uh to kind of begin with like\nwe'll talk about\nuh event sources and targets the\nterminology we'll be using here\num so an event source is uh you know\nlike that kind of like upstream file\nlanding or that upstream workflow\nexecution terminating\nso that's kind of you know like this\nlike bullying condition that when\nsatisfied we'll go ahead and trigger\nthat target and in this case um the\ntarget for our kind of v1 implementation\nis only going to be a flight workflow\nexecution\nbut as part of this implementation what\nwe want to do is integrate with argo\nevents\nso argo itself is actually an open\nsource workflow workstation engine\nthat's kind of i guess you can see\ncomparable to flight\num but they have a fully like kind of\nindependent like uh events project\num which basically handles kind of\nkubernetes native\nand it integrates with all sorts of\nevent sources not just you know like\nupstream files\num but you know like the applications\nand the pub sub keys and everything i\nwas talking about earlier\nand all sorts of different event\ntriggers as well too\nand what we want to do is build off of\nthat for kind of like v1 implementation\nand just support a couple of these use\ncases which includes\nlistening to upstream workflow\nexecutions or file or directory changes\nand then support targets which in this\ncase would just be a flight workflow\nexecution\num but eventually you know like we'll\nkind of build this as like an open\nsource thing within um\nflight but we do want to like upstream\nthat to our go events so that users\ndon't necessarily have to be bound to\nkind of this sdk that we like\nuh define for reactive events and they\ncan actually use argo events to trigger\nflight work flexications you know\naccording to their setup\num so let's go into kind of what some of\nthese example\nlike use cases would look like um so\nthere's a document out there that kind\nof goes over to this in more detail\num it's kind of like in a review stage\nright now too so if you have any\nfeedback i really encourage you to take\na look at that that should be post in\nfuture discussions i can share it again\nafter this meeting\num and that kind of goes yeah again it\nhas these examples in more detail\num but so let's take a look here uh so\nthis is kind of the first example we\ntalked about which is let's say you have\nan upstream workflow execution and you\nwant to trigger a downstream workflow\nafter um this previous workflow has run\nso you can see this upstream workflow um\ni kind of you know just had some like\npseudocode here it produces\nuh different outputs and one of those\nhappens to be a save file um that we\nconsume in this downstream workflow um\nas an input\nso this is the syntax that we have so we\nhave this kind of like trigger like\nuh body that kind of like captures this\nevent target um\nlike syntax um so the event in this case\nis going to be a flight workflow\nexecution\num you can see we can just pass in the\nflight workflow definition itself so you\ndon't have to specify the execution\num we you know you don't even need to\nknow necessarily the version\nuh you just need that workflow\ndefinition um and then here in this case\nyou can see we can also like further\nfilter that event so that we only\ntrigger\na downstream execution of say the\nupstream succeeded because you know it\nwon't produce the file otherwise and\nthat's kind of useless to us\nand in the target syntax in this case\nwe'll have um\nlike a launch point essentially that\nwe'll invoke um and we have this kind of\nsyntax for referencing the upstream\nevent um\noutputs um so we'll pass in that kind of\nlike downstream\ninput file by referencing the the\nupstream workflow outputs\num so yeah any questions here or we can\nmove on to another example\nmove on to another example uh\nwell um so then uh we can take a look at\nkind of like the file change um api as\nwell too\num so in this case we have say a\ndownstream workflow that just takes in\nan entire like you know remote directory\nuh create a launch plan uh doesn't\nassign any inputs or anything\nand then again we have this kind of like\nevent trigger syntax so in this case you\ncan see the event changes\num now we have on directory create as\nopposed to flight workflow execution\nand we pass into prefix so this will\njust watch this directory and\nanytime a new like uh directory lands in\nthis directory\nit'll go ahead and create um\nan indication of this downstream\nworkflow\num in this case you can see the syntax\nis slightly different but we reference\nthat upstream event directory\num and pass that in as is the input um\nand we also have this kind of like\nindications parameter which limits us to\nyou know just say like the first time a\ndirectory lands in this\nprefix then we'll go ahead and trigger\nan execution but we won't watch anymore\nafter that\num so uh on the theme that\num we can kind of customize like these\nevents some more um so one thing we were\nconsidering adding was say timeouts\nlet's say you have like this you know\nevent listener that's you know like\ntriggered for perpetuity\nuh maybe it doesn't really make sense\nfor this like listener to like you know\nlive forever so you can add um\nan expiration time and then we'll\nessentially delete this event listener\nafter that um and then also kind of\nwent over this earlier with the\nindications you can also limit the\nnumber of times that an event gets\ntriggered\num if it doesn't make sense to like\nconstantly you know like pull and\nproduce new workflow executions um\nwell and then also kind of wacky stuff\nlike multiple events\num so this is again like argo has\nsupport for this all under the hood and\nwhat we'll do is we'll be basically like\nyou know taking these user event\ndefinitions\nand um translating them into argo like\nuh\nevent source like objects um so they\nactually\nhave support for like multiple upstream\nobjects so we could also kind of build\nlike an sdk on top of that as well too\nin this case we have a composite union\nevent which means once both of these\nconditions are triggered\nthen we'll go ahead and um produce a\nworkflow or invoke a workflow execution\nuh with these event um these upstream\nevent um\noutputs um as inputs to the downstream\nworkflow\n[Music]\num yeah so that's kind of like an\noverview of the proposal\nagain this is just kind of like a\nuser-facing proposal we'll talk more\nabout the implementation later\num but uh this will again like build all\non top of argos so we'll be kind of\ntranslating like the user-facing code\ninto\nargo events um and again fully open\nsource and like integratable\nso hopefully it should add some value to\nthe argo ecosystem as well as the flight\none two\nyeah any questions\nthanks very much for that demo katrina\nlet's see um any questions comes oh\nthank you niels for dropping in the link\nto the document in the chat\nso if you have any comments to that\nplease go ahead and add them\num any questions comments\nor um ideas\noh quick question for katrina um\nis it is the event framework flexible\nenough to say\ntrigger an event after like say i'm\ningesting data points\ninto s3 and i want to retrain every\nthousand data points um\nsay i'm likely using like some kind of\nsagemaker labeling job or something\nwhich that does dump um\nlabeled records into s3 would it be\npossible to\nretrain a model like on you know\nevery thousand events or something like\nthat\num yeah that's a really interesting\nquestion i think right now the event\nsensors are\nlike for the most part stateless minus\nthe kind of like invocations count\num but i think that's something we can\ndo like the underlying like video api\nwhich would power these kind of file\nchange events\num is you know we get notified on any\nkind of like you know file change type\nthat we subscribe to\num so we could introduce kind of like\nthe separate state that could maybe\nenable things like that like for the\ncustomizations on top um so like\nevery thousand files or whatnot um so\nyeah that's something to consider\nuh but yeah i'd have to explore like\nthat syntax and the implementation a\nlittle bit more too\ncool"
    },
    {
        "title": "Flytectl Walkthrough & Demo - July 27 Flyte OSS Sync Up",
        "transcript": "uh good evening good morning everyone\nuh yeah let me just share my screen\nand everyone see my screen\ncool yep um\nokay uh this will be a\na a short\ninteraction of how exactly flight ctl\nuh is being used for interacting with\nflight so\nuh so just for all the comments this is\nsimilar to\nuh like any uh any other command line\nutility\nuh that we have so currently we have\nyeah\nwe have uh different clis that we use\nwith flight uh we wanted to have\nuh another binary which can be\npretty uh easy easily portable\nand distributable that can be\nthat can be used by all the users\nto interact with flight and i'll go over\nthe use cases\nshortly but this gives you\nlike all the uh all the interactions\nthat you need to do\nwith the flight environment\nso just uh overview of some of the use\ncases that\nare being covered in this uh\nso basically whenever you have to uh\ninteract with slide you create your\nprojects\nuh you create your workflows launch\nplans tasks\nuh you you uh create executions on those\nand\nrun them on on a on an environment uh\nwhere flight is deployed uh along with\nthat flight ctl also allows you to\ncreate\nlike a mini flight environment directly\nusing flight ctl\nso you can just play around with\nthat environment which which is\nbasically\nwhether it runs a kubernetes cluster kt\ncluster within\na docker container and along with all\nthe flight containers running within it\nso that that makes it very easy for\nfolks to uh try the uh\nflight environment uh on their machine\nand uh not have to worry about um\nsetting up the entire thing uh uh\neither on a aws or gcv this is on the\nmachine they can\nuh do this pretty easily and\nuh yeah so just um\nso the existing waves so people who have\nuh\nusing flight there's already an existing\nflight control ui\nto which um uh you can pretty much\nlaunch executions uh uh or launch a task\nor launch a workflow you can provide\ninputs visualize the graphs of how\nexactly your dag is here for your\nworkflow\nso we pre and also\nwe have a command line based flight kit\nuh\nwhich is python based and uh which are\nalso uh allows you to interact with\nfight\nuh in a similar manner but through a\ncommand line interface but\nthe disadvantage with flight kit is\nbasically you have to\nuh have to install all the dependencies\nwhich are pretty much part of uh the\npaper installed it\ntakes uh but builds everything and then\nuh brings your brings you off like a\ncli for you there are other alternators\nwhere you have\na flight called java based sdk\nand which you can use for uh\ndoing a similar interaction of creating\nregistering your flight entities or\ngetting execution status through these\nsdks\nor you can create your own client that's\nanother option you have we already\npublish\nan sdk and flight publishes it for\nmultiple languages i think go java and\npython so\nyou can write your own uh\ncr client on top of it\ngoing to why flight ctl but so\none thing is uh it's pretty easy to\ninstall and set up\nif you have a mac os you can just do uh\nreinstall it\nand you have your flight ctl binary\nbundled with all its dependencies and\nit's it's a cross platform so we have\nreleased it for linux mac os and windows\nand these are continuously built\nthrough our pipelines so any changes\nare being released for all these\nplatforms they'll be probably\nsupporting more later on uh these are\nthe existing ones\nthat we have\nalso it's a very small footprint of this\nbinary\nand so which makes it very easy to\ninstall as well and\nas i mentioned earlier you can uh if\nsomebody wants to\ntry flight with uh deploying a cluster\nor something\nit's pretty easy to do it using this uh\nbinary so you just pre-install flight\nctl and then\nyou want to create a flight cluster you\ncan just do uh\nflight ctl sandbox create cluster\nand there would be more uh demo at the\nend which will\ntell you how exactly to create the\nsandbox environment and\nrunning those uh workflows so so\nit becomes very easy for you to uh\ninteract with the flight environment and\nand see the power of\nof uh flight in general in in a very\neasy steps\nand along with that like if you if you\nwant to use it\nin your automation ci pipelines uh\nthere is also a github action which\nallows you to uh\ninstall it as part of your ci\nand if you want to have some\nuh creation of projects which which are\ntracked to bit of workflows or any other\nflight entities those are\npretty much possible now and\nalong with that uh the main flight ctl\nhas been\nwritten in a way that properly mimics\nsome of the\ncube ctl style uh so you have uh it's\nstructured with\nnouns and verbs so similar to how like\nflight ctl get\na resource or flight ctl uh get\nuh or update or create\na workflow or a task or\na launch plan or an execution so\nyou've structured our documents uh all\nour commands in that way\nso you have um you have this\nthe sub command and within that you have\na particular resource on which you want\nto act on\nso you have flight ctl get workflow or\nflight city i'll get execution and\ni can just show you the main page where\nwe have\nall this defined so this is\nstocks.flights.org\nso within the api reference you should\nbe able to\nsee uh the docs for flight ctl\nthe install and configure as i mentioned\nit's\npretty simple to do it on os x we have\nto install slide ctl and and the\nupgrades are also\nsimilarly very easy we're trying to make\nit much more simpler\nright providing it as part of flight ctl\nitself like flight ctl\nupgrade or something that can\nself-upgrade\nand other operating systems also\nuh right now we have uh just an\ninstaller scripting\num that works on uh\nlinux uh i think we have to uh we'll\nprobably see like you think windows is\nmissing we\nprobably add an installation stuff for\nthat and\nuh the the configuration is also pretty\nsimple\nof how exactly uh you configure your\nuh local environment uh uh\nall so this is a sample configuration\nwhere you want\nuh where your uh flight\nadmin is running uh the local host and\nthe port where it's running some of the\nadditional parameter whether it's secure\nand\nif auth is configured on the admin\nuh and some of the other stuff is mostly\nfor if you're using like faster\niterations\nuh while registering uh but basically\nyou won't be required if you're just\nusing a normal serialize\nand so as i mentioned that\nso this is how most of the commands are\nstructured\nlike uh the command and the verb\nand so create update delete\nregister and and the corresponding\nones on how exactly uh what are the kind\nof resources\nyou will be interacting with are\navailable\nin in the announce page so i'll just go\nover one of those\nlike creating a project yeah so we have\ntwo ways you can create a project you\ncan specify\nall the important stuff uh that you need\nfor creating a project\non the command line uh parameters like\nname id\ndescription or if there's another way\nyou can just\nhave everything in a yaml file\nand similar structure to\nwhat you have in the command line\nparameters and\nit allows you to do the same thing\nnow with the yaml it gives you an\nadditional benefit you can actually\ndo get ops on all these files and\nand this is across all the other\nflight ctl commands you can actually uh\nversion some of these these created\nfiles and\nyou can track them for any changes and\nso creating an execution is also\npretty similar you can any task that you\nwant to execute it\nyou'll you'll uh you'll create\nan execution specifier so this is like\nfor a merge sort example\nand it dumps you\na yaml file which is basically what are\nthe\ninputs required for executing this\nparticular task\nand where you want to execute that\nwhether in a same project domain or you\nwant\na different domain and project for this\nparticular task to be executed\nand what version of the task it is so uh\nwhichever one you are fetching it so if\nyou are doing a get on the task\nand you have specified that you want to\ngenerate an execution spec\nfor particular version uh v2 version so\nit will\ncreate a spec file with that information\nuh the rest of the things like i am role\ncube service account\nor anything related to permissions that\nare required\nfor uh executing this\nin this particular workflow uh will have\nto be specified\nand yeah i mean\num so it becomes uh\nand whenever you want to create an\nexecution you just pass in\nthe spec file so whatever modified file\nyou have\nuh you just pass in for pass in that\nfile and there are other things that we\nhave\npart like relaunching and recover\num yeah i mean\nuh that is pretty much it and\ni would like um raj to go a demo\nabout this and show uh\nthe how to create a sandbox and create a\nsample execution\non it\nover here yeah yes thank you\nand good morning everyone so let me\nshare my screen\nis it brisbane yes we see your screen\n[Music]\nso for today's demo we needed an example\nso we will\ntake one example from flight snack that\nis pmi diabetes\nso we can see in the case study we have\none example in animal training female\nthat\nuh prima diabetes so what it is it is a\npipeline\nfor training and supposed model for a\ngiven data set so\nwe will just like i will explain you\nquickly this workflow\nso like we are taking three inputs data\nset\ntest split seed and then we have four\ntasks\nso these tasks uh is basically like\ntaking the input and then producing an\noutput\nand then passing it to the next task so\nthese are the four tasks we are talking\nabout first is the split\nit will split the task it will uh\nreturns the model\nthe trick task and then we will pass the\ntest\ntheta to predict the output and then we\nhave prediction\nnow we will calculate this code uh from\nthis\nprediction and the test data that we\nhave\nso this is the simple model uh we will\nuse for today's demo\nso first we will we will start the\nsandbox cluster\nokay so first we\nneed to move particularly directly that\nis the ml training in ml training\nwe have all the examples like house\nprice prediction and pima diabetes\nso here we will run the sandbox\nwe will start this inbox cluster and we\nwill pass this source directly to\nthe networking directory and currently i\nhave i have\nthese android spaces so i'm not\nrecreating it but you can create it\nand you will get the output like this so\nwe have console available at 3308\ndirectory so first what we can do is\nlet's start with the demo so now we can\nsee that our\nsandbox cluster is uh running we can\ncheck that\nokay so now like\nif you want to create a concept for this\ncluster so we have a config command that\ni i want to show you so if you create\nthat config file it will automatically\ncreate a config for you\nin the home directory and by default uh\nflight automatically picks the config\nfrom the flight directory\nso you can get the same uh flight config\nusing the same command and if you are\ndoing it for a remote cluster then you\ncan use it\nlike this like by passing hyphen hyphen\nhost\nand now it will create the config for\nthe remote cluster in the same way\nnow when you have config ready for the\ncommunication with the sandbox\nyes and now we will first serialize\nuh we will build our docker image\nso for doing that uh because we are\nrunning it in a sandbox\nmode like a local mode so what we will\ndo is like\nuh we will run the docker build command\ninside the container because we\nmulti-contain\nsource code inside the container that\nmeans we can run the\ndocker build command inside the\ncontainer and in our communities cluster\nthat image is accessible\nso for doing that we have a command in\nplan box\nthat will build the image inside the\ndocker container\ni have the images so it will from the\ncache but\nif you don't have the cache it will\nbuild one and\nnow you your image is available now you\nwant to serialize your workflow\nokay\nso you need to pass the package name\ncheck thing out piece\nand the image that we want and the\noutput file\nand it will serialize the workflow so\nnow our\nserialized workflow is available in the\ndirectory\nthat is flight.tv now we can register\nokay we can register this workflow\nworkflow is registered now we can check\nclick\n[Music]\nget\nokay so like we just registered our\nworkflow\nand then we get our workflow and we\ncreated a dot url\nso it created our dot graph here so we\ncan see our graph we have a split\ntraining data\ntask that has fit predict and score we\nhave total four tasks\nthat is available in graph dot slider to\nrc\nhere\nnow our workflow is registered\nsuccessfully now we want to execute that\nworkflow so for that we will get the\nlaunch plan first there will be launch\nfriend\nso it will create a file execute spec\ndot by men\nwe can edit this file and we can change\nthe input parameters\nso for example\nokay now we can basically create\nexecution\nfrom this file okay\nexecution is created\nokay so the execution is start it\nrunning so we can\nwe can also check it in browser in the\nconsole\nthat it is running or not okay it is\nrunning\nwhat is the input parameter series and\nthat speed is zero\nand while it is running we can like\njust make some changes in the flight\ncode base\nokay so this is the workflow we can\nchange anything for a damn purpose\nlet's add one more input type that is\nmaximum\ndepth\nand default is first\nimmediately fit\nokay\nso we just make made some changes in the\nworkflow and now we want to do fast\nserialize so for that\nwe don't need to build the images\nbecause our dependency we didn't make\nany change in the dependency we just\nmade a change in the code base\nso we will just use py flight to\ncreate fast serialization\nit is running begin with\nwe register the first civilized uh\nproto now we can\nbasically repeat the same step that we\ndid it for the\nserialized uh package like first we\nwant to get the launch plan\nnow if you see the execution basically\nlaunch plan you get the new input type\nand default is focused\nso we can change the value to 7\nsomething\nnow we can create an execution from\nso we can check the principle\nso we we just run this\nuh execution for the fast serialize\ninput\n18 37 we get the output\nand this is the workflow that we run on\nthe fast serializer\nso for this we didn't build the image we\njust see\nwe just did the first serialize and then\nregister this\nit is running\nso while it is running anyone have any\nquestions\nthank you for that demo you've raj and\nthank you for for introducing the\nflight cto walk through um questions\nanybody"
    },
    {
        "title": "Workshop: Building Flyte Container Task Extensions",
        "transcript": "hey everyone i'm samhita i'm a machine\nlearning engineer\nand tech evangelist at union\ntoday i'll be talking about custom\ncontainer plugins\nin flight kit let me share my screen\nright so here's the outline for today\nfirstly i'll be talking about the types\nof flight kit extensions we have\nand then the differences between custom\nbehavior and custom container task\nplugins\nfollowed by takeaways and sql alchemy\nplug-in code walkthrough\nwe have uh four types of flight kit\nextensions in total\nthe first one is custom flight kit type\nso here you see this example wherein we\nhave a task which is called my task\nand in that we have csv file which is a\nparameter\nit has this custom type so the basic\nmeaning of this is\ncsv file has this custom type now custom\ntype doesn't have any meaning so it's a\njob\nto define the functionality for this\ncustom type in\nflight cut python sdk now this is custom\nflight kit type\nnow what is custom container custom\ncontainer is\nauthor defined that means uh it's the\njob of the author to\ndefine the docker image that this sql\nalchemy or\nany other such task has to run in now\nwhat is custom behavior\ncustom behavior uses user's container\nmeaning the user has to provide the\ndocker image\nin which uh this particular task has to\nrun\nand then there's back end only which uh\nis like a combination of flight kit and\nflight propeller you'll have to make\nsome changes in the back end in order to\nmake this work\nlet's look at the differences between\ncustom behavior task plugins and custom\ncontainer task plugins\nin order to know why custom container\ntask plugins fare\nbetter than custom behavior task plugins\ni'll go through each point one at a time\nat\nserialization time a docker container\nimage is required\nthe assumption is that this docker image\nhas the task cord\nwhereas in a custom container task\nplugins the docker container has to be\nprovided by the author\nnot by the user so this statement means\nthat in custom behavior task plugin\nthe docker container image has to be\ngiven by the user\nserialized task contains instructions to\nthe container\non how to reconstitute the task in our\ncustom container\ntask plugin serialized should contain\nall the information needed to run that\ninstance of the task\nthis means that in a custom container\ntask plugin\nthe serialized task in itself has all\nthe information that is required to\nspin up the instance whereas in a custom\nbehavior task plugin\nthe serialized task has to send\ninformation to the container\nwhich again helps in kind of rebuilding\nthe task\nwhen flight runs the task the container\nis launched\nand the user given instructions recreate\na python object\nrepresenting the task whereas in a\ncustom container task plugin\nit all depends on the executor the\nexecutor has to run that particular\ntask the primary difference between\ncustom behavior task plugin and custom\ncontainer task plugin is that\ncustom behavior task plugin is task\ndependent\nwhereas custom container task plugin is\nexecutor dependent\nexecutor is like a layer that's put on\ntop of\ntask\ngoing on to the next difference the task\nobject that gets serialized at compile\ntime\nis recreated using the user's code at\nruntime\nin a custom container task plugin the\ntask object\nthat gets serialized at compile time\ndoes not exist\nat runtime it's because we have executor\nwe don't need the task anymore\nit's the job of the executor to run the\ntask\nat platform runtime the user decorated\nfunction\nis executed whereas in a custom\ncontainer task plugin\nthere is no user function it's the job\nof the executor\nto produce the outputs and accept the\ninputs that are given to the task\nhere are the takeaways of custom\ncontainer task plugins in general\ncustom container task plug-in helps in\nshifting the burden of writing the\ndocker file from the user to the author\nthe user needn't worry about\nwriting the docker file anymore for that\nparticular task\nand it also allows the author to\noptimize the docker image\nwe needn't depend on the user so it\nhelps the author to optimize the image\nlet me do a code walkthrough now of the\nsql alchemy plugin that we already have\nright so here's the directory\nstructure that we'll have to follow in\norder to write a plugin\nit could be a custom behavior task\nplugin or a custom container task plugin\nplugins folder is present below the\nflight cut folder\nin plugins we have to write the plugin\nnow our plugin is flight with sql\nalchemy\nin that we have flight kit plugins\nfollowed by sql alchemy now this is the\nstructure that's\nthat we'll have to follow in order to\nrespect micro lib\nway of writing plugins because that's\nwhat we are following in order to write\nthe flight kit plugins\nmake sure you have the plugin name\nfollowed by flight kit plugins\nfollowed by the folder name now\nyou access this plug-in by uh using\nflightgetplugins.sql alchemy package\nthis is defined in the setup.py file\nthat's present within this particular\nfolder\nhere if you see this is the package name\nthis is how you'll have to install\nusing pip and this\nthis is how you import the code in the\nfile in the code file\nall right let me go through each file in\nhere\nfirstly i'll be talking about the\ndockerfile now since it's the job of the\nauthor to\num write the docker image we'll have to\nhave a docker\nfile it needn't be present in this\nfolder i have put it in this folder\nbecause uh\nit merges well with the code that we\nhave\nso in the here we have python we also\nhave a couple of requirements that we\nare installing\nif you see in here requirements.txt um\nlet me go to requirements.in\nso this is what i'm installing in here\none is flight tip and the other is sql\nalchemy\nyours could be a different requirements\nyou'll have to put the list of\nrequirements that you want to install\nwithin the\nplugin after you have your docker file\nand requirements it's your job to\nbuild the docker image you'll have to\nbuild the docker image and after you\nbuild a docker image just push it to a\ndocker registry it could be anything i\nhave pushed to github registry\nif you see here i'm using a github\nregistry to push\nmy image and i'm using that particular\nimage in here\ni'll go through each code snippet in\nhere\nfirstly i am using sql alchemy config\nwhich is a typical sql alchemy\nconfiguration in order to connect to the\ndatabase\nand then i have two important classes\nthat\nwe have to mandatorily define in order\nto write a\ncustom container task plugin one\nis sql alchemy task and the other is sql\nalchemy task executed in general this is\ncalled the task class and this is called\nthe task\nexecutor class now in task class i\nam subclassing python customized\ncontainer task\nyou will have to do this this is where\nthe task\ntemplate or the custom thing is written\nand then we have the task type which is\nsql alchemy\nthis basically points to the plugin that\nwe are going to use\nif at all this plugin is not present\nthen it defaults to\nthe container plug-in that we are using\nin here\nand we have the init method in init\nmethod you pass in some\nparameters it's test name query template\nthe query that you want to use\ntask config which is this class uh\ninputs a couple of inputs and the output\nscheme are tied by default the output\nschema type is flight schema\nand again i am calling the init method\nof this particular class\ni am passing in the name task config\ncontainer image i have\nas i've mentioned before the task the\nexecutor type which\nis this particular class the task\nexecutor class you'll have to mention\nthis name in here\nand the task type and the rest\nthis is a property that i've defined and\nthen the get custom now get custom is\nused to\nserialize the task template it means\nthat you written some specific set of\nvalues that you are going to use when\nrunning the task\nnow this task is run by this particular\nexecutor\nhere i'm returning four values because\nthese are the values i'm going to use\nwhen trying to write my functionality\nnow moving on to this task executor\nclass\nthis task executor class subclasses shim\ntask executor and it has this method\ncalled\nexecute from model which accepts\ntemplate\nsimply parameter now this task template\ncan be used to extract these values this\nis how you do\nit pt dot custom secret connect args now\nthis\nextracts the value that's defined in\nhere the same way we extracted all the\nvalues that are required\nuh we follow a typical logic wherein uh\nwe are connecting to the database\ninterpolating the query meaning\nreplacing the variables with the actual\nvalues\nand then running the query generating a\ndata frame finally returning the data\nflow so this is the job of the task\nthis is where you'll have to encapsulate\nall of your business logic\nfinally out of the box the task template\ntask will run a command that looks like\nthe following when the container is run\nby flight\nso this is what gets run when you uh\ncall the this particular task sql\nalchemy task this is\nwhat gets uh triggered\nall right this is how you write a test\nthat is it it's that simple\nand then let me show you how you can\ntest this particular task now you have\nwritten you have written the test now\nit's\nyou want to test it so how do you do\nthat you have a folder called\ntests this one in plugins\nin tests you'll have to create your\nrespective folder whichever is related\nto your plugin\nnow mine is sql alchemy in sql alchemy i\nhave my test files\nso these are the plug-in test cases i\nhave written to make sure that my plugin\nis working correctly\nyeah and then this is an example file\nthat i've written\nso this is how you'll have to call your\ntask after you write your plugin\nsay you're writing this particular\nexample in flight snacks then you just\ndo this part\nyou call your rescue alchemy tasks by\npassing in the required arguments\nand then you are running this task\nwithin\nthis workflow by passing in a limit\nvalue\nto limit the number of records that you\nwant to fetch from the database\nand then you get the data frame and i'm\nagain passing it to a task written in\nthe length of the data stream this is a\nvery simple functionality that i have\nimplemented\nto showcase how easy it is use it is to\nuse a task\nyeah so that is it there um and you have\nto make sure that this task\nthis particular file has all the\nrequirements you have\nto make sure that it has the\nrequirements file it has the docker file\nwherein you\ngive this particular plugin you give\nthis flight kit these all have to be\ninstalled on top of this\nuh i'll pass the baton on to haytham who\nwill be showcasing uh\nthis particular example from the user's\nperspective\nawesome thank you samita\nthat was uh it was pretty cool uh so\ni for the\nfor my part of the demo i will be\ntalking about how do you\nuh take it to sort of the next level of\ntesting\nuh so what the meter showed is uh you\nknow\nhow do you start uh writing a new plugin\nright so you go to the\nflight kit repo and make all of these\nchanges and you test it locally and the\nidea is that you catch all of the\nbugs uh as quickly as possible you know\nand like\nuh as soon as possible i should say\nright uh\nbefore building containers and all of\nthat right you're just\niterating quickly on your code locally\nuh you can test your tasks how they get\nserialized how they run\nall of that right um but uh but no\nmatter how much like\ntesting you do uh behavior could be\ndifferent\nwhen you actually run uh remotely you\nmight have missed\nor made some assumptions about the\nexistence of you know certain\ndependencies and so on\num and this is where this next step\ncomes in uh\nwe have this repo called fly snacks i'm\npretty sure most of you guys are\nfamiliar with\nit has all of our examples\nuh that are you know in most cases are\nlike\nuh ready for copy paste uh so if you\nwant uh you know how do you\ndo you want to learn how to do like\nconditionals there are like a bunch of\nexamples that you can just copy\nhow do you use a particular plugin you\nwill find the\ndirectory there you can just copy uh the\nexample and they make file docker file\nall of that\nthey're like ready for that consumption\num it's also a way this is how we\ngenerate the documentation and so on\nso after you develop a plugin we\nask everyone to write sort of a formal\nexample with documentation and so on\nhere\nso that it serves the purpose of\ndocumenting the\nhow people should use the plugin but\nalso has the code that they can just\ncopy\nand use and we can validate the plugin\nworks as expected and so on\num the the the directory structure is a\nbit\ncomplex i have to admit um\nbut where most of those plugins will\nshow up is under\nintegrations uh so these are you know we\nput\nwhere you integrate with external\nsystems in most cases\nyou see aws and others um\nand uh under flight kit plugins these\nare the\nthe we call them we call them here\nflight kit plugins those are the\nthe plugins that uh that\ni was talking about uh where we provide\na custom\ncontainer um that runs the the task for\nthe user\nuh there are already a few examples here\num\nand i just added this one uh to\ndemonstrate\nuh the the plug-in semita showed\num there are a few files here so we'll\ngo over those i know i i\ntried to make the sidebar navigation\nhere bigger but i could not\nuh so excuse that uh uh\ni hope you can read the the text though\nuh so this is a very simple file that\nhas the plugin\nthe the sorry the task that uses the\nplugin we\nimport that and the workflow that just\ncalls this one task\nuh you will notice here that i did not\nneed to add a docker file\nbecause there's no i i i\nas a user i'm not writing any custom\ncode that needs to run\num and the author of this plugin\nprovided\nyou know all the necessary uh\ncontainers that need to run this task uh\nso you have my file\nwith the codes and then i have a\nrequirements file that just needs\nflight kit and this very long uh\narguably too complicated uh\nrequirement and this is how you you can\npip install a package a remote package\nof sorts that has not been published to\npiper yet\nright um at the later phase after you\nhave validated all of this you'll be\nlike\nthis is good to go then you publish your\nuh package and we have tools\nthe two chain in flight kit to do that\num\ntwo pipe but before you do that you\nstill need to run your example and this\nis how you\ncan do it you can reference a particular\nchar you can also go back and make\nchanges to your flight kits\nuh like plug-in and come back update the\nthe shot here um and you can reference\nthe sub-directory this is what i did\nhere to\ninstall this one plug-in right so it\nwill get the\nreleased version of flight kit the\npublicly\nvalidated flight kit version plus your\nnew plugin\nuh installed this is a requirements in\nand we can\nrun the\nrequirements pip compile\nsomewhere here we go so\nuh oh i am in the directory\nall right so while that's running we can\ntalk take a look at the\nrest of the files uh there's a flight\nkit config file this is what tells\na flight kit at serialization time\nuh what well it has a lot of other\nconfigs but the only one we\nreally need is uh this that tells flight\nkit\nwhen uh pi flight when we serialize this\ndirectory\nwhat packages to look at um\nto like serialize or extract workflows\nand tasks from\nuh and in my case this is i'm referring\nto the sql alchemy simple\nuh my oh here we go uh yeah so pip\ncompile finished and you can see\nuh it pulled in flight kit obviously and\nmy\ncustom requirement as well as everything\nelse that flight kit\npulls in um\nso is that yeah and then uh yeah so\nthat's that's pretty much\nit uh so you just need these uh one two\nthree files you can then uh uh\nrun by flight register and i have that\nhere so we say can you is this big\nenough\nthe text here i hope so\nas it says by flight and then there's c\npasses the\nconfig file which is the wrong name\nlike it config serialize workflows and\nuh like it says serialize the outputs to\nthis directory\nyou can run that and\ni need to create directory first\noutput here we go\nwe can also inspect the outputs here it\ngenerated\ntwo three files um this is the the task\ni have one task and then the workflow uh\nthat's represented here and this is a\nthe\nwhat get uh gets automatically generated\nit's launch plan\nwhat we call a default launch plan for\nthe workflow um you talk about that\nin a separate uh discussion um\nand then uh after you're done so this is\nso you validate it now that the task\nand the workflow can serialize correctly\nright there's some validation that\nhappens here at\nflight kit side uh like you know some\ntyping checks and\nothers so this is an important step\nright uh and then the next\nuh step you do is you register those uh\ngenerated files\nuh and for this i am using flightctl\nictl register files you pass in the\nuh these all the files here\nwith the config and project and\ndomain register that i think it might\nyeah they're already uh registered i can\npass a different\nuh uh version let's say that there's\nversion and oss demo one\nand i'll put yeah so everything\nsuccessfully registered\ni can jump back to\nmy\nwindow here in a second here we go\noh this is uh\nour development environment okay i\nhope that's visible um so yeah if i\nsearch for\nsql alchemy you see my uh workflow got\nregistered\nand the version i picked is here\nand and then you can just launch it from\nthe ui and so on you can also launch it\nfrom flightctl but\ni find this for simple workflows easier\nto use\nuh so you can just launch it in or pass\ninputs or whatever\num yeah of course you might find bugs it\nmight not run as you would expect\nand uh and you will like go back to\nthrough the development or the iteration\ncycle again go back to the plug-in\nvalidate your changes locally make sure\nyou write unit tests for them\nas samita showed uh and then you you\nknow you repeat the cycle\num these are the i think the most\nuh important probably steps in like a\ndevelopment cycle for for uh\nfor a new plug-in of that type uh we\ndid really want to go through this uh\nworkflow because i think it shares a lot\nwith\nall the other uh task types or\nplugin types uh that we will be\ndemonstrating in like future\nuh walkthroughs um and for those we want\nto focus more on the\nthe logic and how do you customize those\nplugins\nand then the same workflow applies right\nyou write it first in flight kit you\nvalidate unit tests and so on\nuh you publish stuff and then you test\nit in like a remote sandbox with\ndocumentation\num and uh hopefully yeah hopefully this\nuh you find this work uh\nwalkthrough useful uh encourage more uh\ncontributions for\na lot more uh task plug-ins of this way\nof this type um i think we have we were\nmaintaining a list of uh\nof integrations that we think are\nsuitable for this kind of integration\nand we can we can share that\nmaybe uh afterwards if anybody is\ninterested to get started\nuh we'll definitely love to help you so\nreach out\nstack you're all there um and uh\nand we'll help you you know in the\ndevelopment of these plugins\nanything you want to add samita\ncool thank you\nany questions\ni um i guess maybe i'll ask\na question instead of adding something\nso how would you\nso when would you choose to use\num the the\ndemo custom behavior uh\nwith container supplied plug-in instead\nof\nback-end plug-in\nlet's actually take the simple example\ncontainer supplied uh\ncustom behavior versus a regular flycat\npython task type plugin when would you\nchoose to\nuse which one and why\nyeah do you want to pull up your slide\non that samita\num sure yeah i think that's yeah that's\na\na very good question i think it applies\nto\nall the other types of plugins too\nwhen do you choose to use which\nthe maybe take aways\nslide six all right so the\nthe main the main use for uh this kind\nof plug-in the\nthe custom container uh task plug-ins\nis that uh you as a user uh\noff like the the end user who is using\nthese tasks\nthey do not have to worry about uh\nyou know dependencies or uh you know\nconflict of dependencies\nuh building docker files and maintaining\nthose docker files and optimizing the\nhow the like the runtime of these looker\nfiles all of that\nis can be maintained independently by\nthe\nplug-in author um and they are arguably\ngoing to be the best at\nuh doing so uh they know you know how to\nrun\ni don't know sql alchemy best uh the\nbest type of uh\nlike base image to use uh the most\noptimized and so on\num i think there is also like at scale\nthere are benefits when you run\nthese tasks yeah a lot in your like\ncompany these images will be cast\nuh on on the you know servers where\nflight runs\nuh so like you will see at scale\nuh performance improvements in uh\nrunning these type of tasks um\nuh when you shift it to uh the the\nuser the custom behavior uh with user\ncontainers\nuh you get uh so you lose these benefits\nbut you what you get is access to\nuser code uh which can give you more\num or can i allow the user to customize\nthe behavior even more\nuh right so you can provide the as a\nuser you can provide custom logic maybe\na function or something\nthat will run maybe in the case of spark\nfor example right you you the\nthe spark code cannot be expressed\ndeclaratively\nuh it is it runs like dynamically uh\nuh and the user has to supply that uh so\nthere\nso when when the user has to provide\nlogic that\nhas to run at runtime as code\nyou cannot use the custom container only\ntask plugins you have to go to uh the\ncustom behavior\nwhere you know you involve the user\ncontainer in the process\ncan i paraphrase that last bit just for\nme to understand and probably everybody\nelse to understand\nso the so the time when you should\nprefer\nthe custom behavior the\nyou know author supplied container\nplugins\nis essentially when the the task plugin\nspecification or the task specification\nis completely portable\nthat means you can move it from users\ncode wherever they are on their laptops\nhopefully on the web or whatever from\nthere into\nthe container at runtime which sql\nin this example the one you demoed is\nextremely\neasy right it's just text you can move\nthe text from\nuh from the user's code into uh\nthe container is that right\nokay yeah yeah and if so then but that\ndoesn't mean that\ncode cannot be moved right you could\nmove the code if you could serialize the\ncode as text\nand move it over and you have all the\ndependencies now the problem with that\nis that\nthe dependencies if you depend on let's\nsay a tensorflow for example and this\nplugin was not doing anything with that\nit will not work right yeah\nokay yep and yeah if you uh\nand there is i guess a line fine line\nthere like eventually you'll be\nbuilding rebuilding docker right like if\nyou package the\nuser's code their native dependencies\nthey're dependent right\nyou are essentially rebuilding doctor\nand that's you're\nprobably not what you want to do yeah\ni just wanted to understand okay draw\nthe line\nyep\nyeah but i think uh as a takeaway\nthis these are the types of plugins that\nwe'll see more and more\noften it's a great way to\nintroduce plugins that are controlled\nusing flight kit but not need a back-end\nplug-in so you don't need to\nknow any go to write these kind of\nplug-ins and yet they are\ncompletely controlled by the plugin\nauthor not by the user\nand that has a lot of benefits because\nyou can as\nthey hit the mentioned so\nlike let's say if you want to have a way\nto interact with data proc\nor you know data flow or whatever and\nit's just uploading jars\nit doesn't need to be a go laying back\nand plug-in and\nthere are benefits of using a google and\nmac and plugins get into them into the\nfuture\nuh like no need to run a container and\nthat's a huge benefit\nbut that's not required as a step one\nthis is an excellent step one and the\nand the\ncool part about these plugins is that\nslowly you can migrate them to a backend\nplugin without needing any user code\nbehavior changes\nfor them they were running a container\nand then tomorrow they starts running\nback in\nno container code and it still the same\nbecause their workflow is the same they\ndon't have to build a container\nthey don't have to uh think about where\nit's executing\nhow they are executing what are the\ndependencies and so on they just write\nthe code and\nit works that's that's probably the way\nto think about this\nand i think that's where i saw him in\nhis\nexample had snowflake plug-in because\nyou could do\nsnowflake uh like this essentially that\nyou can write a query that executes in\nthe python\npretty much easily much easy to write\npython plugin for snowflake\nand then later on easily migrate that to\na backend plugin\nfor calling snowflake service right from\npropeller\nyeah and and the nice thing about that\nis from an\nend user perspective uh that code will\nlook\npretty much the same they still\nyou just say my task here's the query\nand that's it\num and then because of how like how\nstandard the serialization everything\nelse is uh this could\nend up being a separate container like\nthese plugins or running in the back end\nin either case you don't need to involve\nthe user\ncontainer and it's completely\ntransparent\nand it's a very nice yeah it's very nice\nlike i guess up skill so you can start\nwith us\nit's an experiment you don't know how\nmany people would use it if a lot of\npeople start using it\nthen you invest more into a back-end\nplug-in and get the benefits that\nyes we will talk about in a future\nwalkthrough\nany questions i'm gonna put jeev on the\nspot\nbecause he clapped\nno this is great i uh i learned a lot\ntoday um\ni haven't had a chance to like think\nabout how to use these yet but\nwe'll definitely look at the examples\nyeah yeah like i think\nuh we've talked about though gleb has\nadded big query plug-in but if you\ndon't want to use bigquery directly this\nis probably\nsuper fast way to get there"
    },
    {
        "title": "How Spotify is Using Flyte to Build Their Financial Reports - June 29 Flyte OSS Sync Up",
        "transcript": "uh today we actually don't have two\ntopics\nwe are we're trying a different format\nuh one of the things that we've been\nthinking is\nhaving um somebody present once a month\nand then the second time around probably\ndo a workshop of some sort\nlike for example deep dive into\nsomething and and\nso today we're gonna do two things one\nis uh\ndylan thankfully has uh agreed to\npresent what he's been working on it's\nreally cool stuff\nand then as the second half we would\nwant to open up discussion\nspecifically around roadmap\nthings that people want or things that\npeople want to hear\nany workshops that we should do within\nthis session because\ni know a lot of people are unable to\nattend\nthe call itself uh but\na lot of them see the videos so we'll\nopen it up\ndefinitely leave a comment somewhere\nmaybe on the meeting note or something\nor at least you know voice it here\nwhere if you like a workshop kind of a\nthing every alternate\nuh meeting and then we can schedule some\nof those\nso that's okay we'll kick it off with\nit all yours man\nuh we don't use zoom internally so just\ngive me one second to\nshare and\ni hope this looks like right yep\ncool uh so my name is dylan oops\num i'm an engineering manager\non a team called vivaldi within spotify\nwe work in the finance department\num and i'm also the tech lead on a\nproject called one model and so that's\nwhat this is about\nand one model um\nis really powered by flight uh and so a\nlittle bit of a disclaimer here\num this presentation is kind of a\nfrankenstein between a few different\nuh presentations um on this topic\ni tried to stitch them together into a\ncoherent story but uh\nyou guys are really the guinea pigs um a\nlot of this stuff too is also kind of\nthe latest thinking on how we're\nthinking about this problem\num and so some of it's more well-formed\nthan others\nbut we think it's pretty exciting uh\nso quickly a bit of background about\nwhat we're talking about oops\num this slide is really about how this\nproject is actually\nnot just this team but a collaboration\nof a number of teams\nwithin spotify and i'll talk a bit about\nhow flight actually empowers this\num but uh we're the team in the middle\nhere vivaldi\nbut we also work with i think a bunch of\nthe guys you know\nfrom spotify who are contributing\ntowards flight itself and managing it\nwithin spotify\nuh we're building a bit of a platform on\ntop of\num flight and then uh the team on the\ntop that you see here is actually\nand there's a bunch of teams actually in\nthis area but they are\nlargely comprised of data scientists and\nanalysts\nand so not necessarily engineers but\nthey\nare our biggest stakeholders and i'll\ntalk about how you know that works in a\nsecond\num but before i do that i just want to\ngive some background on what we're\ntalking about so this is really one\nmodel is a financial forecasting\nproblem and quickly\nwhat financial forecasting is is\nevery quarter as part of compliance\nobjectives\nspotify is required to project two years\ninto the future\nwhat we think our profit and losses are\nand that's part of just being a public\ncompany\num and so it's basically a projection\nacross\nall of spotify's various business\nprocesses\nuh what revenues do we think will take\nin what cost do we think we'll have to\nspend in order to get there as\nin addition to just being part of you\nknow requirement it also forms\na big part of spotify's business\nplanning\nuh you can imagine this is useful for\ninvestment investors it's just as useful\ninternally for the people making\ndecisions the c-suite etc\num unfortunately uh you know it's\norganically evolved over the years\nto be a bunch of different heterogeneous\nprocesses across many different teams as\nyou might imagine spotify is a pretty\ncomplicated company\nwe have a lot of different products we\nwork in a lot of different markets\nwe deal with licensors music law stuff\nlike that\nand so the expertise in order to kind of\nrun this system is scattered across\na lot of different domain experts\nand so as it stands this process takes\nabout three to four weeks every quarter\nspan as many as eight different teams\nthese teams\nwork in silos some of these pieces are\nexcel models\num and then at the end of the day you\nmight imagine team one is handing off a\ngoogle sheet to team two to start\nrunning\nyou know if the number of subscribers we\nget is input to the amount of revenue we\nthink we're gonna get\nteam one has to hand that off to team\ntwo\num and so it's a complicated process and\nso\ngoal number one that we're really trying\nto accomplish may be\nobvious but it's an automation problem\num we want to fix these manual handoffs\nwe want to speed up the\nfeedback time to do this process and we\nwant to\nyou know reduce for errors um\nand a secondary piece of this is that\nbecause it takes so long to run\nwe can only really do this once per\nquarter it's too much of an effort\ninvestment\num and so we think by automating this we\ncan really unlock the second piece\nof the problem which is business casing\nand scenario analysis if we can run this\nproblem\nend to end you know within a couple\nhours\nthen um these people making the\ndecisions can come\nin and say well what happens if we\nincrease\nyou know our subs by 20 million people\nin india\nversus what happens if we open a new\nmarket in eastern europe or something\nlike that so you can start to ask these\nquestions more frequently\nwithout having you humans in the loop in\norder to answer them\nuh there we go\num so this is high level overview what\nit looks like there's\nagain it's a complicated process each of\nthese nodes in this graph is actually\nyou know this isn't one\nflight task per se it's a bunch of\ndifferent things these are just high\nlevel\nlogical pieces to spotify's business\nmodel\num and as you can see here each\nindividual one of these\nmight be owned by a different team um\nand there's lots of you know complex\ndependencies between teams\nuh and there's no i mean this is our\nbest shot at what this thing looks like\nthere's no like person who's in charge\nof like the overall end-to-end thing who\nknows exactly what this looks like\nit's it's very much a distributed\nownership model\num so yeah one model\nautomating all the components uh and\nonce we do that we can kind of unlock\nthis scenario this business\ncase analysis scenario um\nand you know we think that'll be really\ncool uh\nreally quickly um i thought this slide\nwas fun\nuh why did we pick flight this is\nactually\nwritten by our data science team so this\nis not from us\num but we think it's really cool because\nas engineers a lot of this might be kind\nof table sticks for us\nbut for the data scientists being able\nto get up and running on flight and\ngetting all of this stuff for free has\nbeen a really big win for them and\nthey've been really excited about it\nyou know the ability to share models\nwith each other\nand compose things easily out of the box\nparallelism\ni mean again may not seem like the\nbiggest deal but for them when you're\nwriting\npython scripts and everything runs and\ntakes a certain amount of time whereas\nnow for free we get parallelism across\ntasks i think they think that's really\ncool the caching just connectivity with\ngcs\nall of this stuff they've been really\nexcited about it\nso that was the overview i guess of one\nmodel\nand forecasting at spotify um i wanted\nto talk a\nbit deeper about kind of the problems\nwe're wrestling\nwith right now and how we're thinking\nabout them um and then kind of how they\npertain\nback to flight as our main runtime\nengine\nuh so a quick reminder that uh\nthis is really a distributed process uh\nforecast is run by a lot of different\nteams running in parallel there's like\nintersection\num api points between them but uh\nyou know each of these might be a lot of\ndifferent sub workflows being worked on\nby many different people\nand there's really a human in the loop\nin between each of these connection\npoints\nuh so subscribers may finish their\nforecast they need to do some validation\non the outputs does this really match\nexpectations\nonce that's ready premium revenue can\npick it up and they have a similar\nprocess um\nthe other thing i want to point out here\nis that there's really two pieces\nto one model as a system uh the first\nis this forecasting engine flight so\neach of these nodes\neach of these nodes in the graph is like\na flight workflow\nis the way that we're moving um and so\nthis is really the flight piece of it\nbut there's a second\npiece which is equally important there\nwe go\nit's moving a little slow um which is\nwhat we call the assumptions bank\num and these are really uh if we think\nof model developers encoding business\nlogic\nuh business analysts are encoding\nassumptions\nin these tabular data formats which\nreally reflect\nuh kind of their best judgment on what\nthe future will look like across a lot\nof different parameters\nso one of the things you might expect is\nthat\nyou know spotify may increase its prices\nat some point\na year in the future we need to encode\nthat as an input to the system\nspotify might launch a marketing\ncampaign in europe that we expect to\nbring in\nyou know 20 million new free users\nthese are inputs to the system that\nreflect our best understanding of the\nbusiness\nthat we can then run through uh the\nforecasting engine itself and get the\nexpected outputs to all this stuff\num and so those are the two pieces that\nkind of happen\nin the quarterly forecast uh\nadditionally in like kind of the\nbusiness casing section you might\nimagine that someone wants to say well\nwhat if i change the prices what is the\nimpact of that or\num you know other sorts of such\nquestions\nso assumptions bank you can think of as\nthe levers you can pull\num on a forecasting or scenario analysis\nuh process that will change the outcome\nand then the forecast themselves you can\nthink of as our best\nunderstanding of how our business\nprocesses actually work internally\num so how's it going to work on top of\nflight\num again this is a\ndistributed process so it opens a lot of\nsticky questions\nuh you know it's really nice you can say\nwe can automate all this stuff\nbut really this is a manual process that\nrequires a lot of validation\nhas a lot of compliance concerns we have\nthese user and loop workflows\nthat require validation of each step of\nthe process before it can be consumed\ndownstream\nwe need to basically bless certain\nversions of each forecast\nand then advance them downstream those\nmight get updated\nhow do we handle that how do we continue\nhandling that with system\nreproducibility\nhow do we ensure consistency across this\nif someone\nruns a certain version with a given\npricing assumption and then someone\nconsumes that downstream and then those\nassumptions change upstream\nhow do we handle that um\nand then yeah this again there's there's\nabout 25 different\npeople i think or more involved in this\nprocess every quarter\nand so how do we how do we handle that\ncomplexity with so many users kind of\nfiddling with the inputs um\nso this is kind of where our latest\nthinking has taken us\non this problem and we really review uh\nthink about it as a problem of like\niterative alignment or collaborative\ndevelopment um so if we think of the\nwhole one model system\nas just the graph and it's a graph of\ntwo different nodes\nthe first type of node here we have um\nare\nthe assumptions themselves that we're\nshowing in red and so there are certain\nusers that have their business experts\nthey're maybe run the marketing\ndepartment or something like that and\nthey're saying these are the campaigns\nthat are coming up here's what i expect\nthem to bring in\nthey might be people modeling out growth\nprojections in certain areas and then\nencoding those\nin a tabular data set could be any kind\nof thing\nlicensor contracts is another good one\nwe have people who work with the\nlicensors we know which contracts are\ngoing to change or what our expectations\nare there\num and then there's kind of the model\ndevelopers and these are like data\nscientists they know a good amount of\npython and they're really encoding the\nbusiness logic\nthat business logic may change every\nquarter it may change\nduring a given forecast if we look at an\noutput and say we're missing something\nhere\nand so there's lots of different people\nkind of editing this graph all at the\nsame time and trying to align on\nyou know what is the correct state of\nthis graph at any given time\num the only other thing i'll mention\nhere is that uh obviously we have a lot\nof\nuh fax data sets that kind of make the\nbackbone um of like our\nuh forecasting projections so you might\nexpect historical mao\nto be the input that impacts what we i\nthink our organic growth will look like\ngoing forward\num so what we've really begun to think\nof this as\nagain is an alignment on the state of\nthis graph\num that users are kind of\ncollaboratively\nupdating um and so if we think of this\ngraph as an immutable object\nthis really begins to feel like a git\nworkflow\num and so if you can imagine uh\nwe have a master branch where this\noutput of last forecast uh known as the\nf3 which is actually the q2 forecast but\num is the head of master and so what we\nwant to do\nwhen we start thinking about the f4 is\nwe\nyou know make a branch off of the f3 and\nwe start\niterating on like the new business\nencodings our new updates to these\nassumptions hopefully\nlast quarter's assumptions provide a\ngood baseline we can make corrections\nwhere we're wrong\num and we can think about different\npeople\nowning different sub sections of this\ngraph for example someone might be the\nexpert on pricing someone might know how\nroyalties work\netc and we can\nmake these local copies and then commit\nback to sort of a feature branch which\nis\niterating towards the next latest\nforecast\nand we can imagine that the head of this\nbranch is always our best guess at what\nthe forecast should look like\num and when you know all the various\nbusiness owners have signed off it's\nsomething that we can merge back to\nmaster and say this is official this is\nwhat we're sending to wall street\num and so you can think of the process\nthis way\nuh similarly for\nyou know a scenario analysis this might\njust be\nuh someone creating a personal branch\nthat never gets merged back and they can\niterate they can share\num all of that stuff we get a lot of the\nbenefits of\nnormal you know collaborative\ndevelopment um\namongst coders um\nso yeah what is actually being committed\nto this\nhypothetical git repo um or git like\nworkflow um so if we again take a look\nback at\nour object model um we can see that\nyou know we have a couple flight\nworkflows in here that these each of\nthese things are version right and they\nhave some sort of dependency graph on\neach other\num and then we also have these\nassumptions that have like versions\nthemselves\nuh and so we can imagine someone you\nknow multiple people taking\na look at different parts of this graph\num\nand let's say you know someone updates\nthe license or contracts and creates a\nnew version and they update\nuh the revenue to be v3 down there we\ncan see\nand then they do a run so we commit this\ngraph\nback to the repository and we can see\nyou know the top pieces of the graph\nhave not changed at all but these\nother pieces have and so we have no new\npointers to each of these nodes in the\ngraph\nand when we run stuff in flight we can\nsee you know\nwe're going to leverage caching because\nthis top part of the graph has not\nchanged at all\nso you know revenue v3 will change\nbecause it's it's there sorry rerun\nbecause it's changed\nroyalty is what hasn't changed but it\nwill rerun because we have new versions\nof each of the upstream inputs and so\nnow our latest best understanding of\nwhat the forecast looks like is the\noutput of all the nodes\nin the graph on the right um\nand yeah so there's a lot of benefits to\nthinking about this this way\num it allows us to natively enforce the\ncoherency of the system\nso we know that the head uh\nof this git tree um on in a given you\nknow branch\nis always going to be internally\ncoherent in terms of what were all the\ninputs\nthat were uh run and what were all the\noutputs that were used\num and then you know it's natively\nreproducible\nwe can always jump back to a prior\nforecast if any time we get audited\nuh what it was run as a forecast f3\num it's just whatever the get tag is on\nthe 2021 f3 forecast\num you know it's natively composable\nshareable you can share your business\ncase scenario you can share your latest\nstate of the forecast but just pointing\nto a git commit\nuh and then the last thing that i didn't\nreally talk\nin too much depth about uh is that it\nmakes flight caching trivial\num and this is something i know uh i've\nshared with kitan a bit\nand we've been wrestling but there are\nissues around how we make assumptions\ninputs to the system because flight\ncaching\nuh requires you or runs off a specific\nlocation\num on disk and so even if the data\nhasn't changed if you create a new\ncopy of assumptions you're going to\nbreak you know the whole cache\nso the nice thing here is actually\nassumptions now are just a pointer to\na git commit and a specific object in\nthe tree\nand they're only going to change you\nknow if a user is actually editing that\nspecific file or\nin this case data set um\nand so yeah i think this is kind of our\nhigh level thinking around how\neverything works the fact that\nflight supports all of these immutable\num models uh\nall this you know versioning and like\nthis entire\nuh graph workflow and allows us to just\nthink about\nyou know the state of this graph and\nthen rest on\nthe fact that flight is going to keep\nthese things coherent is going to cache\nthings correctly and that\nyou know all of these things end up in\nsome sort of immutable record within\nflight as well\nso that's that i think i have time for\nany questions if anybody has any\nno was everybody keeping an eye on the\ntime oh yeah not bad\nfirstly thank you that actually that\ngave me a lot more understanding of what\nyou were doing and some of your\nquestions\num but i let everybody else please ask\nquestions\ni will have a couple\nno questions okay\num so dylan so when you say you have\nthat\nthat graph uh of the interaction model\nfor the\nentire forecasting engine um\nwould it like i think it's more of a\nfeature question earlier but would it\nhelp for you to visualize the entire\ndependency graph because currently you\ncan't do that today\nin the console flight console you only\nsee that at runtime\nso would that help to visualize that\nahead of time\nyeah so uh i didn't go into kind of user\ninterface here this is kind of like\nour thinking on the data model on the\nback end um somehow i\njumped past this slide actually whoops i\ndon't know how that happens\num but uh\nyes the answer is yes we may\num have to implement some\npiece of this ourselves in fact more\nthan likely we will not be using the\nflight console at all\nthat said uh being able to fetch the\nend-to-end graph\nin some representation is definitely um\nuseful for us\nwe'll end up having to build like to\nsupport this i somehow skipped\nover the slide but in order to support\nthis workload correctly again our\nusers are analysts and so they're not\nreally familiar with git workflows\num we'll need a ui on top of this and as\npart of that we want to represent you\nknow the entirety of the system so that\npeople can understand and reason about\nit\nyeah i'll also add hey is varun here on\ndylan's team that a sort of\ncompile time or registration time graph\nwould allow us to reason\nabout for a given model that consumes\nupstream inputs right\num we'd like to be able to fetch those\nupstreams for a given model\nso we can do other things with respect\nto their relationship to assumptions\nso also adding a yes\nto that would be helpful uh\nyeah good yeah what we really want and i\nthink we can pull this out of the flight\napi right now it's just like kind of a\nit's it's a little bit of work um but we\nwant at any for any run\nlike given a run id we want to know i\nmean as roon said we want to know the\nstate at registration time too\nbut we want given a run id like which\nversion of each\nof these nodes was run what were all of\nthe inputs in terms of the assumptions\nas well as you know historical actuals\nand then what were all of the outputs so\nwe just want like the whole graph\num and i think yeah i think it can be\ndone by\niterating through like flight objects\nand the protobufs but it's not\nsuper straightforward so it's a great\nuh thing so basically i i don't know if\nyou guys have used flight cto\nwhich we should talk about probably the\nnext meeting we will\ntalk about ctl has undergone a lot of\nwork and there's a\nlot of support as you know we don't have\nin our community interestingly we don't\nhave too many people working on the ui\num so we uh we actually\nmake flight ctl as the default de facto\ninteraction piece with the flight\npackage\nand of course that doesn't mean we are\nnot doing anything in the console i\nthink we have one\nengineer now who's going to work out a\nlot of the stuff and he already has\nhe showed me a great preview of the\ndrafting stuff yesterday\nso hopefully we could come and do a demo\nof the thinking over there\nin a bit too but you can use like ctl\nand it should retrieve\nall the the graph and it uses graphics\nto actually visualize the entire graph\nof how it is\nuh oh really it exists uh and it uses\ngraph.flight.org that's uh endpoint\nhosted on our\nuh on the open source uh dns itself that\nuh completely stateless visualization of\nthe graph\nso please try this might be something\nthat's worth linking to in our\nslack channel because i think this is\nsomething we're going to want to look at\nyeah yeah and actually the source code\nis in\ngo so you can and it actually prints out\na dot\ngraph so if you have a graphics render\nor somewhere else you can use that too\nso so that's one second\ni i actually have been hearing a lot of\nfolks ask about the same question like\nhow do we interact with the flight api\nas a kind of construct graphs or\nget data photograph or something like\neven sorry today morning\num a couple weeks ago if not even a\ncouple days ago\nsomebody else then a week ago it was\nkenny\nworkman and so on and i think\nlike kate before the new version of the\nfight gate which is the new api\nfrankit has the capability of talking\nwith back-end and retrieving data\nand interacting like through a jupyter\nnotebook or any kind of a\nprogrammatic interface uh it's just that\nwe\nare reworking that interface and niels\nhere is actually working on the\ni want to say a much more ergonomic\ninterface\nin interacting with the model um i think\nuh\nniels if you can share the rfc in the\nfact we can get we can solve this\nfeedback as to what you guys think uh\nand then also contributions\nuh of course welcome um\nso that should ease like you know\nbuilding getting data\nuh like reasoning things about it third\nthing bill\ni had one more question again another\nfeature question because i i\nsaw that it seems that you do have some\nhuman in the loop type of tab\nlike actions\nlike how are you thinking about that\nrather than me giving an answer\nyeah uh it's a good question i think\nit's it's actually\nthe biggest open question in terms of\nour next step here is really we want to\ntest this uh in the wild this kind of\nworkflow\num if you look the human in the loop\nhere is really\nsomeone will run their model and then\nthey'll have to verify the output before\ngiving it downstream\nand then that may happen iteratively as\ndownstreams notice there are issues they\nneed to may need to run\nupstream exactly how we\nhandle that in terms of a workflow\nthat's\nyou know good for our users i think\nwe'll have to build a lot of ui\non top of uh to make it not a bad\nexperience we're hoping\nthe git model underneath will handle\nlike the internal coherency\nand correctness of the system how we\nmake it like a pleasurable experience\nfor users i think is\nuh an open question it's ongoing\nresearch right now\nso i don't know if that answered your\nquestion but it's something we're\nthinking about\nhe gave me some hints so one\none thing i will okay so\nonce we open source flight probably in\nthe first month somebody asked can we\nget human in the loop tasks\nand that's one of the reasons why i\nbrought this up uh\nthe use case then what i've heard of in\nthe past for human in the loop\nwas essentially something like labeling\nright like you you get a data set\nand then there's a person who actually\nlabels it or approves the labels\num and and that needs a human today\nyou know and and it's kind of your stuff\nalso like you get the data and then you\ncan\napprove the data right you just like\neither this is right or\nno go ahead right it's like a it's like\na barricade\nis that the right way of thinking about\nit or not a barricade a barrier\nyeah i think that's true uh\ni think honestly we were probably\nplanning on handling\nthat outside of flight so thinking about\nhow flight could handle it for us\nisn't really something that we've\nthought about\nbut generally if we jump back to here\none of the nice things about this is\nthat you know\nif someone makes some changes that\nupdates something that needs to be\nreapproved it's very easy to understand\nand track\num and so we can say oh\nit looks like there's changes to this\nassumption that you are an owner on\nuh that needs to be reapproved at the\nend of the day what everything at the\nhead of this\nbranch needs to be signed off and\napproved by\na business owner\nand so exactly what that looks like i\ndon't know the nice thing is\nit this allows us to at least understand\nthe current state of a system\nand what is like not in sync and stuff\nlike that uh\nbut the best workflow i'm not sure\nokay it's awesome yeah it gives me a lot\nof\nthinking to do in the human in the loop\nbecause we have evolved our thinking in\nthat direction as well\ni don't think we will be implementing\neverything as you've said what we'll be\ndoing is giving the right set of tools\nso that\nfolks like you can actually just\nimplement the right\ninterface for their users because i\nthink that that interface changes\nbetween let's say the labeling in your\nuse case and some other use cases right\nthey are all kind of slightly different\neven though the mechanics are the same\nyeah one of the things we want to be\nable to do is like you know compare the\noutputs of two runs and so if something\ngets modified and re-run\nas part of just like the process it\nshould be fairly quick for\nsomeone to take a look at the difference\nand sign off on like any changes\num but again i think that might be\nuh external to flight as a system i'm\nnot sure maybe it's not\ni think the the human in the loop as i\nsaid was asked and i think it's\nit's pretty useful for certain uh\ninteresting use cases so\num any other questions anybody\nyou should look at how rippling is doing\nit which is interesting\nour payroll system basically has a human\nand loop sort of built into it and it\nessentially ties a task to an email\naddress and then a a set of\nuh email you set a nag frequency on how\noften emails go to the person and then\nyou know it's a pretty simple model but\nit kind of works\ni don't know it's all the interesting\nproblem i think about yeah but that's\nthe interface right the\nengine can handle this like just saying\nthat hey i'm waiting for a signal\nyou can send the signal as an email as a\nclick on the button in a ui somewhere\nyeah but it doesn't have the concept of\na person a timeout and a nag frequency\nso i mean it's just a it's like a named\npast type human\nyeah i mean one thing i can think of uh\nis\nmore granular um ownership\nof like components uh whether that be at\nlike a\ntask workflow level um i know\npermissioning is another one that i\nthink\nhey like we were\nsince we're not exposing flight directly\nto users i think we'll be able to get\naround it but like\nhaving better permission and controls\nbuilt into the framework would just be\num very nice i think\nawesome yep roll base access control\nthe hot topics uh good uh\nyeah i think we're getting into a lot of\nfuture discussion but if there are no\nother questions i wanted to\nopen up and like have a you know open\nchat\nwith lots of folks um i will take notes\nuh and then we would like to\npresent to study and like a probably a\nroadmap in the next video\nlet me do it based on this so\nopen up for either questions on this or\njust in general questions or\nfeatures that you think are really that\nwould help your business\nall the features exist awesome so we can\nstop building go home\nanyways uh let me talk about what we are\nworking on um\nwe've been i think uh\nsandra had a\n[Music]\nlike a survey uh with like a few\nprobing questions maybe that might uh\nget people thinking about\nyou know what what topics they might be\ninterested in i don't know if you want\nto post that here\ngive them a 30 seconds or a minute to\nuh just answer that\nsorry i have the link in the chat i can\ndrop that in uh would you like to do\nthat now kitten\nno please do go ahead that would be\nawesome\nthere's some other random stuff we've\nbeen thinking about\ni don't know what the the scope of this\nconversation is no it is open-ended we\nactually\nhave not done an open-ended conversation\nfor a while and we just\nwanted it to be like a place where\neverybody can just share whatever they\nwould like to do\nbasically one of the things yeah i think\nagain because this is\na distributed system uh that we're\nwrestling with how to deal with\num is when we have all these multiple\ncontributors contributing these models\nwith these like\ninterfaces between them uh thinking\nabout\nhow to define better encapsulation i\nguess\num so along\nalong like with the permissioning and\nstuff like that uh you can imagine\npublic and private models\num and then\nyou know uh schema compatibility somehow\nenforcing that i don't really know where\nthe compatibility checks\num enhancing the way that that functions\nwithin the system\ncould be very interesting as well\nespecially because and this is maybe\nunique to us because our users are data\nscientists\nuh i don't think they really think in\nthese ways and because it's we can't\nencode it in the system ourselves\nit's like a little nerve-wracking about\nhow this is gonna function\num you know if it's a bunch of engineers\ni think they're they're capable of\nenforcing these things themselves but\neven then it would be nice to get like\nsome free you know correctness checking\nlike at registration time or something\nlike that\nand so and this is schema specifically\nwith respect to schema\nright uh well there were\nlike two things right like\nyou could have many tasks in workflows\nbut only want some of them to be\nconsumable\nand some of them to be like really\nvisible uh\nand those are like those those are like\nthe interfaces that you define\num and then you could refactor at whim\nlike the other tasks or workflows that\nlike comprise your private\nyou know part of your graph um\nbut right now it kind of just all shows\nup as equal uh\ncomponents in the flight console right\nuh and then another thing that we've run\ninto is if you then refactor things\nyou know nothing ever gets deleted and\nso just the flight console\njust has every record of all your\nhistory and it's difficult to\nyou know go through and understand which\npieces of the system are kind of like\nthe high level\num\nlike interface i guess just to keep\nusing that word and which pieces are\njust kind of like\nthings that uh yeah\nare less important is it like uh\nif i have to rephrase your question or\nyeah let me rephrase it um\nso you're saying that essentially you\nwant a way to delineate\nwhat is exported\nby a project to as a consumable\ndo you think that ties into permissions\nin some way\nuh it could right because you know\nif there's there's like uh i should be\nable to run anything that's like private\nin my repo or my project or\nwhatever but others should not be if i\ndesignate\nyou know someone has the rights to run\nmy public things\nthat could potentially be like you know\npart of role-based access\num so i think they're they're\nintercepting\nintersecting but maybe not one in the\nsame yep\nit would be kind of cool if we could\ngoing down this idea of permissions have\nsome sort of\nyou know in the theme of declarative\ninfrastructure\nright for a given workflow declare\ndifferent levels of permissions you know\nuser\nversus editor versus owner and then for\neach\nrole we could assign like i am group\nowners i guess dylan that's what you\nmean by role based\npermissions right but if we could have\nthat as a sort of flight config\nthat we could check into a version\nrepository and that means that it's\nreally easy to modify those\nthat's yeah that makes sense yeah\n[Music]\nthis i think so uh some time ago we\nactually when\nbefore creating the idea of reference\ntasks\nwe thought about having an export like\nlike you do in\nin c or some other languages right where\nit gets extern\nright only those things are available\noutside\nwe thought about that but then we\nquickly realized that actually it needs\npermissions\nuh to really because exporting is one\nthing\nbut after that also you may not want to\nexport to everybody you might want to\nexport to some people only but i think\nmaybe\nas you're saying we can at least\nexport okay that's awesome\ni think you could do yeah without\npermissions right it's just like don't\nallow things that aren't marked exported\nto be fetched from the\nuh central yeah\nis just that it would be weird that you\ncan see everything but you can't really\nput it right so\nvisualization we can visualize it\nprobably like export it as a separate\nsection\nbut um today\nsince everything's open in the\npermissions model on the\ncontrol plane you can access all the\nentities\nright so just but i i like the idea of\nextern\nokay um\nbut that would be a breaking change so\nlet's say if we introduce external\nas a thing if you're using already stuff\nyou will have if you\ncheck in or if you pull in the new\ncontrol plane you'll need to\nyeah you need to do it the other way\naround which is just microsoft private\nyeah okay\ncool\nany did you guys fill in the form\nanybody\nsandra do you have like a count or some\nkind of response\ni think we lost sandra\ni think we did all right so\num from our point of view let me quickly\nexplain what we are\ndoing and then we would love to see\nwhere we can you know collaborate on it\nso there's an rfc in progress that we've\nbeen doing\nreviewing within union so i we work for\nunion like three or four of us\num and that rfc is about\nreactive workflows so how do you react\nto an\nuh overflow completing or file\ngetting dropped in gcs3\nor some other event happening um and so\nand the biggest\ncontribution over there is the ux right\nlike when i say ux\nthe way you program this such a complex\nthing that\nwhen things land how do you execute\nsomething so that's one thing that\nthat's an rfc\nin progress and we'll share the uh rfc\nthis week\num another piece that\nbeing the question that soren and\neverybody asked is essentially a\nprogrammatic\ninteraction with the flight admin api uh\nthere are pieces already and they are\nunder the name control underscore plane\nuh under flight kit if you open up like\nit\nyou'll see there's it's not documented\nso if something's undocumented it's\nhidden i think\nbut it's uh available underscore 5k\nflash control underscore plane it's not\ncomplete we are\nfiguring it out the name is going to\nchange from control plane to\nneeds uh remote because we think that's\nmore\nuser-friendly uh and then we can do a\nwalkthrough of all of the\nthinking over there and it's an rfc too\nbut it's also in progress\nthe third piece is the entire ui is\ngetting updated\nuh console so we have a new ux design\nfor flight console that we've been\niterating on it's not yet\ndone um once\nit is at a level that we can share we\nwould love to share and get feedback\nfurther\nbut it's not there yet but that's one of\nthe core things that we're working on\nanother thing i don't know if you guys\nnoticed over the last few days we've\nbeen\nupdating 5k package and\na few other commands um basically we are\ncoming up with a completely\ni don't want to say new flow but like a\na streamlined flow from\ngoing from spy flight or flight\nget python to uh rebort\nand nitrating on it and flight ctf\nis almost it's almost ready it helps us\nin\nin taking the flow much further ahead\nand really really improves and\nimproves portability that's a piece that\nthat should land this week\nand then we're working on uh overrides\njust general system override so for an\nexecution time you can overwrite certain\nattributes\num and we have another rfc for\ndocumentation\nwithin flight so for example when you\nwrite a workflow\nor a task you can document it better you\ncan say like\nthis is the description and you can have\nrich documentation that will get\nassociated with\nthe tasks in the workflow and the reason\nis because let's say\nin in dylan's case when you're sharing\nyou would want\nthat information to be much more\nunderstandable and you know uh like when\nyou share you\nyou want to read a document about that\nthing otherwise it's kind of useless\nright\nso that's why that's an rfc too and\nthere will be work\nup with that\nis there somewhere to track these rfcs\nuh we actually that's a great thing\nlet's actually have a discussion what\nwould you guys prefer we actually have\nbeen using google docs and realized\nthat's not really\nthat user friendly to share to share\nthis like i can share a link but\nor else we can we actually put them in\nthe meeting notes always but that's\nnot like i don't think everybody looks\nat them\nuh use i think like either asano boards\nor whatever trello just to like\ntrack the progress of like rfcs like you\nknow draft review whatever\nclosed not\nthe project's available big enough that\nwe might need working groups\nthat\ni don't think that could work\n[Music]\nlet's actually at least start creating a\nproposal and putting all the links to\nrfcs but they can still be in google\ndocs because we want comments and so on\nlike i think\nit's a little hard and cumbersome to do\nthat in github\nso hatham here has been using hackmd\nand he seems to like it i don't know if\nyou guys have tried it\nwhat do you think\nyeah uh yeah i did not realize i was\nunmuted\nyeah i have been using hackmd for our\ninternal\nspecs and design docs\ni i really like it because i\nat some point when you know google docs\nwhatever get complex\nit you lose track of how to do\nformatting anymore\nand you're not sure how the what is\nformatted is what i like you know\njust writing down what you want in md\nuh and then i'm tracking them it links\nto your to a github repo\nso everything is versioned um as git\ncommits\num in the repo i can always go back to\ndifferent\nyou know versions and can like format\nthings as sort of a\nbook uh so you can like browse the specs\nby topics and things like that\num which i i really like you know\nespecially\nas we are working on more and more\nuh specs as kitten was pointing out so\nwe can yeah we can give it a shot\nuh but it will we need people\nwho want to write md\nokay do you can you take the point to\nset it up for all of us and actually\nprobably maybe next meeting let's just\ndo a\nsmall roll out like just explain how to\nuse it\nsure we can probably copy a couple of\nspecs\nthat we're working on there and yeah\nall right\nokay i guess as people are dropping off\nit's time anyways we are over time but\nuh if there is nothing else any other\nquestions we have 10 more minutes first\nanswer any questions otherwise\none thing yes please so\nwe also have\ndata scientists who really are not\nso much into git and\nyou know reproducibility and things like\nthat uh\nyou know what dylan just said and so\nwe we are thinking about um\nhow to actually track\nfor for some cases to track the original\nsource or\nto go back to the original source so\nthis is something interesting\ninteresting to us so because right now\nof course the it's the the sources are\nuploaded and\nput into docker containers or into\nbuckets\nif you use fast register but\nsometimes it can be really interesting\nto to actually go back\nwhen when you have a workflow or some\ntask and\nor even if you have a reference right\nsome someone else\ncreated something you want to use and\nnow you want to go\nbasically back to the source and you\nwant to have the right version\nof that so i think this is something\nreally interesting to us and maybe\nothers also have\nthoughts about that do you have any\nideas on what to do\nwithout git so it's like you're saying\nwe need it outside of gate\nyeah i i don't know i mean\ni think one one thing uh\ni mean we we all if if it's uploaded\num and and you have it in in a bucket\nyou already kind of have a means at\nleast to to access that stuff\ni think the first step would be to\nyeah to do to just build or to build\nthat little to make that link\nexplicit right you have you already have\nthe link\nand if you say okay you use something\nlike\nlike fast register and you have\neverything in in a bucket\ni think this first step is actually not\nthat hard\nyou could even like include something in\nin console also and say okay here here's\nthe source and then you have a link and\nyou can see it somewhere something like\nthat\num yeah and then of course yes\ndocumentation hub part to answer that\nquestion the documentation\nstory that i just said yeah yeah that's\nwhere we are\nwe're going like you're like oh this is\ncompletely not visible\nbut yeah go ahead uh i think you should\ncontribute to that\nrfc that would be really helpful awesome\nawesome\nokay yeah i'll i'll definitely do that\nyeah what was the second part\nthat you were saying besides like the\nlike getting the first track\num\nyeah i think the other part was\npeople don't use it\nactually i'm not i'm not sure anymore\nbut anyways if if you have\nof course git would be the best thing\nbut even\ni think even if you have if you use git\num yeah people would that\nyou don't have this one-to-one\nrelationship people that\nuse different git repos maybe two to add\nworkflows and things like that and\nand i think it's still even if with git\nrepo\nuh it's it's super helpful to\nactually go back to the source repo\nright of course it's even easier if you\nknow if you have a\ngithub repo somewhere um\nyou already have the source code but you\nstill need that\nthat backlink essentially but i guess\nthis is also part of this\ncould also be part of this documentation\ni think\nyeah yeah and i think there's never\ngoing to be\nflight is never going to be the source\nof your or the\nhost of your source code that's that's\nuh\nnot the right expectation in my opinion\nyeah but yes\nit should be our role to link you to\nwhere did the source come from and which\ni think we will\nuh that we have part of the\ndocumentation story and if there is\nanother thing we would love to see if\nyou can bootstrap a project\nlike you know that's yeah you know\nthat's a separate that's an add-on on\ntop of like which would be perfect\nbut like does that make sense in the\ncore it doesn't make sense to actually\nof course yeah yeah\nsource control no no\nokay i think we are over time if there's\nnothing else\nokay awesome uh thank you and so look\nforward to\nseeing you in two weeks bye"
    },
    {
        "title": "Case Study: Building a Weather Forecasting App Using Flyte and Streamlit - Jun 15 Flyte OSS Sync Up",
        "transcript": "um i think next we have niels up\nand running to talk about doing a uh\nsdk only uh machine learning sample that\nhe's been working on\num yep thanks george hi everyone um\nmy name is niels i'm a newish\nperson in the flight and union ai team\num and today i'll talk to you a little\nbit about a little project that i've\nbeen working on\nbut first let me\nmake sure my screen sharing\nis working\nah there you go\nuh does everyone see some slide decks\nwith a title on it\nyou see them in you yes all right\nwell so i'll kind of briefly\ngo over this project and um i guess to\npreface this\nuh i think the thing i'm i'll be\nfocusing on\nthe presentation today is more like the\nflight system and like the\nmachine learning system more broadly and\ni won't focus too much on\nlike the model details because that's\nyou know to be honest a little bit\nprototypey and not\nprobably not the best model uh\nformulation or anything like that so\num yes i'll talk to you about building\nan online learning\nweather forecasting app with flight and\nstreaming\nso a little outline of this talk uh just\ngives a little bit of the goals and the\nbackground and motivation behind\nbuilding this weather forecasting app um\nthen i'll go into how it's made\ni'll show you a little bit of a demo and\nthen some\npotential future work that that uh i\nmight get into\nso first um i guess my background is\nmore in the data science and machine\nlearning side so i'm not\nsuper into um infrastructure stuff but\nyou know i've in previous jobs i've kind\nof had to dabble in it\nand um kind of get familiar with that\nstuff but\nuh yeah my bread and butter has really\nbeen getting data\nfrom non-traditional non-like benchmark\ndata data sources\ntraining models for business use cases\nso really the\nthe goal of this the overall project is\nis we've kind of\ndubbed as flight lab and so you can find\nthe projects of which there are only one\nright now\nbut it's you know supposed to be a\nrepository of non-trivial data\nengineering or machine learning\napplications\nthat showcase flight's unique\ncapabilities so\num what does that mean that's\nthese are like sort of rough guidelines\nof how i'm thinking about these\nprojects so in terms of data i can't\njust call\nyou know package.loaddata mnist or\ndownload uh like a pandas dataframe\num from an s3 file and you know it's all\nready to go\nready to model so that's those kinds of\nthings are good for you know tutorials\nand kind of um getting your hands\ninto flight and you know get a sense of\nhow it\nworks but for this specific\nsuite of projects i kind of wanted to\ngo a little bit deeper and see and kind\nof test the limits of\nflight as a platform in terms of like\ncertain project specific requirements\num so i'll get into that with respect to\nthe\nthe weather forecasting it um the second\none is\nsort of on the modeling side biasing\ntowards a typical training regime so\nlike example non-batch and non-tabular\nso batch is like you know everything\nyou use the model sees all the data\nthe entire data set and might even go\nthrough multiple ebox\nmultiple passes through that data set um\nanother one is like\nnon-tabular so you know text image\nthings that\ndon't come neatly in a nice table for\nyou to model already\nand the last one is in terms of use case\num it has to have at least some kind of\nplausible real world utility so i mean\ni guess you can think of all sorts of\nuse cases that are sort of like\nnot that useful maybe but you know\nso like really like the the main\nprompt here is to try to come up with\nprototypes that\nyou can kind of squint and see you know\nlike a real world application\nbut you know caveat none of what i'll\nshow you is\nready for production or anything so you\nyou still want to go to your weather app\nto get\nthe forecasts uh so for this particular\nproject the goal is to fetch data from a\nsource that involves non-trivial data\nprocessing so as i said earlier like\nit's not just download data set and then\nyou have your target column and then\nyour feature columns and all that stuff\nso\nhere we're we're using the noaa i forget\nwhat it stands for noaa.gov api for\nfor grabbing the weather data and that's\njust a generic api and\nyou know you can get weather data from\nthe past couple of years all around the\nworld\num with this one data set that i that i\nfound on there\nand it has no there's no special thing\nthat's for machine learning specifically\nso\ni have to kind of think about how to\nshape that data\nthe second part is to implement an\nonline learning system producing\nforecasts for specific geographic\nlocation so that's the second bit\nthe third one is to expose the forecasts\nin a user-friendly ui\nso someone who doesn't know about flight\ncan just go to\nsome kind of website and see what the\nforecasts look like\num so what's online learning i hinted at\nit before\nbut if you contrast it with offline\nlearning you have\naccess to the entire data set\npotentially multiple passwords through\nthe data\nwhereas with online learning\nit's a setting where the data becomes\navailable sequentially over time\nand somehow you require the model to be\nupdated\nuh because the data is kind of like\nquite dynamic\nand you can't rely on last year's data\nto make accurate predictions\nfor tomorrow\nso just further justifications right so\num\nagain this is like i don't want to just\ndo online learning for online learning\nsake\num to pick an actual application that\nwhere it's appropriate\num so if you think about weather\nforecasting right it's not static\nthat data is generated as a function of\ntime you can't\nyou can't assume iid data um\nindependently and identically\ndistributed data\nso you can't just shuffle the data\naround right so it's like\ntoday's temperature depends on\nyesterdays which depend on the previous\ndays\nand so there's some dependency there\nwhere you can't just\nassume uh independent\ndrug independently drawn data samples\nand then finally your model needs to be\nup to date as as as\nup-to-date as possible due to the\ndynamic nature\nof the data so a lot of time series\nstuff will apply to this so like\nweather stock prices things like that\npurchasing behavior\nand so the model prototype i'll be\nreferring to when you see the\nword model in a bunch of the next\nsubsequent slides\nis super kind of silly it's not it's\nlike the really toy problem at this\npoint but\nso the target that we're trying to\npredict is the mean daily temperature in\nthe future\num and the inputs are just the past\nweek's\nmean daily temperatures\nuh right so you can think obviously the\nway to make this model better is to\npredict more things like humidity and\nprecipitation but you know\nwe'll set that aside for now and um i'll\nshow you the rest of the system\nand really that's that's the point so\nhow it's made the overview very\npi level is we make a request to the\nnoaa.gov api\num there's a workflow here for getting\nthe training data\nthat produces some training batches um\njust for the past week worth of\ndaily temperatures um there's a little\nother stuff here but\nyou know if people have questions we can\ntalk about it\nthat training batch is is sent over to\nthe get\nlatest model workflow uh what that does\nis it updates\nthe most it gets the most up-to-date\nmodel and then updates\nit with the current data that we just\ngot for today\nand then it forecasts for in this case\njust a week\na week's worth of forecasts of mean\ndaily temperature\nand then some metrics um those forecasts\nare then\nread in by a streamlit app um streamlit\nis\nit's like making it's like jupiter\nnotebooks\nbut a python script that you can just\nrun\nand it's it renders like an app for you\nuh really nicely so i'll show you what\nthat looks like\nuh digging in a little bit to the data\nhow the data works and i yeah this\ni'll just talk through what this is\ndoing so in the get training data\nwe're kind of zooming into it and\nlooking at the subtasks but\nget training data itself is a dynamic\nworkflow\nand it iterates through\num this for loop of a you specify a\nstart date so you can give it some prior\nknowledge so\nif you start training the data model\ntoday\nyou basically give it you specify\nwith the start variable here um how much\nprior knowledge do you want to give it\nyou want to give it\nyou know a week's worth of training\nbefore today or you know three weeks or\nmonths\nso it iterates through here and this is\nan interesting thing that i\ntried and it's you know happily it works\nbut um the data that noah\nreturns is just a csv it's a giant csv\nfor a particular kind of date range\nand so you don't you can compute\nbeforehand\nwhether that csv file has all the data\nor whether it's going to be updated\ntomorrow with some new weather data\nand so here this branching path um\nif the data is complete ie there's going\nto be a new csv file\nand that this the current one i'm\nreferencing won't be updated you know\ntomorrow or the day after\num you can assume that that csv file is\ncomplete and\ni'm going to just cache all the data\nassociated with this particular training\ninstance\nif it's not complete i get a partial\ntraining instance in which case i don't\nwant to cache it so\ni'll fetch it again when it's updated\ntomorrow\nthat produces a list of training\ninstances just batch\nand that goes into the model so that's\nthat's a little like\nfun thing that took a little bit of\ndebugging to get to work but\ni'll show you later why i did this\nmodel is pretty simple this is also\ndynamic forgot to annotate this but\nit just looks through the batches and\nupdates the model\nfor each of those days uh pretty\nstraightforward this is also cached so\nyou could you inv you kind of like have\nlike cached\nand tracked versions of all your model\nupdates over time\nand for forecasting it's a similar thing\nthat i'm doing\num your i'm just iterating from now to\nlike however many days i want to\nforecast for\nand i do a similar thing where if i\nalready got that training instance from\nthe training run\nor previous training run then i don't\nhave to recompute\nand get all this data and in the case\nthat the data is not complete i just get\na partial instance\nthen i create a bunch of forecasts\nand for this prototype i made\nthree launch plans uh one in atlanta\nwhich is where i'm at\nuh seattle which is where a bunch of the\nteam is and then hyderabad\ni think i i might have missed a few team\nmembers but\nthis is just a fun thing to just\nlocalize all of us and\nget a sense of you know i didn't want to\ndo the entire world\nso just had to pick some heuristic\nand um now i'll just want to show you\na quick demo so this is a streamlight\napp that\nthere are some issues with the hyderabad\nlaunch plan\num so right now there's just a line in\nseattle\nbut as you can see this i haven't really\ntrained\ntuned this model very well it's just\nproducing kind of\nsimilar outputs for atlanta seattle\nat least regionally it makes sense so\nseattle i think the predictions are\nlower than than atlanta so\natlanta's hotter so like on you know on\naverage\nthe model is producing kind of sensical\nbehavior but still it's\num i haven't looked at the metrics yet\nbut\nyou know that said uh\nthis app is creating a flight client and\nit's just reading the latest executions\nfor each of these\nlaunch plans um so when the control\nplane\nuh stuff that i'm also working on is\ncompleted you know the\nthe code backing this app will be a lot\ncleaner it's kind of\ni don't want to show it because it's\nkind of a rickety house\num so that's it really so the takeaways\nthat i got uh building this\num i kind of started off with pure\npython code and then\ni wanted to see what it was like to port\nlike my python code base over to flight\nit's pretty straightforward i have to\nsay um\na few things that i learned is you can\nkind of make a lightweight feature store\nif\nif you can make a task that's cached\nfor one training instance that means in\nthis case\nthe input to that the task against\ntraining instance is just date\nright so if i give that task that same\ndate again\nit's gonna just\nall of that computation has already done\nalready been done i don't\nneed to ping um the noaa api anymore\num and the other two are pretty\num intuitive like you can just keep\ntrack of all your model updates and\nmodel metrics over time and actually the\nway i implemented this\nis it it it like reruns the entire\npipeline from the\nspecified start date to now which is\nlike not very efficient\nand i probably need to change that but\nit is fast because\ni've cached all my training instances\nand i've cached all the model updates\nfrom the beginning\nso it just kind of breezes through all\nof those tasks and then\njust the final day for today it'll get\nthe training data and\nupdate the model so that was like a nice\nlittle\nfeature that that i discovered where i\ndon't have to like retrain the model\neven though my code wasn't\nas efficient as it could be\nfuture work um you know again these\nare these are kind of prototypes to just\nshow what's capable what what\nflight is capable of but um if you know\nwe were to do more work on this\nyou know produce hourly forecasts um the\ndata is collected at the hourly level\ni'm just aggregating it for the purpose\nof this\nprototype um we can additionally\nforecast other targets like humidity and\nprecipitation\num which would you know provide more\ninformation to users\nimprove model accuracy you know adding\nmore features\ngetting a wider look back window\nand then enabling user supply geographic\nlocations which would be interesting but\ni'm a little nervous because you know if\nlike you could i you could like add a\nput a query in and then kick off an\nexecution and launch plan\non the back end but uh\nyeah maybe maybe not now but like i feel\nlike accounts\nuser accounts and stuff would be\nrequired for that\num yeah and that's that's it for the\ntalk uh i'm curious if you have any\nquestions\nit's pretty cool yes uh\nwere there any um uh\ndid you have i know you said it wasn't\nthat hard to migrate from your python\ncode to\nlike flight uh were there any things\nthat you had to\nlike think about differently or change\non the way\nto uh get them to work on flights\nyeah i think the biggest hurdle i had\nwas\nchanging my mindset about writing\nworkflows because workflows\nthe outputs of tasks are promises\nand so they you can't do anything with\nthem\nunless except for feeding them to other\ntasks which\nthen unpacks those values so that\ni was kind of thrown off a little bit by\nthat but once\nonce i started thinking of them in terms\nof like async and like whatever\num features things like that it became a\nlot more\num intuitive for me um but because\nworkflows\nlook kind of like python code you it's\njust a function so you're just writing\nstuff\num that's definitely that definitely got\nme\nthank you very much yeah so as you said\nall of this\nis open source uh so please go ahead\ncheck out the\nthe repo and if anyone is working on or\ncan\nshare any uh like showcase uh projects\nuh in a future demo or it can open\nsource it\nuh i think work with neil's to\neven put it in this project in this repo\nwould love to um yeah pretty cool stuff\nthanks nina yeah\ndo we still have time for another\nquestion\ncool uh so that from you swearing away\nokay so\nso one thing i've been thinking about\nbecause in\nflight i mean the cool thing is that\nflight\nkind of captures all the historic data\nso if you\nif you train um models over time of\ncourse you'll have all the\nhistoric artifacts basically for and the\nmetrics\nso have you been thinking about\nintegration with um\ntracking um model track\ntracking things like like tensorboard or\nor um i don't know ml flow or something\nlike that because i mean the data is\nthere right so\ni mean it would be cool to to actually\nget get all the historic data to have\nto compare models yeah for sure i mean\nyeah tensorboard\nlogs are just files and so you can just\nwrite those out\nand then read them in any you know\nwith any arbitrary thing like weights\nand bias is also\nwe've we've looked into um i guess that\nwould be something i would work on\nyeah that's i would be interested in\nintegrating with those\num yeah i feel like tensorboard would be\na very simple thing to start since it's\nwild when you just yeah yeah\nawesome\nare you ever going to do uh pandora\nintegration and sort of\nscrub your input data yeah\nthat's fine i mean it exists so pandara\nis integrated with\nthere's a flightcheck plugin for pandara\num so\nyeah it i guess it didn't really work\nwell for this i mean i do use pandas\nand pandara kind of in the back um\nto just validate the data coming in\num so that's\nyeah that that was a thing for this\nproject\nthanks fields"
    },
    {
        "title": "Using Opta to Deploy a Production-Grade Flyte in Minutes - Jun 15th Flyte OSS Sync Up",
        "transcript": "please take it away guys cool\nthanks for the intro hey them um hey\neveryone i'm ankur\ni'm uh the ceo of this company runex\nand jd is also here who's who also works\nwith us and he's the lead engineer\num so yeah let me just\ni have a quick few slides and then i'll\nshow you guys\na demo let me just share that\num cool so yeah today i just want to\ntalk a bit about this this open source\nproject\nthat we have called opta um it's\nopta is is basically an infrastructure\nis code solution\nsimilar to terraform um but it works at\na much higher level abstraction\nso you can with a very clean interface\nyou can specify\nyour containers your databases any other\nresources you need\nand then it can provision that on your\ncloud account\num it's pretty cloud agnostic and we\nsupport aws and gcp currently and we are\nworking on azure support\num for flight we have been working with\nthe flight team over the last few weeks\nand trying to build that flight opt\nintegration so that's what\ni would be showing you today um\nand as hitham said the goal here is to\nmake it super seamless\nfor people to deploy flight in their aws\naccounts um also i believe opti is now\nthe recommended way to deploy\nflight we have merged the the relevant\nprs in the\nthe flight code base and the flight\ndocks so we are\nvery excited about this development um\ncool before\nyeah before i move forward i just wanna\nshow you guys\nthe the flight architecture that we\nwould like to create today so this\ndiagram i've taken from the flight\ndocs um and as you can see\nit's it's basically running a kubernetes\ncluster with a bunch of flight\npods there and the pods connect\nto various other aws resources like we\nare using aurora for\npostgres we are using sqs i believe we\nalso use\nsns and then we have s3 to store\nsome stuff there and then there is\nan ingress at the front of all of this\nwhere all the\nincoming requests come through so so\nthis is our end goal that we would like\nto\ncreate today and let me just\nswitch to my terminal and i can show you\nfolks\nhow you would achieve this with\nuh opta i do that any any questions\nabout the setup the overview\nyou guys get it we're after fits\ncool awesome um\nokay so the way opta works you write\nconfiguration files similar to terraform\nconfiguration files\nand then you run our cli tool called\nopta\nand uh so for for the case of flight\nwhat we have done is we first create an\nenvironment\nso i'll just show you that\nand what the environment does is it\ntells us which cloud which region you\nare running in\nand it configures a few common settings\nright so in this case we have defined a\ndomain name for the environment\nwe have set up a kubernetes cluster\nwhere we can we have a few configuration\noptions like max node min nodes\num and and all this what this is going\nto do is it's going to set up\na vpc subnets all that stuff for you it\nwill set up an ingress\nkubernetes service mesh um and in our\ndocuments\nin our documentation you can find more\ndetails about how each of these pieces\nare connected together\nbut we see an environment as sort of the\nbase uh\nthat that every application needs and\nthen you can start deploying\nyour applications on this so i created\nthis environment file\ni ran opt apply and that would connect\nto aws set everything up\nand by the way opt is taking aws\ncredentials from my terminal\nso that's this is the particular account\ni have configured here so that's what\nopt is using\num yeah so so you do this\nand uh yeah so i i started with a blank\naws account created this environment\nran up to apply that has created a\nkubernetes cluster and everything else\nfor me\nnow i need to create another opt at yml\nwhich is going to be specific to flight\nand which will show you how flight\nshould be configured in this environment\nso i'll just open that here\num so i'll just walk you through this\nbecause there's a there's a few\ndifferent fields here\num first of all i'm referring to the\nenvironment that was created in the\nfirst step\nuh you can see that this could be used\nto\nif you want multiple deployments of\nflight you can have them\nin the same aws account you can have\nthem in different aws accounts\neventually you'll be able to do them in\ndifferent cloud providers currently i\nbelieve flight is only\noptimized for aws but but i'm guessing\nthat's\ndown the road map for for them um\nso you you pick your environment here\nthen\nyou start configuring all the resources\nthat your flight installation needs\nso we have a postgres database here\nwhich will be mapped to aws rds\nso that's another good thing about opta\nthat we don't ask you to run databases\nand everything\ninside kubernetes which are not the most\nefficient\nway to run these resources we fall back\nto the\ncloud resources so postgres here would\nbe mapped to aws rds\nthere is an s3 bucket here we are\ncreating sqs and sns\nbuckets and queues and and you can also\nnotice that in this\nin this configuration file using\ninterpolation you can connect all these\nresources together\nright if you're trying to do this\nmanually with terraform\nthat's the sort of complicated part\nwhere you have to make sure\nthe right credentials the right sort of\narns and names follow through all these\ndifferent resources\nso here you just interpolate them and\neverything works as you would expect\nwe are creating a few im roles that up\nthat flight needs to operate\nand yeah and then lastly we are\ninstantiating the flight helm chart here\nand so what's happening here is again\nthe same interpolation that we were\nusing before you can use that\nto pass the credentials for all these\ndatabases and s3 buckets that we were\ncreating into the health chart\num another thing to note here is that\nyou can use the domain that we created\nfor the environment\nso if you guys remember we had that set\nup as demo.nx.dev\nso we are telling flight that we want to\nrun flight at flight.demo.next.dev\num a bunch more interpolation to connect\nall those things together and these are\njust common configurations from the\nflight\nhelm chart and\nyeah so so that's basically it you we\ncreated all the aws resources that\nflight needs we instantiated the flight\nhelm chart\nwe used our interpolation to connect all\nof these things together\nand we have pointed all of this at the\nkubernetes cluster and the environment\nthat was created in the previous step\nso once all this is done you just run\nopt\napply here um i've run that before so\nit's going to be faster this time but i\nbelieve it takes\ntakes a few minutes the longest time it\ntakes is for creating your postgres\ndatabases because rds takes like 5-10\nminutes to set up\num but otherwise this stuff is is like\nfive-ish minutes\ncreating the environment which requires\ncreating the kubernetes clusters is\nagain 10-15 minutes because that's aws\njust takes that much time\num and you can see here that we are\nusing\nterraform under the hood so whatever\nyaml\nfile you're configuring and how you are\nconnecting all these resources together\nwe convert everything to terraform\nand then we use terraform to do the\nactual application\nso this allows us to sort of depend on\nterraform for\nmaking sure everything is robust for\nmaking sure we can do\nproper incremental updates when things\nchange um\nand it's showing me here that there's no\nchange to do because i've already\ndone this before um\nand yeah then once once this is done it\ntells me a bunch of stuff here that we\nwere\nwe we have created and just to remind\nyou folks again\num we we set up\nyeah we use the domain name flight dot\nparent domain\nand our parent domain was\nuh demo.next.dev\nso i'll just open let me just switch to\nmy browser\num\nwhere is my tab\nyeah so\nhere i can just go to\nflight.demo.next.dev\nslash console which is where the admin\nui\nis is present and you can see we have\nflight\nrunning here right and this has grpc\nsupport so you can use the flight\ncli and connect to it and create and run\nyour workflows\num it's using all those resources and\nall that architecture that\nthat we were that we created in in the\nyaml file\num so yeah that that's all that's that's\nbasically a quick overview of how you\ncan use\nopta to deploy flight um\nand just a tiny plug for our software\nthis is all\nopen source so if any of you folks feel\nlike you might\nhave other applications for this feel\nfree to\ncheck us out on github let me just share\nthat link\nyeah feel free to check us out on github\nhop into our slack\nor just uh message me or jd and we would\nlove to work\nwith uh you folks as well um\nthat's all so any any questions any more\nclarifications any other part i can dig\ninto happy to do that now\nhey i'm good this is santosh um can you\nbasically compare and contrast a little\nbit of the\ncomplications that one has to handle\nwhen using terraform\nand then how your kind of framework\nsolves it\ni'm very new to this so i don't\nunderstand enough either very well so\nbecause no prior knowledge yeah for sure\nyeah no that's that's a really good\nquestion\num and so there's a lot of ways to\napproach this\nthe the one way i describe terraform\nversus opta\nis that terraform is like the c language\nof infrastructure\nright when you try to do all this in\nterraform you have to configure\nit sorry no no i just saying yes yeah go\nahead\nyeah so when you configure all this with\nterraform you'll have to\nconfigure every single thing manually\nright you'll have to say this is my\nnetwork these are my subnets these are\nmy ip addresses\nthese are the particular security groups\ni want to be opened\num and what what we see opta as is like\nthe python language of infrastructure\nwhere we have designed a bunch of these\nthings\nbased on our multiple years of\nexperience working infrastructure we\nhave used all the best practices there\nwe have figured out how to connect them\nin the right ways\nand you get those you get those\ncomponents out of the box\num so so that's how i think these differ\nyou can obviously do all this in\nterraform\nand that's what we are doing under the\nhood as well\nbut it's just gonna be a lot of time to\nwrite the particular\ntemplates and configuring them the right\nway and secondly you just need to be\nyou just need to have a bit more\nexpertise if you're using terraform\nright you need to know about the the\nsort of details of how\nall these aws resources work and you'll\nhave to make sure that\nyou are connecting them in the right way\nto make sure it's secure\nit's scalable you're doing multi-az all\nof that stuff\num and so our goal with opti is that we\ntake away that complexity from users\nso people who are not like devops or\ninfra experts can\nsort of work with these systems\nokay so in short if i say that i mean\nyou reduce the complexity introduce the\nbest practices\nuh and also make it much more efficient\nin terms of the number of lines or\nconfigurations one will have to set\nthose would be the\nmajor advantages exactly yeah\nokay sounds good thanks\none just one clarification minor\num we have uh companies here i think\nsome are on the call that are running\nflight in gcp\nuh so we know uh like it works well\nthere\nuh i think uh part of the\none of the next steps uh in in this\npartnership\nis getting the right uh configuration or\nthe right\nenvironment yaml for opta to also be\nable to do the same thing for on gcp\num and hopefully it will look very\nsimilar to\nwhat you see in aws except that it will\nnow provision the right\ngcp resources um and once you look at\nthis like bigger picture\nit will look a lot neater and a lot a\nlot cleaner and more maintainable from\nour side to just pick and choose the\nright modules\nto use in the different providers um\nand then make it all work\nand i think uh so encore also mentioned\nthis is all\nmerged in the flight repo and the docks\nare updated so you can follow\nalong the manifests he showed are also\nall checked in\nso you can like use this today uh we\nhave been using it for\nthe past few weeks um uh in that we did\na lot of uh\ndeployments on our own using these from\nscratch and\nupdates and all of that so we know it's\nquite robust and the team is amazingly\nresponsive uh if you find any issues or\nany\nuh any problems with the deployments so\ngo ahead and check it out thank you guys\nyeah thanks so much chaitam and we have\nalso loved collaborating with you folks\nand we look forward to continue doing\nthis\nthanks everyone"
    },
    {
        "title": "Elyra Pipelines & Integration with Flyte - Jun 1st Flyte OSS Sync Up",
        "transcript": "and the second one is a guest\npresentation uh\nluciano a quick background on that\nluciano\nworks at ibm he not sure if he needs but\nit works on the lyra project and in the\npast we've had a chat about\nuh integration of a fight and uh ilyra\nso i'll let luciano you know explain\nhow lyra works and and then we can see\nas a community\nuh if there is other volunteers who can\nhelp try that project\nhey guys how are you uh let me\nshare my screen\nuh can you guys see my screen\nyep uh are you guys seeing\nthe kit\nkit page with the lyra or you guys just\ntrying to see if i'm sharing the right\nuh yup the right desktop\nokay cool so uh\nthanks guys uh for the invitation it's\nreally a pleasure to to be here talking\na little bit about lyra\nuh i also have a few folks from the team\nuh joining\nuh it's good to to to get involved on\nthe community\num so i don't have anything\nlike very like structure prepared but i\ni would just go over\na little bit of what lra is i'll focus\non the pipelines\nwhich i think is an area we are talking\nabout the integration\nand uh feel free to interrupt and ask\nquestions anytime\nuh so we are uh located at github.com\nira\nwe also have a couple of other uh\nripples there\nthat uh kind of like around lyra but uh\nall in that same organization uh\nif you click on the documentation uh i\nkind of like\non the overview you can see a little bit\nof like uh what\nuh kind of features that lyra provides\nuh one of the most popular uh feature is\nactually the pipeline editor but\nwe also bring couple of other things to\nthe the\njupiter lab uh development environment\nwe have uh added better support for\npython\nin our uh so you have a native python\nr script script editor you can do a lot\nof\nlike things that you can do\nin a regular editor like refactoring and\nand autocomplete and things like that\nuh leveraging the uh lsp\nuh integration that we we added in in\nthe latest alarm2.0 release\nbut we also have the ability to execute\nthose\nuh directly from the jupter lab\nenvironment\nyou can execute locally you can execute\nremotely\nor you can run that as a uh kind of like\na single node pipeline\nuh we also already add code snippets to\nthe play\ncode snippets give you a bunch of like\nthe ability to store\nreusable codes and you can later on\nquickly search and start inserting that\nwhenever you're doing your code\nhybrid runtime support gives you the\nability to execute things locally or\nuh externally uh we leverage another\nproject that we created at the jupiter\necosystem culture enterprise gateway\nwith support remote uh kernels remote\nexecutions\nso that gives you very seamlessly\nintegration let's say with cloud\nenvironments and others\ni've talked about uh python and our uh\nthe for version control we have\nintegration with git\nuh also we have been contributing very\nheavily to the table of contents\nuh to make it very usable for uh\nlarge notebooks but not only for that we\nalso because of the\nuh script editors we also integrated\nwith the script editor so if you have a\nlarge python file you can start\nnavigating from there\nand i'm talking about the dlsp\nuh so this is kind of like a high level\nof things that we bring\nto jupiter lab on top of like all of the\nnice things that jupiter lab\nworks so\nwhen you open jupiter lab uh what you\nwill see\nis sort of like the regular uh\njupiter lab interface and you will see a\nsection for lyra here\nwhich starts bringing uh all our\nuh addition\nour extensions uh we have the pipelines\npython editor\nour editor and then we also on the left\nside here you have things like the the\ncode snippet\nuh runtime configuration and things like\nthat\nuh when we talk about pipeline we\nhave the notion of like a generic\npipeline a generic pipeline basically\nare the set of components that can run\nin any runtimes\nand uh we are working for the next\nrelease to\nalso enable runtime\nspecific components that means if\nkubeflow has\na set of components that they they\nsupport\nnatively we'll start exposing those as\nwell\nin prior to that\nwe basically were supporting notebooks\nand\nscripts now we're going to have things\nlike uh\nserve a model train a model uh\nmanipulate some data all those kind of\nthat are natively exposed by the\nruntimes\nsimilarly uh with the apache airflow\nruntime as well and uh by the way those\ntwo\nare the two uh runtimes that we\nsupport today\nuh if we look into a\nuh a\npipeline basically\nyou you have sort of like a multiple\nnodes\nyou can see the node properties\nso you have a associated file in this\ncase\nuh during the runtime this will run in a\na docker image you can\nselect specific images all of that kind\nof like\nall integrated here you can associate\nresources\nuh such as uh cpu gpu\nuh ram uh as you're doing uh\ndevelopment locally and you will run\nthat in a\nremote runtime you can associate file\ndependencies\nuh you can put specific files or you can\ndo like star.csv\nyou can include subdirectories you can\nalso associate\nenvironment variables if you uh for some\nreason customize the run\nuh with different values uh and also\ncollect output files this is\nuh available when you go from different\nones\nand uh adding new ones you have a\npalette that we are expect\ni'm gonna expand here but is as simple\nas just\ndragging and drop something here uh then\nyou can go to the node properties\nand start feeling for example some of\nthe\nthe required properties there\nand that's mostly what someone that is\nbuilding a pipeline needs to take care\nof\nyou can see here like when you hover you\ncan see some of the\nattributes double clicking goes to\nactually\nuh the artifact and if you look into\nthe specific notebook it's a regular\nnotebook\nit's nothing uh basically our idea here\nis the data scientists\nfocus on on building or working on the\nproblem that they have in hand\nuh and not necessarily\nhaving to understand requirements of a\nruntime\nor understand how to package or anything\nlike that\nthat is going to be all abstracted by uh\nelira\nand similarly if you look into a python\nscript this is\nthis very simple python script is just\ndo whatever it needs to do\nsave into a csv and we do the rest\nsimilar thinking to our basically\nno run time or no changes\nrequired to run here and then when you\nrun\nso we have the the run button here or\nour first\nwe we have these runtimes this gives you\nsort of like uh uh the configuration to\nactually\nuh the runtimes that we pre-configured\nthat is when you click here you have a\nlocal runtime that executes\nuh local this is more like a mode or\nif you select the apache airflow for\nexample\nyou will see then what is has been\npreviously\npreviously configured uh similar to flow\npipelines\nand and when you click ok uh we will do\nall the packaging we will\nsort of like if you have dependencies\nwe'll put it to object storage\nand get the execution going there\nlet me give you a quick like adjust a\nlocal\nuh execution local just gonna execute\nuh in my environment and and here um\nif i make it a little bit bigger you can\nsee that\nthings are getting executed and\nright after it's completed uh we'll get\nthe notification here that\nuh excuse me is is done\ncompleted and then all the files and the\noutputs and stuff will be\nupdated here uh\nso that gives you a very\nquick high-level functionality in terms\nof like\nwhat it's doing uh things that are\ncoming so uh\nthis is a a vs code\nso we are also sort of like uh working\non integration with vs code\nthe the idea is about the same um\nwe are looking into places where you are\nbuilding your models you are working\nwith scripts and stuff and needs to be\nintegrated into a pipeline\nand and and looking to enabling those\nto be integrated via elira so in this\ncase i have the same\npipeline that i have there\nyou have the the same functionality you\ncan\nget your node properties and stuff like\nthat\nand uh we will then also uh enable you\nto\nto execute the pipelines here or submit\nto\nan external runtime uh this\nuh kind of like share the same uh editor\nso we\nexternalize the editor so that is is\nusable in in multiple\nuh locations and uh\nwith the next release that we are\nwrapping up\nuh the palette here we'll now have much\nmore\ncomponents so it's going to be even a\nricher uh development\nenvironment for for building training\ndeploying uh\nmodels in a kind of like environment\nthat\ndata scientists are used to work with\nuh if you guys have questions otherwise\nyou can\ntalk about a little bit of like the\ninternals of like how the adding a new\nruntime\nsort of like works or requires\num hey uh thank you for that i\ni uh luciano so would love definitely\nwould love\nto understand how to integrate but i had\na couple questions\num so one is uh you have the editor\nportion\nuh but can you open up one of the\njupiter scripts that you had\nsure yeah maybe not r\nmy r is a little weak um\nyeah so uh this will be run as an entire\nscript any or maybe even if it's a\njupiter notebook has to be completely\nstart to end runnable is that\nis that an expectation\nyes either node\nis run uh to completion\nso if you have a python script will\nbootstrap\na python environment and run there\nsimilarly if you have a notebook we\nexecute for example using papermill so\nit's the same\nuh notebook environment that you would\nget\nfrom inside a a notebook uh or jupyter\nlab\ninstance so it is a native environment\nthat that that runs that and and we\nenable customizations for example\nif you need any specific thing you can\nyou can create a\ndocker image and then use that for for\nrunning that that specific\nyeah interesting that you guys are using\npaperwork we have an integration with\npapermill\nuh within flight um so i think that\nmight make it easier to integrate\nbut uh how do you so i also saw the\ninputs and outputs that is specified\nthey can only be files\nis that right and can they be of any\ntype or they have to be csv\nuh they can be any type of files\nbasically those are\nso the idea is you're doing the\ndevelopment of your script\nlocally so you might have uh\nfor example a csv that you're reading\nor or some other auxiliary file or in\nthe case of notebooks you might have\nimports from other python files and we\nneed to make those\navailable in the external runtimes\nand those are kind of like the local\ndependencies and then\nif you are generating files generating\nthings\nthat needs to be propagated to different\nnodes\nyou can do that now if you have\ndata or a database or something being\nwritten somewhere\nthen you don't necessarily need to\npass that i think then it's more like a\nyou guys sharing that that location or\nsomething like that and then it can\ngo there\nthere is a sort of like\na data aspect uh\nbut currently we don't have anything\nspecific to\npassing data from one node to another\nparticularly because we consider like\na lot of these cases are going to be\nlike big data or\nlarge things and we are not sure\nthe best way there but it's something\nthat we\nhave on the back of our minds and have\ntalked a little bit but but i haven't\ndone\nanything yet i think yeah so in that\nactually flight does the data passing\nand all of that for you right\nright i i think yeah we would love to\nso how do we extend what what is uh\nwhat are the minimal set of things that\nyou need to do\nand what are things are customizable i\nthink\nso for example the components is very\ninteresting\nat least to fly it and and the other\nthings that are interesting\nis we we would love to enter\nlike actually get the inputs and outputs\nlike control the inputs and output\nbecause that's where flight really\nshines so is that possible to customize\nit so that we can\npass custom inputs and outputs and we\ncan provide the files\ncorrectly in the right place so that the\nlra can take over\nyes so uh\nso let me go back here\nso uh the list of components so so let\nme step back\nuh so we sort of have a\na processor which uh encapsulates the\nruntime\nsort of like interface and and that's\nthe main\narea that someone would come\nand extend uh\nwe have the two examples uh well\nactually three examples we have local uh\nthe kfp and air flow and and and what\nyou need to do there\nuh currently uh basically two things\nand that is being expanded for the next\nrelease so the two things that we have\nthere is\nis process and\nexport so those basically allows you to\nprocess a given pipeline\nand then export a given pipeline and\nwhen i say export\nis if you have the ability to generate\nsome\nspecific specific code that\nis this to the runtime so in the case\nfor example kfp is using kfb dsl for\nexample\nand things like that we are adding two\nthings\nuh which is mostly related to the\npalette\nuh so uh we sort of like a pushing down\nto the server the ability to\ngive a list of components and give the\nproperties for each component so in that\ncase then\nthe runtime can tell all these are\nsort of like the additional components\nthat i'm i'm supporting\nand then when you click on node\nproperties for a given node\nuh that same uh functionality\nwill give the specific properties so\nyou can you can have a customized\npalette or properties for each of the\nnodes\nso into that case yeah yes you will be\nable to\nlet's say have different ways of of like\nor like handling data or things that are\nspecific to uh your set of components\nuh and i think the requirement there is\nwhen process is called then\nthe runtimes know exactly what to do\nwith those\nuh so that it executes properly\nand then on the execution uh\nside basically inside the runtime image\nwhen things are getting executed\nuh what we have today for both uh\ncube flow and airflow is is a driver\nscript that basically\nuh bootstrapped the execution of ae\ni'm assuming we will have something\nsimilar to\nflight but then these are things that\nare are much more specific to\na runtime and might defer as well\nthat that gives you a a a very high\nlevel of\nuh things oh one thing that i forgot we\nare also\nsort of like uh\nfor uh the specific components\nthe the actual uh processor the\nthe runtime implementation can provide\nuh a parser\nso that he knows how to parse the\ncomponent files like kfb uses yaml uh uh\nairflow\nuses a a json i don't know how you guys\nare doing in flight but then you\nbasically provide a parser that gives\nyou a sort of like a value object of\nthat component that then\nthe runtime again can pass out\nawesome yeah uh\nwe would uh i think there are there's\ndefinitely some possibilities of\nuh indication here and it might look a\nlittle different from the other two\nbecause it might be richer\nuh because we do have more data than\nthe other two integrations but that'll\nbe we can\nif you guys are open to we would love to\nexplore that\nthe the richer the better and and and we\nare really\nopen uh i think we are really eager to\nto\nto have additional run times uh\nuh we'll definitely collaborate\nand and and help uh as as\nas we can and it will be also good for\nus to to start\nuh uh getting a little bit of like uh uh\ninsights on how uh flight works and and\nand and same you guys on on uh lyra and\nyour turn app in general\nso i think it's a good area for us to\ncollaborate\nyeah yeah you like if you guys have a\ncommunity think i think\nwhat somebody from our community would\nlove to join and talk about flight and\nthen i think uh\nwe will come up with an idea of how uh\nwe think we can integrate\nuh at least what we would like to\nintegrate with and then you know we can\ngo from there\nif that's okay well definitely and i\nthink we already even have\ni think we already have a\nan issue for flight integration\n[Music]\noh yeah this was i i actually made this\nissue a long time ago\nyeah so uh i think we discussed a couple\nof things there\nuh we can uh kind of like\nget get from where we we we left\nor uh and then in terms of community\nuh we we we talk on gear uh we also have\nthis\nweekly meetings uh on thursdays\nuh nine uh nine a.m uh which\nuh basically it's at the bottom of the\nreadme here\nuh how to join that and\nand other than that i think just hop\non on on that issue or some other issue\nand we can start working together uh if\nwe need\nsome ad hoc discussions or something\nto get things started easier\nuh just let us know\nuh we will help in any way that we can\nsure and i think in terms of benefits\nfor your community i think uh\nuh being able to attract more data\nscientists where they can\nactually visually start building their\npipelines\nuh not have to to\nto care about kind of like\nsome of the devops tasks uh\nrelated to a runtime uh might give you a\nlot more\nuh\nbetter user experience in terms of like\nbuilding the pipelines and and\nintegrating with the runtime uh you also\nkind of start getting kind of like\nthe different editors that we start uh\nuh supporting\nuh so in this case jupter lab soon\nuh the vs code as well and then\nso that that i think it's going to be\nanother interesting\nuh uh user experience and value for the\nusers that are building the pipelines\nany questions from anybody\ndon't be shy guys we have the whole team\nhere if we need specific\ni've got a question um this is kevin i'm\nwith the lyra team\num catan you had mentioned um that one\nof the things that\nflight shines about is the inputs and\noutputs\nhandling and that you'd like to control\nthat and um\nyou know we're in the process of\ndesigning this third-party integration\nfacility and i'd like to better\nunderstand\num how how flight\nwould like to control that and and and\nand are your pieces termed as components\nlike\nlike cube flows are or what what's the\nterminology that you would use for\nsomething\nlike that yeah the so\nwe called the dag a workflow because\nit's more than a dag\noften times each\nnode in the graph is a node\nin a workflow as a node but each\nexecution unit is called a task\num and the task has multiple task types\nwhich can be you know the similarity to\ncomponents it's just that\nthe task types are richer because they\nyou don't really think about\na yaml or something but you can actually\nyou know write\npython code or java code or anything and\nthat gets converted to the task\nuh runtime and the way we do the\nspecification is in protobuf\nbut of course you can do it in yaml and\njson converted to protobuf\num so that's the thing what what\nessentially starts happening is you\nnever really specify the tag\nas a set of dependencies between nodes\nyou specify\na producer consumer relationship you you\nsay that in the\nin the in the graph that i can see in\nin the presented screen by luciano for\nexample if factorial.r would produce a\nnumber\nand the generated statistics would\nconsume that number\nthat would automatically create that\nlink between those two nodes\nnow in the ui you could you know you may\nwant to create that link\nby drawing the bits but what you're\nconnecting is\nessentially you're connecting the output\nof a node to the input\nof a node does that\ndoes that yeah that makes sense\nand that that becomes the flow of the\ndata essentially you're modeling the\nflow of the data not\nmodeling uh the\nthe order of task executions\nokay thank you yeah yeah\nand then at run time we do have uh\nvarious uh\nhooks for uh like things like paper mill\nto be started like you can start a hyper\nipi kernel\nyou could actually run a java process or\nyou could run a distributed spark\nprocess and those are just hooks within\nthe\nwithin a container so and we do have two\nruntimes one is in java and one is in\npython but we also have the engine\nthat's actually passing the data\ncorrectly\nto all of them yeah i think from from\nfrom what you're\ntalking uh\nit's more like a a data sort of like\npipeline\ni think uh that will be very interesting\ni think uh uh what we have in our\nroadmap for\nuh after finishing the edition\nthe the runtime specific component is\nactually to work on data stuff\nuh we want to do a lot more support for\ndoing data\nintegration directly with like a sql\neditor\nwith reading\na lot more around that area i think\nit will get even more\nuh synergy with with flight so uh\nvery interesting very good timing as\nwell\nyeah like i think exactly what you said\nso you could write a sequel as\ninstead of the r dot factorial for\nexample you could write a sequel in\nflight today\nand the sequel is run let's say on\nsnowflake red shift whatever\nand the data from there is passed to a\npython script\nand so that's what is supported today\nand all of that is available through\na grpc rest api and so\nyou you can create a full layer on top\nof flight essentially\nthat allows you to do those things yeah\ni think\nflight is probably at the same uh\nlevel as we were seeing with others i\nthink\nthe functionality the runtime and a lot\nof like uh very\nexciting functionality or features\nare there but using that requires a lot\nof like devops skills\nand and i think liar one of the goals\nhere\nis to to to simplify starts bringing\ntogether these\nuh capabilities into to an editor an ide\nthat enables the data scientists to\nto focus on the problem and forget about\nthose\nand we do the magic for integration so\num\ni think it's gonna be very interesting\nif we can uh get that going\nabsolutely cool uh and looking forward\nto it any other questions from anybody\nbecause we're gonna run out of time\nyeah feel free feel free to ask\nquestions guys i i think this is\nsomething that will be\nhelpful but if it's i mean if you keep\nif you\nhave doubts or or anything i mean either\nnow or later on feel free to\nto come and ask questions for us\nso i can i have a small just a quick\nquestion um\nback to the the outputs thing so you\nmentioned that\nthe the outputs and stuff inputs um or\nyou asked if they were limited to just\nfiles um i'm wondering it you're able to\ni'm assuming that means\nflight's able to pass like essentially\nparameters directly to the next node not\njust files\num is that to support um\ni guess um anything specific because\ni've\nwe've been trying to add um or we we're\nplanning on adding like\nhyper parameterizing support um for q\nflow and stuff um\nlater on in in the lyra so i'm just\nwondering is that something you guys\nare doing um when you when you're\npassing parameters like that or is it\nfor some other scenario\nno that that's exactly like it could be\nit's not just for ml\nyou know your pipelines you may have\ndata besides\na large file which is meta information\nlike hyper parameters or\nyou know uh like a learning rate for\nexample\nuh in many cases right and we want to\nmodel that correctly\nand sometimes that itself could be a\noutput of a previous step\num and so that is passed in directly\nbetween\nthe flows so at the other\ntask level though it looks like as if\nit's a file output because\nflight doesn't really deal with large\namounts of data\nwithin the engine the engine is just\nlike from\neven mac said we are basically shuffling\npointers into data sets uh and the data\nset is actually handled\ndown in the larger like either in\none of the big cloud data stores\nlike gcs s3 etc\nor it could be a big data warehouse\nright\nand what we are doing is we are\nshuffling pointers between the various\nsteps\nuh so the overhead of passing data\nbetween steps is minimized\ngotcha yeah and so like integer\nor a file is the same thing right an\ninteger is a\nis one number a file is a set of string\nor a string\nessentially with some meta information\nwith it\nokay thanks\ncool\nso based on what we discussed\ni know your information about the inputs\nand outputs but do you see other areas\nthat might need\nto be tweaked that you uh foresee or\nnot necessarily um\nit's too early for me to comment i would\nsay okay but\nuh yeah if if i could comment right now\nit would be like i would rather do the\nimplementation but\nuh i would think that as long as you\nguys are\nopen to like changing a couple things\nand as we can work together to evolve\nthe system uh which i completely\nunderstand right you know you have other\nthings as well i mean\nwe definitely don't want to break all of\nthat but if you could you know\nmodify a couple things you could\nprobably build a much more powerful\num workflow system\nwithin the within the lrs ui so\nlike model the data flow itself and and\nthat\nbecause we have a compiler so let's say\nfor example our factory generates an\ninteger while generate\nstatistics takes a string flight\ncompiler will fail before runtime that\nwill just say like no that doesn't\nthat doesn't work correctly because the\ntypes are incorrect\nuh so we could actually start modeling\nand showing errors like that\nahead of time even before running okay\nyeah i think we we have been talking\nabout\nadding much more plugability uh uh and\ncustomization things one thing that we\ndiscussed\nand we but we haven't implemented india\nis the ability for the runtime to\nto sort of like uh\nspecify or give hints of the different\nfunctionality that it provides so then\nwe can\nuh morph the ui if\nif we are with one runtime versus\nanother so all of those things i think\nare things that we have discussed i\nthink we don't have a concrete\nimplementation that requires that yet so\nuh\nbut that flight might start becoming one\nof those so uh\nlet's see how it goes yeah awesome\nperfect if you guys don't have questions\nthank you"
    },
    {
        "title": "Dolt Integration with Flyte - Jun 1st Flyte OSS Sync Up",
        "transcript": "yes we can get started we have a lot of\nfolks already anything\npeople will trickle in but uh\nwe can get started we have um\na quick uh agenda\nwe have two folks uh giving\npresentations today\none of them is um\nthe newly released dolt integration with\nflight\nand max is here\nfrom doltop and he's going to talk about\nthe integration and how they see\nhe can improve the community\nand the second one is a guest\npresentation\nluciano a quick background on that\nluciano\nworks at ibm not sure if he did but\nit works on the lyra project and in the\npast we've had a\nchat about uh integration of a fight and\nuh ilyra\nso i'll let luciano you know explain\nhow lyra works and and then we can see\nas a community\nuh if there is other volunteers who can\nhelp try that project\nuh a quick note uh today is the\nfirst of the month we usually do um a\nrelease but end of the month this was a\nlong weekend\num and we we did discover a small bug\nyesterday so we are\npausing the release we're dealing the\nrelease by a day or two\num but keep keep you posted on the slack\nchannel on the twitter\nand all that with the new release coming\nthe lot of new interesting stuff coming\nin the new release so\nstay tuned um but\nlet's get started max do you want to go\nfirst\nyeah yeah that sounds good thanks for\nthe warm introduction\num hi everyone my name is max i'm a\nsoftware engineer at dolt hub\nso i'm just going to share my screen now\nsee if i can get this to work\napologize moving a little bit slowly\nthis morning\nso i had a great long weekend uh\nyeah so thanks um first of all thanks\nfor the flight team they've been very\npatient\nwith uh letting me contribute and\nteaching me about their project\num let's see\n[Music]\ncan you guys all see the presentation\ncool so so i've left this a little\nopen-ended\ni can go longer on some areas uh in\nshorter in other areas i'm planning on\nyou know giving a little plug for myself\nand dolt\njust 30 seconds i have a demo\nof some of the flight snacks that we\ncontributed as part of our plug-in so\njust walking through\na very simple case of how you would\nimplement the flight dual plug-in\nand then i have a few slides describing\nsome\nuh some of the architectural principles\nof how dalt does data versioning\nwe don't have to go super in depth into\nthose but i have them in case\num that that is something that you guys\nwould find helpful as context around why\nyou would use\nthis plugin so you know if anyone has\nquestions feel free to interrupt me\nif i can go into several demos instead\nof just one of the simple\ndemos i can you know curtail the\narchitectural\ndiscussion at the end make it as long as\nas\nis useful or interesting to you guys\njust just really let me know\num but so uh\nyeah adults so i guess first quick plug\non myself\ni i think i have a really fun job i get\nto work on integrations\nso dolt is a database company we have a\nlot of people working the core database\ni contribute features to the core\ndatabase\nbut the best part of my job is i get to\nwork with people like the flight team\nand i get to build integrations in the\nml\ntooling space so uh you know if anyone\nwants to ask me questions about that\nanyone has suggestions for integrations\num you know asking about the flight one\nor you work at a company and you'd be\ninterested in working with us\ni'm on the flight slack we also have\nadult discord\nuh you know we're very responsive feel\nfree to reach out if anyone\nwants to uh sorry i gotta do that uh\nbut um so adults dolt is a\nit's a sql database with git versioning\nit's very unlike\nother version control technologies it\ndoesn't store csvs\num you know it doesn't store these like\ncolumn ordered files like parquets\nit really stores data in a way unique\nto allow git features that sort of make\nsense so you can do really efficient\ndiffs\nbranches and merges on data stored in\ndolt\nwhich is structured very similarly to a\nb tree which\nmeans that we are wire we've made it\nwire the query engine wire compatible\nwith mysql\nso you can drop dolt anywhere you would\nuse my sql\nand you can get the same features but\nyou can also get all of the git features\nbecause of the way that we've\narchitected our database\nso so some reasons why you might use\nthis\na big one is iteration and\nexperimentation\nof ml workflows if you are training a\nmodel or you are doing\nyou know any data work where you care\nabout maybe the differences\nof data quality or processing steps or\nyou want to\nspew the difference of outputs um\nyou know data diffing is a big use case\nthat we see from our customers\nanother one is is data sharing and\ncollaboration\nwhich you know is going to be a subset\nof data ingestion\nas a whole we have a lot of customers\nthat have disparate data sources\nreal estate finance bio and those data\nsets aren't always standardized or they\nmight come out in like quarterly\nsegments\nand dold is a very efficient way to have\nmany people generally collaborate on the\nsame database because you can have\npeople tweak a database\nand then commit small differences to a\ncentral repository\nbut you can also for example have real\nestate listings that are coming on a\nquarterly basis\nand you can merge those incremental\nchanges to your your core data work\nhouse\nwarehouse with the difference and\nlineage\num of those sort of updated over time\nuh if that sort of makes sense so i can\ntalk again about more use cases\nbut this is just my overall quick plug\nof like why why you would use dual\nso last time i was here i was talking\nabout uh our sql alchemy plugin\nas i've already alluded to so one way of\nusing dold is like using any other\nsql database it's drop in mysql\ncompatible so\nwe built an sql alchemy task in flask\nwhere you can\nplug in a connection string to your\npostgres your mysql database\nany rds instance or dalt\nand so what's different about the adult\nspecific plugin\nis that we are really exposing the\nversion control features\nin dolt which so we have to do a little\nbit more work to expose that\nconfiguration um\nso whoops so i'm going to jump into a\ndemo\ncan you guys see screen changes when i\nmove\nto van yeah okay cool so\ni'm gonna walk through a very simple\nsimple example first let me move my\nlittle\nuh dialog box out of the way so i can\nsee\num okay so so we have a simple workflow\nhere\nwhere we are\nso we're going to do is we are going to\npopulate our database\nwith a data frame and then we're going\nto read a data frame so it's really just\nlike a write read operation\nand it's parameterized by a single\ninteger\nthat is going to affect uh that count\ncolumn so it's supposed to just be like\na very simple\nexample of uh how one\nwould use dolt and so the way that we're\nwriting data\nto our adult database is through this\nconfiguration\nso one way of using dolt is through an\nsql string another way is just\nthrough a folder path so locally we have\na foo folder here and in that we are\nstoring\nthe entire contents of our database very\nsimilar to how you would use sqlite\nthese are files just stored on our file\nsystem\nso i'm going to run this and\ni'm going to run it with two different\narguments and what's going to happen\nis we're just going to populate this\ndatabase with different count versions\nfor these different names\nand it's probably going to show up in my\nhistory but\nokay\nand i have this already populated so so\nwe're running this we're going to create\nour data frame we're going to write it\nand we're going to read it um\ni'm going to run this twice and these\ncounts are just going to be twice as big\nin the second example and\nso all we've really done in this case is\nwrite a data frame to adults and read it\nback\nit's not the most useful example but it\nis very simple and it shows like the\nsimplest way that you would integrate\nand um what we've done\nis that on the adult side we have\nrecorded the two different states\nof of this operation so if we let's see\nso if we log this\nwe have these two different commits\nwhich are the two different versions of\nthe database\nand i've taken the difference between\nthem here the difference is not super\nuseful\nbecause we're keying this table on\non both of these values so there's no\nprimary key\nbut this is just a very simple example\nof\nwe have our adult database we have our\nflight workflow\nthat's going to make some rights and\ncommit those rights\nuh in their own like in their own\ncommits\nand um we're taking the difference\nbetween those two commits here so a very\nsimple example\nbut this is i guess the the simplest\nexample that i could make of integrating\ndole with flight\ni do have more complicated examples if\nyou guys would be interested in seeing\nthose\num or i can walk\nthrough a little bit of you know the of\nadult that actually make that possible\ni guess uh ketan do you have any\nrecommendations on\nuh timing or i don't know\nyeah so uh we do have more time for a\nlot\nto you definitely walk through i think\nuh just for folks to\ni think follow what you just showed um\nso gold\nis think about it as an external\ndatabase uh what mac state is modeled it\nas a type\nessentially a total table and as he\nran a task and and and it completed it\nautomatically inserted that data into\nthe database and has a different version\nand it did not affect\nlike it's completely immutable so all\nthe old data exists and all the new data\naccess and there's a diff\nand actually i think this is extremely\nuseful for\ncases in which you're trying out new\nhyper parameters or\ntrying out new data sets for training a\nmodel or so on\num and old is available like a service\nalso right but this\ncan use your local copy or can you use\nany other mysql is that right\ni don't know that part maybe yeah yeah\nso\nso the remote as a service right now is\na product so we have adulthub\nwhich is just a way of of storing your\ndatabases in a remote\nanother way of storing a remote would be\nputting in an s3 bucket or a gcp bucket\nas you know like a central repository so\nlike your entire team can all reference\nthe same version\nso dolph is an easy way to all reference\nthe same version with having a ui\naround inspecting your data and mutating\nyour data\ni guess i could show that too but it's\njust a little ui we can like run sql\nqueries you can see\ninformation about the schemas and schema\nchanges over time\num um yeah and\nyou know if we get big enough then we\nwould make something that basically\nscales similarly to like\nsnowflake or databricks it just\nstores data in a very different way it's\nnot storing data as like csvs or\nparkettes\nstoring data in in i guess\nif i get into the architectural decision\ndiscussion it stores data in this\nway that's very complementary to\nhorizontal scaling but also\nthese sort of identifying structural\nsharing within those\nthat those those chunks of that data\nyeah so i think it would be at least for\nme would be really useful to look at\nsome bits of the architecture especially\nbecause when you did the dual diff that\nwas really quick\nuh is that really quick because the data\nset was smaller is that really\nthat's how it works that's the dataset\nscales it is\nit is partially quick because the data\nset is three rows but it's also\nlogin efficient in the way that we're\nstoring data um\nbecause yeah maybe like i can give like\na high\nlevel overview of this so i'll try to\nkeep this to a couple minutes\ni've given this talk in the past and\nit's like 10 minutes full\nbut so so gold is stored as a prolly\ntree\nso it's very similar to a b tree we have\nwe have tuples\non the bottom ordered by primary key we\nare\nsectioning off the data and we have you\nknow child nodes we have parent nodes in\nthe very top we have a root of our\ndatabase\nwith two main differences between dolt\nand b tree\nthe first is that we are splitting up\nthese data not by\nfixed size widths but we're splitting\nthem up by\nthe contents of the data so we look for\nunique rare events in in these tuples\nand that is how we're splitting up the\ntree\nand what that means is that you sort of\nhave these sticky sticky boundaries\non the data uh and the best way\nso i can go into this for a couple\nminutes but\nin a b tree what happens is inserts in a\ndifferent order\nwill create a different physical\nstructure of the b tree\nwhen you are chopping up the data by the\ncontents\nof the data inserts in any order create\nthe exact same database so that's really\nimportant for dolt\nbecause if you build a database in a\ndifferent order two people separately\nyou want the same database to have no\ndiff in dolt\nand that is partially true or that is\ntrue\npartially because of the way that we're\nchopping up this data the second\ndifference is that you will notice that\nwe are not referencing child nodes by\nprimary key we are referencing child\nnodes\nby the content addressed hash of that\nchild\nand what that means is that when we do\nthings like diffs\nat the very root note node we cannot\nquickly identify\nwhole sub sections of the tree that have\nnot changed\nbetween two commits or between two of\nwhatever you're diffing\nand that means that it's about as\nefficient\nas a way you can of diffing two\ndatabases\nbecause we are um we're not you know\nwe're not storing like the minimum\ninformation delta between two tables\nwe're storing basically you have to make\nnew of these chunks\nevery single time but it's it's a very\nsmall overhead when you're making\nmutations because you're only changing\none of these you're basically writing a\nnew\none of these chunks and then when you\ntake the difference\nyou only have to traverse the tree down\nthrough the path\nthat to reach those different chunks um\nso that is like the i've many slides to\ndescribe this in detail but that's like\nthe minute version of like\nthoughts architecture which i again i\ncan go into more or less as it's useful\nfor you guys\nmaybe maybe a quick question so\nessentially you hash at the point of\nright\nand uh some sort of hash yeah that's\nwhat the structure and so that's why the\nstructure matters because if the hash\ndoesn't\nlike if you if you break it up\ndifferently than the hash would change\nif fast enough is that is that right\nyeah\nwe we store it we\nwe borrow a lot of concepts from git so\nin the process of making a commit\nyou have your working branch which is\nthe state of the database at any given\ntime\nyou have a staging index which is what\nis committed\nand what is like ready to initiate a\ncommit\nand then um you have head which is the\nmost recent commit\nso we borrow all of git's terminology as\na reference of how to like organize\nthought patterns around changes of a\ndatabase and\nthe difference between those is really\nuh additional chunks so if you\nchange this data you're creating new of\nthese chunks so then like if you make a\nnew chunk then the working\nroot now is different and it references\nnot only all the old chunks but it also\nreferences the new chunk and that's how\nit's different from the last one\nso then if you stage that you commit\nthat you've created\na new commit that encapsulates all of\nthose references\nwhich includes right the root hash being\ndifferent because there was a\nchange in one of its children\ncopy and write style but awesome yeah\ni would be very much interested but we\nshould do this separately like\nessentially how do you resolve conflicts\nand so on but\nif anybody else has any questions\nnope um awesome thank you max and so\na quick note on the availability of this\nplugin this will be available with the\n0.14 release\nuh even though it's not tight really to\nthe release it can be\nthe current release process releases\nplug-ins when the entire\nflight kit is released so i should be\ncoming soon in the next couple days um\nbut uh don't it's been wonderful to work\nwith donald just an amazing\ncontributor and super easy collaborator\ni've yeah you guys have been awesome to\nwork with too i appreciate it"
    },
    {
        "title": "Data Inputs for Flyte Workflows Using Datashim - by Yiannis Gkoufas",
        "transcript": "uh i think we have yes from ibm\nhello yes you ready to go all right yes\nit is\nshould i start starting my screen yes\nplease yeah\nso okay so it's um\ni just want to introduce a new\nframe a new framework one year framework\nthat we're working on\nit's called the data stream and uh\njust a bit of a i would just want to\ngive a very high level overview on\nwhat it does you know expose it to that\nand um\njust to explain also the reason i\nencountered flight and\nuh we started talking with ketan\nwas that we are also part of a\nwe're also part of linux foundation ai\nin data we're just adopted as\nuh just like a few months back we were\nadopted as an incubating\nproject so the project originally\nstarted in ibm but it's now\nlfa ibm research actually but\nnow it's lfai data incubating project\nand it was called data set lifecycle\nframework\nand now it's called the data scheme so\nthat's the\nframework that i will be talking about\nand\nwe have started some discussions on how\nan integration would could look like\nwith a flight\num we just uh just a design\ninitial design dock with some thoughts\nand uh yeah we're looking to dig\ndeep uh a bit deeper onto that but for\ntoday just the\nintroduction of what we do and some of\nour initial\nthoughts so let me start\nokay\nokay okay so the\num the project is called data scene and\nit's um\nif i had to describe it would be that\nit's a meta framework\nthat uh for comparing f4 kubernetes and\nopens its workloads that enables and\naccelerates data access\nso the motivation for the work that\nwe started is that we\nfeel that the non-power kubernetes users\nface complexities when accessing remote\ndata and i saw\nalso today in the example about having\nthe aws cli inside the container so\num what we're trying to do is to\ntake that out from the user and help\nthem\nuh on board kubernetes is in an easier\nmanner\nand as on the previous presentation from\nye\nright it's it's becoming increasingly\ncommon to\nstore and retrieve data from remote\nstores like a cloud object store\nin machine learning and ai workloads and\nin workflows\num or any related framework\nand also another gap that we identified\nwas that the kubernetes administrators\ndo have the resources the tooling to\nlimit\ntrack research usage in terms of cpus\nbut not not for data access\nso yeah so basically what we're doing is\nthat\nit's what is like a meta framework and\ncurrently we\nhave first class as first class citizens\ns3 and nfs\ndata sets that are supported within\nthe framework now we\nare bringing one new crd1 custom\nresource definition that we call it\ndataset it's a bit of a legacy term\nbecause initially we're working with a\npre-populated let's say packets\nor uh nfs directories but uh\nif i had to rename i forgot to give it a\nname again i would call it um\ndata source i think it's more\nappropriate and it's like\num a high level abstraction from the\npvcs the personal volume claims and\nbasically what what the framework does\nis that it orchestrates the provisioning\nfor a pvc and the the configmod\nneeded for each data set so this means\nthat\nthe user defines the data set and under\nthe hood the framework provides\na pvc that they can mount it without\nwithout having dependencies on uh\ns3 or nfs clients or whatever\nso yeah that's that's what it does and\nour goal is to\nbasically provide the user experience\nso the users won't have to install\nother plugins related to data access and\ntries we try to be extensible and to\nsupport\nall major types of data sources there's\nno limitation to add\nany fancy packing store\nthat the administrator wants\nand also we're looking at performance\nimprovements as well\nespecially considering caching and\nscheduling\nso this is the high level functionality\nand yeah i have the legacy\nterm here but can will change it so from\nthe user perspective\nuh they just define a data set of the\ndifferent\nformats right and then it's the\ndata set operator is the brain i would\nsay\num for all for all this orchestration\nthe dataset operator that invokes the\nnecessary\ncsi plugin now csi plugin\nfor those don't know it's like um there\nare some plugins um\n[Music]\nthat help you create a pvc out of a\nspecific\ntype of uh of uh storage\nso we are maintaining uh we're hosting\nand maintaining\nsome uh some csi plugins around s3 so\ncloud object store\nand nfs but again we're looking to make\nit completely plugable\nand basically for people who have worked\nwith that\nour goal is to build something like jdbc\nfor for kubernetes and data\nand so basically the data set operator\ninvokes the necessary\nplugin for the specific specific type of\ndata set or data source and gives back\na ppc that the user can just reference\nit in their pods so we have as a\nconvention that\nmaybe you want to mount it on mnt data\nsets\nright and uh but the user can mount it\nwhatever they want\nalso here i'm showing also that um\ni'm giving hints about the how the\ncasting works\nso we don't while we don't do cussing\nourselves and we don't\nplan to provide cashing the core casting\nfunctionality ourselves\nwe're looking to we're looking to\nprovide the interfaces\nso that existing casting solutions can\ncan be pluggable in the in the framework\nso basically in the case where\nuh there is a casting plugin available\nfor s3\nit gets invoked and then the user works\nwith the cast version let's say of\nthe data set without even them even\nrealizing that they're accessing a cache\nversion\nof that bucket so imagine that\nyou can build all sorts of\nfunctionalities on top of that\nso you can have knowledge on uh what\ndataset are accessed and what datasets\nare accessed by\nwhich pods uh you can understand which\ndatasets are\nhotter and also one of the benefits that\nwe\nbelieve there is is that a\nthere can they can be two two personas\nright one could be\non the on a kubernetes cluster one would\nbe the data set\nprovider so it's the person who will\ncreate data sets and make them available\nto the end users\nand the users without getting\nwhat is the interface how it's optimized\nhow to configure they just\nuse the data sets without without them\nhaving explicitly to\nto do something um here i'm showing an\nexample of\nthe current spec for um for a dataset\nhow a dataset looks\nlooks like so you just define the type\nof\ndataset so in this example it's cos\ncloud object store it can be nfs\nand these are the specific specifics for\nthe data set\nso you we need the at least the end\npoint the bucket\nand the access keys which now you can\ndefine it\nwe have updated the framework and we can\nyou can define and define it on a\ndifferent\non a separate secret and reference that\nso we have experimented with all the\nflavors of s3\nobject cla object stores that we know of\nso\nibm calls amazon nuba chef minio\nit works without any limitations and\nhere it's just a\nhelper that we have as a functionality\nso basically if the user doesn't want\nso just to say that once once you create\nthe data set\ntest once the framework kicks in and\ndoes the provisioning you will get in\nthe end the pvc called\ntest as well now if the user doesn't\nwant to\nspecify volumes and volume amounts on\nthe reports\nthey can use it in this function so\nthere is the data set\ndot any number right and then id\nmake a reference to the initial data set\nand use as\nmount so user's mount means that the\nuser wants a mount point\nfor that data set and not the config map\nso they can use it also\nas conflict map for the data system that\nmakes sense you know they might want to\nuse the\ns3 api directly so they just reference\nthat and they don't use the\nthe pvc uh so yeah\nso the benefits that we see from the\nframework is that\ndata scientists and data engineers can\nfocus on the workload\ndevelopment and experiments and not on\nconfiguring and tuning the data access\nthe storage providers can increase the\nadoptions\nbecause the framework is extensible\nwithout having a\nnegative impact on the user experience\nand as i said the data oriented\nframeworks which i think\nis what um\nwhat's the case with flight right we can\nbuild capabilities\non top of that and we feel that it's a\ndeclarative way of accessing and\nmanaging\ndata sources and i think that's all i\nhave for introduction\nyeah so this is my some some links in\nthe\nwebsite in the github and some\npre-recorded demos\nand just to give you in a hint of what\nwe're thinking of\num my idea was that maybe\nit makes sense from from the flight\nperspective\nto have inputs on the tasks\nas being data sets right so\ndata's in providing this this\ninfrastructure of\nconnecting and exposing pvcs for um\nfor a cloud object store and nfs\nand the user will have an option to\nspecify as an input for a task\nuh a data scene dataset beside the one\nthat you already have in flight so but\nthese are\nsome very initial thoughts and we're\nlooking to\nget into more details and flesh out what\ncould be done for flight as well so\nthis is so i have some backup slides in\ncase some questions have uh\nsomething to do with what i showed but\nyeah please free\nto let me know\nand uh yeah i think thank you annis for\ngoing through this uh\ni can you remind me if these are also\nwritable\nuh pvcs or are they mainly for\nyes yes so basically these are you can\nspecified\nas read-only if you want but by default\nyou can write that's that's why i said\nthat it's not exactly um\nit's not exactly dataset anymore because\nmost of the\nthere are some cases on kuberflow\npipelines that they use it that they\njust\nwrite they write the data on this on\nthis dataset it's more of a\ni don't know data connection i would say\nlike\nsomething like this so there's no\nlimitation for writing\nif you want you can specify it as a\nread-only so in the case of s3\nwe can mount the bucket as read-only as\none of the options\nnow we're exploring some ways to have um\nyou know to separate a bit of policies\nmaybe that makes sense to say\nyou know i want this data set of this\ntype of policy\nlike um you know to group them in some\nin this fashion but yes it's up to you\nso the answer is yeah if if you want to\nhave it read only you can have it read\nonly\nyeah so this can also be like an output\nof a workflow or a task like you can\njust\ntest data and produce a data set exactly\nyou can write yeah and also some some\nother ideas we'll be talking with folks\nfrom uh\npachyderm maybe it makes some sense also\nto have\nlike a sort of versioning you know\nbecause\nwhen you execute a task i would imagine\nand\nyou get an input of a data set you might\nwant to associate some\nversioning information with that there's\nanother thing that we can explore\nbut yeah you can use it as an output you\ncan use it as a normal pvc\nwithout limitations\nyeah cool\nthank you very much thanks thanks\nlet me know if you have any questions\nyou can send\nyou can send an email or yeah i'm on the\nslack channel of\nflight as well so thanks thanks everyone\ncool thank you do you want to unshare\nyour screen there we go\nyeah yeah okay thank you um i think\nthat's where we are today um yeah so a\ncouple of plugs i guess\nuh for logistical uh things the the\nuh sandra sent a survey about the timing\num if uh if if it's hard for\nyou guys or uh other people in your\ncompanies to attend\num we're like taking suggestions for uh\nwhen it would be a better time um i\nthink\nreach out to sandra if you haven't seen\nthe link for the\nuh survey um she can send it to you\nand and then the other thing is the like\nwe're soliciting uh\ntopics for future talks i think we have\na few lined up for the next\nfew weeks uh but if there is something\nyou want to\nhear about or a concept in flight it's\nnot very clear you want us to dig deeper\ninto\num yeah please let us know i'll let\nsandra know as well\nyeah thank you okay back to you george\nsorry\noh that's great uh thanks everybody um\nyeah particularly if you have colleagues\nthat\nfor whom this time is difficult um and\nyou want to suggest another one\nthere's been people you know proposing\num later in the evening on a pst time to\nsee if that would be\nmore convenient so this is obviously for\npeople that couldn't make the call so if\nthere you have colleagues that are in\nthat category\nplease don't feel free we're going to\nleave it here unless we get\nyou know enough uh\nyou know community movement for another\ntime um but we're certainly open to that\nif\nif need be um otherwise uh thank you\nvery much annis i appreciate that thank\nyou yi\nyeah i just had a quick uh suggestion\nregarding the topics and i was thinking\nthat that might be very useful for\nuh people who want to adopt the platform\nfor more use cases in their\norganizations and that's basically\nuh to maybe highlight a few\nuse cases for which it is highly\nsuitable so that's one area\nlike iterative machine learning\ndevelopment or\nyou know any kind of linear optimization\nwhich has a lot of you know\nmultiple simulation runs or whatever\nthose use cases are you guys probably\nknow the best\nlaying down top four six use cases\nclasses of use cases that would be very\nuseful people who need to adopt the\nplatform they can just take a decision\nthat yes\nuse case walls in one of those buckets\nit's suitable for it i can use it\nand the second one where i uh kind of\nsee a lot of questions and uh uh\nkind of doubts in people's minds like\nwhy flight why not something else why\nnot airflow why not\nmaybe techtons framework why not\nconductor and all those kind of things\nwhat are some of those\nplaces where it distinguishes itself\nfrom those platforms how is it different\nbut at least the top five ones uh what\nis unique about flight that might also\nbe\nsuper useful people will not have to\nkind of\ngo ahead in a while goose chase and try\nboth the systems\nthat will be very useful as well so i\nknow these are\nbig ass can probably take weeks and\nmonths but that will be something\nhighly useful for people who just want\nto go and use the tool\nrather than actually going ahead into it\ncostly and\nvery broad evaluation mechanisms\nso just a food for thought i mean i'm\nnot expecting i'm such a model\ngood feedback santosh and you're not the\nfirst to ask that question\nso yeah i'm glad yeah totally that's\ngood\nyeah sounds good yeah well said\nawesome that's um that's some\nsomething that they're your first points\nonto something i'm working on\njust like various uh\nand um yeah yeah i might demo something\nvery soon okay okay all right i mean if\nyou\nuh maybe at some point just want to set\nup a call with me because uh\ni have spent some time trying to do this\nuh between\nair flow and flight within lyft uh\nso yeah i mean it might be useful for us\nto maybe just\njoin our approaches and see if we can\nevolve and come up with something better\nand\nmore structured\ni think as an external flight yet\ni would have to say because we have\nexplored um\ninternally we're looking at various uh\nworkflow engines right and we're looking\nat the\nopen source landscape a lot uh my\nmy feedback is that flight is one of the\nmost\npolished uh efforts uh out there\njust my my two cents on that\nright probably yeah\nit's a matter of you know more\naggressive\nmarketing i don't know but it's uh it i\nthink it's\nit's on the top uh framework for for\nthis from from my initial\nuh you know with my soul familiarity\nwith the project\nyeah i mean totally i don't disagree\nwith it i totally agree with it but\njust that we need to basically also land\nit into\nthe use cases a little bit in yourself\nthat\nif you it is a machine learning or a\nlinear optimization iterative modeling\nkind of problem it's great\nuh but i'll give you a counter example\nif i just have to do\na bunch of sql tasks and join them\ntogether in a chain and execute\nit you know regularly it can be done in\nflight\nbut then the preference is very clear\nthat airflow is simple enough\nmore kind of you know well-established\nlot of people know it\nuh so we have to balance it out a little\nbit\nyeah no i'm just amplifying uh what you\nsay i agree that\nuh you know maybe it's\nuh it needs a bit more campaigning or\nsomething like that\nto help in this and uh i i i\nhave it in mind and uh\nwhoever works with uh workflows i'm\ntrying to\nspread spread the message right so yeah\ntotally yeah i mean we definitely can\nhave like maybe a\ni mean and this is for george and uh uh\nyeah so maybe we can just have a\nbrainstorming session we bring in our\nexperiences there's like you know\nwhat are the problems that we have heard\nfrom this set of data scientists\nmls suite and all the folks what\nproblem that they face in deciding\nbetween all the different\noptions of orchestration tools that we\nhave and maybe if you can evolve a\nframework that hey this is the approach\nthat we're going to take and then\nat some point of time publish it it'll\nbe in general useful for flight\nand i think it will help uh the flight\nteam also get focused\nthe use cases where it shines and it's\nreally powerful\nshould probably just double down uh and\nthen the use cases which are like\nyou know barely fitting it's okay to let\npeople go and\nlike you know the tools of their other\ntools possibly\nwe can go to deeper dive i think for you\nknow simple etl jobs where\nyou're you're literally just trying to\nstring together you know three\nsql tasks or some high jobs and if the\nwhole thing fails\nyou can rerun it and it's on a 24 hour\ntime clock like\nit flows awesome for that right it's\nit's that's what it's like or\nexactly where flight where that\nwhere the the the birth of flight came\nwhen\nthe the complexity of the workflows and\nin particular dealing with partial\nfailures became\nlike the thing that kept people up at\nnight\nthat's where the the reliability\nand the ability to find what happens in\npartial failure scenarios at a\nprogrammatic level\num became kind of the driving force so\nif you've got a\nyou know a large you know multi-computer\njob\nmulti-steps they're not all you know\nthey're they're heterogeneous\nand um you need the programmer because\nthey're application specific and you\ndecide what happens in the case of\npartial failures and to be able to\nrecover from those and especially\nnot repeat the work that you've done\nbefore\nthat's where it turns out that the the\nyou know the reliability and the\nscalability of flight sort of became\nimportant and you know sort of surpassed\nwhat was\npossible to do on top of airflow um\nright\nso that you know big\nmission critical jobs that's where yeah\nif you need a simple etl\nand if it doesn't work you just start it\nagain and you know start\nand rerun it airflow's fine and it's\nsimpler to\ninstall right right right here's the\nother place\nyeah the other place which i also found\nto be super\nduper important is that um when you have\ndependencies on specific\nlibraries a lot of tools just mess you\nup like\neach time we have to upgrade one airflow\nlibrary let's say tensorflow or\nsomething\nor even a python library it's a\ntremendous effort because we have to\nunify\neverybody and make force everybody that\nhey check with the latest version of\nthis python library if your airflow jobs\nwork with\nit fine then only will upgrade or if we\nupgrade\nit breaks down it's your job on flight i\nheard from one of our engineers who said\nthat\nthese dependencies particularly in the\nml world i can't get it straight for my\nown projects\ni bring an image processing libraries\nfor map processing from the wild\ni sometimes even patch it and i use it\nand somebody else is on a different\nversion\nand has probably a different set of\npatches and all of us can coexist\nvery easily on flight if i had to\ncoexist on airflow\nmy life would have been held just like\nunifying all those libraries and making\nthem work against everybody's workflow\nso\nthat dependency isolation i think which\ncomes from the containerization\nis a very powerful construct as well\nthat's just containerization right\nthat's\nthat's exactly containerization and\nflight is absolutely is\ncompletely container native like was\nbuilt you know it was the third\nthe first version of flight was built on\ntop of airflow the second version was\nbuilt on top of aws and the third\nversion was built on top of kubernetes\nso it is it is you know and that's all\nthe benefits theirs are not added\nabsolutely they're just that's that's\nwhat you get from a container native\nsystem so\nwe just like just leverages that and\ntreats that\nsort of a native concept like we don't\nknow what's inside the container all we\nknow is the\nall the the data types that are being\npassed between tasks\nand that's what right checked and what\nhappens inside the container is a\ncomplete mystery to us which is on by\ndesign\nright right exactly but yeah i mean i\nwas just thinking of like you know some\nsimple constructs that hey\nif you want to run jobs which has got\ntheir own dependencies or like isolating\ntheir dependencies is important\ngo for flight if you have like you know\nthe places where you need to\ncache the intermediate results and\nrestart from a point or\nyou know you want a kind of workflow\nwhich is\ncheck pointed at multiple places and you\ndon't have to start from the scratch if\nit fails go for flight\nso those like you know five ten things\nand people can map it\nrather hopefully relatively easily and\nthen decide that you can either places\nwhere flight will be really\nuseful and powerful i think that will go\nquite a long way people do not have even\nthese very simple concepts in their head\nat this point of time\nyeah but i do think it's a mistake to\nsay\nyou know throw out airflow and use\nflight for\nfor the simple things that airflow's\nreally good at you know like it's\noverkill\nfor for a lot of simple problems yep yes\nand and that's fine like it's totally\nfine for\nairflow to code i mean we're going to\nco-exist with airflow forever like\nairflow is not going away anytime soon\nit's really good at what it does and\nthey're just in sort of different spaces\nyeah\nfor a converse there as a user that's\nnot\ntentatively in a massive organization\nit's more ideal to not\nmaintain multiple different clusters\nand therefore the second i cross the\nthreshold of needing something flight\nrelated\nideally i only want to maintain flight\nand i don't want to also maintain\nairflow\nand therefore then i need flight to also\nbe good at\nthe airflow type things\nyes in principle i totally agree with\nthat\nbut you know i will cite you an\nencounter example\nand hopefully that clears the air over\nhere\ndo you know that oracle has 40 billion\ndollar in revenues and\nseven sequel dialects today and probably\nin its history probably 25 plus talents\nin spite of the fact that sequel 92 has\nexisted forever\nand the point i'm bringing it because i\ngive this argument again and again\nand i have burnt my fingers trying to\nhave a unified sequel between\nhive and impala two engines developed by\nmy previous\ncontributors and organization we\ndeveloped from scratch we tried doing it\nit's super hard\nthe idea is that when we try to use two\ntools which are really really good in\ntheir subjective\nareas and we just try to kind of file\nthe properties of one onto the other\nand i see a strong pull for that even\nwithin lift\nthe problem is that you end up very\noften with a very mediocre\nkind of system which is neither great\nhere nor great there\nand that's the beauty of it\nand that's the challenge of it that you\nhave to go and evolve flight\nand take up more workloads but not take\ntoo many\nand that way you are best kind of you\nknow set for long-term success\nin the places where it is kind of really\ngreat fit\nand brings a lot of value i know what\nyou're talking about a lot of people\nstart in that direction\nthe problem of like you know that kind\nof unification effort that i've seen\ntime and again\nand unified sequel is one example what i\nrealized is that getting probably 50 of\nsql queries from hype to impala or\nimpala too high\nis super easy and people are highly\nencouraged by that four months or six\nmonths of progress\nbut next thirty percent takes three four\nyears to get it right\nand the last twenty percent can take you\ndecades and that's the probably the\nreason why\noracle has never achieved it otherwise\nit would have been its humongous\ninterest to have a single unified sql\nacross all its\nsystems and making sure that all of them\nwork right really to the\nyou know best performance and\nscalability and all those other\nrequirements as well but it has never\nhappened and there are papers which\npeople have written\ntrying to attempt it and it hasn't\nhappened\nfor a fact so you can dig more into it\nbut the idea is that\nsimilar efforts over here will probably\nalso unlikely to go far\nso we should be selective which use\ncases we bring in\nwhich we probably do not but anyway it's\na big debate\ni don't want to kind of blow it on it\ntoo much but yeah i mean\ni i think that makes a ton of sense\nright i just wanted to make sure\nyou know uh there there's the pain point\nof maintenance from\nyeah thanks so uh i there's a lot of\nbenefits to not targeting\ntoo many use cases as well so yeah yeah\nbalance is probably the name of the game\nhere\nyeah as long as we're cognizant of the\nthings we need to balance\nyes yes\nawesome anyway i mean as a follow-up if\nyou can have maybe a session just to\nkind of where we all bring\nour perspective where flight kind of\nshines\nwhat use cases what core requirements\nand then maybe compile them together\nprobably that one hour exercise would\nalso ease something substantially enough\nto begin with\ngood discussion uh and thanks austin for\nspeaking up i think\nuh you speak for a lot of people\nuh we have we the flight team has a lot\nof work to do to sort of\nbring the simplicity and ease of use\nthat airflow has\npioneered and you know give all the\ngoodies that flight\nuh provides without uh\nwithout the the mental complexity in the\noverhead that\nthat um that sometimes comes with\ndealing with\nyou know just a big containerized system\nas the kubernetes ecosystem sort of\nevolves and it's kind of kind of boring\nout of out of uh\nyou know it's you know it's a brand new\nparadigm for dealing with cloud\ncomputing and we have a bunch of work as\nan industry to bring\nease of use to companies that's\nyou know it's got work ahead of us so\nthat you can have the advantages of both\nbut i think\nright now we can't claim that we're\nthere yet and we are very very actively\nworking on it but\nuh we we've still got work to do and uh\nabsolutely and smiling i was at\nmicrosoft when odbc was\ncreated to first try to solve a database\nand i i could not agree with you more\nthat the reason why there's so many\ndatabases is because at the edge they do\ndifferent things and if you try to unify\nthem you miss those edges\nand yeah yeah and i was telling somebody\nin my organization that's where you\ncannot replace intelligence with\nexperience right right and they have the\nsmartest grads coming\nout of college but until they have\nburned their fingers they will not\nunderstand it i point my fingers so i\ncan tell you\nand so have you yep yep so\num and i and and for those very reason i\nthink even in big organizations\nuh you know i i think air force flight\nare going to co-exist for a long time\nfor those two reasons even though it's a\nhuge pin in the neck and we wish it\nyou know it was a grand unifying thing\nbut the reality is\nthere are different sweet spots for the\ntools and yeah\nabsolutely uh thank you everybody\nit's a lot of fun and uh appreciate the\nthe feedback\nand we will we'll talk a couple weeks i\nthink we've got\ni think senator has already pre-posted\nthe um agenda items for the next few\nweeks so read down if you want to see\nand if you want to get on the list uh\nsend her keith or me\na a note and we'll add you to the agenda\nand you can show off your stuff\nthere's a link to the survey george in\nthe chat if you'd like to fill it out or\nforward it to anybody who's not able to\nattend these times\nyeah please do that yes thanks everyone\nthanks everybody\nyou"
    },
    {
        "title": "FlyteMeet 2021/05/04",
        "transcript": "yes hello everybody uh good morning um\ni think we're gonna do uh hayden's gonna\ngive a presentation on\nthe new auth integration that he just\nchecked in big\ncheck in and then uh caithin's gonna go\nthrough\nsome deep dive analysis that we did on\nperformance of propeller under a lot of\nstress and load\nwhich he's been working on for a while\nand then we open it up for\nfuture design topics any other questions\nthat people have along the way\nso do we have any new people visited\nbefore\nyeah i see tom is\nwe saw him in the channel a couple weeks\nago i think\nwelcome um all right hayden are you\nhey yeah uh sorry kids\nso good morning everyone um let me\nshare my screen here\ni think i'm sharing the light screen um\nso yeah uh i would like to talk today\nabout the\nflight authentication project we worked\non over the past\nfew weeks um uh and uh why\nwe started to do this in the first place\num so if you uh if you have tried to\nconfigure auth today\nwith flight some people have\nsuccessfully done that\nbut uh in most cases like\nthey need a lot of uh uh help uh on our\nside because the\nconfiguration some of the configurations\nare not obvious and um\num it's not quite clear what the\nuh where to put what like which settings\nto set\nwhere uh and what they should point to\nand so on\num and add to that it only\nsupported external idps so if you have\num\nyou know octa or key clock or you know\noften zero or you know one of those\nservices that do support\nuh a wide range of protocols we require\nlike open id connect to oauth 2\nand so on um but if you have a\njust a simple requirement of enabling\nyou know google auth\nuh or microsoft you know\nlive uh accounts uh i i don't know what\nthey\nname them now um yeah it just will not\nwork\nit does not support that um and um\nyeah and then the configurations are\ncomplex um\nand if you just like do one mistake\nsomewhere you don't know\nthe errors were not obvious what\ndid you do wrong and so on um\nand after that you uh you had to uh\nenable and disable auth at every just\nsome point like in flight propeller that\ntalks to admin needs to know that it\nshould be\nenabled or not in you know flight city\nand flight team\nlike every uh client that talks to admin\nneeds to know if it should\nor should not use auth um it's not like\nconfigured in a center place uh so we\npro full i and ye uh\nset out to try to change that uh with\nfew goals and first of which is it\nshould be easy to configure\nuh you shouldn't need a lot of guidance\nthere should be documentation that tells\nyou what to put\nwhere and it should do it in one place\nright you should be\nable to just configure auth in admin\nwhere where art is enforced uh and then\nall the clients should just\nknow that um and uh and start you know\nuh uh prompting uh or login\nmessages um the second which it should\nbe standard compliant we don't we didn't\nwant to like in reinvent the wheel or\nhave our own\nuh off you know weird uh weirdness uh\nor uh store credentials or any of that\nbecause then\nuh you get into this rabbit hole will\nprobably never\nget out of and we'll we're bound to make\nmistakes there so we want to keep it\nstandard compliant um we also wanted to\nsupport\nuh simpler services that just offer\nuh like sso with open id connect like\ngoogle off and so on\num but do not offer uh\nlike an external authorization server\nthat lets you protect your own services\num and um and then we should also\nkeep what we had which is being able to\nconfigure it to to work with those\nexternal services\nuh if you choose to right and uh that's\nthe end of slides\num i uh uh i wanted to go\nover the uh like a simple\nsetup um and\ni had the docs\nuh so we we did uh try to extensively\ndocument\num how do you configure uh\nlet me start from the home page here so\nthere's an overview of uh you know which\nprotocols and which flows\nwe support you can go over that there\nare links to\nspecs if you uh are inclined to read\nthem\nbut you don't have to um and there's a\nsimple link on the top too\nthat walks you through configurations uh\nfor some of the popular services\nand you know contributions here are\nabsolutely welcome\nuh if you have configured it to work\nwith key cloud\nplease put the configurations uh you\nknow other services\num i like would love for this to be a\nvery like give a colonical example of\nhow do you do it for\neach of those popular services uh so\nthat the next person would find it easy\nuh to do so um we uh we did go through\nthat exercise for\nuh google uh google implements open id\nconnect\nit's compliant to the standard um and\nocta\nuh it also does that for open id connect\nyou can just use it for open id connect\nor you can also choose to\nuse it for um uh\nfor uh like as an external authorization\nserver to protect like the other\nendpoints in admin\nuh for you know app authentication\nuh the configurations have changed quite\na bit\nuh hopefully for the better and\ni will let you be the judge of that um\nuh the we'll go through the simple\nexample of how to configure you know\ngoogle auth for example\nright so we when you start up uh a\nsimple\nuh sandbox now after you know uh 0.13\nis released uh it will come up\nconfigured\nlike most of the configurations you need\nfor enabling auth but\nauthor will not be enabled by default\nbecause it requires you to\num add you know an additional step to\nuh to link it to you like some\ncredentials you create at\ngoogle or you know whatever idp you use\num there is a link here for how to do\nthat for uh\ngoogle which i believe i can go to uh so\nit goes\nyou can like read up on google docs\non how to create cleaning shields um\ngo back here i\nuh did that uh um\npreviously uh but it's simple you can go\nto\nlike your uh i think it's\ncloud.google.com you authenticate as\nyourself and this is all free\nuh if you have just like a personal\naccount you can still do that\num and you say want to create\ncredentials\nand it's uh of type what two client\ncredentials which we also tell you to do\num and uh indicated one for flight admin\nyou end up with a client id and secret\num\nand a new bunch of urls here to\nwhere uh where it can go to uh obviously\nthis works with the\nuh sandbox if you have a deployment with\nthe real url and dns and so on you\nshould configure it as such\num yeah so back to here\nuh we just left the note that you know\nyou can do it with key clock i know\nsiren did\nuh i just didn't get the time to\nget it set up and put like write down\nthe actual configuration how to do it\nuh if you did please uh contribute\num and then it goes through how do you\nconfigure this\nuh in the the at a high level there are\ntwo things you need to do\nright you need to set a secret somewhere\nand we use\nkubernetes secrets for that uh the\nsecret the coin secret we just\nlooked at um and then you need to tell\nadmin\nuh you should start enforcing uh\nauthentication now\nand use this service right like google\nor and so on\num the secret is\nuh simple commands here uh and if you\ncan\ntry to go through them\noops uh i thought that would zoom in it\ndidn't uh\nokay uh so you can\ntake your couple either secret\nand slide it\n[Music]\nuh the the deployments now come with\nuh empty secrets um that uh will\nwill uh populate the first time you\nuh you you like started admin deployment\nclients next all right\nlet me just set my uh cube\ncouple here here we go uh\nand so so all the secret like there's no\nsecrets checked into the\nrepository they're all generated uh for\nlike unique to each deployment\nuh and they stay as such like the first\ntime you deploy this they will generate\nuh new secrets and uh and then after you\nupdate admin or whatever they will just\nremain the same it will not reset\nuh existing login stations and so on\nwill continue to work\nuh but we just want to make sure this is\nuh uh we don't like to leave secrets or\npeople start sharing the secret or\nwhatever\num unknowingly and um\nand you know uh and we compromise like\ndeployments\nuh yeah so if you looked here uh we\nalready generated those three secrets\num they are used for uh you know\nvarious things uh cookie encryption\nor claim encryption in certain tokens\nand it tells you to add a secret under\nstring data\nand don't worry yes we are pasting a\nclean\nuh uh text secret but uh oh this became\ntoo big\nsorry zoom out okay um the\nkubernetes once you save this object\nit will immediately\nencode it and encrypt it on the server\nthink i can copy from here give me a\nsecond\ntalking from a real browser\nuh here we go all right uh so\nit provides this convenience uh access i\nguess to uh\nplain secrets you can add clean secrets\nuh once you do once you save this object\nthey will get encoded and so on and\nconverted into\nthis data field\nso let's save this oops\nall right so now the secret is there\nnext we need to configure admin to start\nenforcing\nauthentication and tell it to go you\nknow uh to do that against\nuh google uh this is a bit awkward but\nto\nuh find the uh the config map\nuh that's currently used uh with flight\nadmin\nuh you can do this dance where it\nsearches\nuh the the you know the current\ndeployment for uh\nuh for this string uh and the reason we\ndo this is because every time you change\nthe\nthe config map uh in the deployment and\ncustomize we generate\ngenerates like it hashes the contents\nand generates a new name\num and it does that so it forces like a\nrestart on the deployment if the only\nthing you\nchanged is you know a config but then\nthe side effect is you end up with those\nrandom name config maps you don't know\nwhich one is currently being used\num so yeah let's edit this\nuh and you will notice here there is now\na section that will come\nuh with you know any new deployments\nafter 13\nuh called loss and it has you know a lot\nof configuration already filled in\nuh some of those have default values\nconfigured in admin but others here that\nuh that we think are common are common\nto be edited\nuh are configured are like displayed\num there is an authorized uri\nthis is uh what urls you can like access\nadmin from\num and currently it comes with like the\nuh you know the ingress endpoint for\nsandbox uh\nas well as internal endpoints that are\nused within the cluster\num if you have like obviously a dns you\nshould add that here\nso it uh it allows people to like\nredirect to those urls\nto you know for the various\nauthentication flows\nand then there are two main sections\nbelow that there's an app\nauth and user auth uh so user auth is\nwhat we have been talking about so far\nuh this is what we use for uh that was\nsupports like\nsso essentially uh and the only current\nprotocol supported is open id\nconnect um and it's filled in with some\ndummy values for\ngoogle auth but you should you know\nreplace this client id\nwith the one um you know you created at\ngoogle here um and uh and pretty much\nleave the rest the same\nright um and uh and then you flip\na switch that says use auth just say\nyep um\ni'm just doing the changes in line here\nuh ideally you would change this in\nlike your customized deployment and like\nyou redeploy so you have it\nin a consistent state um but\nuh you want to assume and then you can\nrestart admin and\nif we did everything correctly\nnow if i visit\nif i visit flight admin it should\ntrigger off which she did not\noh there are other errors i think\nwhoa obscene connect\ni'm having i think ingress problems\nstill restarting i think again oh that\ncould be\nyeah okay let's see\num looks like it's running and it\nredirected me to\nlogin but i am unable to access that\num okay i will look into that um\nthat's what makes a live demo live demo\nright uh\nwe'll i will look into that uh\nafterwards but\nuh but yeah that should be all you need\nuh to just do\nopen id connect i will not go through\nthe oauth 2 but if you have a server\nthat supports\nan external authorization server\nexamples again are octa\nor key clock there are there is\ndocumentation here for how do you\nconfigure octa to\nto be your uh app auth provider uh and\nthat lets you\ndo things like uh so admin comes with\nthis like simple implementation of this\nbut\nif you configure opta uh you can have a\nstandalized like dashboard for\nlike the company uh apps and you can\nenable disable\nthose apps in like a central place um\nyou can enforce like a consent screen so\nevery time you know you somebody tries\nto authenticate with a new app\nit will ask the user for you know do you\napprove this app of sorts right so it\ngives you a lot more fine-grained\ncontrols\num and flight is not an identity\nprovider we don't want to\nimplement any of that um they do it much\nbetter so\num so you should do this uh if you have\nyou know depending on the\nconfiguration in your company uh and\nthen it goes through telling you how do\nyou\nhow do you configure that in uh for\nadmin\num and again uh this is all you will\nuh have to edit uh and then the other\nclients will just\nstart prompting the users you know if\nyou're using cli um\nit will just start prompting the users\nto authenticate\num and consent them or whatever uh it\nwill like discover the right\nyou know authorization server and so on\nthrough admin like we added a few\nuh new endpoints in admin to uh make\nthis discovery\neasier um if you are coding a completely\nnew client on your own that's\nwe should talk uh we can like show you\nthe we don't have documentation on how\ndo you do that\nbut i think we at some point we will um\nuh we can show you at least to the\nexisting patterns to use\nuh so you can you don't like hard code\nany assumptions\nin your clients and yeah make it all\ndiscoverable i think that is\nall i have uh if you have questions\nabout\nyou know authentication or how do you\nconfigure soft please\nyou know where to find us thank you and\nback to you george hey hayden i had a\nquestion\nuh if how is this\ndifferent if you are enabling a user to\nauthenticate who might be\nthere in an idp versus a service let's\nsay airflow wants to submit a job or\nmaybe an ml platform or submit a flight\njob\nis that going to also go through idp\nitself or how will it differ if at all\nyeah that's excellent question um so uh\nyeah you uh you can configure there is\na concept of a service uh\naccount flow uh in those in all two idps\num and uh yeah they will you will go to\nand again this will not be like\nhardcoded these are what can be all\ndiscovered by all the urls to go to and\nso on can be discovered through admin\nendpoints but yeah you will go to the\nidp\nyou get a token through your like\ncredentials\num and then that will issue you a token\nand then you can\nsubmit this against admin the reason\nit's different is because\nuh most of like sso uh\nor like open id style stuff are more\ncomplicated than just the username\npassword like you will\nuh maybe prompt the user for a\ntwo-factor auth and like you know send\nthem a text on the\nessay on like on their phone and so on\nright um\nand you don't you can't do any of that\nas a like a headless service\nrunning or you know airflow server or\nwhatever um so they\nare different and and and because they\nare different it lets\nthe administrator who is configuring the\nidp uh decide which\nuh like permission levels uh they can\nget\nfor if you are just a service showing up\nwith the username password maybe it will\ngive you like very limited\npermissions um versus a user who can get\nlike you know\na wider range of permissions were like\ntheir full set of permissions\nso that answer your question yeah yeah\nso you're saying that that will be a\ndifferent flow\nbut that's also supported yep yep okay\nsounds good thanks\ni believe we have a section down there\nfor uh uh like ci\nyeah uh how do you like we have a i\nthink this scenario have a ci or style\nsome service running in in\ncron job or whatever doing this uh how\ndo you configure that\nsounds good i have another question\num so what's the time frame when\nis this already in or um\nyes so all of this is uh merged uh the\nremaining pr is the stocks\nuh the dox vr uh so i yeah uh hopefully\nthis will go in today as well\nand uh it will be awesome access on\nthere\nyeah that's cool because i mean i have\nuh with with key cloak as he said but\nwith\nuh with the old way which is\nmore was a little bit more painful or i\nhad to ask you all the things\nand um but when if this is in now i i\nthink i\ni'll try to to to upgrade um\nand then i'll add key clock\ndocumentation that i just didn't want to\nedit for the old setup\nyes yes please that would be awesome\nthank you yeah\nyeah so we'll be releasing today or\ntomorrow\num 0.13 and that should include this\nlike as the entire platform level\nthrough everything\nuh one thing i don't know if it was\nobvious to everybody but this is\nfull single sign-on support across\neverything across cli\nbrowser like everywhere you\nusually is not available in most open\nsource software so when i was looking at\nthe pr i was like\nholy crap this is my language that the\npr was huge\nand i was like we are implementing a lot\nyeah\nbut the way it's been done is it's\nalmost a standalone service that you\ncould use for authentication across\nother products if you want\nas a gateway so it's pretty awesome\nthank you nathan thank you\num kaitlyn i think you're ready to go if\nyou want to start talking about\nsure open heart surgery on propeller\nmine uh is not going to be as\ninteresting as uh\nit's it's more of a and there's no demo\nper se but we'll talk about performance\nuh i've seen some questions\num from various people about performance\nlike how do you you know\nalter it so most folks when they start\noff with flight it continues to work\nis what i've noticed and then it\ncontinues to\nscale also for decent workloads but then\nyou start running it for extremely large\nworkloads and then there could be\nproblems\nand so it's important to optimize the\nperformance at that point\nand so what i'll do is i'll try to walk\nyou through how do you reason about the\nperformance of light and how do you\nreally\ntry and tweak it and improve it for your\nspecific use cases\nand what are the things we're working on\nfurther\nso for anybody who doesn't understand\nthe flight architecture here's a quick\nvery short overview uh essentially we\nhave the user plane where it's all the\nsdks console\nand then we have the control plane which\nis essentially storing all the inventory\ndata\nessentially all the workflows all your\nexecutions recording it showing it to\nthe users\nuh both the user plane and the control\nplane are not\nreally in the performance critical path\nand when i'm talking about the\nperformance i'm actually talking about\nhow fast is it to execute a workflow to\nschedule a job\nto run a node and so on\nand this is by design that they are not\nabsolutely critic in the critical path\nthere is one flow that is in the\ncritical path and we'll talk about it\nuh though the user plane has\nuh if you're using not using flight\ncopilot if you're using the cricket or\nflyq java they have their own data\nmanagement system so they do download\nand upload the data on their own\nthat is different performance in my\nopinion that is the performance of\nactually how fast you can get data from\ns3 or write to s3 or you know\nand can we use caching over there that's\nnot what i'm going to cover\nin today's topic this is purely the\nengine performance\num okay so that's what i did define\nuh and so one of the things is like\nthe reason why we went with the design\nuh of flight\nin the way that we talk in this way\nwhere we\nsee the details is because we are not\njust scheduling\nparts on kubernetes we are scheduling\nparts and communities we are scheduling\njobs on stage makers consuming jobs on\nbigquery and\nand and we wanted to actually optimize\nthe entire\nperformance as well as security of this\nentire system so that's a few reasons\nwhy we decided to\nfollow this path of developing flight\npropeller\nso again before we try and dive deeper\ninto the performance let's\nrecap on what exactly happens when you\nstart an execution\nwhen you start an execution flight admin\nis the end point that you communicate\nwith\nnow flight admin we call it like one\nservice is essentially a bunch of\nservices grouped together\ninto um behind a rest slash grpc\ninterface\nuh behind one ingress endpoint um\ntoday we write it like a monolith\nessentially but\nyou could imagine that it could be\neasily split up into multiple\nso let's think about the execution\nservice well it's called the execution\nservice\nfrom the uicli what happens over there\nis it retrieves a previously compiled\nworkflow closure converts it to a\nexecutable format and currently the\nexecutable format is targeted towards\nkubernetes\ncrds so it actually generates a crd\nobject\nand then submits it to kubernetes um\nwhen it's doing this it's actually also\ntaking the inputs that you passed in um\ndispatching them into the right system\nand and uh converting all the\nobjects so that uh like converting the\ngraph and\ndoing a final kind of a smoke check on\nthe graph and running it onto\nthe coconut is this is not touching any\nof your containers this is not touching\nany of your\nother metadata so at this point\nwhat has happened is now the crd is\navailable on kubernetes\nwhat happens next is the uh\nwe run this thing called as a control\nloop control loop is essentially\ncomes from control systems right it's uh\nyou you\nwant to take it what is sorry\noh sorry no no worries so crd stands for\ncustom\nresource definition in kubernetes um\nand please ask me questions if i'm going\nover some of the\nconcepts so uh kubernetes\nin my opinion kubernetes is essentially\nthis mega service\nthat allows you to define resources\narbitrary resources\nand some of them are first class and\nsome of them are you know second party\nresources\nthe first party resources are\nessentially\npods containers within pods deployments\nservices and so on but kubernetes allows\nyou to extend that api\nto add other services like you can now\nsay kubernetes\nrun a spark job kubernetes run um\na query in in bigquery kubernetes\nwhatever like\nand what the beauty of that thing is\nthen it allows you to\nspecify a control loop or an\nactor loop for a specific definition\nso let's say i wanted to check the\nweather\nuh every few minutes you can actually\nwrite that as a crd\nwithin uh within kubernetes i will not\nget into the details of how to do that\nthat's a completely different session\nbut essentially you can extend the api\nand then you can use cube ctl\ncube clients and all of the known\ntooling\nto access those services got it thanks\nso what we did is we essentially made uh\nflight\nworkflows as an uh crd now\nwe are debating whether that's that's a\ndecision that needs to be done at the\nmoment because uh\none of the things we wanted to do uh\ninitially when we started is like\nmake entire flight system completely\ncomposable so\nflight workflows with flight propeller\ncan be run\nindependently uh and so if you wanted to\nbring in a separate workflow engine you\ncould\nyou wouldn't get all the power of flight\nbut you could you know re-implement some\nof the protocols and\nyou would be able to run it the other\nreason why we did it is because\nkubernetes offers a very easy way of\nwriting control loops\nand uh rather than easy a canonical way\nof writing control groups and what are\ncontrol loops\nso you uh whenever like i said let's\ntake the example of i want to run\na job this is the request that comes\nfrom the user we call this as the\ndesired state\nand then the actual state of this is to\nrun the job\nbut the running the job may require us\nto let's say pull a container first\nso so you would first say go pull a\ncontainer\nand that might take a while so you you\ncould yield control\nand then after some time you can again\nlook at the desired state which is like\nrun a job\nand the current state which could be\npulling a container or maybe container\nwill complete it\nthen we might say run a command line\nempty point or something with it\nand then this is a hypothetical example\nbut that's what a control loop\nis if you're constantly looking at the\ndesired state and effecting\nan actual state uh and whenever you look\nat the uh\nbefore you create an effect on the\nactual state you look at the current\nactual state\ndiff it with the desired state and do\nonly the diff bits\nright so this this is a control loop\nit's a very simple pattern\nthat is very effective because it's it's\nresilient from failures right\nany time the desired state and the\nactual state drift you can reconcile\nthem\nto bring them together so but how does\nthis fit with flight workflows\nwhat we did is we model flight workflows\nas a\nas a desired state that we want to have\nan execution of the workflow\nand the actual is to run every single\nnode\nuh within the workflow the desired\nreally is\nthe output of a workflow right that's\nwhat we are saying\nhere's a workflow give me generate the\noutput and flight propeller is the one\nthat's running the control loop and\nconstantly evaluating and fixing the\nactual state which could be run the node\nrun the task on a big query all of that\nstuff and then\neffecting the actual which is the output\nwhich may take hours or days\nand it's okay it keeps on looping over\nand doing this again and again\nso uh if you know loops there is one\ninteresting thing this is a common\npattern that also happens in\nin on your phone every day on in your tv\nscreens\nright you are constantly running loops\nyou're reprinting the screen it's static\nimages that are run so fast that your\neyes cannot make a difference between\nthis is actually static or is it moving\nand this illusion of movement\nis because you you make every\nframe that's what we call it or every\nloop runs so fast\nthat uh and so many frames run within\none second that your eyes cannot make a\ndifference\nso that's called uh you know this is the\nrule of\nmost on the holy grail of rendering a\nbeautiful thing you run it at 60 frames\nper second\nbut the same thing doesn't quite\nliterally apply to us but it applies in\nsome sense\nwhere you want to make sure that the\nloops run really really fast\nbecause if the loops can run really fast\nwe can run\nlots of loops within one second one\nthread\ncould do um you know if you could do 60\nframes per second then you could do 60\nloops within one second\nright and that means you could run 60\nworkflows in one second\nand you would think that they are all\nrunning concurrently or with the or with\nthe one second delay\nso how do you uh so how does that relate\nto propeller so basically\nin one loop in flight propeller this is\nwhat it does it\nloads the entire workflow specification\nit\nloads the previously recorded state\nwhich is what's the workflow in a\nrunning state or what the workflow\nrunning a bigquery job or so on uh it\nobserves the real world state so it may\ncall it big query and select hey is the\njob running\nit it actually works with the data in\nsome cases because\nit has to collect the data from upstream\nnodes and pass them down into the\ndownstream nodes\nso it has to materialize the inputs\nessentially so it may have to do that\nyou may calculate the difference between\ni wanted to run two big big query jobs\none after the other\nand the difference is i ran the first\none so the difference is i have to now\nrun the second one\num and then it may also\nafter doing all of like and then it may\neffect the change that means you may run\nthe second big query job\nand then record that i ran this bigquery\njob\nand then finally also informs the\ncontrol plane that hey i've done all of\nthis\ngo show it to the user or go show it to\nthe observer\nthis seems a lot it's a lot of work that\nit has to do within one loop\ndoing this in super tiny amounts of time\nis extremely hard because\nif you think about it it's reading data\nfrom multiple databases\nit's reading data potentially from other\nservices which have\nlow throughput limits like maybe\nbigquery doesn't even allow more than 30\nrequests per second\ni'm giving an example i know some\nservices that don't and so\nhow do you make that work so\nwhat do we uh so again\nwhy why is the performance can be slower\nbecause you know loading and storing\ncan be slow observing can be slow um\nand and one thing is true we have to\ncreate an effect\nand again remember creating an effect is\nlike launching a pod launcher query\nthe goal of life propeller is to do all\nof these things\nso the performance penalties over here\nthere is no way of hiding that\nit's just going to happen but everything\nelse we can do something about it and\nlet's see what we do so first thing\nflight propeller if you start reading\nthe code you'll just notice there are\ncaches everywhere\nand the reason why we have so many\ncaches\nis because we want to hide the penalty\nof loading data from any remote system\nand but caches are dangerous right like\nwe all know that caches can be very\ndangerous the only way caches can be\ndangerous if you start mutating the data\nbehind the cache\nso flight propeller works on the\nprinciples of immutability almost\nonce it touches a data bit it never\ntouches it again\nso let's say it works on a node and it\ngenerates an output\nwe know that that node's output will not\nbe regenerated\nso we just store the output in a place\nand we never\ntouch it again now when i'm storing it i\ncould cache it\nthrough a you know right through cache\nwhich so that subsequent reads are\naccelerated\nsame thing happens when you when you're\nreading data if you know that it's not\nmutated\nyou could just cache it in memory by\nusing a read through cache and then\nmultiple downstream reads will be much\nmuch faster\nso currently the uh and so and i'll show\nyou\nsome graphs in the end and there are\ngraphs available for all of this\nthe the most\nthe observe or the desirable um cash hit\nrate\nis 100 it's impossible because we have\nalways have to generate new data and so\non\nbut we observe upwards of ninety percent\nin crashing\nthe second thing that happens is how\nfast can we inform the control plane\nuh about um hey i have observed this\nlike you know i'm done this happens for\nevery single node transition every\nsingle workflow transition\nuh so this is one place where the\ncontrol plane is in the critical path\nso if your control plane goes down\nflight propeller will pause\nit will stop sending events and you'll\nstart making progress\nand the reason is because we think if\nyou cannot record\nuh the the progress then into the final\nstate\nof the local state then it's it's\nessentially useless progress\nso there is no need of making that\nprogress this doesn't mean your jobs if\nthey're running\nwill halt they will not only the\nrecording of the progress\nwill be halted um so this usually is\nless than 10 milliseconds\nuh so one of the ways to solve this\nproblem\nis uh make sure your database is uh\nwell sized so if you're running large\nworkloads then the postgres database\nshould be\ndecent sized um don't run it on a t1\nmicro\nand expect like extremely high\nperformance uh\nbut you rule of thumb you can look at\nthe graph again\nthe latency should always be less than\n10 milliseconds it's usually in the\nin the single digit milliseconds of four\nto five milliseconds in p99 in case\nuh another thing is use some sort of\nservice mesh if you're within kx the\ndefault kx proxy will bind to one\nhost on on admin and if you're running\nhundreds of thousands of workflows\nit will be a problem because that\nactually becomes a network\nuh problem within that one host\nthe the second thing is how do you make\nlike the i said the effect there is no\nway out of doing the effect so\nthe only way to do it is to make sure\nthat you can\ncreate as many effects as you can within\none second\nwhat that means is you should have\nhigher bandwidth communicating to\nkubernetes so if you're launching\nhundred thousand parts\nyou should be able to launch hundred\nthousand pods at some\nin some in a reasonable amount of time\nthe default kubernetes client is\nconfigured to do only 20 requests per\nsecond or 10 requests per second\nso if you calculate by that anyone\nlaunch hundred thousand parts\nit will take a long long long time and\nthat's not really flight preparers for\nit it's essentially cuba\nis protecting itself from launching too\nmany parts\nuh same thing is true with the queries\nand things through it\nuh all of those things and what flight\npropeller does is essentially creates a\nbarrier around\nall of these services so that you don't\nbrown out any of these services it will\nbreak limit everything\nthis pr has a bunch of these tweaks\nin the documentation and it's getting\nmerged soon it allow it\ntells you that how do you upgrade the\nlet's say the kubernetes client\nwe have safely increased the kubernetes\nclient to do more than 3 400 requests\nper second\nbut that's not required 3 400 is pretty\nhigh\nyou can just do 100 with a burst size of\n50. and that works really really well\nanother thing is you may if if you're\nrunning your own kubernetes cluster\nmake sure that the lcd is running\non like a nvme drive\nso that the performance of xtv itself is\npretty good\nthe other thing that and these are all\nlike some of these are already\nimplemented internally um\nso what flight propeller does is like\nwe said that we have to actually fetch\nthe workflow and we have to fetch the\nstate of the workflow\nso instead of just directly fetching the\nworkflow from cube api\nalways we have a cache uh it's it's a\npattern called as the informer cache\nand these are in memory caches with the\nworkflows uh in there and they are\nasynchronously synced\nand they are synced using a stream\nprotocol so every time\nsomebody uh creates a new workflow\nflight propeller doesn't hold for it\nflight property just gets it using a\nstream api\nand it keeps it in memory after that\nit also we've also taken this pattern\nand further implemented it for all\nexternal services so if you use the\nflight plug-ins uh web api module\nor if you use the um kubernetes api\nmodule we are automatically\nusing informal caches for all external\nservices\nso what happens essentially the idea is\nthat if you are using bigquery and you\nwant to update the status of the query\nall the time instead of calling bigquery\nyou consult the cache\nand the cache updates itself\nasynchronously\nuh from the background uh and that way\nyou can have limited number of queries\ngoing to bigquery\nand even if you are not like even if you\nare not really within the loop of\nchecking\nthe cash you will asynchronously be\nupdating the cash so next time when you\ncome and check the cash it's own it's an\nin-memory lookup it's\nextremely fast\nthe other thing that we realized was\nlike but caches come with eventual\nconsistency\nright you bring in caches that means\nasynchronous caches especially\nthat means you got everything into the\ncache\nbut you have to wait for the next cycle\nto get everything into the cache again\nand that causes redundant loops what\nthat means is like every time you run a\nloop\nyou may not have an update and so\nand you may actually be looking at all\nthe states later so to prevent that\nwe actually use um an interesting\nuh thing within the system so\nhcd stores everything as a multiversion\nconcurrency\nuh there is a multiversion concurrency\ndata store that means\neverything is versioned monotonically\nalmost\nso what we do is we we look at the\nversion numbers and we\nplay a trick if we know that our version\nnumber is\nolder than the last written version\nnumber\nor it's equal to the version number in\nour cache\nwe just drop weight on the floor we say\nthat we have to wait for the\nsync and this reduces the total number\nof cycles we're running and that\ninterestingly improves the throughput of\nlight propeller\num and this one\nis another thing we realize is that when\nwe actually affect a workflow\nwe know the full state of the new of the\nupdate\nupdated state of the workflow what that\nmeans is let's say i ran for\nnodes uh i know that i ran these four\nnodes\ni know they were launched four big query\njobs were launched\ni know that they must be running now\nso i don't really need to wait and\nretrieve the result\nfrom hcd what we have it in memory so as\nlong as flight propeller continues to\nrun\ndeployments will change that but as long\nas it continues to run it\nuses this in-memory copy to run another\nloop\nand it does this as fast as it can\nto make sure that the workflow can make\nas much progress as it can\nwhile also being resilient from failures\nbecause it always records the\nstate in between and this can be\nadjusted using mac streak length\nagain it's not part of the pr um it you\nhave to be careful with it\nif you make so i'll give you an example\nlet's say\nwe have a three-step workflow where all\nthe three steps are memorized\nthen uh you you\nprobably if you set the max peak length\nto whatever n uh which is maybe 15\nit will go through the entire workflow\nin in\nless than 5 milliseconds or 10\nmilliseconds or some some number which\nis\ndependent on cache lookup speed\nflight propeller is also a single\nruns as a single process today in the\ndefault mode\nand it uses go routines to run multiple\nworkers within it\nand go routines if you know are not\nthreads they are i o\nuh they are like they use the concurrent\nscheduling pattern uh and so\nwhenever there's an i o it will yield to\nanother go routine\nand hence you can run lots of them in\nif you have n number of cores so let's\nassume you have four cores you can have\n500\nuh workers but it doesn't mean you can\nrun infinite number of workers\nbecause there's only diminishing return\nafter some point\nso again in the pr dock we have an\nexample of how many to run\num you how to how to alter them default\nis around 20\nbut you can easily change them we have\ntested\naround 500 and it works absolutely fine\nthe other problem that you will see and\nthis is a this is a user problem\nuh i mean a user uh\nfacing problem uh they create a workflow\nuh\nwhich has a very high span out now what\nthat does is\nfor one loop now i have to let's say i\nhave completed the start node\nthen in my next loop i have to evaluate\nall of these work\nall of these nodes in parallel in\nparallel and sequentially one after the\nother because flight propeller only runs\nsequentially for one workflow\nthen if you have hundred thousand nodes\nhere it will take a long time to\nevaluate them\nspecifically if you're causing an effect\nlet's say you're calling cube api server\nnow there is a rate limiter on keyboard\nserver and you can only do 10 requests\nper second\nto go through all of them will take a\nlong long long time\nand that will slow down the refresh rate\nhence cause\njarring and performance is slower so the\nsolution for that is we should\nlim we limit the total concurrency on\nhow many\nnodes within one level can be evaluated\nby flight propeller\nand this is um already in\nwe just have not exposed it through the\nexecution endpoint and we're going to\nexpose it on the execution endpoint so\nwhen you launch an execution you could\nsay\ni want some cap on the max concurrency\nand there will be global defaults\nbut the other better solution in this\ncase is to use\nwhat we call as map tasks map tasks are\ninteresting especially when the same\ntask\nis repeated hundreds of thousands of\ntimes so let's say you are trying to\nbuild a model for\n10 regions and 100 regions 1 000 regions\nor you are trying to build the map of\nthe world and you are breaking the world\ninto like 100 000 pieces\nit's essentially running the same job of\nmapping and\nyou are passing just different data so a\nbetter way to do that is run\nmap tasks please refer to the\ndocumentation for it um\nthe reason why it's more efficient and\neffective is because there is a very\nuh simplified state storage that happens\nwithin five propeller\ninstead of taking many many megabytes\nand kilobyted stores\nit stores in one sub 10 kilobytes\nof the entire state which is which is an\noptimization that we have done and i\ndon't think anybody else does it\nit was done specifically for map tasks\nanother thing that it does\nis it automatically enqueues instead of\nrunning\nthem in parallel it enqueues by creating\na concurrency control in there\num but besides loop performance there\nare a few other things that you have to\nbe careful about one is the size of the\nworkflow itself cannot exceed two\nmegabytes\nand this is a limitation on xcd uh lcd\nmax object size is only 2 megabytes and\nso we've seen that this gets breached if\nyou have a task\nwith unique tasks in thousands of tasks\nor if you have an individual task where\nyou use extremely large queries and i've\nseen queries like which are in hundreds\nof kb that not even tens of kps\nwhich can you know you add a few hundred\nof those queries and\nyou will basically breach the threshold\nbut a solution for that is use dynamic\nworkflows dynamic workflows don't use\nthe xcd cache um hcd store to store\nthose they\nthey get uploaded to blob stores uh so\nthat makes them really\npossible to write extremely large\nworkflows in dynamic workforce\nand coming later we are going to offload\nall workflows and that what that does is\nallows you to\nuh not rely on series dependent\nokay so now was a lot of stuff um\nwhat do you do do not optimize anything\ntill you measure\nso we publish a bunch of flight\ndashboards and\nhere's uh them on grafana labs just\ndownload them\ninstall them and use them to understand\nthe performance of your\nflight deployment uh\nmost we are following a model for having\nthe\nbest defaults which are like any other\nsystem defaults that work in most cases\nthey may not be the most performant but\nthey will get your job done\nso we have we are doing that throughout\nthe system\nthis pr will get merged soon which\nactually gives you these knobs\nthat you know or use your documentation\nto know which knobs to\ntweak and don't be shy to ask on track\nthis is complicated stuff\ni completely understand it's um can deep\nuh it can be very deep so please ask\nquestions\nas a benchmark um i ran this on a\nsingle propeller with two cores and two\ngigs of ram\nwe ran about 10 000 workflows\nconcurrently\nand what i had done is created a fake\nload that\nactually blocked a worker in propeller\nfor one second you just put it to sleep\num\nso if i have 100 workers if i start\nrunning hundred workflows\n100 of them just sleep for one second\neven with all of that we were we had an\noverhead of less than nine seconds\num for completing all the workflows\nwhich means\nit was purely blocked on the one second\nsleep\num in conclusion like one profile is\nfrequently fast you can really run\nmore than 10 000 workflows in one\npropeller with minimal memory footprint\nand cpu usage and there's only c2 cpu\nyou can\nraise it to 4 cpus and increase the ram\nand increase the number of workers\nbut that's not enough we need to scale\nout flight propeller already supports\num sharding by namespace that's trivial\njust add the hyphen iphone namespace\nflag\nor we are actually working on using kx\nleases and dynamically\nuh scaling up and down based on the load\nuh we've not done that because it makes\nit complicated for users to understand\nany kind of failure scenarios um so look\nout for that but by the end of\nby mid or towards the end of the year we\nshould have this out\nwe should automatically scale up uh\nwe still think the the bottleneck is\ngoing to be cube api\nso we should work on that a little bit\nmore but\nother than that we should be able to run\nhundreds of thousands of workflows\nconfidently\nand that's it it's a deep dive these\nslides will be shared\nand please ask me any questions\nthank you very much\nhey kitten um if\ni am a workflow owner\ndoes this help me in identifying\nbottlenecks for my workflow\nand for me the simplest model is that\ni wanted hundred thousand i requested\n100\n000 containers i got 10 000\nand it's kind of blocked on that one\nbecause other people have taken the\nresources but\ncan i see where my workflow has been\nkind of not getting enough resources and\ntherefore not progressing fast enough\nusing this framework yeah so the\nexample dashboards that i that we have\nshared on\nprofana labs let me again share my\nscreen\nso this dashboard there's a dashboard\ncalled as the user dashboard this was\nactually a problem that\nuh adam on the slack channel had like\ntwo days ago\nwhere uh he had a\nworkflow that was running\nvery slowly according to him he was\nrunning under or ten thousand a thousand\ntasks sorry and they were\nrunning slowly and what the reason was\nnot properly the reason was the\nkubernetes quotas for the namespace that\nwe had were were said to be very low\nthey were said to be like 10 10 cpus or\nsomething and\nso you could not use more than 10 cpus\num and so flight profiler was backing\noff saying that there's not enough cpus\navailable\nbut if you look at the user dashboard it\ndoes come up with\nlet me open up the dashboard um\nso you can actually start uh finding uh\nyour\nworkflows here right it's not showing\nanything you're not running anything\nit's a demo\nbut it can show you the current quota\nusage for your\nuh project uh and domain combination\nyou can show your current task stats how\nmany are pending and what's happening\nuh i don't know if i've seen this one or\nnot but\neffectively does it show that my\nworkflow actually\nneeds 10 000 uh tasks or it is basically\nmy tasks are waiting\nit shows like what is being executed uh\nthe fact that overall quota how well it\nis utilized or how much of headroom is\nthere\nbut does it actually show that my\nworkflow needs x amount and\nout of that x 25 has only been allocated\nthat's the kind of thing that i wanted\nto know about\noh like the current progress\nis what you want current request versus\ncurrent allocation\nyeah uh so that it should show right\nlike this is the request versus\nlimit right this is the max usage versus\nthe actual header\nuh the cap what i think is the other\nthing that you probably need\nis uh in the console\nwe have actually been working on some\nimprovement to the console and and they\nshould\nnot to the control with the backend\nsystem to actually show\nyou uh more details of when you run a\nworkflow\nuh so uh\nso if you run an extremely large\nworkflow we you may want to\njust want to find an example it's always\nhard to find one uh\nokay i'm not finding one uh but like our\nextremely large workflows\nuh we do show that these are all queued\nwe are proving the messaging in there\nlike why is it cute like for example is\nit waiting for resources\nis the gpu is not available are you\ntrying to schedule for\nsix cpus and the max number of cpus on\nany nodes in your\ncluster is only two so we can't really\nschedule it\nso all of that is coming in the ui it's\nit's available in the control plane it's\njust not shown in the ui\nthe ui is a little behind okay\nsounds good thanks great question\nanything else\nyeah one more information i needed\nyes so yeah so actually\nin my company we were using the netflix\nconductor\nhere and i saw that more of the\nbasic components are same that flight is\nusing and but uh\ni can see flight has this cache here and\neverything you showed\nthat is pretty good so is there any uh\ncomparison study\njust to get a benchmark or something\nlike\nuh we are actually working on publishing\na comparison but we won't be comparing\nwith\neverything uh yeah yeah\nit's extreme just get a benchmark yeah\nso\nfrom my point of view netflix conductor\nis more of a micro services orchestrator\num and flight is more of a like a\nworkload orchestrated cycling\nalso actually we used necklace conductor\nas a\nas a base we built on top of that to\nmake it uh use as a metal ops also\nokay yeah yeah so\nthis is the first time i'm hearing\nsomebody using this conductor for that\nso\nuh it's great uh but yeah as as you can\ntell i\nwon't be able to be like i don't think\nthe community will be able to compare\nevery single\nuh schedule but we will be comparing\nwith the majority and which we have been\ndoing like one of them is airflow\nanother one is and that we've been\ncomparing with is\nuh argo i think and another one is\num q flow pipelines or something which\nis\nhardwood is it there in documentation of\nlight\nif you notice nobody else has compared\nanything you know because you have to be\ncomprehensive and clear\nand so we are trying to be as correct as\nwe can\nwe'll also come with recommendations\nessentially like you know if you are\nrunning some sort of\nuh queries where you don't need data to\nbe passed or so on property airflow is\nbetter right in terms of the maturity of\nthe and the ecosystem\num and yeah so we'll be doing that\nplease stay tuned and if you can help\nwith like comparing us with you know\nuh netflix conductor we would love to\nhelp you\nget the data and get there yeah sure\nthanks thanks for that yeah\nuh yeah for for a timeline on the\ncomparison let's\nuh say bye in by the next\nmeeting we'll have at least a draft in\nthe documentation\nwhich is two weeks from now\num anything else\nby the way suraj which company do you\nwork for if you don't mind\nsharing general electric healthcare\noh awesome nice to see you guys so yeah\nwe were looking into\nflight and i think if we find it better\nwe'll migrate\noh thank you so much yeah let let me\nknow how we can help we can also do a\ndeep dive on like a total session for\nyou guys just so that\nyou guys can understand all the\ndifferences\nokay so yeah performance related\nproblems definitely raised to us\nuh we care deeply about the performance\nthis has been working at scale\nat lyft and other places so\nnow but like you know it's a pro\nperformance is always a progressive\nproblem like\nlike on day one you don't feel anything\non day two we probably feel a little bit\nand then\nthen when you really are going to run a\nhundred thousand workflows or something\nthat's when you start feeling it\nso we want to be we want to help you get\nthat point\nsmoothly sounds good\ncool\nthanks everyone i want to be sensitive\nto not going over an hour if we can\nso we're pushing um\nuh that was a lot of information it was\na great overview\nof just how propeller works which was\nsuper so not only the performance but it\nwas good\nconceptual overview from um\nthat's up anybody want to present next\nweek please drop sandra or two weeks\nfrom now drop sandra or me a note\nand um we'd love to see some of your\nstuff in your internal implications\nespecially and see what crazy things\npeople are doing\num and uh thanks everybody for joining\nthank you\n[Music]\nhi everyone hi everyone okay"
    },
    {
        "title": "FlyteMeet 2021/04/20",
        "transcript": "hey how's it going guys\ngive us a few more minutes max yeah i'm\ngetting a bit set up too\ngood morning everybody good morning\ngood evening what's up\ni think we have a quorum um today i\nthink\nmax was going to show some of the adult\nintegration i believe that's on the\nthing and then\ncatherine and uh we're gonna go over\nthe magic of helm\nso max do you want to take it away\nyeah sure uh thank you all for having me\nuh so i'm going to i'm going to try to\nbe brief i'm happy to you know go into\ndetails as much as necessary\nor talk about dope more if you guys are\ninterested in that\nbut what i um what i've built\nis an sql alchemy specific integration\ni think there's been a little bit of\nconfusion about it so i posted a link\nto a document in the chat that gives a\nbrief summary\ni had a little trouble running the\ndocker image to show a live demo\nunfortunately but the the core of the\nintegration is fairly simple i think the\ndifficulty is understanding sort of the\nintersection of\nthis database management software in\npython so\ni guess as i wrote in that doc i sort of\nthink of\nsql alchemy as a couple different parts\nbut it's sort of like\nto sql databases in python as pandas is\nto data frames\nsql alchemy is sort of this higher level\ninterface that library users use to\naccess like arbitrary data sources\nin sql format um so the comparison is\nsort of like\nyou use pandas data frames pandas\naccesses a lot of different data sources\ncsv json\nit for the most part lets you not worry\nabout what the data source is\nand where your data is coming from so\nfor flight\nflight is a library and flight is not\nopinionated about what data sources\nits users are using right they may be\nusing postgres\nor mysql or oracle\nand so sql alchemy that single\nintegration basically lets you connect\nto an arbitrary set of databases\nbecause sql alchemy has logic to\nsort of handle all the different use\ncases for a variety of\nbackend databases like that's what sql\ncompanies\nuse is another example so like\nsome of you probably use django and\ndjango's logic is tied pretty closely i\nthink\nto my sequel um if i'm not mistaken\nsomeone on my team knows\nmore about that than me but uh i guess\nsomeone on my team is having a lot of\ndifficulties with\nwith django because it's so tied\nspecifically to the logic of one\ndatabase\num some of you may have used fast api\nit's sort of\nsimilar to django it's like another web\nserver in python\nand fast api's approach was to use sql\nalchemy as an integration\nso then their users don't have to use\nmysql as a back end\nthey can bring their own database fast\napi's code is generic\nso that's sort of uh that's i think the\nhardest part about understanding like\nwhat this integration does so all i did\nin the code\nwas create an sql alchemy task you give\nit\num the the uri for your specific\ndatabase\nwhich is composed of a couple different\nparts if you see the first\nexample uh oh i could share my screen\ntoo that might make it easier for you to\nfollow my\npointer\nokay so uh this is the entirety\nof of what a user needs to do to connect\nto their database\nso here we have the dialect we have a\nspecific\npython module for that dialect so mysql\nhas a couple different python\nlibraries that have small differences\nbetween them pi mysql\npython my sql connector um but basically\nthis is just telling us mysql and we're\ngoing to use pi mysql as\nthe underlying like python connection\nand then this\njust points us to our database so this\ncould be an\nuh you know an rds instance running in\namazon it can be running\nlocalhost like i'm showing here but uh\nyou have this database uri you pass it\nto a task and you can give it um\nwell in this case like a select but\nbasic\nbasically it just um\nwhat happens under underneath is sql\nalchemy will connect to the database\nand it abstracts sending the right\nselect statement to your database\ndepending on\nlike what you've configured here and it\nshould work exactly similarly to the\nsqlite3\ntask that you guys have existing in your\ndatabase\nwith that task you you know you point it\nto a database file\nand you do the exact same thing so i\nkept the config as similar\nas possible to the sqlite task\nand that's that's i mean that's most of\nit\nso katan i hope i'm saying your name\nright sorry if i mispronounced that\nuh so i added i added some secret\nsupport too\nso one of the reasons so we still have a\nlittle bit of\nuh i think edge cases to hammer out\nbefore the pr is merged\neven though the core of it's very simple\ntesting gets a little tricky when\nthere's so many different databases in\nthe back end\nand just integration testing databases\ncan be a little tricky in itself\nbut something that you couldn't do with\nsqlite that you can do with most other\ndatabases\nis have fine configuration over\nlike usernames and passwords and\nconnection\ncredentials so we exposed a way for you\nto\nhave secrets in your environment\nand this is the flight syntax for\ndeclaring like a secret\ndata class and you can pass those as\noverride arguments so that you're not\nyou know\nencoding like protected passwords in\nthis uri string\nso that is that is most the entirety how\nyou use this in a task\nor i guess in a workflow sorry is as you\ndeclare a task\nyou have this workflow dependency\nand because we're returning data frames\nfrom this task\nuh you you flight does its magic and you\nget a data frame\ni guess that's uh so again none of this\nis adult specific\nthere's a little bit of confusion about\nthis this should be you know useful for\npostgres my sequel\netc i just need to do a little bit of\nwork\nmore to get the uh yeah tests\nto the satisfactory to the flight core\nteam before we can merge this\num yeah hopefully hopefully that was\npretty brief i'm happy to go into more\ndetails i can also\nmaybe i should give a little background\nand adult in myself too\ni could probably could have started with\nthat but i'm a software engineer at\ndalton i'm not here primarily to\nlike this is this should be useful to\nothers not not using dolt\nthis should just be generally useful to\nsql database users but adulthub\nis an sql database and that's something\nthat i work on and i work on\nintegrations\nwhich probably makes sense given the\ncontext of this talk uh\nso maybe a 30 second on adult dolt\nis a a version control like\ndatabase with uh that is sql compliant\nso that is complicated because it has\nseveral meanings\nbut that means on one hand\ndolt has created this storage engine so\nthey've taken\nthe way that git stores data and deltas\nand blobs\nuh in this commit hash format and\nthey've optimized it\nfor tabular data rows in the database\nand\nfrom from that like lower level like\ndata storage format they've built the\nprimitives\nin order to also expose it as a mysql\ncompliant database so don't can run as a\nserver\nand in that server you can change rows\nyou can add tables you can do schema\nmutations\nand you can add commits and create this\ncommit graph the exact same\nway you would with git except\nyou know you have both everything you\nwould expect from git and everything you\nexpect from sql\nso that that is\ni think i think i've covered everything\nthat i've wanted to um\ni'm happy to answer any other questions\nbut otherwise\ni'll probably give it back to you guys\nyou guys should call the blockchain\nit'll be worth three times as much\nuh there's yeah i mean github\ngit is a blockchain i mean\n[Laughter]\nso get as good as the original\nblockchain i i'm not sure we want to\nbrand ourselves that way\ni think that sounds awesome something\nthat's cool\ncheck it out hey max i had a question\nabout the\nabout its way of handling the versions\nyou\ncan you uh select buy a version\nyeah you can you can select from um as\nof so you would do let's see select\nextension on the query yeah so you can\nsay as of\nand you put a commit here i'm hoping to\nget\nso i've spent a lot of the last week\ntrying to formalize\nuh\n[Music]\nyeah yeah so you would just you would\njust do an as of commit here\nand i think we might have branch support\ntoo so you might be able to say like new\nbranch\num you can do it there are a lot of\nweird ways that you can like insert\nget style stuff into uh sql commands\nwith\nthought that's really cool\nyeah doing this with performance is\nprobably extremely hard\nthat's awesome yeah i'm i'm definitely\nnot the one\nwho's best to ask about that but we have\nsome really smart guys\nwho have been spending years optimizing\nthe performance of the storage engine in\norder for me to be able to show you guys\ncool stuff\num and uh just another question on door\ntop so is that a completely hosted\nmy is that is it free for some\ni see some like data sets over there is\nactually for those open source data sets\nor what\nyeah yeah it's free it's free for open\nsource data sets it's free for private\ndata sets too\ni think we recently um let that open\nit's we do have hosted options we have a\nhandful of companies that do\nlike private hosted i think we're still\nworking out our funding model there\nuh but yeah it works we\nbasically our founder is very set on\ndoing everything you would expect github\nto do so if you want hosted\nprivate data sets or you want enterprise\nversion\non-prem we're sort of\nflexible and supporting what people want\nany other questions from\ni guess the there was a there is a team\nthat wants\nintegration with sql style databases\nright they're not here but\nuh so basically this adds that\nintegration\nplus don't so it's awesome\nthank you\num yeah george to you\nyeah i think um lester any more\nquestions thank you max\nthat's awesome uh i think\nis it kaithin and sornero yeah\nuh serena do you want me to talk or do\nyou want to talk\nto you yeah sugar go on yeah you'll have\nto button and help me\nbut i don't have a present i actually do\nhave a presentation but that's what like\nthree slides that i just wrote\nwhile max was talking thank you max um\nit was uh essentially\nwe have as a\nuh so last few weeks we've been thinking\nabout moving to\nuh or we've been talking with various\nfolks within the\nsorry one minute yeah uh last few weeks\nwe've been\ni think soren probably demoed or talked\nabout him\nmaybe a month or two months ago and\nin the last few weeks we followed up\nwith a bunch of\nfolks who have deployed it in flight in\nproduction and we asked them what do\nthey use\nwhat do they prefer and the answers were\nvery very surprising\nto me some of them i think\nthere are two teams that use customized\nbut most of the others use\ncompletely uh self-described deployment\nsystems that means\nthey are actually handwriting the ammo\nor or\nsomething to that effect and using\nterraform sometimes using\nyaml directly in some cases also they\nare updating the config completely\nautomatically\nand and\nwhat that led me to understand is that\nmost folks who are going to deploy\nflight in production\num they actually take it on themselves\nto add\nto convert flight into the way if it's\nin their production ecosystem and and\nthere is no one size fits all in the\nproduction ecosystem\nthere are people doing all kinds of\nthings so then we started thinking about\nlike okay what is the\nway that we can get new users the most\nvalue\nfrom flight how do we get them to start\nusing flight really quickly\nuh and uh\nlet's let's try and understand but why\nwe you know thought about for help\neven so flight is so extremely\nextensible\nthat it's a problem um it's like it just\nhas configuration all over the place in\nthe back end\nand what has happened um\nit's a reflection on or not\nunderstanding who the user is for the\nconfiguration\nuh when we built the configuration we\ndid not\nor we do not actively as a community\nthink about\nwho's consuming the configuration and\nthen the configuration is an extremely\nimportant consumption point\num so what happens is sometimes we have\nand maybe let me\ntry and open up a config to show you an\nexample\nof what i mean\nso\nokay so uh this configuration is for\nsending out notifications from flight\nline number\n313 to 330.\nif you see it talks about notification\nthis is\nspecific to aw so we do a great job of\nlike allowing people to configure aws\nseparately in gcp separate\nthe problem is we don't know what this\nstructure is right as a user\nso that needs documentation but okay\ndocumentation decides the point\nthe structure of this is also very\nconfusing because there is a publisher\nand then there is a processor and we\nhave\nno idea as a user like why is there a\npublisher and a processor in a workflow\nnotification system right\nfrom a point of view of the user it\nshould have been hey here's a\nqueue send my information to me here's\nthe email body that i want\nand cool that's it but that's not the\ncase so\none of the things going forward of\nbringing this up today is\nwhenever we are writing extra\nconfiguration bits we should think about\nwho the user is and the user is is like\nalmost like a user of like it we think\nabout\nusers of flightgate we try to make it\nsimple we try to make it understandable\ni think we should do the same for\nconfiguration as well\nbut how can we systematically improve\nthis and that's where we started\nthinking about him\nand and soren and ruslan did all the\nwork actually i am just kind of\ntalking here so um what we\ndid is we started thinking what if we\nwere able to break down helm\nuh our our deployment system into\ninto a basic uh outline of what we think\nright\nright consists of components right and\nthese are\nthings like flight admin\npropeller and so on right it consists of\ndependencies so we need\ns3 gcs one of them right for example\nuh we need a database\nuh we have some core configuration like\nnumber of threads to the\nnumber of replicas to run and so on and\nthen we have uh backhand plugins like\nspark\nlink for example maybe one day uh in\nback end\nuh so etc right so\nwhat if we lay out the structure to\nmatch this way\nand then allow users to pick and choose\nwhat they want to deploy\nand to do this we i started\nexperimenting\nwith some stuff and this is the branch\nthat we're working on\nit's called the helm working branch it's\nworking and i'll do a demo of\nhow it looks and works um but\nthough some cool things about him once\nyou create the configuration\nit automatically generates a\ndocumentation for you so in this case it\nhas generated the documentation\nof what are the various configuration\nknobs that you need that you have\nat your disposal it also gives you\nexample configuration so in some of our\ncases most of our config is yaml so it\nconverts it into\na json um not that useful but it can be\nmade useful and i'll show you an example\nso this is the configuration for flight\npropeller\nand what you can see is we can put a\ndescription that says core propeller\nconfiguration follows the structure\nspecified here\nand if you click on that\nyou see an example of the configuration\nand ideally and\nthis is one of the pieces that i'm\nworking on is that every configuration\npiece has such an example and this\nexample is automatically tested\nbecause we can write a golang test\nexample\nthat way the users can see what is what\nare all the fields that are possible in\nthe configuration\nhow do you configure here's an example\nand the actual fields are below\num just just described below in the\nconflict structure\nso that way at the point of creating the\nconfig the users have access to the\ndescription of the configuration and\nthis is one of the things that we're\nworking on\nso once you have this how do you really\nuse help uh\nfor it's a one second overview a hem\nconsists of\nthese thing called as values values is\nessentially all the configuration knobs\nthat you can tweak\nand templates for most users you don't\nneed to worry about the templates\ntemplates are like\ndeployments and they get auto generated\nusing go templating language\nbut values is uh the most important\nthing when\nwhen new users want to install flight\nand the good part about values is that\nit actually\nallows you to have defaults for\neverything so what we have done is we've\ncreated a values.demo\nthat follows the structure that i just\nlaid out in the in the presentation\nso if you go through it these are all\nthe components\nand all of them have some same defaults\nalready specified so flight admin\nuh you can use the\nthe image to use the resources\ndata catalog for propeller and do you\nwant caching\nfor console and then you have other\nsettings like dependencies\nuh or in specific in sandbox case we\nhave dependencies that\nshould not be used in production but we\ndo have dependencies so that would be\nredis\npostgres mineo contour\nuh if you want to deploy kubernetes\ndashboard\nand then uh the actual core dependencies\nof flight which is\nuh storage and db only two real\ndependencies that we need\nthe storage is sv or gcs and db\nand then comes down a bunch of\nconfiguration and the entire\nconfiguration for\nvarious components of flight is in one\nbig configuration and the way\ni'm improving this now is that for every\nconfiguration there is an associated\nlink\nto that place that i showed you so that\ngenerates the documentation\nautomatically so from here you can jump\nto the actual code\nand it started documenting and but more\nuh most most importantly all of them\nhave same defaults and we'll see how the\nsame defaults come into play\nuh and and then you can add other\noptional\nsystem components like workflow\nscheduler\nworkflow notifications uh cluster\nresource manager\num and then if you want to add plugins\nyou can add spark operator you can add\npython operator and so on\nnow let's say we want to build a config\nfor sandbox\nand here's an example for that config\nyou actually don't even need to specify\nthese but um\nyou can say that okay these are all\ndefaults i'm not changing anything\neverything's just the default oh i\nwanted to add a different tag\nfor the image that i want to use\n[Music]\nand i want to use kubernetes dashboard\nand i want to use these specific configs\nso i enabled it besides that\ni wanted to change the storage so i want\nto use my own\ns3 bucket the default for uh storage\nconfiguration allows you to have\nminio as the sandbox default so that's\nwhat it's doing\nuh and the database configuration\nand if you want you can now optionally\nand this is just to show that hey i'm\nupdating the s3 bucket\ni can actually change my task resource\ndefaults\ni can change the default environment\nvariables that should be injected\nand that's about it this is all the\nconfiguration of like\nnow including flight admin data catalogs\nlike propeller\nall of it and i don't need any scheduler\ni don't need any notifications\ni'm keeping the cluster resource manager\nas is\nand i don't want spark operator and\nbacktouch so this is the total\nconfiguration of flight and\n[Music]\nnow if i want to install it\nwe say help install\nit doesn't actually by default we don't\ncreate a namespace you give it a\nnamespace and you say create a namespace\nand you can give it the values file in\nthis case the values file is value\nsandbox\ndot and you call the deployment a name\nin this case we are calling it flight\nand you give it the path where i find\nthe helm\nchart and go for it\nthat shows you everything's coming up\nuh it will take a bit let's make sure\nbut as it comes up uh your ui should be\nexposed on localhost 3008. so this was\nsandbox configuration\nfrom helm and the reason why we actually\nwent with them is this\nit for most users of\npython pip install is the way that they\nwanna you know\ninteract with the problem and that's why\nfor a system like\nflight which is used as google related\nit's extremely complex to use\npip install because we want to install a\nbunch of components in kubernetes\nso we wanted to give them analogous\nthing to pip install and that's help\ninstall in my opinion\nagain there is no right answer this is\none choice that we are making we are\nstill keeping customized\nuh till we absolutely are stable with\nhim\nthen it comes a decision and we want to\nopen this up to the communities like how\ndo we want to\nmove forward uh with customize and him\ni don't think we can maintain both\ncompletely all the time\nuh we can always make one primary and\nthe second secondary\nwhich means that the primary will be\nmaintained\nand the secondary will need help from\nuh community to maintain it\ni think both can be maintained as a\nregression suite\nuh so we should be able to figure out\nbugs and problems and things like that\nbut\nit's still harder uh to do so that's the\ncurrent take and let's see if the system\nyeah everything's running\ni can now again share this thing\nyep so that's up so it came up from\nuh him with minimal changes\nhopefully uh and the idea is\nthe sandbox ammo will be the default\nammo\nso when you do help install flight you\nwill just get sandbox by default\nuh and i'll show you an example of how\neks looks\nand this is mostly working so\nyou can add annotations because that's\nwhat i am need you can add the extra\nsecurity groups\nif you need to update you can update the\nserver ports and so on\nyou can enable more plugins\nand one of the things that happened is\nif you're using spark operator for\nexample you can add configuration for\nsmart\nplug-in itself within the spark operator\nuh deployment\nso this is um\n[Music]\nvks and it's it looks very similar to\nuh the regular values.aml and\nall of this will be available in our\ngithub repo and we will publish the\nchart the to the main chart\nso then do you want to add something\nyeah yeah just one thing\num so um right now what you showed\num was still uh how\nuh the repo did the git repo\nlocally so that but\nwhen we when we publish the thing the\nchart\nto to artifact hub which is the\nway to deploy hand charts and then\npeople can actually\ninstall uh the chart from from\nfrom there just without having to to to\npull the git repo so it's really like\nyeah like like it said like pip install\nyou can just do helm install\nrepo name slash flight\nand then it will install sandbox so this\nis really uh\nshould really make it easy for people to\nget started\nyeah yeah our intention for moving to\nhell\nis that day one experience for our day\nzero experience for users will be\nlike i think the day zero in my opinion\nstill is tripping stuff like it and you\nrun everything locally\nand then day one is help install flight\nonto\nany kubernetes cluster that you have and\nthen from there you keep on\nyou know updating it\nany suggestions questions in that\nand i know jeeve has uh uh\nquestions about him versus what\ndo you want to chime in\nno i don't have any problems with him at\nall um i actually like helm\num i think i think they're also you know\nheading in the right direction in terms\nof like getting rid of tiller\ni think it's on the cluster and stuff\nlike that um\ni think eventually freenom will also\nmove to something\na little bit more modern than customized\num but you know what i was telling\ncatherine is that like we are\npretty deeply entrenched and customized\nso we're going to at least try and\nprovide some support for customize over\nthe next\nuh few quarters or so for for the uh for\nflight\num and try to keep it in sync with with\nhelm and but but i think that for\neverything else like helm is probably\nthe right choice\nwe're on board thank you uh no i think\nwe will keep customization for the next\nfew quarters\nourselves i think we are not going to\nbreak it and i think the\nthe right way to do this is again\nwhoever who's adding new config\nlet's have same defaults in the code\nitself there's a\nthere's a nice way to have defaults uh\nin\nflight configuration system just have\nthose defaults\num\nyou know hey kevin this is profile i had\na question\nso you showed the values.yaml file\nuh so can that be generated to show what\nall dependencies i can\nuh customize to\nadd some like whichever ones i want to\noverride the first thing is like what\nall dependencies i have\nif it's a huge definition that we\ncurrently have for customers it's very\ndifficult to\nknow what all dependencies we have and\nis that\ndoes that become simpler when we book to\nhim\nyeah soren uh we were discussing this\nright like basically\nhow do you discover the configuration\nvalues without the documentation can you\njust dump the default configuration\nlet's say locally\nor the configuration options locally and\ni think there is a way right\nyeah yeah i think there's a\nhome helm show command something like\nthat\ni'm not sure if it works it might only\nwork if you already installed\nthe chart but um i\ni think it should be possible yeah to\njust\njust dump them complete that these\nvalues out\nyeah yeah there is a way prefer at least\nthat's what we are thinking and that's\nwhy we\nare trying to add all the documentation\nand show on so it actually when i look\nat i'm sure it shows you the read me\ndirectly it shows you all the values uh\nit seems and so we will aim to do that\nand\nimprove the documentation through that\nprocess\nyeah it does show actually we were local\ni'm sure value start\nokay thanks\nthe only problem is help doesn't allow\nyou to have multiple values\nlike you can apply but i don't think\nthat is shown by the\nlease like you can't say help show\nvalues eks right\nsorry at least that's what my\nunderstanding is i could be wrong\nokay that's it from my side\ni think that's a wrap today unless\nanybody has any uh questions or any news\nthat\nhas come up but uh\nthank you seren thank you max look\nforward to\ndigging an adult looks awesome thanks\nfor having me guys\nall right well thanks everybody we will\nsee you in a couple weeks\nuh i think senator will send out an\nagenda if anybody wants to present\nyou're always welcome just grab the\nmeeting notes and write in what you want\nto talk about\ndrop standard me a note and you'll get\non the agenda it's real high\nstress process um\nuh and uh we'll see you soon\nthanks everyone bye bye everybody thanks\neveryone have a good day\nbye\nyou"
    },
    {
        "title": "FlyteMeet 2021/04/06",
        "transcript": "i i can start\nyeah so i have presentation\nuh about gcp plugins\nand if somebody can tell me how to share\none\none chrome tab in\nzoom i would appreciate it\ncan you do it somehow i can share the\nchrome tab no\nyou cannot share a single tab but you\ncan share a single window\nso pull it out all right all right\nuh can you see my screen\nyeah uh hello everyone and uh\njust hold on yeah and i have\npresentation about gcp plugins\nso it's not more not a presentation\nlike regarding gcp or how like what gtp\nprojects exists but it's more\npresentation how to develop a backend\nplugin\nand why you need it and before we start\ni have some\nfree weather words that i want us to\nunderstand\nbecause everything is complicated\nnowadays so\ngsa it's a google service account and\nksa is\nkubernetes service accounts uh jk\nis a google kubernetes engine uh it's a\ngoogle product for managed kubernetes\nservice and gcp is a google cloud\nplatform i would use a lot of words ksa\nand jsa when i'm going to describe how\nservice accounts work\nso before we start when we want to\ndevelop gp plugins\nwhat kind of requirements do we have so\nideally we want to make it super easy to\nadd new task types\nfor google cloud because there are many\nproducts that constantly\nadd new products and you want to be up\nto date with it\nyou also want to scale to a lot of\nprojects and workflows uh\nso as a ballpark we can say that we will\nhave like 1000 projects\nand overall we would have one million\nworkflow executions per day\neven though we might not need it so we\nmight not have the scale but when we\ndesign it we want to aim\nfor such skill and we also want\nfor each project uh to have own\ngoogle service account why because we\nhave one 1000 projects\nthey are shared by different users and\nwe don't want all of them to use one\nservice account because otherwise it's\ngoing to be a security risk\nor you want to have a separate billing\nfor each of them and so on\nmany products for google for instance\nbigquery or data files have\ntheir own job management mechanism so\nyou submit a job and then\nyou actually just wait for this job to\nfinish so idea if you want to have this\nresource\ncontrol like we want to control how many\njobs we execute in paradise\nwe want to control how we retry behavior\nand so on and we want to do it cluster\nwide\nso we want to control it globally for\neach flight task that is executed\nwith that requirements in mind actually\nthat's why\nfight better plug-ins were developed\nit's like a perfect\nuse case for them now we compare them to\njust using the plain containers so it\nhas much\nbetter cost and it scales better because\nyou don't need to run containers\nso imagine if you run one million\nworkflow executions per day\nand each of them has just one task so we\njust saved one billion container\nexecutions\nand so if you just do some calculation\nit would take\nfor you for instance like like something\nbetween 10\nseconds like 30 seconds to start one\ncontainer and if you just have a plug-in\ncode it can do like a request\nto the service like in like 100\nmilliseconds or 200 milliseconds\nso we just optimized our performance\nlike 100 times and i'm not even counting\nthe case when you want to wait for\nsomething to finish\nand in many products you can spend time\nin the queue like for many hours\nand then your container can die and you\nwas just ever since that you have been\ncompleted\ncompeting uh we also want to have more\nfunctionality\nand so it's just a perfect way to write\nplugins because it actually actually\nallows us to control our task life cycle\nso you can say the task is in the kill\nthen you say the task is\nretrying or stack is waiting for quota\nyou have access to all these apis as\nwell you can access to\ntask template custom fields config and\nother labels\nnowadays you can actually access the\ntask template differently but\nbefore it wasn't the case and by\nimplemented plugin you basically\nhave one implementation for all five key\nsdks\nso it will be implemented as a plugin\nand this plugin can be used\nwith any sdk by you implementing a small\nlayer\nwhy you don't want to write a plugin\nit's more complicated than\ndoing container tasks in general\nrecommendation\nyou start with a container task and once\nyou have a problem with it\nor you want to scale more you'll start\nto implement\na plug-in as well as there is a\ndeployment problem\nso it's kind of a bubbles a benefit a\ndrawback\nso as a benefit you upgrade everything\nat once as a drawback if you broke\nsomething you break everything\nas a whole everyone who uses this\npropeller instance if you want to change\nthis plugin a little bit\nyou have to rebuild the whole propeller\nis a drawback\nbut also it can change in the future\nthis is a different bug in architecture\nso if you just deploy\na flight on google cloud on kubernetes\nengine\nyou would have a setup similar like this\nso you have multiple nodes\nin our example you'd have two nodes and\nit would run pods\nso you would have a system name space\ncalled fight where you'd run propeller\nand you'll have your own namespace for\nyour project in this case you would have\nfights next\nproduction namespace so what actually\nhappens\nuh when your pods will start to\ninstantiate google credentials it will\ngo to special\nmetadata server and get a token from it\nbased on ip address\nthis metadata server will give you\ncredentials and this would be one\ncredentials for\nthe whole cluster it works well if you\ndon't\nif you have like a small deployment of\nflight like let's say you have a couple\nof teams using it and you don't mind\nthem sharing one service account\nand as a drawback basically anybody who\ncan who has access to your cluster has\naccess to the service account\nand can do whatever\nas second iteration of this you can\nactually set up something called workout\nidentity\nuh it's a special uh setting you just\nenable it in a ui\nor through some deployment scripts you\nhave\nand basically what happens it creates uh\nsome proxy server\nfor your uh node so each node will have\ntheir own instance of this processor\nthat the cd point has a daemon set\nbut basically what happens each port uh\nrequest metadata server gets proxied\nthrough this\nproxy server and then this processor\nactually knows\nabout the ports it knows about the\nnamespace spot runs and so on\nuh with this you can actually access you\ncan assign\na new kubernetes service account\nuh on google search account so\nme i can create my project uh\ni can code fight snacks and i can assign\none search account to this\ngoogle search account is this and then\nsomebody else can create a\nproject called fight tester and it can\nhave a different source account\nit's very nice it's very easy to set up\nand use but there is one\ndrawback with it it doesn't work for\npropeller plugins because if you go back\nall preparer plugins run in the file\nnamespace\nand they use one source account the same\nkubernetes service account\nit means that you cannot really get away\nfrom it so and if you develop plugins\nthis plugin will use a system service\naccount\nand you cannot really separate uh and\nhave like one plug when one plugin\nruns task one name space it for yes one\nservice account\nand then the same plugin will run that's\ndifferent space it will use different\nzeros account you cannot do\nthis because they all use the same uh\nthey are all in the port in the same\nname space\nactually given that a very obvious\noption\nokay what if we just run one preparer\nper\nnamespace uh you can do it out of the\nbox\nuh so you basically run your set\nsay propeller okay don't take any tasks\njust take the tasks from this namespace\nand you'll deploy one propeller open in\nspace\nyou can do it if you have like three or\nfour or ten namespaces but the more you\nadd\nthe more it becomes a problem because\nit's harder to update and scale\nand actually each of them can break\nindependently right\nand you have to monitor so it's not a\nreally good idea basically\nif you want to do it like that but if\nyou don't want to make your life\ncomplicated\nif you have like very few namespaces\nyeah very few projects in fight you can\ndo it\nin our case we want to have thousands\nand it becomes not visible\nif you look around actually google has a\nproduct called config connector\nwhat it allows us to do it implements\noperator of kubernetes\nit allows us to create resources for\ndifferent google products\nand it will it will take care of\ncredentials and everything for us\nuh so you can create a custom resource\nwe say called bigquery job in your\nnamespace\nand then the operator will pick this\nresource up it will see that it was\ncreated and it will do necessary\noperations in necessary apis\nwe don't know how it works because it's\nnot open source\nbut potentially what it does it goes to\nsome service\nit has some superpower to get a token\nand then it uses this token to post a\njob to bigquery\nthis is pretty nice way of integrating\nit because you don't need to implement\nany security mechanism at all uh\nand it's very easy to promote kubernetes\nplugins and fight\nyou basically just create a structure\nlike extracting\ngo and you're done and you need to\nimplement some glue code\nyou can also use kubernetes resource\nquartus\nit's a very simple way of doing it but\nit's also powerful it basically allows\nyou to control concurrency of\nhow many resources you can have at once\nas a drawback it device on external\nkubernetes operators is not even open\nsource\nuh it would create additional wallet on\napi server for kubernetes\nso if you plan to do a lot of this\nresources you would have a problem with\nscale\nand as well there is the um existing\nmechanism to control the source quartus\nis very\nuh limiting so you cannot so the only\nthing you can do for now is just\ncontrolling currency\nthe main drawback in this list is\nreliance on external operator\nbecause if you want to extend something\nif you want to support for new product\nor if you want to slightly just how it\nworks you cannot do it\non the other side if you happen to\nintegrate with some other product this\nproduct already has\noperator you're in a good spot because\nit's super easy way to integrate\nthen the next option okay what if it\ndon't\nuse operator but remember we had this\nspot\ncalled operator that could become any\nservice account somehow\nand then create a jobs for us what if we\njust do the same but we implement this\nin goal\nin a propeller as a code so because\npreparer knows\nin which namespace you're on you can\npotentially get a service account\nassigned to this namespace and do the\nsame kind of\nacrobatics that operator does right\nand you can actually do this there are\ndifferent apis to do it\nand you can perimeter exist and you can\nthey can even make it secure\nbut basically this way you can implement\nit and\nthe first implementation of gcp plugins\nactually did it\nso it's very easy you can use just go\nlibrary for google\napis you can use all features of core\nplugins you want\nbut as a drawback you have to basically\nimplement yourself talking retrieval\ncode\ni need to implement authorization code\nto make sure\nthat you know nobody uses service\naccounts that\nyou don't have access to because your\npropeller becomes somebody who actually\ncontrols access\nand gets different tokens and rotates\nthem and does a bunch of things\nso you need to be very careful and you\nalso need to authorize all requests when\nimplemented\nbut it's possible and as another\ndrawback\nif you run pods in the in the parallel\nyou would use a different mechanism to\nget these tokens actually\nyou'd use some search account\nimpersonation and\nradio mechanism uses a different\nmechanism called\nwork quote identity for pods so use a\ndifferent mechanism\nit works like that but it's a bit scary\nto implement it honestly\nbut then what if we just combine the\nbest of all attempts\nand uh we have the last option now\noption number three that is the ultimate\none\nso apparently if you would read google\ndocumentation on enough\nand you would read some source code to\ndifferent products you realize\nthat everything that operator does it's\navailable as an api\nuh so there is some way you can\nbasically get tokens the same way that\noperator does it\ni'm not going to dive into dip into this\nbut basically what's implemented right\nnow it resembles exact\nmechanism how operator\nthat was provided by google called\nconfig connector does token rotation\nand by doing that we basically removed\ntwo drawbacks we had before\nso if you don't implement our own\nvariation code anymore you only\nimplement\nin the retrieval code and we use the\nsame mechanism as\nworkload identity does so once you set\nup workload identities there's a\ncouple of clicks in ui everything works\nwe don't know for sure if it does the\nsame thing as operator\nbut we kind of only can assume only can\nassume it\nso as a conclusion in general more\nrequired they want to\ndo a plugin so definitely if you\nintegrate with products like google\ncloud\nor similar it's a great fit\nif you have operator that happens to do\neverything you need\nyou might have a more simple life but if\nthis operator is not open source you\nmight be in position when you want to\nchange something a little bit\nand you cannot do it there is something\ncalled web api plugins that i do not\ndeep dive into it's called also remote\nplugins\nbut basically it's a super way super\neasy way to implement anything that uses\nsome restful apis\nmost of the cloud providers they provide\nclients so it becomes super easy to\nintegrate\nuh if you want to integrate with gcp\nthere is workload identity it's super\neasy\nto set up and it provides a secure way\nto connect kubernetes source account on\nthe google service account\nand actually propeller can use this\nmechanism so we don't have to invent our\nown naked using to rotate these tokens\nthe current status there is a pull\nrequest that implements this mechanism\nthe exchange tokens\nthat i just described is going to be\navailable for each\nplugin so if you want to develop your\nown plugin that will do your gcp you can\ndo it\nthere is also a big query plug-in work\nin progress it is a simple request that\nuses web api\nyou can check out so you don't have to\nuse workload identity if you don't\nbecause there is like a default mode\nthat uses one source account for\neverything\nbut if you have if you happen to have\nthousands of projects\nyou likely want to change it\nand as a next steps for bigquery plugin\nwe would extend it with this and sdks\nbasically to provide the small layer\nof user api to use this plugin\nand there is also some future work that\nwe briefly discussed that we actually\nwant to extend schema support\nto be more compatible with\nproducts like bigquery but it's a topic\nfor a different\nsession\nif you want to check out this\ndocumentation here it is\nyou can read how to exchange tokens all\nthe day i'm also happy to talk about it\nbecause i have spent a lot of time\nreading different source code files to\nunderstand how it works\nuh yeah that's it thank you\nawesome work yeah thank you for uh for\nadding the\nthis i guess uh capability new\ncapability to plugins um\ni think it does uh set precedence for\nuh integrating with other uh cloud\nproviders\nthat do some gymnastics too to syndicate\num so and i hope if you\nif anyone else is using uh you know gcp\ncheck it out uh if you are on a\ndifferent cloud that does something\nsimilar\nuh you know follow the the same\nguideline\nuh i think we have uh\ni know if philip is here\nfelipe is not i don't think felipe's\ngonna be able i don't think he got\npermission\nhe's ready to go but ah um\nokay um\ni can do a quick uh overview on uh\nsecret stuff that uh we released last\nuh milestone uh so my story 12 is out\nuh and it has a bunch of bug fixes\nuh performance and stability related\nand one of the notable features we\nreleased is secret access\nto container tasks\nso let me uh i haven't\nprepared a demo but i can show you what\nuh\nthe examples we we wrote and we can use\nthat\nto talk about the feature uh\nso it's um let me share this\nright uh so one of the and we heard this\nfrom\nuh quite a few people uh one of the very\nmuch needed\nuh feature was um\nsecrets because in containers because\npeople there\nas gleb pointed out they start extending\nflights\nuh with writing code in containers or\ncontainer tasks\ntest plugins um and as soon as you\nuh want to communicate outside your\ncluster right it's fine if you are\ncreating maybe crds or other pods in a\nspecial way or something right but once\nyou go outside the cluster\num you typically want to authenticate um\nand uh there are and you know once you\nwant to authenticate\nregardless of how you authenticate right\nyou're using\noauth or api keys or what have you uh it\nboils down to\ngetting access to some secret string\num that you don't want to just publish\nin your container\nor uh in your like you know yellow or\nyour\nworkflow definition or whatever right it\nhas to be securely delivered to the\ncontainer and you want to uh also\npotentially manage access to that key so\nnot every container task can get it\nwith only the ones maybe in a specific\nnamespace or\nuh and so on um\nand now you can um so i'm uh\nopened the the flight logs if you\nhaven't checked them out in a while\nthey got a fresh look\nthanks to quite a few talented people\nwho worked on this\num and uh uh yeah if you go to the\ntutorial section\num i think is it in intermediate\ni'm trying to remember yeah uh so we\nadded an example here\nfor uh how do you use uh tasks how to\nuse secrets in tasks\num and it should be familiar if you\nwrote any flight code\nuh just uh just an extension to how you\ndecorate a task you add\nrequests to secrets that you want access\nto\nthere are two parameters here and they\nare\nnamed somewhat generically because\nthey apply to a wide range of\nsecret engines uh or secret management\nengines\nuh they are they're called the group and\nkey\nright so we looked at uh quite a few\nsolutions for secret management\nand they all have this concept of uh\na group uh so you can like bundle a few\nyou know passwords let's say in a group\nand then\neach password would have a name um so\nyou uh on the task you specify the\nyou know and this can be a list right of\nall the secrets uh\ngroups and keys that you want access to\nand then in your code you can\nget them this way right um\nuh this also enables us to like we can\nuh\nuh uh you can\nit's not available now but you can uh\nyou know\nthis code this sorry this declaration\nexists in the task definition\nuh so you can later on also haven't have\na kind of a view on\nyour tasks and know which tasks are\naccessing which secrets\num so you can do even uh like\nauthorization rules\nuh to like fail registration or uh\nor do audits and so on uh just by\nlooking at like the flight admin\ndatabase\nuh and you know quite well that this\nthe task will not be able to secret\nunless it was\ndeclared uh in the task division so it's\na very\nit's following you know the same um uh\ni guess the same uh\ni don't know what do you call this but\nlike you know our uh uh\nuh uh strongly strong typing uh for\ntasks passing inputs outputs it follows\nlike the same mentality i guess\num we have i think a few other\nuh slightly more advanced uh examples\ndown there\nbut that's the the crux of it um we also\nhave one other doc\nthat walks you through how do you set up\nyour like flight\ndeployment to start uh you know exposing\nthose secrets to tasks\nuh so if we search for secrets here\nuh how to inject secrets into tasks\nthis walks you through the flow how like\nthe communication happens\nand how do we securely deliver uh\nsecrets\ninto your containers um the the\nbase of it is we added a new\nweb book uh it's called uh pod web hook\nthat\nlistens for uh specific uh labels on uh\non pods um so once uh\ntasks once we you know propeller all the\nplugins figure out that the task needs\ncertain uh secrets it will label it that\nway\nand then this webhook will uh will be\ninvoked by\nthe api server when the part gets\ncreated to\neither directly inject the secret into\nthe\nthe pod uh or just inject the annotation\nthat will\nyou know later result into the injection\nof the secret into the pod right so some\nother systems if you're familiar with\nwolves\nor confidence or you know others um they\nuh they rely on some special annotations\non\non pods to mount a secret as a volume or\nas a\nyou know uh environment variable or\nwhatever\nuh and this takes care of uh\nyou know annotating the part correctly\nas per\nconfiguration uh to work with whatever\nsystem you have\nuh currently it only supports uh what we\ncall global secrets uh so those are\nsecrets that are\nmount or like available directly to this\npod web hook itself\num or part or secrets that\nare uh that exists in in um\nas kubernetes secrets uh in potentially\nin the past\nin the name space where the the you know\nthe task pod is being created\num they uh we added the the all the\nmanifests to\nprovision this part and manage you know\nit's like tls certs and whatnot\nuh into all the deployment uh manifests\nso if you're using you know the\nsandbox or eks gcp whatever uh they will\nyou automatically have the pot uh\nrunning and it will\nby default only uh look into the local\nsorry the global secrets uh or default\nor assume the secret is a kubernetes\nsecret\num i think that's\nuh what i wanted to talk about uh we\nalso talk into construction how do you\nscale it uh horizontally vertically\nright now it just\nprovisions one pod and we think that\nshould be enough\nto handle most of the loads but uh if\nyou\nuh as you monitor the clusters if if\nthis becomes\nuh uh kind of a bottleneck in uh for\npot creation uh you can increase you can\nlike horizontally scale\nuh this deployment um and it should\nyou know work fine\nyeah um i i think i think that's it for\nsecrets uh i\ni want to say that this is a part of an\neffort like a larger effort to\nuh enable more uh also as\npointed out writing uh plugins in tasks\nin task containers\nuh is very easy because you\nprobably all started with just writing\nyou know tasks and workflows and writing\nuh in python and extension seems like a\nvery natural easy step\nas opposed to uh you know either\npotentially learning go\nand developing in the back end learning\ndifferent interface and so on\nuh so we want to make sure that that\npath is\nis easy well maintained and\nis powerful enough uh to implement most\nof the functionality you would need\nto get started writing a plugin\nuh and secrets was uh you know one of\nthose\nhurdles along that path uh there are\na little bit there are a few others but\nwe'll hopefully address those\none at a time and uh and make this\nexperience much better\num any questions about secrets\nso i had one comment you guys guessed it\nso secrets are available already today\nbut the\nthe difference is that you either have\nto use the pod plugin uh if you want to\nwrite\nsecrets into your like mount the secret\ninto your\npod or you have to use your own method\nof annotation\nand so on which is what lift and couple\nother places do you want to just build a\nstandardized way of accessing\nsecrets so that they just are easily\nportable and easily understand\nthat's one of the reasons why we added\nthis um\nyeah but besides that i think i i just\nrealized\nyou should always run with more than one\nreplica of the secrets\ninjector because i think it's an\navailability loss\nlike if the the secrets injector is down\nyou will\nnot be able to launch parts propeller\nwill back off and retry and do all of\nthose things but\nit will there will be a momentary loss\nwhile the new part is being created if\nthat happens\num yeah besides that\nwhat yeah i think also writing that uh\nflight kit site plugins\none of the things maybe we should talk\nabout it a little bit whenever we get a\nchance but\none of the things we are working is\nability to pass\nthe template information into an\narbitrary container without\nreally building the container and this\nis useful for like query type plugins\nand plugins that do not really have code\nin them uh like they have been\ninterested for\nfolks to write plugins that talk to a\nsql database for example\nand we don't want to do that in flight\npropeller because talking to a sql\ndatabase actually uses memory\nand we want to avoid any of those memory\nusages in propeller\nuh one other thing i think haita\nmentioned about the performance fixes\nthat went in 0.12 so i have been\nworking extensively on just on pure\nperformance of propeller and\nbenchmarking and coming up with numbers\nand we'll share more details probably in\nthe next talk\nuh yeah gist of it like we\nwere in our goal was to run 10 000\nconcurrent workflows\num very like\nwith the minimal overhead and so i'll\ngive an update on that it should be\nit's it we've unraveled a few\nperformance\nbottlenecks that could have caused those\nproblems and we fixed them\nand we should be looking into that in\nthe next\nnext meeting or something i'll probably\ntry to go over in detail\nyeah is there any other topic i think\npuffle had something right like ctrl\nexecution or no\nyeah yeah if we have times we can do a\nsmall demo\ni don't have a presentation or anything\njust\nuh i'll show through the command prompt\nhow exactly to\ncreate executions there's no one that\ncan\nstart off go ahead it's just how we like\nit\nokay great uh just as i can\nshare my screen\nright\nokay\n[Music]\nyou guys can see my screen\nsomeone confirm you can see the screen\nyep\nokay great uh\nso so i'm just pointing to the docks\nthat we have for\nflight ctl uh these are on the\nthe same flight main docks page\nuh if you go to uh api references\nagain uh this is flight ctl\nand you have a bunch of commands to\ncreate executions create projects uh\ngetting launch plans\nuh getting execution center so the one\nthing that\ncouldn't demo last week was last time\nwas creating an execution\nso that's over here so the documentation\nuh gives you an overview of how exactly\ndo you\num create these executions\nuh so it's a it's a actually a two or\nthree steps process\nuh right now so you can uh you can\nlaunch a task or a workflow\nand uh if everybody's familiar with\nconsole that\nuh you can just go in the ui and launch\na task or a workflow uh search for\nthe task and pass in the parameters for\nthose tasks or workflows and\nlaunch them so you try to keep a similar\ninterface over here for\nflight ctl uh but in in this particular\ncase like uh\nwe have additionally added\na uh like a github's\ntask where you can actually create an\nintermediate\nfile for your execution which you can\nuse for\nuh launching it multiple times you can\nmodify the parameters you can\nuh modify any of the uh the auth roles\nthat you require for your\nexecution uh and relaunch those\nuh using that file so so if\nso basically uh whichever workflow or\ntask you want to execute you'll just\nuh do a get for that particular\nuh task or workflow and\nand there's an option to create uh\nto output an exec file i can just\nshow you quickly how to get that so\nso so\nin the get task there is a new option\nthat\nuh it provides you like if you want to\nlaunch this particular task it allows\nyou to dump an\nexecution file um what this does is\nbasically dumps an output for the\nall the task parameters that this\nparticular task requires\nuh for execution uh if there are any\ndefault\nvalues for those parameters it will dump\nthat in the file\nand uh and as is as a next step\nthe user can go in and modify\nthose uh those parameters and\nuh pass it to the create execution and\nthe similar thing we have done done for\nthe launch plan as well\nuh where we can say that i want to\nlaunch a particular launch plan and you\ncan specify\nan execution file for the same and it\nwill generate an execution file\nuh those parameters so here i was given\nan example like\nuh this is a a merge sort example that\ni'll also\ndemo uh later so so this\njust gets the launch plan from a\nparticular\ndomain and project uh the name of the\nlaunch plan\nand the option of creating an execution\nfile for it uh whatever name you use you\nwill be probably you'll be using those\nfor\ncreating the execution uh the sample\noutput\nover here shows like how that execution\nfile will look like\nuh it has the roles uh it could you\ncould use an imro or a kubernetes\nservice\naccount role uh whichever these are the\ntwo ones which are supported right now\nuh you will be allowed to use either one\nof them uh and if it's a\nif it's a task you're mandatory required\nto pass in\nuh passing this uh one of these\nparameters\nand the other one is basically the\ninputs\nthat are required for this launch plan\nso in this merch for example you have\nwhat sort of numbers you want to sort\nthem what's the total number of count\nhow many of them you want to run locally\nand\nat this stage you can also specify uh\neven though the\nthe the exact launch plan of the task\nwould be in a particular project or\ndomain\nyou could actually specify where you\nwant to uh\nrun that uh whether you want to run in a\ndifferent\nenvironment completely you want to run\nit on production or you want to run it\nin a different project\nuh that can be specified by default if\nyou don't give anything\nit will just use the same uh\ndomain and project where\nthat particular task or launch line\nexists\nand yeah so say so the create execution\ni mentioned that it will take the input\nfor the file that was created and\nyou can just go and update the inputs\nfor that particular task and the way\nwe've done that is basically that\nall the inputs whatever supported by\nflight\nyou can uh write them in a yaml\nstructure\nuh if it's a list you have like a\nyaml uh list that will get converted\nwhen we\nactually try to launch it\nso so in this particular\nexample you have like\ntwo sorted lists this is basically a\ntask execution where you have\nuh we're trying to create an execution\nfor this uh\nyou have two sorted lists you want to\nmerge them and\nuh yes you're not specifying any target\ndomain project so it will use it\nthe default one and uh\nthis is basically a list structure uh\nof yaml but uh given the\nthe definition of sorted list one and\nthese two parameters for the\nuh for the the merchant task\nthat is registered uh with that they\nthey are basically saying that this is\nthis has to be interpreted as a\nlist of integers then uh\nthe create execution will read this will\ncreate\na list of integers that flight\nunderstands and\nuh create an execution for it\nand this allows us to uh specify\nuh nested like any sort of uh\nuh nestor structures of uh types\nbasically uh whatever flight can support\nand\nuh the similar representations and yaml\nyou should be able to create executions\nfor those\nso you can specify map types you can\nspecify collections\nuh uh primitive types and those should\nbe\nuh you should be able to represent them\nin the yaml structure\nand uh while creating the execution\nuh uh the execution\nuh command will check what all\nparameters you have passed in what are\nthe types of those\nthat are registered for those for that\nparticular task for launch plan\nand it will try to interpret\ninto that type if it's not able to do so\nuh\nit will throw an exception it will uh it\nwon't allow you to run it\num so\nyeah i mean i can show a small demo of\nhow\nthis works\ni'll just show you a launch plan example\nover here uh just i have like uh\nuh three versions of the launch plan\nthat i have registered\nuh in my local flight\nand should be able to\ncreate an execution for this so the\nfirst step i mentioned\nis to get the launch plan and uh\nand you can generate the exact spec file\nfor it\nuh if you specify minus latest it will\nuse the\nlatest version of that particular launch\nplan\nso here says that i\ni'm using the latest version it also\nalerts you if you have\nsomething already written so\nhere it generates a execution spec yaml\nfile which dumps\nuh all the inputs that are required for\nthat particular\nlaunch plan um here it tells you what\nworkflow is going to be run in that\nlaunchpad what version of it as i\nmentioned by if you use\nversion three so it will uh that's the\nversion it will get serialized in this\nfile\nand uh you can specify\nuh you can go and modify these inputs so\ni already have\na file i can just show you\nso just uh simple file that i've\ngenerated\njust like five inputs uh integer inputs\nand\nit's going to uh sort these using\nthe merge sort workflow and\nthe way you do it is now create an\nexecution for it\nand as i mentioned like uh\nwhen you specify the execution spec uh\nyaml file you\nalso want to pass in the the same domain\nand project where this workflow resides\nright now this is not part of the\nserialized file uh\nso and once you launch the workflow it\nwill\ncreate an execution for it uh you can\ndrag the execution using the get\nexecution command and this allows you to\ntrack it in the same command line you\ncan write scripts to\nmonitor this we also plan to have like a\nwatch execution\nuh later on so here you see that we just\nlaunched it using this file\nand it's currently in a running state\nsame thing you can actually go and check\non the flight console\nso if you go in executions you'll see\nthis is the one that we triggered\nand it's running these are the inputs\nthat we passed\nand should be able to\nrun completely and return the task\nstatus over here\nright and yeah similarly\nyeah you can do the same for uh\ngetting x tasks so\nfor a merge sort you have for the same\nwords for task you can\nuh you can say that i want to uh\njust run the merge task so the merge\ntask is basically taking\ntwo sorted areas as input so you can\ngenerate\njust uh the execution spec for\nfor that particular one now here i\nspecified that i want to use version two\nfor this particular\nuh task and uh you can do the same\nthing yeah and i've written to the same\nfile doesn't matter\nuh you can do it in a different way and\nso this particular task has two inputs\nuh\nsorted list ones whatever let's do so\nyou can do\nuh the same things with the task as well\nuh similar to how you have uh in the\nflight console\nso so\nyeah i mean it's uh we're trying to\nuse this exact same semantic so you have\nuh\nso even you have uh you can launch tasks\nfrom here\nand the same things\nfrom here uh so but\nthis the same executions that uh you can\nnow\ndo it through the flight ctl come on\nyeah it says\nfinished uh you can see the\noutputs as well\nyeah i think uh that's said from my site\nplease try it out i think the\ndocumentation is available here\nand yeah please let us know\nif there's anything additional feedback\non this\ncool that's awesome thank you\nlet's stop sharing i would like that\nflight ctl moves ahead of the ui sooner\nis developing flight ctl is probably\nlike more people here can do that\nit's awesome\ni think that's it unless anybody else\nhas any questions that's three awesome\npresentations and good work all of you\ngentlemen thank you\nthanks sandra for keeping us organized\nthis time appreciate that\nno problems\nall right see you in two weeks see you\nin two weeks everybody\nsee you bye"
    },
    {
        "title": "FlyteMeet 2021/03/09",
        "transcript": "good morning\ngood morning\nwho's taking the notes today\ngeorge you're muted if you're saying\nsomething\ni got him today thank you\ni think we're ready to go\num good morning everybody\ntoday it's mostly tech demos um\ndiscussions of upcoming features that\nhave recently been reviewed\nand some ones that have been in the\nworks for a while\nuh so\ncaptain you want to start training\nyourself\nsorry yeah one minute i was just asking\nthe few folks in the channel who were\nasking a bunch of questions i was just\ntelling them\nif they want to join the sink meeting uh\nanyways i guess if they reply i will\ngive them the link\nbut i can start i have uh two very quick\nthings\nand then i will give it to he and i\nguess katrina\nso uh last time when we met\nwe we talked about that we were going to\npresent\nat uh linux foundation um\noh one second one of the folks wants a\nlink\nlet me give them the link or can you\njust post a link to thomas\nkeslin there's somebody in the section\nyeah so last time when we were talking\nwe uh we said that\nwe would be presenting to the linux\nfoundation\nai and data dac\nabout uh adding flight to the\nto the to their projects roster\nand uh so we did a presentation thank\nyou for some of you guys who\nattended the presentation it was early\nin the morning at 6am\nand that was the thursday before last\nand we were approved with majority\num and let me share my screen\num the the\nis done but we have not completely\nannounced it yet we are\nwe are slow in announcing but if you see\nthis there is flight as part of the\nlinux foundation and data interactive\nlandscape\num and we are getting the entire thing\nset up\nuh if you still scroll down to the\nprojects within the lfa and in umbrella\nyou won't see flight\nbut that should be coming probably next\nweek\nthe current set of things that we are\nworking on is\nuh getting everything set up\nwithin the flight arc so that lfa and\ndata owns a trademark\nand so on the next set of things is\nessentially\nwe we want to create a proper\ncontributing\nguidelines and so on that\nare in line with the usual linux\nfoundation project\nthe cool part about linux foundation is\nthey don't really\nthey don't really control how a project\ngovernance is managed we can control it\nas a community\nbut we would like to adhere to some some\nset of standards that you know other\npopular projects have and that way\nit's clear transparent and open so\nideas are welcome over there we we have\ndone a few things we'll share that in\nthe next\n[Music]\nfew meetings one of them is we're trying\nto create github teams\nfor every uh repo and we would love\nuh folks who have been contributing to\nvarious repos if they\nthey're entrusted with the\nresponsibility of maintaining those\nrepos\num that would be would be amazing\nuh beyond that as the contributors come\nin the usual style is to create a fork\nwork on the fork and um and then pull\ncreate a pull request from the fork\nunless you become a committer uh into\nthe other\ncompute committee into the repo then you\ncan directly open a pull request against\nthe people\nso those are some couples simple\ncontributing guidelines\num please let us know if there are any\nquestions\nthe the path to so\nwe are currently an incubating phase\nfrom incubation you move to a graduated\nphase\nand for that we need three things really\nuh one of them is\nusage in terms of like five major\ncompanies\nor five organizations in any it doesn't\nneed to be major or anything like\nfive organizations uh distinct\norganizations need to be contributing to\nthe court\nthen you'll want to work with five other\nprojects within the landscape\nor three other projects and so some of\nthem are very interested in\ncollaborating with us\nwhich including including data\nobservability\nuh machine learning um like neural\nnetwork\nuh serialized formats like onyx or\ndistributed framework like\ni call a word and then the third part is\njust\nmore than some number of github stars um\ni want to say that we are over the\nnumber of github stars that we need but\nit would still be like if you know that\nhelps us spread the word so please\nstart the repo if you have not already\nand then if you have then thank you\nso that's a quick update on linux\nfoundation\num and we will be forming a\na technical advisory committee within\nthe project and it will compose of the\nfolks that you see\nare more active in the flag channel that\ndoesn't mean\nthose people have all the rights they're\njust there to make sure that the project\nis governed and\nmoves in the right direction but that\ncome\nthat committee constantly will keep on\ngetting updated\nmore folks will get added and and we'll\nkeep on improving\nuh how we govern the project so uh thank\nyou for being part of the communi uh\nthe community and and uh which is\ngetting started\nin this bigger journey now\non the other front so i had a second\ntopic that i wanted to talk about\nuh oh rather this is just a demo so now\nif you go to the docs page and you say\nhey how do i monitor my\nmy deployment you can click on\nthe monitor deployment it gives you a\ntip and you can read through but\nbasically what this\nis is a bunch of grafana templates that\nhave been posted to the grafana\nmarketplace\nuh and so the way the graphina\nmarketplace works is you can go to a\ngrafana dashboard\nand you can say add sorry import\nand you can give it the id so in this\ncase let's say we want the admin\ndashboard\nyou can give it the id i think i have\nthe dashboard already so when i try to\nload it it's going to say like oh it's\nalready exist but\nif you see yeah it gets an identifier\nand you can select the data source\nin this case we're going to select a\nprometheus that's local to this cluster\nand once it's set\ni'm just directly going to jump to the\ndashboard it should directly open up\nthis dashboard so this is for admin\num in the admin dashboard for every api\ni hope i have covered every single api\nif not i will show you how this is\ngenerated\nbut for every api we have a bunch of\ndifferent metrics\num you can change the time and\neverything else as in grafana\nso for example we are running something\nright now so you can see like how\nthe latency is and how many apps are\nrunning and how what's the latency and\nif you notice the latency's high at this\npoint\num something you looked up for every\nsingle api\nthe other more interesting ones\nis from the user point of view so when a\nuser runs something they usually want to\nknow\nwhat's happening in their project domain\nand if\nif their workflows are running so then\nthis gets generated automatically this\nis the flight user dashboard\nuh again this is vr prometheus because\nthat's the only one that we're going to\npublish\nif you want to use other data formats\nyou're free to take\nour or even contribute that's even\namazing or take our base\ntemplates and modify them for your\nformat uh right\nor your back ends like cloud watch\nsorry to interrupt i think your screen\nisn't sharing we're just looking at the\noverview of your dashboards\nwe're not actually able to see the\nindividual lens oh really\nyeah this happens all the time it just\nstops like\nand i is it better now\ncan you see the flight user dashboard\nit's all black now for me at least\nuh okay so i don't know what i can do\n[Music]\nyou can stop sharing again\nno\nmaybe\nshared something else in some other way\num\n[Music]\nlet me try and share my entire desktop\nif that might work\ncan you see something now\nit's all black\nokay so my computer doesn't want me to\nshare but trust me when i say\nyou want us to try yeah if somebody else\ncan share that'll be awesome\ni can give the link here sure\nactually you have the link right\nprobably\ncan you see anything now yeah\nokay is this the link you want me to\nshow\nyeah uh so can you click on the user\ndashboard\nyep so these are the user dashboard\nessentially from the user point of view\nat the top\nyou can select the project and the\ndomain that you work on so in this case\nwe just have one this is from a sample\ncluster that has one and you can select\nthe workflow um\nyeah and you can use all as every\nworkflow i don't have much data here\nwhich i should have generated\nbut if you scroll down uh if you're\nusing kubernetes then it will show you\nwhat's the current quota usage in\nkubernetes\num so are you using\nuh how many bytes and maybe the\nmaybe i need to tweak the the units here\nand if you scroll down further if there\nare any errors that are happening are\nthese\nuser errors system errors if specific\ntasks it will show you\nbreakdown by type of task like if you're\nrunning queries versus you're running\npython versus array and so on and how\nwhat's\nwhat's failing what's succeeding from a\nuser point of view and this is just a\nstarting point we can definitely improve\non this further so if we scroll back\nsame thing is available for propeller uh\nand if you\non the left on the left pane uh yeah if\nyou go to dashboards home yeah\ngo to dashboards not to home\nyes it doesn't show you all the\ndashboards\nuh i guess you have to search is it is\nthat the only way so there is another\ndashboard that i recommend\nadding and that is um the go dashboard\nwhich is just\navailable for free on the um\non the grafana dashboard if you search\nfor gometrix\nuh the cool part it actually all of our\nservices are written in go so you should\nget all of these metrics for free they\nshould show you what the current memory\nutilization\nand and will help you in optimizing\nperformance or debugging any issues with\npropeller or admin or anything um\ni have been i've been using them to\noptimize uh\nsome of our backend systems\nall right now where is this code written\nso if you go to\nuh the flight repo\nunder the flight repo there is a stats\nfolder\n[Music]\nso all of these are using the grafana\nlib library\nwhich is a library that's published by\ngrafana\nso you these are programmatically\ncreated\ndashboards so you just create the\ndashboards the\nthe query is the or the expression is\nthe most important thing the expression\nhere\nis prometheus query language uh so if\nyou are using different backends you\nhave to alter those\nbut if you are using any prometheus\ncompatible backend like tsdb\nuh you know not dsdb the the uber one\nand a couple others\nthen they all should just work and so if\nyou write them\nuh you can just import the dashboard and\nset the target to be your data source\nand it should work\nif you want to add anything more please\ngo here these are\npretty straightforward dashboards and\nthey are generated using\nmake stats on the top in the root\nand that's it so and i i\nthere's documentation on it so you can\nuse that to refer to the to the home\nlink\nof where things are that's it from my\nend\nlet's see uh you want to go\ni think katrina is going to go first\nsure\ncool um awesome let me share my screen\num all right so mouth tasks um let's\ntake a look at an example here\nso map tasks are kind of a new concept\nin\nnew flight kit there is an equivalent\nfor them which we'll go over in just a\nbit\ni'm an old like it but a map test is\nkind of a specialized case of an\nexisting dynamic test in which case you\ncan take any\nsingle python task in this case we have\nthis really simple mappable task that\ntakes in an integer\nproduces a string and from that we can\ncreate a map test which will basically\nmap over a range of input\nand they call this mappable task with\neach input\nso in this case this is the syntax for\nproducing this map task here um this is\nkind of new it's a little funky it's\ndifferent than the\nway we kind of uh define other tasks in\nflight kit and we're always looking for\nfeedback so the syntax does seem a\nlittle bit\nintuitive i'd love to hear your thoughts\num but as you can see it takes in this\nmappable task it can also take an\nadditional metadata\num that every individual task takes in\nsuch as retries you can pass some\nresources here as well too and this all\ngets applied\nto the individual map task that gets\nyielded when this\nuh overarching map test is called for a\ncollection of input\nin terms of using this you can just call\nit just directly like any other task\nthat you would\nin a workflow definition so in this case\nyou see that it takes in a list of\nintegers which is that\nlist collection of that interface for\nthe underlying task\nan important caveat for matas is they\ncan only take in one\ninput and produce one at most one output\num we're going to work to change this\nbut there's some limitations with uh\nflight kit right now that we need to\nkind of refactor through\num before we can get to that um and as\nyou see here they produce output map\ntest produced outputs just like any\nother tasks and you can pass those as\ninputs to other tasks that take in that\nfull collection interface\nand you can always run those locally as\nwell too\num and we get our weird string of 34567\nawesome um we can also run this on\nhosted flight which is kind of where the\nadvantage of map tasks comes into play\num so here you can see here's our our\nmap task and then our coalesce task\nwe've run them here um sorry\nso again we pass in this\nlist of inputs we produce that\ncollection of outputs\nuh we actually produce logs for every\nsingle um subtask\nunfortunately the pods get read too\nquickly if you're using kubernetes\num so the the logs don't quite work on\nthe kubernetes backend but you can also\nuse aws batch to\num uh as your uh like map\nexecution environment and that's kind of\nreally where a map tests shine because\nthis lets you scale up\nuh to say like thousands of tests that\nyou can run on\nan aws batch queue remotely and all\nthose can be configured with different\nsorts of\npriorities um and rules and whatnot um\nwhich lets you kind of scale beyond your\ncluster that you host flight on\nuh we can take a look here here's our\ngraph um\nyou can see our map task our coalesce\ntask awesome\num cool so going back to\num uh dynamic tasks\num so and\ni guess kind of as an equivalent for map\ntests and um wolf like it and i guess\ncurrent blanket as well too because\ndynamic tests have been ported over to\nnew flight kit\nyou can see you can kind of approximate\nthis map task pattern\num by yielding a bunch of the same task\nfor\na range of inputs but this is all done\ndynamically which is to say that this is\ncompiled at execution\ntime um it's going to be a little bit\nslower to run through and you're not\ngoing to get all the kind of like type\nsafety checks that you would ordinarily\num in a statically defined task so this\nis where map tests are also kind of\nan improvement on the existing paradigm\nbecause this will all be kind of\nstatically defined\na little bit of performance improvement\nas well too um so we definitely\nencourage you to use those\nand again feedback is welcome on the\nsyntax as well\nand that is it katrina\nquestion so map tasks can be used inside\ndynamic tasks as well right\ni think yeah yeah they're like any other\ntask yeah\nyeah i think it's important to mention\nthat so you it's still\nrecommended to use map tasks when\npossible instead of the for loop\nway of iterating and probably important\nto understand what the distinction is\ndo you want to go into that um\nyeah so i guess the like the map test\nlayer have kind of like a limited\ninterface as well too and then a dynamic\ntest you can do that for loop style like\niteration and then coalesce those tasks\nor do additional processing within the\ndynamic task\nlab tests you have that kind of stricter\ninterface you would need a separate task\nto consume those outputs\num so that's just it's i guess kind of\nlike a yeah a stricter interface but\num at the same time it can still\naccomplish the same thing as dynamic\ntasks\nuh but dynamic tasks can also yield all\nsorts of things right like it doesn't\nhave to be the same instance of the task\nlike range for an input they can you\nknow use\nall you know different types of tasks as\nwell too and you don't quite get that in\na map test so\njust strict yourself set the\nfunctionality\nyeah but uh just to add like from my\npoint of view the the reasons to use map\ntasks\nis a much tighter control over the\nperformance characteristics\nwhat happens if you if you create a\ngraph in dynamic which is\nthousands and thousands of nodes they\nactually consume\na lot of memory in the back end\nto to represent and they consume because\nthey have to be compiled and they have\nto be\nthey have to be stored and they have\ntheir status while executing has to be\ntracked and flight propeller has no way\nto know that they are all the same now\nthere is some work over there to\nactually coalesce\ndynamically into our array task but\nthat will happen in the in the longer\nterm\nbut if you use map tasks essentially\ngiving a hint\nto propeller that they all are they all\nare essentially\none one unit and so they get represented\nin the execution point as one unit and\nthere are lots of advantages of doing\nthat like for example we know that the\nshape of the task is\nidentical that means the container is\nthe same\ninputs are kind of similar and so on so\neventually we could actually reuse\ncontainers\nas they are coming up we could uh\nwe can actually prioritize uh throttle\ncontrol them the status itself is\ncompressed we use bitmaps\ninternally to represent how they are\nprogressing uh so\nso they are very optimized in the back\nend and specially handled\nso my recommendation would be if you're\njust going to run one\ntask or a large corpus of data\num just you know partition then it's\nbetter to use map tasks\nand that's why we formalize that pattern\nas a separate entity within the system\nokay\ni'll go next okay so i everyone can see\nthe screen right\nyes okay um i wanted to\ngo over i wouldn't call this a roadmap\nbut just some\ntalking points so maybe what people can\nexpect in the next\nfew weeks few months\nso flight kit can be thought of from\nthe contributors perspective at least in\nthree\nand user perspective the three main\nareas one is the authoring side where\nyou're actually writing\ntasks and workflows and launch plans and\nall that stuff map tests and dynamic\ntests and all that\nuh one is execution side which is what\nhappens when the container containing\na flight kit defined task is\nspun up and executed on a flight back\nend\nand one is control plane objects these\nare previously\nand still currently known as sdk task\nworkflow launch plan\nuh workflow execution i think is one\nthat's very commonly used\nthese allow programmatic uh in python\nscript access\nto control plane kind of like\na flight cli or flight ctl for python\nobjects\nwith the map task pr we're mostly\npretty much done with the new api\nwe there's some cleanup that we need to\ndo we'd always need more testing\nwe're kind of looking i think maybe\nsomeone's helping us look into better\ntesting frameworks in general and i'm\nsure we'll find a lot of bugs as people\nadopt it more um and\nthere's a question of whether or not we\ncan or we can't but\nhow to define workflows imperatively um\nso like\nyou instead of having a user write an\napp workflow\ndecorator to decorate a function um\nuh we allow the user to be able to\ndefine a workflow\nlike with a for loop for instance\niterating over the nodes\nso that needs to be done but that's a\nsmall addition\num and\nafter that on the authoring side we'll\nhave\nlike very broadly speaking just continue\nto plug in work\nuh we want to add support for\nuh things like local caching and\nintra-task checkpointing these are\nfeatures that i think will let users\nget more out of flight kit by itself um\nas a standalone thing and um improve\nuh utility of it in general even on the\nback end\nand we there's some um\nenhancements we need to do to the typing\nschema that's not\njust flick it that will go beyond um\nand then we are soliciting ideas in\ngeneral for\nuh developer experience improvements\nand contributions of course uh\nthe control plane side i think is the\nnext big\nchunk of work that will start we kind of\nwant to revamp everything to work with\nthe new type\nengine that's uh deployed with the new\napi\nso that will take that will probably\nstart on that\nthis week or next and will continue\nuh hopefully won't take more than a few\nweeks\nbut we'll see and lastly on the\nexecution side\num some someone i think\nspotify um asked us to\nfor a pattern that we couldn't support\nand we realized the reason we couldn't\nsupport it was because\nof a limitation in how tasks are\ninstantiated at execution time uh the\ntask\nbeing the python instance itself um so\nwe have one i put in a pr\nuh it's not merged yet for people to\nreview\num it is pr number 404 so\njust to give a quick overview of it\ncurrently when\na container comes up the python instance\nof the task is instantiated by\nlooking for these two options to the\nclick\ncommand the module and then\nthe name within the module to uh to load\nand this only this has a bunch of\nlimitations\nnamely the\nthe module and basically the name of the\ntask has to be at the module layer it\ncan't be nested\nso we introduced this thing called a\ntask\nresolver you can take a look at the um\nthe doc string that i added but\nbasically\nyou still need to start with\nwe're giving it a resolver and the\nresolver itself still needs to be\naccessible from the module level so you\ncan't\nthis default task resolver can't be\nnested\nbut from there you can pass any\narguments you want to it\nso the arguments for the default one\nwill look very similar\ni stripped out the dashes but basically\nit's just what we have today\nand\nthis is what the new one would look like\nso um\nno actually let me this is another\nexample of what\nso if you sorry let me\ntake something uh you can we basically\nadded this mix-in\nto the workflow class itself in this pr\nwhich allows you to do things like this\nuh\nbasically which is not previously\npossible this\nlets you define a task inside the body\nof\nthe workflow itself uh this is nested so\nit is typically uh\nwithout this pr not possible but with it\nthe the workflow itself becomes a\nresolver and the workflow\nknows how to locate the um\nthe which task it is to run\num the because the workflow itself\nends up being the resolver the workflow\nitself definitely has to still be at the\nmodule\nlayer but if it is then you can do\nuh stuff like this which will allow you\nto define uh\ni think it's more useful for sql tasks\nbut\num or at least it looks cleaner for sql\ntests but\nuh you can decorate tasks and call\npython function tasks or any other test\ntype you want\nuh and yeah i think that's it\ncan you scroll back to the resolver\npattern thing and show the example where\nyou were using the builder oh sorry\nyeah can we just parse that one second\nuh number 35 right\nyes so the point of this is that this\nbecomes this is a nested function that\ngets retrieved from this thing here\num and this becomes the body of the task\nso i think a typical use case would be\nif you want to write a class\nwhere you want to put inputs and outputs\nas a query\nbut have the process inside just you\nknow process method\nlike very simple right this is the\nsimplistic workflow that anybody can\nhave like take inputs\nprocess and produce output you can now\nuh\nencapsulate that in this kind of a\npattern\nwhich is what's shown on like number 35\nthere's no outputs here but you could\nhave\nput outputs is that right e\nuh the task has an output i believe\nno no when i say like as an output like\nif i want to take\nsome data from bigquery generate a you\nknow\ntrain a model and then take the model\nand you know deploy it to a model\nserving layer or\nactually write the output of the\nvalidation\nor you know execution of the model to\npick query back\nyou could write that as one line yes i\ndon't have a follow a subsequent step\nafter the process\nyeah and\ni guess like to maybe give\nsome context about the spotify use case\nbecause i found that\nalso interesting use case uh i\ni imagine others might want to replicate\ndo you want to speak a little bit about\nthat\nuh i'm not entirely sure what what it is\nuh yeah so the the uh i think the\nthese the basic version of it is they\nthey wanted to provide their users\num their own dslr they want to write\num you know some codes in here and then\ntheir users would use\nuh uh would just write or they won't\nexpose\ncertain constructs to their users um\nthat don't quite follow the paradigm of\nwriting\nworkflow tasks all statically defined\nand so on\num and and with this uh now they can\ncompletely customize uh how those\ntasks and workflows are loaded at\nexecution time\num the maybe caveat is\nthere should be zero um like\nrandomization or non-determinism\nin how that logic works uh it should\nalways load\nlike the same task every time you call\nit with the same inputs obviously\nuh but it's it's a very powerful\nconstruct you can imagine others would\nwant to expose different dsls the same\nway\nyeah actually this makes it possible to\nessentially write new dsls\nand in any any order as long as they're\ndeterministic\nbut you can write a yaml dsl using the\npython sdk\nyou can write a um you can write like\nthe single line dsl which is very very\npopular for\nsome of the machine learning projects um\nand\neven for some simple data processing\nparadigms\ni have one question this looks great\nlike it's very similar how it works in\njava sdk where\nbut in javascript you can just give a\nstring like one name\nbut here you can actually pass more\nparameters so i think i like it more\nbut like i mean when i was thinking\nabout it uh like\none way to go is just to serialize the\nwhole object\nyeah that's why people don't like it\nokay\nlike but but does it oh it does realize\nthe whole object\nokay uh and i played around with this\nand then\ni added i should make this an explicit\nwarning but basically this even like the\nhello world example uh once you add a\nclosure it becomes\nvery large uh then the command line you\nbasically it has to be offloaded\nsomewhere so\nthat will uh require changes to admin as\nwell so\nwhich is why there's a huge caveat in\nthis\nyeah so gleb but it is we don't\nserialize all the time this is an\nexperimental\ncloud pickle resolver so that means you\ncan extend how you want to\ndo it so tomorrow if you want to support\njupiter based fully cloud pickled\nobjects and if that's your game flight\nkit won't stop you you can do that\nyou can completely upload it and data\nwill just run from jupiter notebooks\ndirectly onto flight\nuh that comes with its own caveats you\nshould you should know what you're doing\nuh definitely but uh yes it's possible\nand and i think the serialized object is\nthe way to make it\nthe interaction really fast uh\ni think that's where you get into right\nyeah\nbasically the problem with combat line\nand what java sdk does\nit needs to pass the list of jars so it\nactually creates a json file with\na list of jars that are also and then it\nuploads this file and then it links it\nbut if there was like a way to read\nlet's say custom from container\nlike even just have it on gcs or\nwhatever it would solve it\nyeah why can't you do that why can't you\njust serialize let's say not a json but\nlike have a protobuf format which\nis too small and then compress it and\nsend it on the command line\njust yeah yeah just you have to do it\nbasically yourself\nbut because there is like a custom field\non each task\njust for me it would be comfortable at\nleast to have it like accessible\nit's like extra overhead to always pass\nit but\ni mean you can just pass so many useful\nthings there and as well as there is\nlike extra things like labels and\nannotations\nand just have a way to access it from\ncontainer tasks as well it would be very\nuseful i think\ni think you are absolutely right i think\nboth we were discussing whether we\nshould take the task template and just\nsend it down into the python sdk\nand that would that would solve a lot of\ndifferent types of problems and one\nof the things today is that if you want\nto run a sql task\nin python you need to have the container\nbuilt with the sql even the sql is\ncompletely portable\nand so what you could have done is\nessentially have one a library of\ncontainers that just are doing let's say\nhave sql alchemy in them\nor other orm in java\nand then you could just pass the query\nto it and it could do a query on any\ndatabase\npotentially that the orm support so yeah\nso that's\nwe we did talk about it i think we will\nbe doing it\nuh we would love to experiment with if\nyou are ready with it\nlive i i think that's a trivial change\nin the back end we can pass that in\nit's pretty true uh if you pass it in i\ncan take it\nokay yeah that's awesome let's actually\ndiscuss because it's not a big change at\nall\ngee you've asked a question in the chat\nabout uh when does the static\ntype chipping occur when we're doing the\ndynamic workflows\nor do you want to verbalize your\nquestion\nyeah i was just wondering um you know\nwhen you when we register the workflows\nand tests and stuff like that there's a\nlot of\nbenefits of uh having uh our tasks\neverything be type checked does this\nalso still work with this model\nwhich model does resolver stuff uh yeah\nsorry\num yeah i mean\nyes the the command the command to the\ncontainer is the only thing that changes\nall the other compilation steps are\nstill being\ndone okay gotcha yeah\nso the the in this during the\nserialization phase you will run\nthe same loader command essentially\nto load your tasks serialize them so\nextract\nthe inputs and outputs including the\ntypes and so on and then\nregister it and then at run time it's\nagain run to actually\nload it back and reconstruct the task\nyeah so it's it's essentially the same\nit's like just a different way instead\nof using\nimport module all the time we are saying\nlet's have a customized import module\ngot it thank you\ni think that's it for the agenda today\ndoes anybody have anything else\nthey want to bring up\nall right thank you everybody for coming\num\ni think the map tasks look awesome\nkatrina thank you very much\nuh and we will\ni look forward to deep more deeply\nunderstanding what\nyou just described because it's a little\nmind-bending from my perspective but\nuh\ni'll look forward to digging into a\nlittle bit uh thanks everyone\nwe will see you in a couple weeks uh\nthere's on in the meeting notes\nit's posted uh the top of the meeting\nnotes are\ntopics that are sort of up for\ndiscussion anybody wants to volunteer\nthat list is totally edible anybody\nwants to\nchange add to that list go ahead it's a\nreal low tech system but\nit should be free to comment in the\ndocker you can dmd if you want to add\nsomething to that or you want to demo\nand we're always welcoming any you know\nnew presenters who are working on new\nstuff\nto show off what you're doing that's the\nmost fun demos we do\nall right thanks everybody see you in a\ncouple weeks\nbye\nyou"
    },
    {
        "title": "FlyteMeet 2021/02/23",
        "transcript": "hey santosh\nhey can you hear me\nyeah yeah okay awesome\nhey you're right good to see you man\nsame here yeah how are things with you\ngood good hanging in there\ngood morning\nso quiet for a second i thought my\nspeakers weren't working\nyeah sometimes it's like very\nyou really silent they've just gotten\nreally good at the noise cancellation i\nguess\nthe expense of all other usability and\nexperience\nfeatures i guess most people actually\njust mute i think\nhi thank you for sharing everybody's\nfaces\nit's always good to see yeah it's like\nthe time of the week or a couple weeks\nthat i see a bunch of folks\noh so this provoked jeep to share his\nface that's awesome\ni've been getting a lot of for it\neven at work so\ni'm just gonna get more on the camera\ni think we didn't send uh it like the\ninvite\nagain on the slack yesterday so i think\nsome people missed it but that's okay i\njust posted the link i didn't even know\nthat people\nactually knew of that one so\nand this is an interesting problem like\ngetting your google and your\nslack membership to be in sync\nand resharing the invites as people join\nin\ndoesn't really work well\nhey jake\ni think we should get started\ngeorge you want to kick it off\nyeah i'm logging in hang on a second\nokay now we're all set um\nmorning or afternoon everybody depending\non where you are\num i think\nthe main folks up today\nare powerful talking about our new\nflight cli\nthat they have been working on\ni think that's it i think that's our\nmain agenda enough for the day\nno uh gv is going to go over the getting\nstarted guide in the onboarding\nso yeah uh\ncool all right sorry they didn't make it\ninto the agenda\num i've also heard some um slides about\nthe hands and support so if you still\nhave time\ni can show that uh otherwise i'll the\nnext time that's fine too\nno no we have time i'm supporters\nactually we've been talking about it we\njust want to move away from customers\nthe more we think about it it's like\ngetting paid for\nso uh we would love that\nshould should we do it in order is it\nokay sir and like would it be okay to go\nlike the\nthird in line yeah sure\ncool then uh jeev why don't you take it\noff\ni thought we were going in order you are\nlike what are the priors at liberty here\nso oh yeah\nokay uh let me share my screen\nof course everything is messing up\nokay sorry who's taking notes\njust to make sure we have one person\nwho's taking notes\ni will thank you\noh sorry who what are you\nokay thank you\nokay let's do this um\nso so uh the flight team has been has\nbeen really amazing in putting together\nthis dock\nfor essentially helping new users kind\nof onboard\nonto flight uh as you all may uh you\nknow whoever's\nkind of operated on flight before you\nmay know that there's a non-trivial\noverhead to bringing a cluster up and\nstarting to play around\nuh and this is typically not something\nthat you know new users uh\nwant and they just wanna like basically\nyou know have a couple of commands to\nfire things up and start\nuh playing around with the api\nregistering running workflows and seeing\ntheir results and so on and so forth\njust kind of getting used to\nthat experience what uh that flight\nprovides and so\nyou know here's a i'm gonna start with\nthere's actually two docs um\nbut i want to start with this one\nbecause i want to this is the kind of\nlike the first thing that we\nworked on last week and then we'll talk\nabout some of the some of the\nimprovements that are probably uh that\nare coming\nuh inevitably next week or this later\nthis week and so i just kind of want to\num uh this might this might not be super\ninteresting to everyone but like for\nnew users might be you know interested\nin this i'm just going to kind of walk\nthrough all the steps in this\nuh and kind of share the experience of\nyou know getting\nuh getting the first workflows going on\nflight so let's get started\num so i'm just gonna i'm gonna copy all\nthese commands um\nlet's just uh create a virtual\nenvironment first and this is a live\ndemo so bear with me if they're errors\nwhich there might be so\ncreate a virtual environment for myself\ninstall flight kit\nyeah perhaps i should have done this\nbefore you know to sell for the next one\num and then simultaneously i think we\ncan basically like go through and uh\nand start the cluster and stuff\nsimultaneously this this is a block\nso you know uh if you're if you don't\nhave the fly flysnax repo on your local\nlaptop you can go ahead and\nclone it and cd into it i've already\ndone that and so i'm going to go\nstraight to this make start command\nand run that as as you can see my\nflight kit is getting installed in my\nlocal environment but this does not make\nstart does not depend on\nuh flight kit being installed in your\nlocal environment this all this does is\nit it creates a\ndocker container that will run a sandbox\nkubernetes cluster and deploy flight to\nit\nand so we'll kind of walk through all\nthe steps and so here you know it says\nfiring dependencies and\ncurrently the dependencies are just\ndocker and uh coupe cuddle\nand uh core utils uh in mac as it turns\nout\nthat i learned yesterday and so it\nstarts uh k3s within this docker uh\nin docker environment uh and we and you\nknow\nuses docker and docker here because uh\nthis allows us to basically build the\nimage nicely on the container itself as\nopposed to building and pushing to\nregister\nregistry or building and kind of loading\nit into the image\nso after it creates the the cluster it\ncame up pretty quickly\nuh and this time might vary for people\nuh you know\nfrom on my laptop it's actually pretty\nfast but i can imagine that this can be\nslower\nif you're on a slower laptop or if you\nhave fewer resources kind of allocated\nto the docker\nvm on your mac and so\nnow it deploys flight and you know it\ngoes through the process of essentially\nloading the sandbox environment in the\nflight repo\nonto your sandbox uh cluster kubernetes\ncluster\nand at this point it will you know it's\nessentially just kind of waiting for\nthe flight deployment to come up so it's\nwaiting for like flight admin\ndata catalog uh flight propeller and so\non and so forth so it's kind of just\nmonitoring and waiting\nuh and once that is gone uh once that is\ndone the next step will basically be\nto kind of register auto register your\nfirst few examples\nto get started with right so that so\nthat is essentially you know whatever uh\nimage that has been pushed in flight\nsnacks um\nas like the latest image and so that'll\ncontain like uh\nsome simple examples to base to get you\nstarted\nand when this uh when this is done so so\nbasically now flight admin is up and\nrunning\nand so we're pulling down uh flight\ncookbook which is basically based off of\nthis repository\nand we're registering uh that image and\nall the workflows within that image\nwith the cluster that just came up and\nso that's part of the part of the make\nstart experience here\nagain this might be a little slower or\nfaster for people depending on your\nnetwork\nhey can i ask a few questions here yeah\nsure of course\nuh so just wondering like i mean uh\nthere are a lot of things kind of\nthrown into this entire uh demo at this\npoint of time i think\ncluster management for example bringing\nup a cluster is not something that a\nuser would do it's more like an admin\nthing so i was just wondering like if\nyou can separate like okay these are the\nsteps that the admin does\nand probably most likely in an\norganization you should just get it out\nof the box\nhere is what steps you need to do maybe\none or two and then you can get started\nor start writing your jobs uh that kind\nof separation might have been more\nhelpful um just a thought but i thought\ni\ni'm a product manager so i found it very\nhard to kind of figure out where is the\nadmin part separating out and where is\nlike how can i build a pipeline so\nuh if you can kind of not now but\nsometime at some point of time when you\nguys can actually think\ni think that's it's an excellent uh\nobservation i think the goal\nwith this uh first tutorial\nis uh uh for for\nvery like this your first interaction\nwith flight right\nyou don't have an organization that's\nrunning flight you don't have\nuh admin you know sitting on the side to\nset up\na flight cluster for you it's just you\non your own you found flight on\nyou know google and you want to give it\na shot and we wanted that experience to\nbe\nas simple and straightforward as\npossible so we packed as\nmuch flight as we can in this one little\nbox\nto give you the the full experience this\nis\nby no means the way you will end up\ndeploying\nflight in your uh organization makes\nsense all right sounds good\nyes i thought this just too that your\nquestion was\ngetting admin to set up right this is\nnot this is actually running it locally\nthere is no i don't know i i i get that\npoint uh but the point is\nstill that you know in a user's mind uh\nif he knows that okay in the stereo\nstate i don't have to do\nall these things launching a kubernetes\ncluster for example\nor all the other work it's just a\none-time thing it's still very relaxing\nfor me\nbut this is a one-time thing for\npipeline work i\nstart with this step and they're only\nthree or four or whatever they are\nsimpler ones\nso just i mean just like whenever you're\neven writing it down or demoing it's\nlike okay this is one time thing\none two three four five boom you're done\nyou don't have to repeat it ever\nthe second steps where you will do\nregular work for your each pipeline\ndevelopment\nthis starts with here uh mentally that's\na model that i have in mind when\npipeline development is like you know a\nthing so\nmight be helpful for people to grok it\nbetter\nabsolutely that's a fair point i think i\nthink it might make sense to kind of\njust update the dogs to say\nuh you know kind of like the\ndependencies or the components of flight\nand like how one would bring them up and\nlike what the responsibility is\nfor all the different parties uh just a\nlittle bit of context here\nuh at freenom we were actually running\nsomething similar where we had a flight\nsandbox\nrepo that we would essentially use to\nkind of bring up these these\nthese ephemeral clusters that people can\nuse to work on um\nand i you know i found we found that\nlike earlier on when we were on boarding\nour\nour engineers on delay we found that to\nbe like pretty useful\nin basically allowing them allowing them\nto like move much faster\nuh and kind of just you know firing up\ntheir own playground either locally or\nsome you know some cluster and just kind\nof playing around with things and\ngetting getting familiar with like the\napi and stuff like that uh and we didn't\nyou know\nthat did not require some admin to set\nup a cluster and and i in a way like\nthat almost allowed them to move faster\nand kind of like learn the\nsdk and get familiar with it faster so\nso you know uh\ni think i think this this is a good way\nto kind of just get started\nbut we should i definitely do agree with\nthere's we should probably call out that\nlike\nyou know some some things are not uh\nsome things are the admins\nresponsibility but they're just kind of\nsimplified here in this\nin this demo experience\nthank you for that suggestion um cool so\nso now\nas you can see like you know we've gone\nthrough and kind of registered all the\nworkflows in our latest\nflight cookbook example um and the\ndeployment is ready now and you can\nbasically you know\njust go to this site here and you can\nsee\nyou know your examples um the the\nand we should probably kind of uh\ndescribe this as well but all the\nexamples are essentially in this flight\nsnacks development\nproject um and you know i'm gonna for\nthe purpose of this demo like look at\nthis one\nexample which is the hello world\nexperience a hello world pipeline\nand we'll just kind of go ahead and just\nlaunch it off um\nand you know and you'll see you kind of\nfollow the ui and see what's going on\ni also like to see what's going on in\nkubernetes so that's\nthat's a good way to kind of know what's\ngoing on there and so like i will just\nstalker exec into my container\num and this is and this is not a part of\nthe experience but this is just\nsomething that i want to show\num in terms of like what pods are coming\nup and like how they're they're you know\ngoing through their life cycle and stuff\nlike that\nso we'll just watch so you can see that\nthere's this pod which is\njust just come up in the flight snack\ndevelopment namespace and it's running\nand completed now so if i go back here\nyou can see that uh you know this this\npart is not complete and you can look at\nthe logs and look at the inputs and the\noutputs there's no inputs obviously\nand the output basically says hello\nworld right\nso let's let's go to the let's go to the\num hello world\nuh workflow and that's and this and\nthat's this one over here and you can\nsee that\nuh essentially it takes you know this\nworkflow takes no inputs it calls this\nsay hello task\num and then returns whatever the uh\nwhatever the say hello task returns\nbasically\nand so you know uh you can imagine like\nwhen you run this workflow obviously it\nwill have printed hello world\nuh as the output so let's let's make\nsome as part of this like i would like\nto uh\nyou know following along here um you\nknow we went through all of these\nprocesses\nand i'm gonna try and uh you know make\nsome changes\nto this workflow and do a fast register\nand see\nif i can you know those changes that i\nmade to my pipeline are now reflected in\nthe new version\nof the workflow that will run on flight\nso i'll just you know like i\num you can kind of follow along and just\nkind of make the changes that are\ndescribed here but i will just uh\ni know what they are because i've kind\nof practiced this a few times\nso um so essentially what i'm doing here\nis i'm passing\na name argument into this hello task uh\nsorry the say hello task\nand then and then when i'm returning i'm\nbasically printing hello world and then\nthat name\nright and so the input uh you have to\npass that input into the work\nso uh when we're launching the workflow\nwe're passing the input into the\nworkflow\nand we're gonna pass then that input\nfrom the work the workflow input into\nthe task\nuh so um and then you would expect you\nknow uh this return result to basically\nbe hello world and whatever name you\npassed in\nso um you can also test whatever changes\nthat you make your workflow locally and\nso like\num i can i can essentially modify this\nthing\num you know to to to be able to run\nlocally\num so and so like i can basically go\nhere\nand run this command this python\ncookbook core basic hello world command\nand that should you know make sure that\ni do see a hello world atom printed\num so this is a way to so yeah so\nthere you go and so this is a way to\njust kind of make sure that whatever\nchanges that you make\non your uh on your pipeline is just\nlocally tested and it's ready to go\nbefore you ship this\nnew workflow off into some remote uh\nenvironment and run it uh run it in a\ndistributed cluster so\nso now i've got it running i can\nbasically just run uh make\nfast register and what make fast\nregister does is it essentially like\nserializes all the code\nin this repository and ships it off as\nlike a tarball uh\nto uh to the cloud and then that will\nthen get pulled down by the image\nuh pulled on by flight and and run\nin the cluster basically so this is all\nyou know\nreally nice sort of uh convenience\ntools uh i'm pretty sure it's like you\nknow fairly uh it's doing a lot of magic\nfor you in the background but\nuh that's hopefully like all that magic\nis hidden from you you can just kind of\ndo your work\nand enter in on it quickly so i'll go\nahead and make fast registration t like\nthis should go pretty quickly\nso i made a small change um and\neverything is registered and is ready to\ngo\nand i can go back now into my sorry i'm\njust moving around my uh\nmy images here and go back to my cluster\nand you can see that when i launch now\nthere's a new input that i can enter\nwhich is the input that i was expecting\nand i'll just type in a name here barney\nand launch it and so then you can follow\nalong this time i'll kind of just stay\nin this ui and\nsee the task coming up and whatnot\nyo it's going it's running\ni'm sorry you need my new performance\nturbo boost mode\ni will give it a kick ah\nmaybe the laptop's just i can feel my\nfans are spinning up right now so\nso you can see everything is like\nslowing down there you go so so\num if you look here you can see that\nlike it came it came up data grading\nrunning and it's not completed\num and and and that is also reflected\nhere and so you look at the task or the\nworkflow output\nso um\nthis is still getting ready there we go\nso it's complete and i can look at the\noutputs and it says\nuh oh i guess i didn't did i not pass\nout the outputs here\nthat's that's odd we should look into\nthat but but\nlook at it now it was succeeding\noh i see okay got it so outputs um there\nyou go so hello world barney as opposed\nto just the hello world that we saw\nbefore\nand the only change that we did was just\nin the python code we did a fast\nregister\num and that doesn't require like a\nrebuilding of the image right\nit just you know registers all the code\nthat you currently have and uses the\nexisting image\nand kind of a caveat there is that like\nif you change if you change the\ndependencies\nuh that you have then you probably will\nneed to rebuild your image but\nmost of the time like people are just\nreusing like you know whatever image\nthat they're using\num you know with sci-fi and all that\nstuff and then they're just launching\nthings off and they're just modifying\ntheir\nuh their code and so this is a good way\nto to get things registered and\niterating quickly on it\nso so that's that's kind of uh where we\nare with this particular\nuh tutorial so uh oops so essentially\nwhat we've done\nis uh you know we brought a we brought\nan ephemeral cluster up\nuh we ran a workflow on it we made some\nchanges to the workflow and then we\nre-ran it again\nuh first locally to make sure that all\nof our changes were correct\nuh and then we we did a make fast\nregister and we ran that workflow again\nuh on the on the cluster and everything\nworked out\nas we expected so that's that's this uh\nhello world tutorial experience and\nthankfully everything worked\nfor the most part um the improvement\nthat we're making is that we want to try\nand simplify this even more\num for people that are you know just\nkind of want to work off their own repos\nand whatnot\nand so what we started to do was\nessentially put together a\ndocker image that will you know as when\nyou run it it will run\na a sandbox uh flight cluster for you\nso this is this is this is kind of uh\nyou know to santosh's point\num you know this is this is what the\nadmin uh you know if you were running on\na cluster like all the stuff that\nhappens in this documentary is what an\nadmin will run\nbut what this will allow you to do is\nlike you know you can just spin this up\neither on some remote cluster or\non your own local laptop and then you\nwill have\nyou know a fully fleshed out like flight\ndeployment ready to go and then you can\nstart iterating on your\nuh pipelines as you would normally so\nthis is you know this doesn't require\nmake sure\nand stuff like that but and what we're\nplanning to do is we're going to take\nthis image now and we're going to\nreplace some of the the core internals\noff that make start command with this\nimage\nand try to make everything a little bit\nmore seamless um and so\nand so um you know this this is still\nkind of work in progress uh one thing\nuh this brings up a cluster but one\nthing this doesn't do is kind of\nregister example workflows\nand so you know that is work in progress\nbut but for now i can just show you what\nthat\nlooks like so i can fire up this command\num let me actually let me just go and\ntear down my old cluster and make sure\nthat doesn't exist anymore\nso it's tearing down my sandbox\nokay and so no more containers running\ngreat\nand now i can i can create this new uh\ncluster so it doesn't find the image\npulls it down it basically is currently\na\nuh a hundred whoops 139 megabyte image\num and i think we can probably make it a\nlittle better but you know it's\nfairly small and it should should be\npretty quick\nto download on most laptops so again\nthat's the same stuff starting the it\nstarts the kubernetes cluster\nit deploys flights um and then it waits\nfor the flight it waits for flight to\nbecome ready and that and that goes to\nthe process of you know checking that\nall the deployments are up and running\nand we actually have two different\nversions of this image\none is what i'm currently using right\nnow and then there's another one that is\nbased on\ndocker and docker and what that allows\nyou to do is\nuh you know the same thing that we did\nwith our example where you can build\nyour images\ndirectly on your cluster if you want and\nso you know there's no overhead of like\nhaving to push the image to a registry\nand then pull it back down to your\ncluster\nyou can just deploy uh you know uh build\nand register the images directly on your\ncluster\nso here you go everything is coming up\nit looks like and we're just waiting for\na couple more things to come up\nuh and once and what sorry\nwas the question how was sorry just a\nquick question on the docker buildings\nhow's the comparison of like the time\naside from the net the network\nstuff just like do the builds run pretty\nmuch\nas fast yeah so the builds run locally\num and the thing is like you don't have\nto push right so typically like if you\nif you build say like a three gig image\nyou have to sit there and wait for three\ngigs to push and then for three weeks to\npull down back into your\ncontainer that's going to take a while\nwhereas if you just kind of do it\nlocally\num then you don't have to overhead after\nyou push the registry\nand and as it turns out like you know\nbuilding people's computers are fast\nenough to build\nbut they're but their internet is always\nlike kind of you know a little bit\nshitty to like push three gig images up\nand so it's it's nice to have you know\nthings working locally\nso so now we have a does that answer\nquestions\nyeah it sounds like the building itself\nis like not that intense and it runs\nreally\nthe actual building runs really fast\nyeah so the building is is depend on\nlike how\nyou know it depends on your image right\nso like you might have\nyou know like a hundred or a thousand\ndependencies that might take a while but\nlike if you just want to\nif you have like a sandbox type of\nenvironment that you want to play around\non like that should be fairly quick to\nset up yeah just depends on how many\ndependencies you have\ncool so so now you know there's this\nimage so there's this message that says\nthe flight is ready and it's available\nand i can just like click on this and\nnow i have a brand new cluster\nready to go and so now you can do the\nsame stuff that you do build your images\nand register with the cluster and so on\nand so forth unfortunately there are no\nexamples\nuh yet in this it's a it's a brand new\ncluster um\nand we're and we're working on base on\non being able to register our cookbook\nexamples into this cluster so people can\nhave a much nicer\nuh first run experience and so please\nstay tuned for that\nuh that's all i have i'm happy to take\nany questions\nhey one quick question so what is the\ndevelopment\nor recommended development cycle for\ndevelopers looks like now i mean\nyou had earlier i think you were showing\nthe development staging and production\nand then you have something\nlocal sandbox so i mean what is the\nrecommendation\nrecommended way to kind of go through\nall these\nwhen should one one to the other i will\ni will defer to the flight team on that\ni think um\nand i'm happy to talk about like how we\ndo things at freedom but i'll let the\nflights\nanswer that question okay let's talk\nabout how you do it at freedom first\nagain yeah so in freedom essentially\nlike we don't have uh you know like\nthese different\nso so so you know there's there's one\ncluster that has multiple\nuh these things called multiple domains\nand so like development staging and\nproduction\nin freedom we only have a single domain\nbut we split a cluster\nwe have different clusters entirely for\nuh development and\nand production so it depends on how you\nwant to end up doing it so you can have\nlike one flight cluster and have both\ndevelopment and staging end production\nrunning on it\nor you can have multiple flight pluses\nwhich is what we do at freno and then we\nhave\njust one which is like either a main or\na default domain\nfor that environment so you know when\nyou're developing you're on the dev\ncluster\nand then when you're ready uh you just\nyou know update all of your\nuh you know your customized templates\nand then when you deploy it into uh\ninto production uh we have you know\nit'll like automatically register all of\nthe workflows and stuff and then\nuh and all your production workflows\nwill be available on the production\ncluster for you to run\num we do have a local environment and\nthe beauty of that is that\nlike if you don't if you just want to\nunderstand the api like sometimes you\ndon't know if like this thing is going\nto work this thing that you're trying to\ndo with the flight api\nand you don't have to like you know sit\nand try to keep pushing your image to a\nregistry\nto understand that right the beauty of\nthat if you have something local you can\njust like\nwrite a quick like foobar example uh you\nknow\nwhat you're trying to accomplish uh and\nthen just run it on your local and so\nat free gnome we have and as well as uh\nflight\nflight snacks here there's a lot of like\nfood bar examples that you can play\naround with and see like what works and\nwhat doesn't work and really try and get\na sense of what the api is\ncan do for you and so that's what we use\nlocal for we don't we don't actually run\nany uh you know real workflows on on our\nlocal environments okay\nokay so uh to answer the\nfollow-up question to that or or maybe\nthe prior question\num the the way we\nlike where we started building flight is\nwe realize that not\nit's impossible to run everything\nlocally it's just\nespecially with the hardware constraints\nsecurity constraints\nuh and there are a lot of demos out\nthere\nfor a lot of products that say things\nwill run locally they're lying like it\njust doesn't run\num so and again i'll go on record saying\nthat\num so but we do\nrealize that running locally is very\nvery very powerful\nand that's why all the demos talk about\nit right it's just because it's so\npowerful it's so\num quick to get started with and that's\none of the reasons why we\nwe put so much effort into this getting\nstarted actually\njust kudos to both geo and hate them\nthey've been working on this\nthe the the thing that i actually\ndiscussed with them once was like i want\nlike to get started with less than\ntwo minutes and literally it's less than\ntwo minutes today which is just\nphenomenal\num so the experience that we wanted to\nprovide is like\nmost of the companies eventually will\nhave a hosted\nflight cluster if they're using their\nusers and and once you have a hosted\nfight cluster you\nwill use the host effect cluster right\nand that's why we have domains in there\nnow you could divide the entire like the\nclusters themselves to be separate\nsegregated which is what freedom does\nuh while spotify and lyft and few others\ndon't do that\nbecause it's not necessary internally\nyou could divide that\nthe benefit of either one is that you\nknow isolated domains in one of them\nright you know which is actually pretty\ngood you can always test and deploy\nthings very very isolatedly\nwhile in the lift case right you will\ndeploy\nonly the data plane multiple times but\nthe control plane remains steady and it\ndecides when to send things to other and\nthe users get a consistent interface\nso that's how we develop domains uh and\nso the ideal iteration structure\nis knowing that there is a hosted\ncluster\nis uh you write code in python\nor in java sdk you run it locally now\nthat run it locally comes with a caveat\nbecause that's why we\nspend time that python itself runs\nlocally but like if you're using\nlet's say uh distributed or something\nyou can't run it locally so it's okay\nyou you run whatever you can locally and\nit will try to run it on one machine\nuh and the idea is to get a sand sanity\ntest the\nright unit test for each of the tasks\nyeah if you want to you know even\nfurther improve it which i highly\nrecommend\nand so everything is unit testable uh\nyou and and then the last step is you\nuse flight type system\nso the if you saw like he wrote name\nstring\nnow if you try to pass an integer to it\nflight will complain\nright will not let you pass an integer\nto when you expect a string\nthis is another additional level of\nsecurity where it\ntries to prevent or protect users from\nhaving problems at a later point in time\nbecause pipelines invariably tend to be\nvery expensive\nand so if you can find all of these\nproblems right at the point of writing\nthe code\nit's much easier uh to debug and\nyou know prevent uh you know\nmistakes that can you know invariably\nhappen when they're in the users\nso once you are happy with all of this\nyou should push your code\nto a flight development environment okay\ntest it on remote because no amount of\nlocal testing is you're ever going to do\nscale testing\nso then you can test it on remote and\ndevelopment can be as scaled as you want\nor as slow\nand you can have like you know quotas\nand so on that's what all flight comes\nwith\num once you're but you may want to\nupgrade now you might find a bug in\nremote and that's why we\nare again it's experimental i want to\nremind that but make fast register or\nthe fast register stuff that\nuh jeep showed is essentially to take\nthat point\nafter your remote you find a bug\ntake that bug and fix it and reiterate\nand rerun we wanted that\niteration cycle to be less than five\nseconds literally that's the goal that\nwe went with\nand so it is less than five seconds\ntoday uh it comes with caveats\nif you add a dependency it's not less\nthan per second but you're not right\nyou're right rating on bugs exactly\nonce you're happy with the bug you\nshould never use fast register\nin the ideal world to move it to staging\nor to you know pre-product production\nand once it's in production uh every fix\nshould\nbe subsequently done using a full full\nloop right like this\nso you're now testing locally you're\ntesting remote and then you go to\nproduction and you get this entire\nlife cycle of uh pipelines\nuh naturally through the system so that\nthat was the idea\nand and the idea is also development has\ndifferent access rights\nyeah each domain can have different\naccess rights different roles and so on\nproduction has different access right so\nyou can't like go and\nuh corrupt data from production you when\nyou're running in development you can\nthough like it's up to you now it's like\nthe\nadministrator and the users of how they\nset up the flight environment but\nuh that was a good got it got\nso just to make sure that i understand\nso when the way i look at it is that\nyou do whatever your testing you will\nrun only your unit test when you are\ndoing\nthings locally once it goes into the\ndevelopment\neffectively all the in functional tests\nuh more tests than what you have written\nprobably for your model all of them will\nbe running and\nnow that's passes and probably that\nhappens at regular cadence every few\nhours or something like that\nso you know you run your own test right\nwhen you go to development i mean i mean\nlike you run your own integration test\nlet's say you're using sagemaker or\nbigquery you can now run a bigquery\nquery\nand get results from bigquery and pass\nthem into your algorithm\nthat's the test it's not like the user\ndoesn't care about what the system is\ndoing like\nsystem tests are only administrator\ndependent right\nuser shouldn't think about it users\nshould expect\nthat flight is fully reliable as a\nhosted endpoint like an aws service\nright that's that's our gcp service\nright it's just not\nthat was the intention yeah uh one quick\nquestion you said that is basically\ndetermined by the control plane so\nin order to move from dev to staging to\nuh\nproduction do i have to make any change\nin my code or is it like just that i\nspecify the domain and it will run on\nthose domains or something\nokay yeah no changes in code whatever\nchanges are\neither by convention that you guys want\nto do or by\nwithin the company or you know by\nrecommendation so for example at lyft\nwe have different roles like i am roles\nuh roles uh associated with different\ndomains\nand that has and we have different life\ncycle\non the docker containers themselves so\nthings that once get to push to\nproduction\ndon't get pruned but while the doctor\ncontainers in development get pruned\nlike every seven days\nuh so you know and the data also right\nwe have policies on data\ndata generated in development gets\nproved very quickly data generated in\nuh production doesn't get proven with\nlifecycle policies\num so this can be enforced just purely\nbased on\nits convention list right we just\nprovide the names and\nthese names are arbitrary you can come\nup with your own name called santosh's\ndomain right it should be fun\nyeah last question do you have like uh\nidentical data\nfeeding into all these environments from\nlocal to production\nor can you actually set uh to read from\nproduction data directly because i don't\nthink you modify data internally\nno so yeah you can set whatever it's\nrole based\naccess control so you might want to\ncreate sample sets which only are\navailable in development\nif your organization or the data\norganization is mature enough to\nhave those kind of things you can\nenforce but yeah today we\nwe we cannot really control for what we\naccess\nwhat is possible that's what like i'm\ntalking about here okay sounds good\nthanks a lot\nyes um so for at freedom we do have uh\nyou know\nwe we can read from production buckets\nin research if we want so\nthose are all just those are all handled\nby the administrator and you hopefully\nthe user doesn't have to worry about it\ngot it got it makes sense\nall right no more questions we can move\non i think we're uh like 35 minutes\nhow big is your computer i'm just\ncurious it is it is not as big as\nhaytham's\nuh as i as i've determined but\ni mean it's like a i9 with 16 gig or\nsomething like that\nuh inaudible\nokay so not the biggest mac you can buy\nbut close\nno definitely not the biggest mega\ncompetition okay\nall right thanks um\nprofile do you want to take it away do\nyou want to show the cli\nuh yeah sure uh let me just share my\nscreen\ncool\nokay you guys can see my screen\nokay cool uh so i'll go over\nsome of the work that me kaiten and your\nraj has been working on\nso so we have a\nflight console which is the ui for\ninteracting with flight for\nuh running your workflows and uh\nand checking their outputs and\nregistering uh new workflows uh\nthrough the ui uh so we all\nwe have a corresponding slide kit\nuh which is a python based uh cli\nto actually uh interact with the same uh\ndomain objects that flight provides now\nwhat\nwe are also working on is parallely\ngetting a flight ctel which is\nuh synonymous to cube cpl and\nuh to have the same experience that you\nget through\ncube cpu in slice ctl and uh the same\nkind of semantics\nthat you have in the ui and the python\nkit you will get it in the flight cpl\nso you can create executions you can\ncreate projects\nuh you can register uh workflows uh\nuh so currently we rely on\nall the uh the workflows and the tasks\nwhich have been registered through\nflight kit and we consumed those as\nuh tar files or uh plain flat files and\nwe try to register those through flight\ncpu\num so that's a nutshell uh what exactly\nare we\ntrying to do with the flight ctl uh\ni'll okay just a second so i have a\nwe have the documentation already\npublished\nin the main slide page i'll just go over\nthat so this is the link\nthat we have for all the api cli\ncomponents\nwe have\nso we have the i talked about like we\nhave a\nuh python kit uh so i'll go over the\nthe flight ctr that we have been working\non\nand we also have a flight kit java\nversion\nuh so this documentation goes over how\nexactly\nyou have to install uh flightcheck here\nlocally just local and install this\nand there are certain minor\nconfigurations that you need to do\nuh to tell tell flight ctls where\nexactly your\nadmin is running and what sort of\nsettings do you need to communicate with\nthat\num i already have something running so\ni'll just uh\nshow you that but uh and the way we have\norganized the documentation is also\nsimilar to how\nptl is like you have verbs and nouns\nuh over here or how exactly uh you want\nto\ncreate some resources and uh gets\nget information about resources or\nupdate certain resources\ndelete something or register is\nsynonymous to\nuh uh creating some resources over here\nuh and config uh\nthe nouns with uh so\nif you want to use a noun over here\nyou'll see like creating a project or\nuh you're updating a project or\nsomething like that\nso uh it says give a small demo on how\nexactly these things work so\nthe documentation gives a run by of how\nexactly\num you want to go about\nrunning these commands so um so this is\na simple slide ctl create for creating a\nproject\nuh it tells you like what all flags are\navailable for that particular\nuh option uh so for project\nuh country there's no other additional\noptions other than the\nfile uh so i'll just\ngo over it yeah so\nthis is the example slide ctl uh create\nproject option so\nsorry it has options uh it's not there\nin the main page over there\nso here you have uh creating a project\nrequires the name of the project\nuh the id of the project uh and the\ndescription\nuh there's another way that you can\ncreate a project is just\ngive us the input of a flat yaml file\nuh with all the contents and you should\nbe able to just\npass in to the file flag and it will\nregister that uh\nonce you have so this is the first thing\nthat you usually do you create a project\nand uh and once you have the project\nthen you\nstart registering all your tasks and\nworkflows in there\nso uh over there\nonce you i have created the project then\nyou'll start\nregistering stuff uh into that project\nso uh this is similar to how you do\nin uh flight kit python or java you\nspecify the where are\nyour all your serialized files\nuh so in this particular case i say\nflightctl register\nmy files are over here in this directory\ni want to register\nit only in the development domain uh the\nproject is basically flight snacks\nuh other options you have is basically\nyou can just\ngive a tarball and it can be an http\nendpoint as well\nhttp or sdps endpoints or it could be\njust the gun zip file of that r\nand uh so flightctl would be able to\nread\na read those and register with the\nthe running slide admin\nwhichever slide cpl is configured to\npoint to\num so just you know\nstart with some examples over here so\nthat would be easier so\ni'll just demo something so i have\ni have a local version which has uh some\nminor changes so i'll just\ngo for that that will be helpful\nso this is the same command that for for\ncreating a project\ni can just say that i want to create\na new project it says that the name is\nflight demo id is this and description\nis this\nand says project created successfully\nthere is\nother ways that you can do the same so\nhere\nthere's a file option that you can give\nas well so i have a\nyaml file that i've created for this so\nuh project.yaml file i'll just show you\nthat as well\nso here you can specify similar to how\nyou do\ncube sql deployment so you can specify\nall the configurations in this file\non how exactly you want to define your\nproject\nand it should be able to pass in this\nyou should be able to pass in this file\nand create the project\nokay so i can just show you that\noption as well so this\ngets you that project created now\nyou can as well understand what are the\nprojects that you have\nregistered uh so here you can say get\nprojects uh which will give you like\nwhat all projects\nthat you have so uh we had\na flight ctl demo and flight ctr demo\nfile this\nthis particular one was using our demo\nfile so that gets\nregistered over here and uh\nyeah so you would be able to do the same\nand along with the inside the\ndocumentation we have\nadditional options that uh\nthat you can do with the get is\nbasically you can specify\nuh the name of the uh the project itself\nit will get filtered on that\nuh we'll be adding more filters to it\nget more details from the project and\nallow you to filter based on that or you\nyou can get the output of the project in\na yamaha\njson format so that's also like pretty\nsimple to\nuse over here so you can\nso this dumpster and diamond definitions\nof all the\nuh projects in here\nokay so this is uh\ncool now i'll show the\nfile example that i was talking to you\nabout so\ni already have a serialized version of\nsome of the work workflows\ni have put them locally\nso this is a simple launch plan uh this\nis from directly from the cookbook\nuh which just does a great thing uh\nbased on the\nthe time uh that the time parameter for\nthat particular uh and\nwhat i'm going to do is i'm going to\njust\nregister this\nokay cool so yeah so this one says that\ni already have something\nwith version one running so\nyou can additionally pass that okay i\nalready have some versions already\nregistered\nlet me register a new version version 10\nand\nokay so it has successfully registered\nall the other higher versions if i\ndo it again it will fail the same way\nthat it failed earlier because it\nalready knows that there is a version\nexisting\nif you want to continue on errors there\nis a flag option minus c\nwhich it just tries to skip over\nany of the errors that it gets so that\nit tries to register the rest of them\nall of them are failing with the same\nissue but\nif i just change it to version 11 now it\nshould be able to\nsay that i have registered this\nas a version\nokay and\nso that was for some of these i also\nalso have\nyou can also do a register using\nan http endpoint\nso i have an http endpoint currently\nrunning as\nuh in python so just in this directory\nand it will be doing the same thing\nthis one will complain again because it\ndoesn't have uh\nthese registered files already exists so\nit doesn't matter\nlike we already have the same workflow\nas the latest versions of those\nuh running uh registered with slides\nnow we'll try to see how\nhow these registered workflows actually\nlook\nso you can do uh\nguest launch plans or get the actual\nworkflow objects to see um\nwhat what exactly is the uh registered\nversion with flight\nso this will tell you like uh you can\njust do a get\nfrom the launch pad okay this these are\nthe versions that i have\nuh that i have registered if you want\nmore information about\nit you can do\na yaml version of it and dump the\nmore detailed output of that\nand yeah so you can interact uh\nwith the same way\nuh once you have registered the next\nthing is basically you want to\nexecute the launch plan and so currently\nwe are working\non creating an execution uh uh there's\nalready ones that exist in\nuh flight five kits so i'll probably use\nthat\num so\ni just have this\none which is for flight cli\nuh this is the the python version i'll\nalso show you what happens\nwith the\nthe cto version also same way\ni think my ip has probably changed here\nlet me try using\nthis is inside ctrl\nsomething soft uh\npointing to the strong endpoint\ni'll remove them\nyeah it seems like it's using a tls i\nthink i have turned it off\nuh probably some some some configuration\nthat i'm missing right now\nuh yeah it seems to be correct\nokay uh i i think uh\nrunning all the time but uh basically\nonce you have those\nexecutions created you would be able to\nsee it\nin a similar way in the flight console\nso\nuh even in the ui so here these were the\nexisting ones that were running uh\nthe these were the projects that you had\nand the new one\nshould show up the one that we created\nokay yeah i think my admin went\ndown for some reason that that might\nactually cause this issue\nokay anyways like uh we have uh\nthe documentation uh please go through\nthat and please provide\nfeedbacks uh and\nyeah we would love more feedback on this\nthanks are there any things that are\nstill\nsort of largely missing between what you\nhave now on the flight and the python\nversion\nyeah yeah there are uh quite a lot this\nis\njust like the the basic version we are\ntrying to get out like where you can\nuh register create executions uh and get\nsome of these executions and\ndominate those executions uh there are\nmany other flavors and\noptions that people however you want to\nadd to these\nuh to be compatible with what we have uh\nslightly\nand so if somebody wanted to help and\nsort of jump in is there\nlike a list of issues or how would i\nmean it sounds like there's\ni mean there's a lot of surface area\nright that yeah\nso we have the same uh we have flight\nctl issues\nuh in github so people can pick up from\nthere\nany new ones that are missing cool\nlooks awesome yeah it's also nice\nso i think anybody can pick up and\ncontribute pretty easily\nuh if you are interested in learning\ngoals so this is one of the best\nways to start learning go and it's fun\nto write like gta stuff\nyeah this was my first time learning go\nso i'm mostly a java developer with\nuh my entire career so this was a good\nchance for me\nand it was uh yeah there were initial\nhiccups but you're getting used to it\nokay thanks thanks a lot yeah add one\nthing like i think\nthe flight city and the reason why we're\nbuilding is i i don't know if everybody\nknows it's because\nuh you don't have a virtual environment\ninstalled everywhere\nwhat we want to do is have a portable\nbinary that just you can just do a brew\ninstall or\npip install uh not girl installed right\nand then the other thing is we have\nprojects and so on which doesn't have\ngithubs today\nwhat professor is essentially you can\nhave project camels that you can\ncheck in into some master repository if\nyou want to use full githubs\nfor everything including managing\nprojects and so on you should be able to\ndo that now\nwith this um and more and more so\nright going forward so that those were\nthe intentions\nlike uh like for example flight admin\nhas ways of managing cluster resources\nas ways of managing\nthrottles quotas you can use github for\nall of that\ni think we're close to out of time\ni'm not sure soren if you want to do you\nwant to try to sneak it in or do you\nwant to\nwait till next time\ni'm sensitive to folks in india who are\nit's like late at night and their\nfamilies are\nwondering what the hell they're doing\nstill on this call but\nperhaps if i do it next time so\nwe don't have to to rush to too fast\ni don't know what do you think\ncabin\nup to everybody else like i i don't\nthink i have also followed as much\nthough i've like talked to sultan a\nlittle bit but what do others want like\nbasically what soren is helping with is\nbuilding the um\nchat for flight which is probably one of\nthe most\nrequested features by people so\nit's fine by me\nokay so well go for it and we'll take\nyou know the video be up\nand we'll okay if people have to peel\noff then they can\nwatch the video i'll try to shake notes\nokay correct we'll just we'll just we'll\njust power on so apologies to anybody if\nyou need to go\nwe understand all right and let's do it\num okay let me just\nshare my screen\ncan you see my screen\ncan you see my screen yes is it the\nright\nright one or is it the\nit's in presenter mode i think it's in\npresenter mode so it's\nyeah but i think it's fine you can see i\ncan see at least we can read it\noh let me switch maybe better now\nyep okay great uh so\nokay let's uh talk a little bit about\na little bit about helm flight and helm\nuh so we are currently in the process of\nintegrating slide into our\ninto our system into our platform and we\nuse helm and so it\nmakes sense for us to yeah to basically\num uh help with the\nsound support for for flight and there's\nalso interest\nfrom other people as well so um\num what i what i want to do first is for\npeople who\ndon't know helm just to to give you a\nbrief\nuh overview and and then move on to\nuh basically what uh\nthat more the the flight part what\nwhat's been done and what's what's\nwhat's coming um\nso um what is hell\nso it's that's from the the\nhelm website it's package manager\nfor kubernetes basically a way to\nto find and share and distribute\npackages of kubernetes resources\nand i i i don't have any numbers but\ni think it's by far the most popular\npackage manager for kubernetes right now\num so the main\nkind of the core concept of helm are\ncharts which are basically packages\ncollections of files that describe\na related set of kubernetes resources\nand there are\nthree main ingredients of chart so one\nis\nmetadata like name\nand version of the chart version of the\napp and also dependencies\nand we'll see that in more detail in a\nmoment\num it's all and usually in a\nfile called chart.yaml\nand then there's templates which are\nbasically\ntemplated it's basically templated the\nammo templated\nkubernetes resources\nand and third there are\nconfiguration values which\nare uh which can contain different\ndeployment options different variants\nfor instance\nand then they can be applied to these\nto these templates to to create um\nyeah releases and different different\ndifferent variants of these resources\nso here's an example\nof our a chart dot yaml\nwhich is actually from um\nfrom the flight pr\nso it just specifies the basic metadata\nlike\nyeah name and description and versions\nand so on\nand it can also um\n[Music]\ndeclare dependencies which are um\nbasically other charts\nyou want to include in in in your chart\nand\nand use and basically deploy um\ntogether with your chart and also\nconfigure\nso here we have um contour and\nthe spark operator but yeah of course\nbasically everything\nthat is packaged as a chart you can use\nin\nyour own so you have some\num yeah which enables you to\nkind of decouple things and enable\nlike soft engineering best practices\nbasically\nuh yeah so um\ntemplates are as i said basically\nyeah templated yaml so it's just yaml\nwith\nuh text templates in it and you can\num it's\nyeah basically kubernetes resources so\nhere we have a deployment\nand what we can do\nis essentially save for certain fields\nwhich we want to\nto uh to vary\nfor for different uh releases we can\num add\nthese this template mechanism and say\nokay this\nis something i want i want to change\nbased on on my values and\num\nalso it's not only the values but you\ncan also\nuse like template functions um to make\nit a little bit more more\nflexible but yeah it's basically\nkubernetes\nresources with uh with some\ntemplate values and um\nfinally we have these the values which\nare\nwhich we can apply to to the templates\num and\nthe uh which allow us to\nor which allow chart users to override\num to customize um\nthe the chart with\nwith some some some values usually you\nhave\nthe values yaml that comes with the\nchart\nis usually uh\ncontains some default values um so here\nwe have something for\nour flight admin for for the template we\njust saw\num like which\nuh which docker image tag and resources\nand so on but this is usually\nsomething you can override\nas a user\nso one part of helm is this\ntemplate engine so basically we have we\nhave these three ingredients i\ntalked about metadata templates and\nvalues\nand what we can do is to run helm\ntemplate with the helm cli\nand out comes um just raw kubernetes\nresources\nuh which we can apply\nyou could apply to a cluster just\nyeah just this something you\nget out of customize for it for instance\nbut helm is also a release management\ntool so\num we can also use the helm\ncli to actually apply these things to\nthe cluster directly and\nto to manage our releases which brings\nmore more functionality so\nif you use helm that way\nwe can actually\nmanage\nmanage our releases and\ndo all kinds of things to to to make\nsure\neverything is properly properly\nproperly installed in the cluster um\nso there's a bunch of commands um\nin in the helm cli\nfor instance you can yeah you can\ninstall we\nwill see that in more detail and also\nof course upgrade when you when you\nchange things\nwhich will also prune\nresources so basically helm\nhas metadata in these kubernetes\nresources which\nallows it to to track which uh resources\nit tracks and uh when it is able to also\nremove things you want to get rid of um\nin an update and yeah also things like\nlike rollback if you if there's there's\na failure\nand of course uninstall releases\nand stiffing with with the help of a\nplugin\nbut of course you you are not um\nyeah you don't have to use the the\nrelease management you can also use just\nthe template\nengine basically and use it\nand then use another tool for for\napplying it to the cluster\nso\none one interesting thing is that you\ncan\npackage your charts um\nup into yeah basically\nthe files and upload upload them to\nwhat's called this what's called a chart\nrepository\nand there's all kinds of repositories\nand\nthe home website has artifact hub\nwhich basically aggregates these\ndifferent\nrepositories and you can just\ngo there and look for for helm charts\nfor different\nsoftware and then install it with helm\nso it's\nquite easy to to basically get started\nwith some\nexisting existing charts and try them\nout\num so this\nkind of looks nice but helm also has\nsome\nsome issues and uh actually we had had\nsome discussion in\nslack um we're using it but\nthere are yeah there's some problems\num like for instance the the text\nbased templating um\non yaml is kind of error prone as you\ncan imagine\nand there's also the problem that\nhelm values are kind of static\nso if you it's especially an issue if\nyou have\nuh if you combine multiple charts um\nyou can't use values themselves as\nvariables so that's a little bit\nrestricting depending on your use case\nand also if you use the release minute\nuse helm as a release management tool\nit can be sometimes a little bit\nfragile but\nthings are getting better\nwith every release so um\nwith l3 um\num there there was a huge improvement\nbecause before enhanced ii they\nthe um\nthere was actually a server-side\ncomponent which did all this\num talking to the kubernetes\ncluster and this was quite kind of kind\nof problematic and they completely\nremoved it\num and now it's more more client-side\nand that makes\nthings also debugging and\nso on makes it much much much simpler\nand\neasier to use less error prone\nand they also introduced three-way diff\nso if there are some\nchanges not from helm in on some live\nresources it won't\nthrow them away and lots of other\nfixes also yeah as i said it's very\npopular\nso uh that's always a plus because many\npeople already use it um\nyeah i think the last point i said\nalready we can just\nuse it as a as a template engine if you\nalready have another\ntool to for a release management\nso for instance teca and\ncdks they all\nprovide help support so you can import\nhelp charts if you\nsaid so so i think if you\nprovide something as a helm package\num there's a good chance that many\npeople\neven if they use other tools can can\nactually\nuse that hand char so i think this can\nbe quite\ncould be quite helpful for uh for flight\nas well\nokay so let's uh move on to to\nthe the flight helm integration\num so the first um\nthe first thing or the the the\nthere's a that's the pr from ruslan\nstanovich\nwho did quite amazing work to actually\nkick this off and\ncreate a a first-hand chart\nand he\ngot the end-to-end test working with\nwith his chart\nand yeah basically provided\ntwo two variants with different values\nfiles\nfor for sandbox mode and eks deployments\nwhat's interesting is that he has\nin spr most of the or i think all of the\ndependencies\nactually in line so it's more\nin that sense it's kind of kind of more\nsimilar to\nwhat we have now and with customize so\nall the\nall the resources or the templates are\neven\nfor things like the spark operator and\npytorch operator and\ncontour and so on they are integrated in\nthe chart\ndirectly so it doesn't use um\n[Music]\num like the external\navailable charts um for\nfor external dependencies um\nand that's actually what i'm i'm working\non right now um\nis based on on his amazing work\nso all credit goes to him basically he\nhe's done most he's done all the work\nuh is to to build on that and um\ntry to use um chart dependencies\num where they are available so\ni've started with uh contour and the\nspar operator um\nbut i'm looking into into adding more\nor moving more dependencies\ninto like a real chart dependency\nand i've also done some some updates for\nof the chart itself for the image\nversions and so on because of course a\nmaster\nthe customized version\nhas moved on and has changed\nor is this changing all the time\nso my focus is on getting the\nsandbox running first and\nthen on pke because\nthat's what we uh that's where our\nplatform runs and\nyeah hopefully i'll have a draft pr\nvery soon ready so people can actually\nhave a look and\nand try it out try it out but of course\nruslan's pr can also\njust use this branch and should\nwork even though the the flight version\nis\na little bit older\nokay so let's have a quick\ndemo um\n[Music]\nso\nso here is the um\nchart in its current version um so maybe\nyou can kind of have a little look\nbefore i actually try\nto install it um this is basically\nwhat we've seen in the slides right\ncan you see is it large enough\nokay cool um so we have the chart\nwe have um the values file with um\ndefault values and then there's for\ninstance\nthis is the values an extra values file\nfor the sandbox\nwhich is mostly empty but has a few\noverrides where we say okay i want\nthis local postgres for instance\nwith these settings and so on\nand then of course here are the\ntemplates\nso let's see um\njust reset that\n[Music]\nso the first thing i have to do because\ni\ni have these external dependencies is\num is to update them or to fetch these\ndependencies\num so i'm gonna do a helm dependency\nupdate\nand hopefully this will download\num\n[Music]\nthese two charts and that yeah\ndownloading now and we should be able to\nsee that\nuh in the ide uh\nhere here they go so what this\ndependency update\ndoes is just just downloads\nthe these two charts from the uh from\nthe repo\nor from there their repos could be\ndifferent\ndifferent ones and\nmakes them available locally so i can\nactually use them when i want to deploy\nthe chart\nand um yeah intellij is nice\nhere with its helm support because i can\njust expand it\nand i can have a look and see their\nchart\nso we have the contour chart which with\nall all the settings\nand of course also the the templates and\num and the default values and\nuh so what's nice is these are the the\ncontour values and i can\nwhen when i use that um\nas a dependency in my chart i can just\num over overwrite\num um let's see\num yeah i can basically i can override\nall the settings of\nany dependency any sub-charge in here as\nwell\nso that's quite nice\nokay so\nuh once i've prepared\num the or downloaded the dependencies\ni should be able to do a helm template\nwhich is this\nwhich just renders basically renders the\ntemplate so when i uh\ni give it usually i give it the full\nname of the chart but because i have\nthis locally here in\ndevelopment mode i just do it on the\ni just give it the current directory i\ngive it a namespace\nand i also give it a values file\nto be applied to the to the template and\nonce i do that\num it will just render the template and\ngive me\nthis huge yml and of course i can also\nwrite\nwrite that in into a file and\nuse it similar to uh\nyeah to the generated channel\ni get out of customize\nyeah but the other thing i can do is um\nto actually use\nhelm as the release management tool\num so let's give it a try\num so here we do the helm install\nwe also give it the namespace and the\nvalues and just\nfire it up and\nwhat i can do here in the meantime\nsee what happens in my local cluster\nand you can see it\nthe deployment was successful so it just\ndeployed flight um and\nin that case it doesn't wait for all the\nresources to\nto be to be ready but it can also\nset of flag something like wait for\nreadiness\nand you know it should be i can also\ndo in helm ls so if i have multiple\ndeployments um they will all be shown\nhere and\nand i get the the yeah the name the\nrevision\nwhen it was updated last time what's the\nstatus and so on\nand the versions and\num it should be complete now i guess\nso let's give it a try\nyeah here we go so we have um\nthis running and should be able\nto actually execute something so here i\nam\nin the flights next repo\nso let me try to just register\nsomething\nand here we go should\nhave worked yeah so now we are\nable to just\nrun the workflow\nas usual\nyeah so that's running um okay\nso now what if i want to\nchange something um\nlet's go back to the to the ide\nso let's say i need\nanother\nreplica for contour just because i'm\nhere now\num so i changed the the values or\nusually i would\num change the\nthe value here but yeah because i have\nit in development mode i can\njust do it on the novelist on the\ndefault value file as well and\num let's see what happens\nwhen i uh\nlet's let's first look at the diff right\nit's always better to\nbefore you deploy something to look at\nsome diff what is actually changing\nthere's a um that's not baked into helm\nactually but there's a div\nplugin which you can install\nand see what's\nwhat's what's changed so\nlet's let's call this helmdiff upgrade\nwith our values\nand um\nthen we can see that it should\ni just changed the replicas and the\ngenerated yaml\nand if if we actually want to apply that\nwe can do and do an upgrade\nand yeah it will just\napply to the to the cluster directly\num yeah i think\nthat's basically it um\nyeah are there\nany questions\nnot for me but this is amazing stuff\nlike i think\nthere's only one team that uses him\ninternally uh\nand and makes it like\nvery easy to discover and install is\nwhat i've realized\ni'm sure there are many other problems\nwith it but at least the initial\nlike the initial getting started is much\nmuch easier especially with cloud\ndeployments\num but i had a question so i saw that\nyou\nhave dependencies on all of these other\nchat is it\nso should we model like flight admin\nlike propeller\ndata catalog as its own helm charts and\nthen\num like make them as dependencies\nyeah that's that's actually a good\nquestion um\ni think i think\nfor for flight for something like we\nhave under control i think it's\nactually it's definitely easier\nto to to start with like a a monolith\nbasically and put it all in in one hand\nchart\nbecause it's easier to to iterate\nand also the question is\nuh what would be the benefits of\nactually putting it into\ninto its own chart i think the most of\nthe benefit actually\nis from is when\nsome someone else provides a\na home chart for some some software and\npackages\nup so you can use so you don't have to\nto maintain it essentially but when you\nwhen you um when you're maintaining it\nyourself\ni think it's not really worth the effort\nat least not to start it\nuh to break it up into a smaller\nsmaller charts but that's that's my\nopinion so um\nmaybe um there could be a reason to do\nit\nat some point um if you want to\nkind of um\nmake it much more modular if you say if\nyou have like\nif you want to only install certain\nflight components if you at some point\nif you get to that point uh then maybe\nit makes sense but\ni think uh\nfor now it's easier to actually have a\nsingle chart but then\ni use the external dependencies that put\nthem into\nto charge my opinion again\nno i think that's fair i think the only\nreason why i\ni think it might be good to not break it\nup into those many components but like\nprobably two conferences because the\ncontrol plane and the data planer\ncan be separated and you can run in a\nmulti-cluster mode\nuh which can be beneficial at times\nbut when you're starting off that's not\nthe good part of probably having and i\nthink you mentioned that in your in your\nslides is that you can't use variables\nin values\nso if i understand that correctly then\nlike let's say if i want to have one\nuh gcs bucket or s3 bucket\nacross the two like the control plane at\nthe data plane\nthen i cannot really i have to\nre-specify it for each of the components\nis that right for each of uh\nyes um\nit yeah i mean if you put it into\nlet's see if you control the chart\ni think you can model the templates in a\nway\nthat if that it could come from the same\nfrom the same value maybe then it's\nstill possible but yeah\nthe problem is more if if you have like\nif you have an if you have a dependency\non a chart\nand it says okay i have like this\nfor instance this contour replica\naccount\nand you want to make this replica\naccount\ncontrived example of course dependent on\nsome value of your own chart that's\ngoing to be hard\nbut i think this is another\nanother issue actually i'm not sure\nyeah i think that was probably what i\nwas talking about it would\nbecause it would be a dependent chart\nand\none value for both\nbut so another question in that so you\nthink\nwell would this solve the config problem\ni think the real problem with flight\ninstallation is the convey\nit's just so vast uh\nwith the flexibility comes like crazy\namounts of configuration\num there are very good same defaults and\nwe usually do them in the code itself\nbut\nstill there are some same defaults for\nlike a\nsandbox deployment aws\ndeployment or a gcp deployment let's say\ncan\ncan we hide that and config maps\ngenerated as well or\nis that not a recommended technique\num sorry\ncould you re-repeat the question yeah so\nwould do you like is it recommended in\nhim to actually\nuse values to substitute\nsections of a config map because oh yeah\nyeah yeah i see\ni see it yeah it is i don't know if it's\nreally recommended but i think for\ncertain things it's basically uh\nthe only way uh so kind of\nto put like full yaml for instance into\ninto a value and that's which\nthen will be the content of the config\nmap\nthat's what you mean right no i mean\nlike the config map can that be\ntemporarily\nitself oh okay um\nyeah no that's not not possible with\nwith helm so it's really the templating\nhappens at installation time\num i mean of course you could have a\nconfig map\num the config maps are also templated\nuh basically but it all happens at\ninstallation time right so you yeah but\nthen\nthe difference is the config map is\nactually a string even though it looks\nlike a gamma plate\nthe data yeah oh you mean the data part\nis yeah okay okay now i get it yeah\num\nactually i don't know um i\nthink my understanding is that you\nactually can yeah you can put templates\nwhatever you want\noh really i think so replacement it's\nnot really it is\nyeah that's actually one of the\none of the issues uh that it's\nwhy it's kind of fragile sometimes right\nbecause it's just tank text templating\nit doesn't\ndoesn't really know yaml or some\nstructure\nsome some uh even type source things\nlike that\nand that makes it but that\nmakes it also kind of flexible so it\nshould be possible yes definitely\ngetting the indents right in your\nyamalized big map that will be the hard\npart but but it does work\nyeah exactly yeah so this is uh let's\ncan we see some example here\nso you see these these functions here\nright so you type that to\nthis and then because yeah it's\ndemo it's uh you have to make sure that\nit's\nuh that has the right indication\num interesting yeah because my my\nbiggest thing is to actually take config\nmap\nsomehow and make that easy now i've been\nthinking of\nvarious different ways of doing that\nit's like i think besides config\nlike one of the core fun things about\nonward was that that configured\nwritten in python you write the thing\nand it generates the config photo\nbecause it's error to generate things\nbesides that i just couldn't come up\nwith and i thought like maybe them could\nhelp us a little bit but i don't know\nyeah perhaps there's a way maybe we\nshould look into this\nmore more more closely\nif you're caitlin if you're just kind of\nlike saying if you're kind of writing\nyour keys in the\nin the configmap template and you're\njust like templatizing the values\ni think that would be fine and you know\nlike you won't run into indentation\nissues with that but like if you're\nif you're starting a string and then\nadding your template values uh\nsorry your yaml values in like as as\nblocks then\nthen you have to get the indication\nreally correct otherwise it it screws up\nbut yeah yeah but even if the value is\nlike what if the user writes something\nwrong\nwe will not fail at at the point in\nwhich it is built it will fail at like\ninstallation time and things will come\nup and crash\nso this is yeah yeah\nyes maybe mlint or something might help\nbut then you have to generate the config\nmap separately\nand then plug it in into as a i don't\nknow if that's possible\nso yeah it's not it's not necessarily a\nlinting issue as well right it's like\nis the value actually correct for that\nthing because then it would just kind of\ncrash\nthat caused the pot to crash or whatever\nafter the fact yeah but like\nlike crashing because of badly formatted\nyaml is worse than\nbecause of bad values probably we can do\nimprove over there but like you know\nright rightly formative values that will\nhappen with this right\nyeah okay but\nuh no but i i actually still like it\nmore than customized customizers has\nbeen\nlike becoming a little nightmare almost\namazing for flight\nstupid i think i think it can reduce\nreally some some repetition that you\ncan't avoid with\ncustomized at least that's my impression\ni'm not too familiar with\ncustomizing basically only know the\nflight\nthe slide deployment but i i think\nwith templating if you do it the right\nway you can\nif you have some like cross-cutting\nconcerns uh\nyou want something to want to apply to\nbasically all\nlet's say all deployments or um\nor all parts and\nit makes it easier if you control the\nthe\nthe chart so if you that's something\nthat's something that makes it a little\nbit difficult with uh\nto work with dependencies of course you\ndon't really control the templates\nin these dependencies and\nif something you want to change it's not\ntemplated there\nyeah you're out of luck you have to fork\nor\nfor the the dependency basically and\nthat happens often\nquite often actually um so\nbut if for your own charts where you are\nin in control of the templates i think\nit's it's much easier yeah flight has\nvery few dependencies to be honest like\non the\nand the kubernetes part of the app\nsystem but\nyeah maybe it will increase at some\npoint\noperators and sagemaker operator and uh\nspark operator so those dependencies are\nquite a few to be honest so yeah\nanyways but this is awesome thank you\nyeah\nhi caitlyn like i just want to\nlike i just want to add one thing in the\nhelm thing\nso like the config part i understand the\ncon\nconfiguration is hard and help but like\nwe are using helm as a packaging tool so\nyou package your flight tool\nas a like by default values maybe you\nhave some by default configuration in\nyour config\nbut if i want to add more config and i'm\nrunning flight on production using the\nhelm\nthat means there is a two way one way is\nlike i will follow the\nuh helm chart and then i will make\nchanges in the\nhelp right one way and the second way is\nlike i will not fork the chart i will\njust\nfork it and then i will use the\ncustomize so that means i want to write\nsome configuration like i want to change\nthe config map\nthat means i will pull your helm chart\nand then i will do some\ncooking in the customization where will\ni will change the entire basically\nconfig using a config\nfile maybe maybe i have a config.iml\nand i can use that file inside the\ncustomize so that i can replace the\nconfigmap data value with the file value\nthat i have\nthat means i don't need to change\nanything inside the helm or anywhere\ni just need to add the customize apart\nfrom help\ncombination of help and customize yes\nactually that's the right way as people\nare talking about\nso use helm as a packaging tool but\ndefinitely each company make\nsome changes according to them like\nmaybe some volume related changes and\nother changes\nso it is good like people will use\ncustomize on health rather than directly\nusing help\nto build the chart and then deploy\nsorry i muted i was just going to ask so\nare you do are you modifying with\ncustomize the\nthe templated yaml files in the helm\ntribe directly is that what you're\nsaying\nno so basically in the helm chart we\nhave some plugins for the customize\nso you can use the helm plugin to use\nthe customize\nso basically hand will do if we gives\nthe templating like a vml format because\nit's like a templating\nengine that will convert your templating\nand using the value file gives you\nthe raw kubernetes file right and gotcha\nraw kubernetes file that means you can\nuse customize to figure out the right\nresource\nand then patch the resource and then\ndeploy the\nthat means i have the basic config using\nthe helm chart\nbut i'm using the config map so that i\ncan pick the exact resource that config\nmap i have to change\nand now i can use a config like\nbasically i have\nentire config in somewhere in my\ndesktops okay and i can use that config\nfile\nand customize to just replace the data\nso that means i\ni will not use config like if you see\nthe other charts like\nnginx has a good thing so nginx and this\ncontroller will give you helm chart you\ncan deploy it\nbut if i have to make some custom\nchanges like nginx configuration\nthat means i have to manually change the\nconfiguration\nor else what i can use it i can use the\nunchart and then use configmap\nbasically customize so that i don't need\nto do manual changes\nthat's the right way so we can set the\nexample so that\nwhenever someone start using the helm\nchart he know how to write customize\nhow to put the exact gcp or aws file and\nthen use it basically that's the right\nrecommended way to use help\nyeah i mean i think this is\nthis is something that of course um\ncombines\nthe advantages of helm and customize\nand i think there's\nyeah there's people doing\nyeah using customize or something else\nto actually\nthen patch the\noutput of of of helm or the customer\nkubernetes resources on the other hand\nof course this also adds\ncomplexity here so you have now two two\ntools and yeah\ni think this it doesn't make make it\nmuch easier\nso in general i think i i try to\nto improve the helm templates in a way\nthat they are flexible enough that uh\nthat helmet is enough essentially\nbetter\ncool but thank you this is awesome uh\ni have one live question actually um\nmore for cayden so so if we move to helm\nsupport like what happens to customers\njust curious\ndon't ask this question to me i will let\nyou guys answer it\ncommunity\nsomething about him i said i have no\nunderstanding of him like i don't\nunderstand him\ni did not know customize to the point\nand then i started when i rewrote the\nnew customized template that you see\nthe documentation was in chinese mostly\nand i don't know chinese so\nit's like if i wrote something in\ncustomized if it doesn't work i\nsorry i couldn't read the docs\nso so yeah i don't i i'm not the right\nperson to answer this\nuh question at all i'm absolutely the\nworst person to answer this question\ni can just say that we can't so i can\ntell you\nwhoever that we are whoever's using it\nin production at\nsome scale i'm not using customize as\nmuch except for probably free norm\nuh so what they do is essentially have\ntheir own\nweird ways of deployment everybody has\ntheir own deployment\num for for flight propeller is where\ni think uh customize is used more often\nat least at\nlift and five and so on\nand even at spotify i think but other\nparts\nspotify also doesn't use it um but\nyeah but i i like if the question was\nwill it break people if we change from\ncustomers to help\nit probably will some people and then\nwe'll have to think about the right\nstrategy but\nit's not like a end game in that\nwe should do what the right thing is but\ni don't know what the right thing is so\ni don't want to\nmake a call over there that's fair\ni think that's an elaborate i don't know\nright kitten\nthat's an elaborate way to say i don't\nknow right i have no idea okay\nyes we will figure it out well uh that\nwas\ni think the lesson learned is we need to\nonly schedule two things because that\nwas a lot\num and a lot i did my best to take notes\nbut when we got down to\nusing customize as an input to the\ntemplating engine for helm my head sort\nof wrapped around an axle so\nyou might want to clean those up\nthanks everybody i think we've got two\nweeks till the next one\nplease ping me or captain if you want to\num show something\num but this is uh this is a lot of fun\nenjoyed it thanks everyone have a good\nday\nall right bye\nyou"
    },
    {
        "title": "FlyteMeet 2021/02/09 - Events, Lyft L5, Planning",
        "transcript": "or katrina if you're not talking\noh sure thank you\ngood morning everybody\ngood morning good morning\nfrom seattle\nwe've got at least one from india and\none from atlanta that i know about\ni don't know gleb where do you sit here\nin stockholm\naustin nice beard man\nyeah i'm really i'm really deep in this\ncovid business\nall right and maybe we'll get started\ngeorge\ni don't see miguel yet we miguel's there\noh my gosh oh there you are your name\nisn't true\num i guess\ni think we should just let you take over\nthe show miguel\nwe've got some news people welcome\nsamita down in the lower\ni'm on my screen left hand corner\nwhat else new yeah hi hi george\nhey\ngo first right now i'm running having\nsome technical difficulties with my\nworkstation\nand the end do you want to go uh yeah\nsure screen\nuh can you see my screen yep\nuh today i'm going to talk about the\nfight egress event\nso as we know currently uh from\nuh flight the propeller to fight me\nthere are a few types of events\nworkflow excursion events notes\nextrusion and task\nexecution events this event ascending\nfrom propeller and to notify\nabout the status update when there is\nexecution\nongoing and\nwhy would we want to get this event\nbecause we are currently working on our\nliving\nlineage integration with flight\nworkflows\nand missing this\nevent could help us to uh like filtering\ntheir needs message and eventually we\nare doing some enrichment\nfor these events and then send\nto our lineage system\nuh that's uh how we saw it and so\nuh it's very simple setup the\nthat this is how it looks like in our\nsetup and\ncan see should i zoom a bit or\nit's a little small yeah\nif you hide the themes i guess it will\ngive you more real estate\non the right hand corner press decks\nfull screen\nokay\nyeah so basically from flight propeller\nwe were sent uh\nit wasn't a workflow or start event\nand notes event and was a task event\nto fly to me and uh\nfrom there uh we are currently working\non a publisher\nuh which sends this raw events and then\nto a pub sub topic\nand then uh we in this we are have\nanother\nuh enrichment process uh which is that\nthey're going to be\nimplementing slide with me which is\nessentially just this\nred uh uh and if you are in reach the\nthe exclusion event\nwith uh additional data and then we\nwe were sending it to our streaming\npipeline for another service in our\nslide and this always works this service\nfor streaming public work convert that\nformat to our internal\nlineage command and that's how it works\nand how we may imagine this would work\nand\nwe are working on prototyping to have\nthis\nlow and uh\nyeah this this our use case\nand uh when we talk about this thing uh\nwith kitten\nwe think maybe it can be a more generic\nthing like we have\nuh uh like publisher for the all-round\nevent\nand then it's a configurable\nfor like if you want the workflow or\nnotes or task executions we were\nsent that from flight with me\nand then we there is uh like downstream\nconsumers uh it can be our like\nenrichment process for\nlike uh for our linux purpose it can be\nlike\nfor the no email notifications and it\ncan be other\nconsumers so it can be it's going to be\nlike a timeout\nuh yeah\nso the next slides the ongoing work is\nlike\nwe have a pr implied with me which the\npublishing all the raw events\nand we are defining uh the schemas\nuh for the english event\nand uh yeah uh how were their like\nnew publishers configuration looks like\nthat's pretty simple it's like you have\nthis\nsection in flight admin config file\nand it says like external events which\njust enable that\nand seeing the type of the cloud\nprovider\nin our case we chose tcp and it set the\nproject\nand yeah and you should you will set the\nthe topic name and\nthe event types it can be or it can be\nworkflow\nuh not and tasks and\nthat's how the publisher will work\num i guess that's it for\nme do you guys have any questions for\nthis before\nhey uh can you go back to the this is\nsantosh can you go back to the wires for\nthis event a little bit\ni believe we saw that okay\nso this is mainly for the lineage\nis it yes yes okay\nand when you are providing the lineage\ndoes it actually\nconnects the execution so it doesn't\ngive the data lineage\nas well as your um execution lineage or\na combination of two that\nthis data was transformed so data a got\ntransformed to data b\nby this particular version of this task\nis that the level of\ndetails it will provide uh i think\nin our case it's more like the uh\nnote that is like a workflow and then\nthe the data set\nis actually combining like changing\ndifferent workflows on the data set\nyou got it okay thanks\nbasically i think that was my question\nto like is it\npossible to get a little more uh insight\ninto your lineage service just like so\nthat we can\nsee what you guys are doing okay okay so\ni think in our case our linux\nworks like like these um\nwe have basically it's all the\nthe tasks the task as a upstream\nuh maybe i can write it\nso it's like this we have a uh it's\na basically flat structure and task\nhas a strong property called name\n[Music]\nand\nupstream\nlike this could be a list\nand then we have an\n[Music]\noutput you are on\nyour eyes and\nit also has this field called\nwhat do you think about\nit's a like\ndata set something like this\ndata sets what what kind of uh\nit's a flat structure and we also have\nthe\ndigital field it's called it's\nexternal so basically saying if the\nuh this this thing produced\nit starts look up for external data sets\nand at the wrong time it will build up\nthe dependency there's a\nthe service yourself were just chaining\nup these things\nlike it will look up for like maybe\nanother field for i think there's\nanother\nworkflow of\non text it's basically\nclosure about what this is about\nand it looks like this way and so it at\nthe wrong time when you\nsearch for specific workflow it will\ngrant all the slab tasks\nand they find all the dependencies for\nthis task and the invert transfer\nthrough the graph\nand find out strings using this dataset\nand like external to find a dependency\nto other\nworkflows or data sets that's how it\nworks more than\nso is this that the questionnaire is\nthis for visualization\nand like discovery or is this more than\nthat\nit's a bit more than that so we have\nactually uh\nanother layer but on this learning\nsystem that\ndoes our internal stuff like uh\nuh for like data sets basically we're\nhaving some alerting and systems for the\ndata sets\nand they're they are using the linux for\nlike figure out what is\nupstream and downstream and dying and\njust visualizations destroying your\nspace and other\nbusiness\n[Music]\nsantos you had a question sorry yeah no\ni was just wondering so\ni mean based on my understanding\nuh there is a flight execution id\nand you can pretty much change the\nintermediate data sets\nas well as what versions of tasks have\noperated on them\nuh so i was just wondering like uh why\nis\nthat insufficient and you have to\ngenerate these event based so like\nwhat's the delta that this event-based\nuh lineage is creating because from my\nperspective that was\nsolving substantial amount of problems\nso\nwhat i'm not clear about is that what\nare those\nextra new issues that you have\nfor which event-based lineage is going\nto be a\ngood solution does it doesn't make sense\nthe question\nand\n[Music]\nuh so one thing that our lineage\nservice has that doesn't isn't available\nin fight is an internal model so there\nis a\nterminal model that has the concept of a\nworkflow this is not the right workflow\ninternal models that has a concept of a\ndata set and assignment to ownership\nand then you can imagine the case when\nthere are two teams and they interact\nbetween each other through publishing a\ndata set so one team publishes the data\nset and the other consumes it\ntheir work workforce is not really\nconnected actually one team can be using\nlike a completely different processing\nframework or like\ndifferent service accounts and like for\ninstance a case\nlet's say is there a second an incident\nmanagement situation when one team\nhas a problem and they need to\ncommunicate with all their downstream\nconsumers\nto say that okay don't wait for data in\nthe next three hours we have a problem\nso\nlike they can actually do this because\nwe track this information\nand they can also say announce like a\nbreaking change so say we are going to\nbreak this\nthis way or we want to add this new\nfeature to this data set\ngot it done so uh if i'm correct\nif i can summarize like an execution of\nan end-to-end workflow is already\nchanged by default in flight what you're\ntrying to change\nnow is that a number of these workflows\ntogether\nand potentially it's not only about\nlineage but it's about\nproactively figuring out alerting and\nmonitoring\ntwo on top of that okay that's\nthat's very clear thanks\nand i i i actually gleb and thank you\nfor the presentation i just wanted to\nadd one more thing so the the eventual\ngoal of data catalog was to reach\nin this direction so whatever you guys\ndo and like we learned from you guys we\nwould love to\nsee if you know we can incorporate at\nsome level within data catalog\nand with some other open source\ncommunity systems like maybe a munson\nand things like that right odpi and so\non so uh\ni think the event work is useful beyond\nthis even\nso it's just an amazing so thank you\nwe have actually investigated the data\ncatalog but the\nthing that we are saying why we chose\nthe event\nis because we have to pull the data\ncatalog\nuh like frequently to get well to get to\nfetch what's happening\nhere so why would no\nso we decided to go from the other pros\nlike when there's happening\nwe should get the information\nimmediately instead of just polling\nand that's why we chose to use events\ninstead yeah one way to\nput that would be that you guys already\nhave a mature infrastructure right\nat spotify but most of the other\ncompanies don't so\nprobably uh we could help that through\nyou know data catalog\nbut this is awesome thank you yeah\nthanks guys this is great\nuh stop sharing here\n[Music]\ndo you guys have other questions\ncan you all see my screen\ni'll take that as a guess yeah yes and\nyou know you might want to give\neverybody a little background what i'll\nfind it\nwhat how you're different from living\nyeah\nso level five is essentially lyft's\nautonomous\nvehicle division um basically lyft's\neffort of\nproducing their own self-driving vehicle\nuh software\num we uh are\ni guess the way i kind of phrased it was\nat an initial point kind of a startup\nwithin a larger organization\nand we didn't necessarily adopt all of\nthis infrastructure so we kind of\num basically started from scratch\nand about i don't know a couple\nyear and a half ago or so i reached out\nto the flight team\nin the rideshare core organization about\nuh deploying flight at\nuh level five so we manage our own uh\nflight deployment outside of\nrideshare which is the core course\ncore lift it's essentially everything\nrelated to the app\nso we deploy flight on a managed\nkubernetes service uh via eks\nand um there are a couple distinctions\nbetween kind of\ni guess most of our normal users and\ntheir workflow\nthis is largely because we also use a\nmonorepo\nmost users about eighty percent of the\nusers\nuh at level five um basically interact\nwith this\nmonorepo we called av software\nand uh in that monorepo we basically uh\nuse bazel\nwhen people first i guess when level 5\nwas first starting\nmajority of the people coming in were\nfrom google\nuh so this is also kind of why the\nmonorepo decision kind of got started\nand then uh bazel is basically the open\nsource version of google's\nuh build system it has multi-language\nsupport\nuh support for building containers and\nsupports custom rules\nuh basically each um and i'll go more\ninto\nexamples um but each kind of you define\na build target\nand each build target uh corresponds to\na rule\nwhere it will basically run\nsome sort of uh like basically\nessentially compiles\nor interprets the code in that specific\nlanguage so handles all that for you\nand you can write your own um which we\nkind of did for flight itself\nto better support our users um\nand it's kind of as far as some of the\nuse cases and\num you know basically we have a\nperception metrics\nuh which is for example you make a\nchange in the perception stack\nyou want to then run that\nsame perception stack across a fixed\ndata set see if you\nincorporated you know any regressions or\nimproved those baseline metrics ml\nautomation pipeline is something that\nis currently ongoing basically this\nwhole\nprocess from the data gets adjusted from\nthe car\nto s3 that triggers then\nsome drive data then there's some scene\nselection basically which scenes are\nkind of\nvaluable to train on and to generate a\ndata set\nonce that data set is generated run it\nthrough\na specific model and then produce new\nweights\nand then run metrics again and this\nongoing process and so before this used\nto be manual\nuh now there's been a huge effort to\nbasically automate this with flight\num scene generation is also um\none use case of flight basically uh all\nthe\nscenes essentially scenes are basically\njust a start time and end time that gets\ncaptured by\nthe vehicle um this could either have\ndifferent representations could be a\nimage lidar point clouds or\njust radar or a combination of all of\nthem most of the times combination of\nall of them\nwhich basically then that they generate\na scene and kind of store in a database\nfor example\ndoing searches like hey give me scenes\nwith bikes\nor give me scenes with pedestrians\num and as well as data set generation\njust uh\non kaggle there were two data sets that\nwere\nused by lyft that well\nbasically with the data that was\ngenerated from the avs\nand uh both of those large data sets\nwhere\nflight was being used to generate those\nflight and spark\nso these are some of the use cases and\nkind of uh one of the complexities is\nyou have this essentially code that's\nrunning in the car but you also want to\nrun in the cloud most of this code is c\nplus and uh bazel itself is honestly\npretty good\nfor uh very great at c plus building\nthat\nso um that introduced a lot of\ncomplexities\nuh the original workflow was you\nbasically published an artifact from\nuh this av software repo and then in a\nseparate repo\nyou would um download that artifact most\nof the time it was a debian package\ninto this other basically installed into\nyour workflow code in a separate repo\nin your docker file now that caused a\nlot of\npain points from users because i make a\nchange in navy software\nand this one repo publish it go to this\nother repo update the version\ntest and then something fails and i have\nto\ngo across multiple repos and do the same\nthing\npublishing these artifacts isn't as fast\nas we'd like but\nso that introduced um\nbasically a use case for uh\nour own custom bazel rule\nwhich in this case a user would\nbasically similar in\nthis one single mono repeat av software\nwould define their workflow this is the\nold syntax\nbut we are working on migrating to the\nnew\nand then they would define a pi binary\ntarget\nso this is just a standard python\nbinary target that you would define um\nin any\nother python regular python file and\nthen\nafterwards you would define this base\nrule called abs workflow\nand here these are a couple parameters\nthat we kind of specify\nname of the target something um\njust needed main so this is kind of your\nmain\npython workflow code\nspark whether you leverage spark or not\nit'll use a different base image\nwhether you specify true or false which\ndomain you would like this to be\nregistered in\nthe name of the actual workflow workflow\npackages\nthis is kind of similar to the flight\nconfig file\nworkflow project and then just workflow\nengine\npi binary now under the hood\nwhat this all does is actually\nbuilds an image uh with your workflow\ncode and\nall its dependencies and then also\nwill then basically register your\nworkflow with flight\nand then prepare a script for you to\nkick it off\nso before i dive into a quick demo any\nquestions\nokay um so\nhere is basically i have my uh total\nrecords\nworkflow and then i have\nmy build file which where i define my\nabs workflow target and essentially\nto build this i would basically run abc\ntail build\nbuild artifacts and then specify the\npath and the name of the target\nthis should be pretty quick because i\nran it already pre beforehand\nand if i dude\nso here we can see actually the image\nthat got\nthat was built and this also got\npublished to our\nartifactory which is our docker registry\nand\nif i decided to instead\ntry this with run and this also\nwent ahead and already registered as\nwell so run basically just makes a call\nout to the flight clin now that it's all\nbeen registered\nand everything with flight and here i\nlaunched an execution\nwe spit out an execution url\nand here it is running\nand then and that would basically be it\nso user would then just kind of make\na modification to their workflow go\nthrough the same process\nbuild run and it dropped down from like\nhours i guess developing for our core\nusers\nto a couple minutes and in some cases\nthere have been some optimizations\naround darker layering\nto within less than a minute and then\nwe're\nalso hoping to integrate like fast\nregistering faster live seeing how that\nworks\nand experimenting with all these use\ncases but we also expect\nthe new flightcast sdk to help improve\nthe developer workflow because they\nno longer will have to run uh this\nworkflow\nor this abs workflow when they want to\ntest locally\nthey can just run this pi binary just\nlike any other python file\nso we had a demo to a couple of our\nusers and they were pretty excited about\nthat yesterday\nand that is about it if\nanyone is interested in the actual kind\nof implementation behind\nthe scenes uh it's kind of this file you\ndefine a\ndot bazel file and then you use starlark\nwhich is a kind of like\nnot as great python is\nthe way i describe it but um and then\nbasically all of this\nis kind of all the magic underneath the\nhood of that one\ntarget\nthat's awesome question when are you\ngoing to open source this\nuh i'm hoping once we do the rewrite now\nwith a new\num what's it called the new flight uh\num yeah new flight kit\nawesome thank you\ni have a question how does it work if i\ntry to like if i have like my commit i\nchange some workflow\nand i try to register the workflows that\ni didn't change\nuh so let's try it basically\nlike so like if i didn't change\nsomething does it hash it or like would\nit register a new instance\noh for right now we actually enforce a\nuh git commit every single time um\nyou make a change and that's just uh for\nthe same reason of\ntraceability and kind of no easy way of\ngetting a version\num that will kind of where it talks\nabout trying to find a different way of\ndoing that with um\nfast register and faster life so we're\nhoping um that version\nwas already there so\nyeah and uh gleb what happens is because\nhe's created a single\nuh bazel target por workflow\nbazel is very smart but it will only\nlike\nserialize and register that one target\nright if you change it\nso\nyeah it'll only compile against its\ndependencies if i make a change in\nsomething that\nisn't relevant it won't uh it won't uh\nbasically\nbuild yeah i was more curious like\ni used very little bazel but i know\nthere is some question\nhashem mechanism that tries to make a\nreproducible builds and\nwould be great like if just everything\nwould be integrated with this hatching\nmechanism as well like\nso like you can just run the build it\nwill register everything and\nif nothing needs to be registered it's\nnot going to register it\nyeah i think that's what it's doing\nright in his demo right now it didn't\nregister right when you re-run\nthe time yeah when yeah when i rerun it\ndoesn't uh re-register\num because it's already um essentially\nbuilt\nlike what it uh at the end of the day\nit produces kind of a final shell script\nthat then\ngets invoked when you do run and that\nshell script kind of sets up things like\npython path and everything like that and\nit i guess does uh\ni'm not sure about the internals but\nonce it calculates like\nit's already been built it will just uh\nnot go through that whole process\nagain yeah as far as i remember it was\nbuilt on the idea of the c\ncache stuff in gcc so basically\nwhen the object files are not updated\nthat's where it started but it grown\nwell beyond\nit so it computes the entire graph and\nfigures out what's changed and what's\nnot\nso gleb though on the remote side it's\nokay right to re-register\nbecause it's not important but yeah if\nyou could reduce the\nnetwork itself it would be pretty\nawesome but you could completely\nyou know delete the workspace and\nrecreate it and flight admin will\nrecover the state because it has its\nlocal remote state so\nuh or remote state rather yeah because\nit's kind of a problem today for like\nbig reapers if they don't have bazel\nthat you\nhave to like let's say you have a ci\nprocess and you have to run everything\nbasically\nbecause you don't really know what has\nchanged you don't have a good modules\nin the system yeah so you could say you\nhave a reap of these hundred tasks and\neach task has integration test\nuh you have a problem yeah yeah we get\nthat for free\nwith uh kind of bazel and leverage as\nmuch\nof bazel bazel also provides like\ncontainer rules that we use kind of\nto build out the docker image pretty\nmuch\ntry to basically it's taking all these\nlike rules that are already\nuh open source and try to bring them\ntogether as much as possible to define\nthe entire\nregistration workflow there's some\ncustom stuff that we do but it's very\nspecific to like the register command\nand\nflight specific stuff yeah we would love\nto\nthis to be open source so one of one of\nthe lift internal users\nworking on pricing or something had 400\nworkflows and\n1000 tasks or something and then the\nregistration time would take\nstarted becoming more and more like it's\nlike initially it was like\n10 seconds or something and it went to a\nminute or so like or\n30 seconds um and so that becomes a\nproblem at some point\nand so and mostly users are not really\nchanging\neverything right they're just changing\ntheir bits so\nbut finding the diff is extremely\nextremely hard\ni guess what's next on the agenda\nuh next is you if you want to do\nplanning for\njust sort of open up the floor for\nwhat's going on in the core\num all right so um i guess i won't take\ntoo long but uh i just want to share\nwhat\nwhat are what um i\nuh hate them and katrina and like a few\nfolks within the\nnew company are working on um\nso we have been focusing on getting uh\nlike the first use scenario and flykit\nthe new api uh really\nreally in a way that the users find it\nextremely easy to adapt\nand this includes documentation and this\nincludes the process of\nfiguring out what to do next tutorials\ncookbook\num so and and i think we we we\nenvisioned that we'll continue doing\nthis for the next one month\num one of the uh great things that\nour and i i did a jeep came up with it\nand i i think i would love\nfor jeev to demo this some point uh here\nin this conversation is uh at reno they\nhave like a\ntestbed environment for a\nflight where every user can just like\nclone a repo and hit a button and they\nget like a local cluster\nor a remote cluster within one named\nspace in a cooperative\nin a shared kubernetes cluster so that\nway\nevery user has their own flight\ndeployment\nuh where they can learn flight um and\nthat's\nthat's really uh i think awesome\nand so jeeve has been very gracious\nwhere he's\ncontributed some code uh which\ncan help us get the initial time\nfrom zero to start to be less than with\nflight less than\na minute so we are coming up with goals\num so we have about five goals one is\na user should not have more than one\nline or they should not need to use more\nthan one line to get started with let's\nsay it's like gate or\nso on they should not need to use more\nthan two lines to get\nfrom there to a cluster running locally\nuh and or an existing kubernetes cluster\num\nand not more than again two lines if\nthey want to use just one namespace\nand the total time for all of this we\nwant to be less than two minutes\nuh so and having\nexamples deployed into that cluster and\nrunning those predefined examples\nso that's our goal um and we are we're\ngoing and literally\nwith the timer on we want to go with\nthat goal so that users really can\nget to from zero to you know uh\nrunning a remote cluster in no time and\nfrom there the next\nnext thing would be to actually uh use a\ncloud\nhosted gcs or s3\nand then you know slowly slowly add more\nproduction readiness to the\ncluster so with that goal we started\nworking on\nthat's why for the last two months if\nyou've seen we have not done a lot of\nreleases we don't need a 0.10\nuh one of the other reasons is because\nlike we're really making it we're\ncompletely changing the\nonboarding experience um\nbut that doesn't mean we're not working\non other things\nso let me share a screen what i did is\ni wanted more\nfolks to easily\n[Music]\ncontribute and know what we're working\non\nso i updated the roadmap documentation\nagain if you've seen this part the\nentire getting started and roadmap and\neverything is is getting an overhaul\nwe'll share that probably in the\nnext meeting the new structure\nbut till then in the roadmap section we\ndo have upcoming features and issues and\nwhat we've decided\nthat you we've created labels now so\nthat you can just click on each of these\nissues\num and that should pop up\nonly all the issues related to that\nlabel\nand i've cleaned up the labels there\nwere some weird labels and we removed\nall of them\nnow that we are in flight org we can\nactually set up bots and we can do all\nkinds of\nthings that we should be we should have\ndone earlier which was very hard\nunder the lift organization but here we\ncan\nso for example if you're interested in\nlet's say getting started\num and then i think these are uh you\nknow\ngreat first issues um the other things\nthat are great\nuh to contribute on are plugins uh and\ni'll go through some of them\nand there are lots of features requested\nand you want to see if\nusers are interested in more features so\nplease contribute\non that front there's an open source\nhighlight\nruffle i don't think he's on the call\nbut uh he's been working on flight ctl\nhe's added a lot of features to it\nincluding register\nuh and so on so flight ctl the goal is\nthat flight sequel becomes the one cli\nthat\nan idea would be to brew and stuff like\nctrl and from there on\nyour journey begins with light so it's\nvery uh\nstreamlined and it and and you raj\nboth of them have been really going uh\ngreat with flexi two so thank you for\nall the open source contributions and\nplease\nplease keep on uh working on them so\nif you're looking for things to do and\nor add\nmore issues i have i'm actually in the\nprocess of\nupdating the issue template so you can\nactually select this is the ui issue or\nwhatever\nwhen you create the issues itself but if\nyou're looking for issues to work on\nthese are bugs and these are good first\nissues and most of the flight cto ones\nare\ngreat first issues actually it's pretty\nstraightforward and if you\nif you don't have any if you not not\nhave a lot of go\nworking knowledge that's okay flight ctl\nis one of the best ways to get started\nwith go in my opinion it's really\nreally well defined easily testable and\nuh and then i guess most people like\ni think buffoon never had worked with go\nand i think\nhopefully his convert now is let me go a\nlot\num same thing as plugins\nand if this is one of the areas where we\nwant to really\nfocus on specifically because like we\nsee a lot of users using different\nsorts of data stores or\nhosted services and i think uh\nsome folks are working on uh so i think\nthere is a flink plug-in that's in\nprogress\nat spotify and there's a bigquery\nplug-in we added a\nplug-in for athena so um\nwhat hatham did recently is he created a\nsimplified interface for\nand this documentation is coming through\nbut creating a web\napi style interface what that does\nsorry\nuh is the interface\nyeah so you just have to implement\noh this is complicated there is the\ninterface\nso i'm trying to find the interface\nsorry um so there is a very simplified\ninterface if you want to integrate a new\nuh web api uh plugin uh oh sorry\nyeah so if you want to integrate a new\nweb api any web api in the\non the internet you probably just have\nto implement\nthis one interface either if it's an\nasync plugin or if it's a synchronous\nplugin\nso synchronous here means that you call\nand the response is returned immediately\nuh or if it's an async plugin means you\nyou launch a job and you wait for the\njob to\nthen your screen isn't download oh\nreally again\nat least for me it's not which part are\nyou looking at\nit shows me green it's still at the road\nmap\ni don't know what to do then maybe\nre-share your screen once maybe that\nwill work\ndesperately\nso uh maybe also hatham's here hit them\ndo you want to quickly explain web api\nuh interface before um\nsure if we yeah we have the time\nso let me share my screen hope i have a\nbetter luck at doing that let me see\nyour screen\ni share it again but\nnothing is i guess\nyeah anyways my my meta point being if\nwe run out of time is that please\npick out the issues uh add more\nissues if you guys are working on it and\nuh\nnow you should be able to assign an\nissue to yourself just\ngo ahead assign it we will have better\ncontributing guidelines throughout the\nentire project but\nright now we are small enough and we can\nmove quickly\nwe do have a lot of hooks and lots of\nissues there\nand there are lots of great first issues\nso please go further\ni import people to share my skin as well\ndid that work yeah okay awesome\nuh yeah sorry uh who was saying\nsomebody was asking something it's okay\nask later\nokay uh yeah so this is the interface uh\ncaitlyn was talking about there are two\nuh main interfaces here there's an esync\nplug-in\nand sync plug-in um the esync one\nis well as the name suggests if you have\na\na service you want to integrate with\nthat you\nit's async so you issue a create request\nonce and then you monitor it\nin the background and once the request\nis done if it's maybe a long-running job\nor a query\nor something like that and then\nyeah so you need to implement these\nmethods uh the sync one is obviously\nmuch more simple uh the expectation here\nis we\ncall this once and it works uh\nthe the thing to note about sync plugins\nis we\nwill call this in the i would say the\ncritical path\nof executing a workflow um so this\nshould not be an expensive operation uh\nwe expect a network call obviously but\nnot uh\nwaiting for uh uh or asleep waiting for\na query to execute kind of thing\nuh for those you should definitely go\nwith the async plugin\num the rest of the interfaces here that\nwill get passed to you\nuh that give you like you know to your\nmethods\nall the context about what's running uh\nthe task and so on\num and and i have added that link to\nuh all the issues where we need a plugin\num the other thing we also added is a\nis an implementation uh for one of the\nasync plugins it's for ews\nathena um it's a\npress-to-based engine running on aws\nthat\nyou know developed by ews\nand this is all you really need to do uh\nfor like or i guess similar to this um\nuh it you know uh translates the\nuh flight lingo uh so from flight like\ntasks and proto buffers\nuh to uh amazon's\nuh own uh library uh\nbluetooth3 whatever library uh to call\nto call the service and so on um so yeah\nuh if you have any questions uh like we\nwe have\nwe did add a a list of plugins that we\nthink uh\nare useful based on conversations with\nother people in peace and conversations\nwith\nsome of you uh but if you have a need\nfor a\ndifferent plugin please uh find an issue\nwork on it\num and i'll be happy to answer any\nquestions on the open source channel\nback to you kedan oh yeah and one one\nmore thing i wanted to add is like\nwriting a\nflight kit only plug-in and i think some\npeople may have questions on this like\nlet's try and understand what is a\nflight kit only plug-in and what's a\nback-end plug-in\nmaybe it's a rehash a flight kit only\nplug-in is where you don't\nlike everything can be written as a\nflight kit only plug you could have\nwritten athena as a fractured only\nplugin\nuh the reason why we didn't write it so\nis because that has an overhead of\nrunning a container and that is not\nalways preferred\nbecause uh with the total number of\ncontainers\nas it increases it actually causes\nkubernetes to have a problem\nthere's lack of visibility into the\ncontainer cleaning up resources harder\nand so on so but that doesn't mean you\ncan't start off with writing\nflightcheck plugins and so friday plugin\nwriting is actually even easier than it\nwas\nand it can be done in a separate library\nyou can just make a new library\nyou can publish it completely into pi pi\nbut if you would like you could also\nwrite the library as part of like it\nthere's a plugins folder\num and and then there are some plugins\nthat are just\npurely syntactic sugar uh and one one of\nthem is neil's actually added one thank\nyou\nwhich actually is not only like it\nbrings in great value along with\nsyntactic sugar uh one of like the value\nhere is like you know\nchecking uh and making sure that the\ndata frames are correct\nand probably neil's maybe you should\nlike\nprobably have to add an example or\nsomething and you can walk through it\nwith\nuh for the folks here the other one if\nyou're interested in writing like that\nis\ngleb and some other folks have talked to\nme about this\nis wakes is a different data frame\nformat for auto core\ndata frame processing and that may\nreduce the need for using spark in many\ncases because i've seen spark misused in\nmany cases in the world\nwhere you have like 10 gigabyte of data\nand you're running a massive spark\ncluster it's not needed\nso that might be a great\nplace to start too that's it from my end\ni think that's all we have today\none thing so i see soren here uh sorry\nwould you be open to\ndoing a con deep dive or a conversation\nnext time about helm\nand customize i see that we are talking\non an issue and\nsure sure i could do that yes totally\nokay yeah yeah\ncool\nsuper all right everybody um thanks for\ncoming\nwe'll uh same drill it'll take a while\nto get the video\nsort of ground up and posted but um i'll\nping the channel without a here\nwhen it's all done so you can forward it\nto anybody that missed the meeting that\nwants to see it\nand then we'll see you in a couple weeks\nall right\nbye-bye\nyou"
    },
    {
        "title": "FlyteMeet 2021/01/26",
        "transcript": "awesome\nso\nhello hello\nhello\nhey there here we go\nhello can you guys hear me\nshall we get rolling captain or do you\nwant to wait for a few more\nlet's get started i think\nthat's fine i think people will trickle\nin and i know some of them are not able\nto join today but\nokay um\nyi or katrina do you guys want to go\nfirst\nall right sure you first thought i feel\nlike you have the bigger news to\nannounce at the empire lease\nuh sure i\nwait i thought you were announcing that\nbut well whatever\nlet me go over the\n[Music]\num\nokay so after i think seven\nmonths of work we finally merged the\nmassive pr\num the screenshot right yeah\nuh the the pr for the\nnative typing api version of flight kit\num we made two we found a mistake of\ncourse right afterwards\nso there's actually two beta releases\nbut the the bulk of the notes\nwhich is right here is in the\nzero version um\nwe the main focus of the beta release\nwas to try to get the api\nto a stable place where we to a place\nthat we can commit to and commit to not\nchanging\nso the last piece of that was the plugin\nstructure\nwhich has been updated um following a\na micro library type\nstructure that is detailed in\na blog post that we found but basically\ntakes advantage of\nthe python namespace packaging\nconstructs so you'll find all the new\na little bigger you can think of flight\nkit plugins as basically\na separate repo and in the future\nit may actually be a separate repo\nso every single plugin will\nbasically mirror the structure that you\nwant ultimately in the\nin the python in the pi pi package so\nyou hip install in this case flight kit\nplugins\ndash hive and then um\nthe top level folder is empty uh so\ngithub just shows the\nactual plugin folder so it's\nthey all follow plugins plugin name\nflykey plugins plugin name\nso that is what enables the micro\nlibrary\nconstruct um\nand yeah i so going forward\num the square bracket\nuh version of plugins will still install\nthe old\nones so if you were doing uh\ni think spark 3 or the the\nones in the setup.high they will\nthey still will they won't fail but they\nwill not work and they will not install\nany of the new dependencies uh these\nthese things here um\nyeah check out the blaze notes and check\nout the blog post if you're interested\nthat's it\nyeah i think uh you may want to point to\nthe new cookbook also\num i thought katrina's doing that okay\nokay i guess i can do that yeah\num cool okay well with that wonderful\ntransition\nmaybe we can start taking a look at the\nnew cookbook then and just how easy it\nis to\num iterate using the new fly kit um let\nme share my screen\noh okay bummer i guess i have to quit\nzoom and rejoin because i've never\nshared my screen before so see y'all in\na sec\ntry no\naccording to hatham you don't actually\nhave to do that\nokay hello again um anyways let me try\nto share my screen for\na little this time see if this works\nwait\num awesome so we've been hard at work on\na\num a new uh\ncookbook uh with all sorts of examples\nfor the updated plugin structure\nas well as um uh all the\nkind of like getting started and kind of\nworking our way up um into more\nconcrete examples um we have this huge\npr open right now i think we'll be\nmerging this soon um invite you to take\na look at it\num i guess the diff is probably not the\nmost fun thing to look at\nokay then feel free to chime in with any\nexamples um we have a ton of new\nexamples in here that are super\ninteresting\noh sorry let me just show the rendered\ncookbook maybe that's better okay for\nsure\nuh did we push that to read the dogs no\nno we haven't right you'll have to show\nour local render\nokay okay got it um give me\nuh one sec um\n[Music]\nsorry i guess there's a communication\nproblem um\nlet me render that unless somebody has\nit open on their machine already\n[Music]\num yeah the nice thing about this new\ncookbook is that we have a bunch of\ninteractive examples that you can run\nfrom the get-go\nwe're using the new kind of native uh\ntyping flicat sdk\num so uh it's really easy to kind of see\nhow you can kind of begin iterating with\nyour code and then slowly adapt it into\na flight workflow\num and along with that we have all sorts\nof plug-in examples so i'm\nkind of increasing complexity there as\nwell too\nso let's take a look at\nnew rendered cookbook let me share my\nscreen one more time\num okay can you all see this\nawesome thanks george um so i think i\nmay not be on the latest latest kids and\napologies\num but we have our examples here\num again our basic examples um you can\nsee we have this kind of sweet new\ngallery format or we can take a look at\noh uh here we have an example of a task\ncall ads you can download code run it as\na jupyter notebook\num we have more extensive examples here\nas well too\nwith our plugins um hi sagemaker\nexamples\nuh things you can kind of use to get hit\nhit the ground running\num as well as kind of inline\ndocumentation as you can see here\num yeah keep the knee if you have\nanything to add\nuh yeah but once we publish the docs i\nwill post that in the flight slack\nchannel and encourage you to take a look\nif you have any suggestions questions\nfeedback always appreciated\num great so uh\nwe can take a look at actually how to\nuse how easy it is to use a new flight\nkit to iterate now\num so let me show you kind of an example\nworkflow i have and\nthe steps we'll be taking to make\nchanges to it\num so here i have a really simple kind\nof example workflow to celebrate our\ninterview\nthis is santosh um sorry first time uh\ncan we go to the cookbook a little bit\nright uh uh\nso one of the things uh you know this is\nspace is\nfull of so many different uh\norchestration system from airflow to\ncadence to conductor to you know what\nnot\ni was just wondering if it will make\nsense to maybe also add a section\nwhich talks about okay what's unique\nabout flight or like why\nyou should be using it and also\nsometimes like which is cases\nyou should not be using it for uh\nso that section will probably be useful\ni mean just a thought\ni mean i see the example and i think but\nhigh level framework when somebody is\ncoming in\nto understand a full layer of land like\nwhat would be\nwhat would be fitting use cases would\nnot be that would be great i really like\nthe\nexamples that you have put there the\nreal world examples\nbut um i guess from that somebody has to\nstill decipher it like\nokay based on these examples what it\ncould be what would be a good use case\nso something like a two pager or\nsomething two screens uh that\nspecifies that that will be very very\nuseful\nlike that's absolutely fair um so this\nis the walkthrough uh of using freight\nwe have a entire documentation up for\nflight\nokay is that arguably that has to\nimprove and one of the biggest\nover there is uh addition of all the\nor the comparison pattern is a moving\ntarget\nso yeah yeah comparison is a moving\ntarget right\nyeah as of at some point and we would\nlove\nmore community involvement hey this is\nwrong\nbut yes look forward to it hopefully\nokay yeah i can provide some inputs\nmaybe we can have like a smaller\nconversation sometime half\nan hour 45 minutes to maybe just based\non my understanding of like what this\necosystem\nlooks like and then we can definitely\nbuild on top of that that is amazing\nthat would be\nand i will uh it's it's on my plate and\nthat's probably the next bit that i'm\ngonna\nuh work on okay\nagain it's not going to be a one-man\nshow it's just two\nno no totally i understand yeah thank\nyou but that\ncompletely taken and awesome yeah\nthanks a lot please go ahead yeah\nyeah yeah i think that's kind of a\ncommon like piece of feedback wiggles so\ni\ndefinitely heard the address um also\nthank you for uh\nsharing the latest link for the polish\ntalks um so you can take a look from\nyour own machines if you're interested\nwe'll be sharing this link like i said\nbelow um but again this is yeah mostly\njust the walkthrough docs as it stands\num is this not it\nokay cool no that that's the default\ndocs that's not that's awesome\nthat's the\nyeah so this is uh the existing dock so\nthis kind of has more of an introduction\nof like what is flight etcetera etcetera\num yeah thanks again okay cool\num uh any more feedback on docs or shall\nwe take a look at interactive examples\ncool um all right so\nlike i was saying before we have this\nkind of very simple dummy workflow i\nprepared for this as a demo\num celebrating new awesome like a beta\nrelease where are you gonna talk about\nhow great it is\num so uh to run this workflow um we'll\nhave to build a docker image which i've\nalready done previously\num and then we can use our fancy new um\nout of container serialize\num let me go back here\num so as part of this kind of\ninteractive like docs revamp um we've\nadded if you can make file targets as\nwell\nas well as a docker files in order to\neasily\nrun and build these examples on your own\num and\npart of these changes uh to serialize\nhere\num allow you to now run a pipeline\nserialize outside of\nthe container which should hopefully be\nan improvement in your like kind of\niterative workflow\nso you can take a look here a simple\none-line command we're going to run that\nnow\num\nlet's run that and um i'm\nscoping our packages so we're only\nserializing that demo workflow that i\nhad\nbut as you can see it's pretty fast\npretty straightforward um\nand then after we serialize uh so this\nis converted all of our workflow\ntasks launch plan code to protobuf we'll\nneed a register\nand in order to register um we'll\nactually be modifying these photos just\na little bit\num in order to\npass in the project in this case flight\ntester and the domain\nand set a version\nwhich is necessary now at the\nregistration phase because these protos\nare actually completely portable so\nlet's say you've serialized them in one\nyou know user's machine you can share\nthat and register that um across any\nproject or domain combo\num and those protos um have kind of like\nno\nreference to that those like\nregistration type parameters\num cool so we've gone ahead and\nregistered our entities\num we can take a look if we want to um\nsorry this\nscreen share oh my gosh\nright in the way of my browser tabs okay\num\nso we can go and launch that new\nworkflow uh that we just registered\nyou can see our latest version even cdf\num it's like it is awesome so let's go\nahead and launch that\num cool well that's writing i'm running\nthis locally but\num we'll take a look at um how to\nuse this uh administrator let's register\nloop in order to fast register\num so traditionally serialization\nrequires you to rebuild a docker image\num each time you make a code change but\nif you're only making a code change you\ndon't need to update your docker\nfile dependencies uh we can actually go\nahead and use fast serialize\nso to demonstrate that i'm gonna add i'm\njust gonna make a code change here\nadd a lot of extra enthusiasm um we're\ngonna go back here\nrun the make fast serialize target um\nagain this is documented in the latest\nvocal changes\num go ahead\nand do this well\nso once again pretty speedy to serialize\nand then we'll go forward\nand um in this case run the fast\nregister files command\nso we can go ahead and do that\nand just like that didn't need to\nrebuild the docker image nothing at all\num we've gone ahead and registered our\nworkflow again\nuh with updated code changes so we can\nsee here that our previous\nuh workflow succeeded\num so let's go ahead and launch that\nfast register workflow you can see that\nnew version is here\nand voila um\nso yeah hopefully that was a helpful\nkind of introduction how to use new\nflight\nand how fast it can be if you use fast\nregistration\nto iterate through your code changes um\nany questions\ni probably will let me add a little bit\nof a comment or a plug for\nfast register because some people may\nnot know uh\nand some people are new so just uh the\ntypical flow\nwith flight before this point was\nyou write code you write a doctor file\nand and the docker file is uh simplified\nnow but like you have to write a docker\nfile and then you have to\nuh so you have to build the docker file\nand then you have to\ncall by flight register inside the\ndocker file\nwhat that would do is pick up all the\ncode figure out\nall the you know serialize it into what\nflight understands and then push it to\nflight and from that point on\nit was like you know it's currently and\nin the future it's remaining the same\nwe have changed this initial part like\nuh what we realize is that you\nyou only build docker files when you\nbuild dependencies in there\nlet's say for example you want to add\ntensorflow you want to add pytorch or\nyou want to add\nspark or you want to add any other\nlibrary\nthat needs in python specifically that's\nvery very hard you have to\nmake it really portable you docker is\nprobably the best format\nthat we know pickle doesn't really work\nbecause pickle only takes your\ncode in the user space so\nwhat we did is uh if you are now\nmodifying any dependencies\nit is still recommended to build the\ndocker file and\nregister that docker version right the\nthe image version\nbut if you're not doing any\nmodifications to the dependencies which\nis the typical workflow for most users\nright they\nthey figure out the version of\ntensorflow that they want to use that pi\ntouch\ngoing forward they don't do that so then\nthey can jump on to using this thing\ncalled as fast register\nwhat it does is keeps the image\nbuilding phase and directly uh uses the\nentire code base\nnow granted you need one access from\nyour user's machine\ninstead of ecr you need an access to\nsomething like\ns3 or instead of gcr you need access to\ngcs\nbut as long as you can push to gcs\nto a specific directory that flight has\naccess to\nthen you can use fast register and that\nwill\ntake the entire code base and make it\navailable to flight at runtime\nusing your previously registered docker\nimages right\nso this this way if there are three\nusers\nusing the same workflow but with\ndifferent dependencies still can go\nindependently\nbecause you can now use tensorflow 1\ntensorflow 2 at the same time and by\ncharge probably right\nweirdly within the same docker images\nand\nyou can iterate on the same task and the\nsame workflow\nand yet in your in your daily routine as\nyou're working on it\ni trade on it really quickly because you\ndon't have to rebuild the images you\njust have to push your code\nwhich is only you know to your\nspecifications\nso that was the flow that we wanted to\nclean up\nuh we did try pickle we did try a bunch\nof things and we saw a lot of errors in\ndifferent cases\nand the the simplest uh way we thought\nwas to actually\nuh side load the code into the container\nand that's what we are doing\num in java flight kit java\nuh gleb and nelson have already\nimplemented a way\nbecause java is way more portable than\nuh spotify so they\nthe default way in flightcheck java is\nto actually\nmove the code directly into the\ncontainer side load the code\nand so and that works beautifully so now\nboth of them look the same\nalmost so hopefully that gives you a\nquick overview and\nand the idea was to bring down the total\niteration time\nfrom whatever a minute sometimes to\nbuild a container and push it to\nless than five seconds and it is it's\nprobably\nclose to a second but we want to\ncontinue\nimproving that time hey keith uh also i\nthink\nwith once we have a a place to put all\nthe\ndocker images with every um every flight\nkit release we'll also release\nwe'll make a release on the github page\nof cookbook and then we will\nupload all the docker images somewhere\nand serialize\ndue to serialization step basically for\neveryone so that you can just\ndownload a zip file of all the protobufs\nand register immediately\ndoes that make sense and to that end\nactually prefer\nhere on the call is building a flight\nctl register which is\na go binary that anybody can install on\nany machine\nto easily assess standalone binary to do\nregistrations and so on\nso this the entire process to just\nsimplify this entire flow\nuh also the dependency on s3 and gcs we\nare thinking if we should just add a new\nendpoint on flight admin that will proxy\nuh gcs or s3 to upload your\nyour site loaded code that way the users\ndon't need any more permissions besides\nhaving access to admin and so we would\nlove to hear feedback about that and see\nif we want to do that\nhey katherine i had a quick question um\nor i don't know if it's quick but um\nwhat's the thinking around like\ntraceability\nas far as like what version of my code\nis actually running\nin the cloud um i know we historically\nhave used like the git sha and forced\nusers to use com myths um but with this\nwhole\nnew process kind of lose that\nyeah we don't lose it so it's it's in\nand i would still recommend to use\nif you have the right ci cd system setup\nfor pushing images that is still\nprobably the best way because it's\nguaranteed right like that will run yeah\nright loading still has potential\nfailures that can happen at runtime\nand so it's not foolproof\nbut a very good question so the way we\nare tracing is\nevery time and katrina can please uh\nif i'm wrong but um every time we build\nthe shower uh the the artifact which is\nthe serialized and\ncompressed version of your code it is\nhashed using a hashing algorithm\nand assuming there are some changes and\nthe hashing\nthere are not collisions happening\nwithin the same stream of changes\nuh you will get unique identifiers now\nthe good question now the important\nquestion here is how do i\ntrace back because the code is stored in\ns3\nwe have a unique link always to all the\nolder versions and that's why we think\nthat proxying it to flight admin\nmight actually make it fully traceable\nbecause we might be able to build\na nicer reference to like hey this is\nthe code that ran and this was the you\nknow version\nbut replayability as long as you don't\ngo and delete the code from s3\nthe repo is guaranteed uh the other bit\nover here which i think which you've hit\non um\nis uh is like total like\nwe know we have reproducibility and so\non but there's no easy way of\nvisualizing all of this\nand there's a story that katrina is kind\nof trying to uh\nflesh out and that is documentation uh\nwe basically for every single workflow\ntask and version\nwe want to have an associated\ndocumentation\nand automatically associated to the\ngithub sha\nif you're using github or git sha in any\nof this\nand the fast registered uh\narchive automatically so then from your\nexecution you will be able to go to the\nactual\nuh reference entity from there you can\ngo to the reference code\nthat that would be the flow hopefully\nthat answers your question a little\nlong-winded but\nyeah now that that helps um just trying\nto think through\nkind of how to like get the developer\nexperience but also\num traceability because one thing we've\nhad complaints we force everybody\nto make a commit and sometimes they\nbasically had complaints yeah yeah and\nand doing this like you know\nprobably we are not even thinking hard\nenough for this problem the the doing\nthis\npervasively throughout the entire\necosystem of flight design like this\nmeans\nif you fast register your code is now\navailable on\nlet's say if you're using sagemaker on\nsagemaker\nwithout building a docker container\nwhich they don't have a you know uh\ncapability of on spark without building\na docker container natively\non um uh if we tomorrow at\ngoogle cloud platform uh services then\non google cloud platform and so on\nso so that's uh doing that pervasively\nis the hard one and i still\ncall this an experimental feature i'm\nsure we will find some bugs\nso please be\nvocal about it bring it up we love to\nfix them uh\nin our testing we've seen that it works\nin most cases um\nbut yeah we would love to know if you\nfind it okay\nlet's add one thing to like your concern\nmiguel is like one of the things that\nwe're envisioning is that you can kind\nof use fast register as part of like\nyour development iteration process but\nonce you're kind of like pushing code\nall the way through to production maybe\nthat's the case where you end up using\nthe kind of traditional\nlike register commit method so then you\nget all the benefits of like peer review\netc if you are using that for your code\num and you're not pushing just like code\nwilly-nilly production but fast register\nis kind of like a tool to just improve\nlike the development integration cycle\nuh but like ethan mentioned hopefully\nwith like the inline kind of code\ndocumentation hub will have kind of you\nknow\nthe ability to understand what's going\non regardless of your registration\nmethod\nso yeah no that makes total sense um\nthat's one thing at least like\nhere we don't really use separate dev\nproduction for the most part\nso kind of this will be our forcing\nfunction for users to\nuse that yeah yeah i think\nso we still want to make sure the flight\nthe the\nend goal for flight is to make you know\nyour production workflows really really\nwe want to guarantee that things that\nhappen in production really works well\nbut we realize that iteration is a\nkey user story in this scenario so we\nwant to\nnow probably say that you want to\naccelerate from local to\nuh production\nhey one question here uh so that\nboundary when you have to build a\ngate image and when you don't uh is that\nvery clear in the programmers that\ni don't program so much so i would not\nuh\nbut the idea is that if i'm doing this\niteration at some point of time i make a\nchange\nis it very evident to me that um i mean\nuh at the coding level when i compile\nwhen i deploy or something like that\nwhich will tell me that hey you have to\ndeploy the k8 because you've changed\ncertain dependencies if i'm making a\nmistake\nis that does that happen or is it too\nhard to implement\nso this is a very good question santosh\nso\ntwo things it's very hard for\nflight to know that\nyou should be building an image at this\npoint\nwhat will happen in the case you do the\nwrong thing is at run time you will get\na failure saying that\noh you asked for tensorflow but your\nimage doesn't have tensorflow\nright but that's it's okay i think okay\nit's okay\nmoving fast right but on the other side\nfor a\ndeveloper i think once you get used to\nit that\nthat like let's say if you are working\non a typical ec2 machine or something\nthe way you would go it is go and loop\nthe install right first and and what we\nneed to say that when you are doing the\npip install\nis when you build the image otherwise\nand to an astute right like so a\nperson who is now understanding what we\ncan do with this is that there can be a\ngallery of images that are pre-built\nwith a set of um\ndependencies already pre-installed so\nfor the\nuh i don't want to call it knife because\nthat's not the wrong but a\nuser who doesn't really care about the\ndependencies they are like hey i'm\nbuilding a tensorflow algorithm i'm just\nbuilding a spark job i don't care about\nwhat it is\nthen they can just choose the gallery\nimage\nand now building the gallery image is\ndependent let's say on the platform\nuh developers or so on and that's that's\nthe direction that we are going in\nso uh okay awesome okay all right\nanother idea i was just thinking about\nis that will there be like a template or\nyou know there are some structures like\nyou have to basically dot you know\ncompile file used to be there where you\ncan see or something where you\nspecify all the libraries or dot h files\nuh\nif that kind of template can help here i\nmean that's just an idea here but yeah\nno that's excellent like we we do have\nin python there is requirements\nso we are actually the thing is not\nevery developer uses it right so\nlike python has this huge range of\ndevelopers which\nextremely like deploying production\nservices to\npeople just trying out things right so\nuh what we want to do\nas a going as in one of the future steps\nis and we would love contributions on\nthis is write a cookie cutter\nproject template or something a project\ntemplate that you just say hey\ninstall this template or edit this\ntemplate and it will build the right\nfor you to write flight uh things and\nthen within that environment it's more\ncontrolled\nyeah we can probably you know exactly\nkind of you know predict when the user\nhas modified the requirements that in\nbut not rebuild the image\nbut uh still hard yeah i mean i would\nsay that yeah this is great progress and\nonce we get the feedback and if you see\nthat that's actually a pain point then\nwe have some solutions regarding that so\nuh yeah i wouldn't be surprised if there\nwill be people who will be a little\nconfused and i know\nand hate them and all of them have voice\nconcerns saying that\ni know this makes it fast but we are\ngoing to have weird issues where people\nwill not have built and they'll have run\ntime\nand they'll ask like but i think it's\nthe\nyeah i'm more concerned about yeah i'm\nnot concerned about that libraries are\nmissing i'm more concerned about\nyou know you have to increase the\nversion of your library or something\nmistaken and then\nfour six eight hours later you are\nfiguring out that oh i did not make\nthis change or library version or some\nsmall thing\nso the problem is not that if the\ncompile time failures get caught\nlike something is missing the compile\ntime is failure awesome that's like\nyou know good situation to be in i'm\nthinking more about the bad situation\nthere like okay one library version i\ndid not change everything ran but ran\nwrongly\nand maybe i got to know only after i\nchecked the data a few days later so\nthat\nthat kind of cases become like really\nproblem\nyeah hopefully you use memoization and\nuh\nokay yeah\n[Laughter]\nso we're trying to cover all bases\nthe idea is not to kind of fully fledge\nout all the possibility possibilities in\nwhich things will fail i think we should\njust basically\ntest and you know then figure out which\nare the areas where we need to improve\nso that's fine i mean i just wanted to\nbring this up to you no this is great\nlike i think hopefully we are taking\ndown the notes and we'll keep them\nyeah the way the ambition is that uh\nany project will have their cicd\npipeline already configured\nso you will use like a fast iteration\nfor local testing and\nuh yeah this is working but at the end\ni will expect that developers will go to\ntheir\ngeek repo and make a commit and then\nwe'll trigger cidc\nthen maybe we'll do the the billing\nmachine\nand then uh hopefully these kind of\nthings will be\nreduced because of that\nyou can also think of it as we are\nalmost requiring a commit for every code\nchange and with this\nwhen you're just doing fast iteration\nyou you don't necessarily have to commit\nbut when you're ready for production you\nstill expect\nthe full normal commit build cycle with\naudibility and everything else this is\njust speeding up iterations\nyeah one of the things though we might\nwant to do probably at some point is see\nif it is feasible\nto make sure that you can do local\ncommits right local git commits might be\nforced and that way\nwe might even get a better trackability\nagain it's not great because we don't\npush it\nthe history is lost but now is something\nbetter yeah\nokay sounds good great\nuh nelson would you like to go next i\nthink you're gonna\nyes\nokay um hi everybody\nand nelson from spotify uh\ni've been in the company for\nover six years and i'm in the\ndata infrastructure department we have\nlike a fancy\nawards for regular things so we call it\ntries but it's\na department and i've been in\npart of the team that we have been doing\nflight\ninside of the company uh we started\ntalking maybe\nlike a year ago by now i don't know\nmore or less and so i will explain\nwhat uh what are we doing uh this is the\nagenda\nwant to have like a description of how\nwe do\na very high overview of how we do data\nspotify\nuh yeah one of the main contributions to\nflight is flaky java so i have a look at\na smaller light about that\nbut uh yeah we did a presentation on\na full presentation of our flagpole so\nit would better for you to\ntake a look at that instead of this\nlight\nthen okay how we have our setup\nwhat we are doing right now and things\nthat we\nwe're going to do later in the future\nso okay um i assume that you are\nfamiliar with spotify we're a\nstreaming music company with hundreds of\nmillions of users\nworldwide uh but here we are\nwhere i'm going to talk about data so\num we don't we are a big company so we\ndon't have\none data team uh\ni guess uh most of big companies work\nthis way\nwe have platform teams that give support\nto many\nfeature teams so\nmy team is regarding\na scheduling orchestration bash\ndata pipelines and then we provide\ntooling and services to let's say the\nsearch team that\ndoes the uh is in charge of\nwhen you search for something in the app\nit gave you a it will produce some\nartists tracks or whatever a playlist\nso we are the way we have data engineers\nin all these feature teams\nand we as a platform we make their life\neasier\nwe have more than ten thousand distinct\nback shops and these\nuh we have we are much\nmore of a batch heavy than stream heavy\nand we have this uh\nis a wide range of workflows\nand most of the workflows uh\nare using a luigi uh python\nlibrary the spotify build years ago\nand these the you define a workflow\nuh the data using um\ntask and python and everything is\nrunning as a library so it's a\ni would say it's a fad library instead\nof flight that is uh\nyeah you compile it in a protobuf and\nthen send it to the service\nhere is running inside the machine and\nso yeah it's not as convenient that's\none of the reason\nwhy we want to migrate to flight\num we\nhave our own gk cluster to run these\nuh luigi workflows but most of workflows\nwhat they do is they spawn\nuh heavy jobs in gcp we are\nusing google cloud products\nso we use data flow\nfor pipelines we have some\nlegacy pipelines running in the data\ndata provides like a managed hadoop\ncluster\nwe use heavy bigquery so um\nbut all of these tools where they have\nlike\nas a key is that they run outside of the\ngke cluster that we manage\nso we have like a\nin our all stack we have like a\n120 nodes for running\nall these bachelor jobs\nand so\nwe are have a mix of workflows written\nin python\nbut we have also a big chunk of their\nwritten in java\nso um those\nare written in java they have like the\nthe paint that the\norchestration in the workflow\ndefinitions written in python\nbut there are number crossing side is\nwritten in java so\nthat's a great pain for them\nuh and we we also built an\nopen source uh scheduling\nsystem the it's called recorded sticks\nit's also open source and sticks\nincludes the scheduling\nis very very good to say okay this is\nour workflow partition\nby time and so we have like a retries\nbackfields and that kind of stuff\nuh and in\nincluded like the execution the handling\nof the gk cluster to\nrun the the these luigi workers\nin their own gk cluster and what i have\nbeen doing is\nextending these sticks to be able to\ntrigger\nuh flight uh large plants and now we can\ndo that so we have this mix\nenvironment okay\nvery briefly if like java\nyeah with a sdk to define tasks and\nworkflows\nfor flight is\nuh we have\nminimum level product features uh we are\nusing\nthe in teams so what i\nwant to say is still having achieved\nfeature parity with a flykey\npython a regular flaking\nuh but we are working on that\nyes there is a the link to the github is\nopen source and we also\nonce the um\nwe we will donate these two flights so\nonce the flight have their new home uh\nflaky java will be included\nokay um yeah i won't talk about this\nanymore because gleb\ndid the presentations a few weeks ago so\nyou rather\nwant to see that presentation nelson can\ni interrupt you now it's a real quick\nquestion\nis sticks is your stick scheduler pretty\ntightly welded to\ngcp or would that run on aws or\nmicrosoft as well\nyeah it is um yeah very tied up to gcp\nokay that's what i would say yeah yeah\nwe are\nthat's what i would expect i'm just\nwondering yeah yeah we never thought\nuh we we open source it as\na side product right\nwe first built it to want to solve our\nproblem just say\nhey let's make it open source um\nwhat is uh very tied to\nthat's what i expect i didn't mean to be\nreally i was just curious but go ahead\nyeah okay so\nnow let's talk about flight we\ndo things on gcp so we use a\nmanaged kubernetes cluster with google\nkubernetes engine\nuh postgresdb user cloud sql\nuh we use popsup and\ngcs google cloud storage for artifacts\nso we don't use s3 but ecs\nwe have our cluster managed by terraform\nso\nthat way we configure our cluster how\nbig it is\nfeatures it needs um\nbut we have uh\nspotify also have like a back inside of\ninfrastructure they we are taking\nadvantage to deploy\nfriday mean and data catalog to the\nmanuscript native cluster of spotify\nwhite so\nit's not we have the our\ncluster for flight is only only half\npropeller\ninside everything else is in the this\nsystem-wide mass coordinated cluster\n[Music]\nat the moment we are using a\nvanilla components where it means that\nwe are taking the\npropeller admin as it is just\ntweaking the configuration to use the\ngcp\ncomponents inside the fried\nming a propeller\nwe contribute a part of this code\nmonths ago so our contributions are in\nthe\nfly open source\nbut we would plan to write a qualifier\nspecific plugins\nsome of them could be\nuse yes some of them could be open\nsource because uh\nlike triggering data flows uh uh data\nfor\nblogging or bigquery blogging something\nthat will be wide\nuh um interest\nbut we have like internal systems like\nuh\ni will talk about one of those uh later\nis uh\nour lineage service this is a\ninternal uh podified product\nand so eventually we will write uh\nplugins for those systems but we are not\nthere yet\nwe have a common task repository\nthis is uh we have a project\nwhere we are building\na common task that we made to be used\nfor\nseveral teams at spotify and\nthese uh for example how to launch a\ndata flow jobs using this repo\nthat does is not that blogging yet is a\ncontainer base it does at the moment but\nwe\nplan to rewrite it to be a plugin base\nin the future\nand we have\nwe use internally something called like\nyou can imagine like a dns\nfor data when\nwhere i can get this data so we give it\ngive it a name and give it a\na partition like a time i will say okay\nthat data is\nis in this uri so that can interaction\nwith that system it's also a task that\nwe have in this central repo\num those stats are mostly written in\nflicky java\nand we also build like a language\nlibraries to\nto get reference to this task in the the\ncentral reaper so\num instead of using\nuh it it will be just a as a library\nyou include like your workflow like a\nvery\nit is a very sure\nvery few lines of code to to make\nreference to these tasks\nso this is like okay get this task get\nwe take a\nwe do a call to get the latest version\nof friday friday for these\ntasks and that's the one we use uh the\nrepository\nand we have these uh language libraries\nin java and python\nso we have a\nteams testing\nflight and we have on both sdks\nuh what we are doing right now\nuh we just at the end of last year\nuh we release our setup as a\nalpha how we call it internally that\nmeans that is\nready for other teams so feature teams\nto start\nexperimenting\nwe are not full produce production yet\nfor some components i will\ntalk about the we basically we need\nbetter integration with the\nspotify infrastructure and that's what\nwe are building\nso um the one we are doing at the moment\nis our linear service\nand so what we want is that every time\nwe secure\nour flight workflow we are able to\ncommunicate communicate with this system\nand for doing that we are looking at\nflight amin it has the property\nto send events but the event the\nflag of mid sense is more mostly like a\nemail style notification events we want\nto\ntake a look of that and and make it like\nuh\nstill keep the functionality to send\nemails but be able to be\nto send data basically protocol messages\nthat\nthen we can consume those events and\ntalk to\nour linear service so this is what we\nare doing at the moment\nuh that is on the what to\ndo with flight internally also we are\nlike uh\nimproving on boarding for these feature\nteams\nuh for example you talk about cookie\ncutter we build a cookie cutter\ninternally we have like a um\nuh in internal in the internet we have\nlike a\ncreative like project and people\npress a button and we create a a\nproject with cookie cutter already put a\nbunch of stuff in place\nuh they still need to do some manual\nuh they have to ping us for example to\ncreate a projecting\nflight so there are some integration\nstill missing\nand documentation also is submission for\nthat\nso uh oh\n[Music]\nthis uh i forgot to\nreplace this this is our internal\nbuilds cicd system\nso uh\nwe have a way right now the cookie\ncutter project that we produce have like\na\nvery noisy for the the build steps we\nwill prove that by\ncalling creating something called\ntemplates to make\ncicd easier and\nalso a little bit further in time\nwe will scale our cluster uh\nharder than our setup that means that uh\none of the first things that people ask\nus he says\nokay are you on call for this and we say\nnot yet we are still busy doing this\nintegration stuff\nand but once these roadblocks are solved\nthen\nokay we will pay attention now um\nthis is something that lift has done\nbefore but we haven't done yet so we\nneed to learn\nokay how we manage flight at scale when\nwe throw a lot of load how we behave is\nour\nmonitoring in place alerting what we\nneed to be alert for\nso this is something that we are we are\nnot doing right now but we are do\nvery shortly and last\nuh what we are interested in going\nforward\nis that um you want to spend the\nflight security features and\nsomething that uh we implement\nin spotify is that uh we use\nuh service accounts to have access to\ndata\nand everything have their own service\naccount we've got like a strict policies\nthey're not allowing people there\nbasically what we want is that if we\nhave a\nworkflow we don't want uh developers or\nfrom other team be able to trigger\nworkflows\nuh my workflows we want to have some\nauthorization in place and for that we\nwill work with uh\nwith you uh we're not there yet um\nthere we will do this uh the middle of\nthe\nyear and then we will do\nback-end plugins for data flow than\nbigquery that is\nwe spread these to donate to to the\ncommunity\nbut also for this internal task for our\nsystems that we have\nas a container breakdown we also plan to\nmove it as a\nplumbings and last\nwe have 4 000 rigid workflows\nand we cannot expect\npeople to migrate to\nspend that time uh so we are thinking\nlike uh\nsome kind of luigi emulator for fly\nworkflows\nuh we have we did some proof of concept\nuh last year uh but we are not focusing\non that yet\nyeah uh that's it uh do you have\nquestions awesomeness and i had a\nquestion if you\ngo back to slide number six maybe\nuh-huh so i guess just for everybody's\nunderstanding the common task\nin central you know the common task\nproject\nis essential flight project in which you\nhave\ntasks registered by the platform team is\nthat right\nyes yes and yeah we\nwe ask platform team and we we know that\npeople will interact with these spotify\nservices like these like a\nas i mentioned dns for data or\num people will spawn a\ntask for will spawn data flow jobs or\nwe'll do\nlike a big query or notes meaning that\ni have some data in gcs i want to upload\nit to bigquery\nso these are common tasks in the\nin spotify and we have that common task\nas a luigi task in our internal\nrepository\nso basically we are taking that internal\nrigiditas and free writer that's a\nflight test we use\nbecause they are commonly used across\nthe\ndata engineers at spotify\nthat's a good idea yeah\nuh anybody has any other questions i had\none more question so i think there are\ntwo other teams right that\ni don't know how the relationship is uh\nor\nhow are you guys the platform team for\nthem but\nthere's the financial engineering team\nand then there is the flat map team\nright\nwhich yes fl flub map team\nis also uh is another platform team\nuh at least they are the maintainers of\nuh we don't write a worth\ndata pipelines in data flow directly and\nthe this thing build a scala\ndc the tsl on top of\njava data flow so people can write their\npipelines in\nscala uh using some\nan api similar to spark scouting inside\nthe\nhybrid there and\nthey uh they want now\nto see to explore like okay\ncan we do\ninstead of that we are very data flow\nheavy but we are writing\num data flow\nis i don't know how much you know there\nis a apache project called apache bing\nand this was like a uh execution model\nthat google\ndonated to about you so about that\napache beam have several implementations\nthe the gcp implementation is called\ndata flow so you are able to write your\napache bin\njobs and running data flow but they have\nother runners\nand they have this flink runner and we\nare exploring\ndoing that so and this team\nis a platfor team and they're exploring\nuh have a fling uh\nplugin for flight so we can\nsee see how easily to run this\nwith flint so they\nthey are prototyping that\nand probably say\nthey will donate that fling\noperator or fling plug into\nto the community and the financial\nengineering\nthose are feature team as we call it\nand they they like a lot\nbe able to use notebooks and they do\nforecasting\nso they want to use\nthey love python and they have a flag is\nuh\nperfect for them so we have they are\nrunning on our clusters they don't have\ntheir own flight setup\nand we provide for them that's the top\nand we're interacting with them\nthank you for that clarification that\nwas awesome\nhey nelson what's the state of the\nbigquery\nplug-in you mentioned it on the last\npage um\nyeah yeah the site is\neasy to do this is like\nwhat we are going to interest going\nforward\nokay so it hasn't no it hasn't been\nstarted yet right no\num we\nuh we have uh another platform team\nis uh the the one glibs is\nbelongs and\nthey are taking over the ownership of\nbigquery\nso they will be working on that\nsoon like a i think uh\nin the coming months we will have uh\nthese\nuh back-end plugins for bigquery\nawesome but today if you want to run\nbigquery do you use a java common task\nlike the common test thing or do you not\nrun big query\nvery easily we are not running bigquery\nyet we have the container based tasks\nfor\ndeploying a data flow job and\nthe bigquery is still running with luigi\nwe haven't been we haven't built these\ntasks yet but uh\nthat will change soon because uh\ncollecting\nis actually taking that yeah it's\nawesome\nyeah at lyft also there is a team that\nhas\nsome bigquery queries and the way they\nhave done today that they were they\nwrote\nuh basically a python task right which\nis a plugin\nuh which has the query and everything in\nthere\nand so yeah yeah the way we think to do\nit\nis to use the google have\nthese kubernetes operator for data flow\nand\nbigquery uh so uh\nbasically we we are going to piggyback\non that and\ncreate a um\nour plugin to generate those uh\ncrd resources so let\na google operator do the work\nso it shouldn't be that problematic\nwe're going to have the bigquery one\nready before\ndata flow because\nwe are the ones who care about data flow\nbut we're still busy doing this\nlinear integration and then with\nauthorization\nbecause for us right now the\ncontainer-based task is working so\nokay that's working let's focus on\nthings that are missing\nuh but the gleb's team\nwe we proceed with the this big query\nback and looking\nwe're coming up on an hour and i kind of\nwant to be respectful of people's time\num\num so let unless there any more\nquestions i think we probably ought to\nbrought\nto a close pretty soon\nare you planning now or do we need to\npostpone that for next\nlet me postpone that i just have a quick\nplug though uh i will again post the\nlink\nto the sheet here what i have done is\nthere are two sheets now\num one of them has themes\nplease add any more themes and then the\nfirst sheet is i don't think\nyou're just crowdsourcing ideas what\npeople would like\nto have and please add them i'll put the\nlink here\njust for everybody\n[Music]\nand so just go through the themes and\nthese are some of the themes that we\nwill be working on in this year\num and some of the ideas in there uh\nsome of them actually we already\nlike i got individual messages from\npeople\nso i tried to pull them together into\nthere uh but yeah\ndon't be shy because i think to the\nsheep\num like for example like google\ngcpr platform integration is interested\nuh freedom is interested in that and so\non\num so please please add things and if i\nmissed out\ndefinitely add more that's it and then\nwe'll probably go over\nit in the coming one or the one after\nthat if more time is needed\nbut we want to do like a clear layout\nfor new people to come in so like hey\nthese are the things that are getting\nbuilt these are the things we cannot we\ndon't have people\nso if there are anybody who can\ncontribute will be awesome\nthanks nelson especially yes uh thank\nyou\nthank you for all your hard work to get\nthat thing merged thank you trina\nall right um thanks everybody i'll\nzoom processes this video for a while\nthen we upload it to youtube and then\nyoutube processes it for a while so it\nusually doesn't get all finished until\nthis afternoon but we'll post notes in\nthe video\num sometime later today for anybody that\nhad to drop off\nthanks everybody i think we're going to\nget a preview of lyft's\num deployment which looks quite\ndifferent which is interesting um\nnext meeting from on maul um he couldn't\ndo it today we wouldn't have time anyway\nbut\nso if you're interested in that make\nsure to show up a couple weeks\nand uh and we are always open for demos\nof projects in\nuh in flight so to speak\neven yeah even like other things that\nare not related to flight like i think\nthe sticks and everything i would be\nreally interested in seeing more\ndetails about that and how we can even\nthere's a collaboration over there\nyeah we've certainly felt the need for a\nreliable skill\ncloud-based scheduler\nokay all right\nthanks everybody in a couple weeks\nyou"
    },
    {
        "title": "FlyteMeet 2021/01/12",
        "transcript": "hello good morning\nor good afternoon if you're other places\nmorning\nforeign\nhey you raj it's it seems you use the\nfireflies.ai note taker\nare you just sending the bot are you\naround\ni think he's just sending the butt\n[Music]\nyes\nso i guess i'll be the note taker today\nbut we want to start\nrotating this responsibility across the\ngroup\nhey george you want to take the notes\ntoday\nyou are muted all right yep i'm happy to\ndo that today\nawesome\nhmm oh\ni guess we can get started\nor maybe wait one more minute\ni think we're ready to go okay then if\nyou want to kick off i'll take notes\nso um so the agenda for today so\nwe are going to going forward we have\ncreated a backlog of agenda items\nbecause i think the spreadsheet was\nbecoming harder for people to\nadd in so just add yourself to the\nbacklog\nor if you can talk about some of those\ntopics that are there in the backlog\ndefinitely let us know um\nand so we will try to have this agenda\nready\nat least a few days before\nuh or locked few days before the meeting\nuh and it does everybody have access to\nthe meeting doc\njust can make sure yeah good\ni think we can just make everybody\nprobably\nhave right access to it sometimes that's\na problem\nyou don't know who actually edited\nsomething that's one of the reasons why\nwe don't\nwe haven't given right access you can\nalways put a comment\nand we will add that cool\num so today we have\nthree uh items uh specifically in the\ndemo\nand then a fourth kind of a request and\nan\naction for everybody here to do\nso let's get started with the first\nthree items um\nhanding over to yi to talk about uh\nflight kit\nwith the new release hey\nmy screen is coming through okay right\nuh we made a release um i think last\nfriday\nthis is the problem for sure the final\nalpha release\nfor the annotation the native typing\nuh the python native typing version of\nlikey that we've been working on\num i wanted to specifically call out i\ndid this it's\nlisted in the change vlog but i wanted\nto specifically call out the breaking\nchanges as well\nand when i say breaking is just um minor\nchanges that\nif you happen to use these things you'll\nhave to make\num if you yeah\nbut it may not be applicable to everyone\num\nso this cleans up the implementation a\nlittle bit better on the back end as\nwell\nuh moving the configuration for every\nsingle task into its own\ndata class effectively so instead of\njust indiscriminately\num having all the configuration in the\ntask\ndeclaration you put them inside the\nconfig object itself\num certain things have been moved inside\nthe spark context\nthings like parallel i'm not sure what\ni'm not too familiar with spark but\num look for it in the spark context if\nyou can't find it in the session\nuh uh test metadata previously it was a\nfunction we've made it a data class we\nthink it's a little bit cleaner\ndown below we've also added the workflow\nmetadata as a data class\nthat you and it's available through the\ndecorator args as well\nthe types have been moved to each to\ntheir own subfolder so this is the way\nthat we'll continue to add\nthe natively supported types at least if\nthere are any more to come\nand\nthere was a small bug in the\nhandling of named tuples that were only\none element\nlong so that has been fixed\nand i think everyone who uses it should\nread this section as well\nwhen you get the chance this is\nimportant because\num the registration process has long\nbeen\nwe felt a little bit awkward and now\nwe're getting to the point where we're\nspending\nwe're trying to clean it up as much as\npossible so there will be more changes\nin this regard\nin the next couple weeks\nso currently you have to run uh\ncurrently you have to run you always\nhave to run two things\none to compile your code down to\nprotobufs\nand the second thing is to actually\nregister those credit bus\nwe're taking out certain settings\ncertain values\nfrom those protobufs and then basically\njust waiting until registration\nactual network call time to assign them\nand\nthis way you anyone can produce any\nartifact\nany flight artifact and then give it to\nsomeone else\nand under a completely different flight\ninstallation you can again re-register\nthe changes that we're going to continue\nto make is pertains to the the top part\nuh currently our recommendation is that\nthis is run\ninside the container we that's pretty\nslow and leads to poor iteration time\nso we are adding some more configuration\noptions\nto when we say we and katrina is adding\nsome more configuration options to this\nuh to the command that will allow you to\nspecify everything that you need to\nspecify outside of the container\nand it'll also work with fast\nregistration and all that\nbeyond that because if you take a look\nat this\npr it's entirely new code effectively\nand it touches the pr number 136. it\ntouches the old code in very very few\nplaces\nso the alpha and beta moniker for us has\nalways been\num alpha really means\napi changes to the user uh\nthings that would break existing user\ncode and beta\nis more like uh\nwe're testing whether or not the\nold code that's currently on master is\ncompatible and will break\nso we're thinking of releasing data\nprobably by the end of this week and\nremoving the entirely\nuh making an official release very very\nshortly thereafter maybe another week\njust\nfor testing that the current code isn't\nbreaking\num along with that we'll i'll update\nthis stock again\nwith the final changes that's just the\nproposal where we laid out the spec\neffectively\num and we will\nsee what else i wanted to say um uh\nwe'll be better about\nlaying out all the tasks and hopefully\nonce it's merged\nuh we'll have a more concrete list\nsmaller list of things currently it's\nall just in a google doc\nof features that we want to add in bug\nfixes and whatnot and\nthe community at large can collaborate\nbetter\nso they'll probably be flight issues\nand i think that was it\nso uh summarizing we we are in alpha\nwe might introduce breaking changes but\nwe plan to go into beta\nlater this week um or maybe early next\nweek and that that\nat that point we are saying essentially\nthat flight kit new api is stable\nand we will we will maintain that api\ngoing forward\nno matter what uh till the next\nbig change of course um\nand i think we want to actually\nrelatively shorten the timeline between\nbeta to\nfinal release and that's what you\ncovered\nso um i would recommend everybody to try\nout the new\nuh fight kit you just have to do pip\ninstall\nflight kit 0.16 a2\nso that's the alpha 2 and\nthe examples are in flight cookbook they\nare published to\nread the docs please go through them\ni think ergonomically and this is my\nopinion\ni find it really uh really easy to\nwork with and and and very easy to\nextend so please\ndefinitely give it a try and let us know\num i think the next person\nis anand uh he wants to\nhe wants to share about like note to\nnode relationship and what's the status\nwith that\nand give a little more detail about it\nfor the new people\nuh sure hey thanks kitten let me share\nmy screen just confirm if you guys can\nsee my screen\nawesome can you see my screen now yeah\nokay beautiful so uh what i want to talk\nabout today is\num we have rolled out a new\nevent version uh within\nflight when a propeller actually sends\nevents back to admin uh why did we do\nthat the first main reason was that\nour current modeling uh\nof the events for example let's say if\nyou are\nlooking at an execution and you look\nclick on an execution you look at all\nthe nodes inside the execution\nthe layout here is actually incorrect\nwhatever we show here\nthis layout directly maps to\nthe data which is stored in the admin\ndatabase\nnow we have three issues going on so the\nfirst thing is we actually\nwe're storing the data in the flight\nadmin\nin a wrong format number two propeller\nwas\nactually not sending the correct\nrelationship and so on\nnow before i go on to explain what the\nissue is let me\nactually show you the difference between\nthe correct one the\nincorrect one so this is the one after\nthe fix\nso technically you can see that there is\nactually\nthe node 0 and node 1 were actually\nwithin the\nparent node 1. so this was a sub\nworkflow and the sub\nworkflow actually had other nodes\nthis was actually not modeled in the\ncurrent uh\nevent version so actually this is how we\nwere uh there were a lot of bugs for\nexample let's say if you had a sub\nworkflow with the same node id\nwith a node node in the parent\nessentially we will just overwrite it\nso everything will run correctly in the\nback end\nbut when you see it in the flight admin\nyou will actually have\nonly one execution for the node id and\nalso in the ui\nthe the guy which writes the last or the\nnode which executes\nlast will only be displayed everything\nelse will be swallowed\nuh and so on so these are the main\nfundamental issues that we're going on\nso what we did was we introduced\na new event version uh so\nessentially if you want to add uh the\nevent version\nyou have to enable it in flight admin\nuh so here's the here's the way it works\nif you update your code to the latest\nyou will have all the bits\nof propeller and admin but you will not\nhave the fix\nfor you to get the fix you will have to\nupgrade the event version\nto the new event version and i will be\nshowing more information of that so\nthe way of doing it is you have to\nenable it in flight admin\nand once you enable it in flight admin\nwhat happens is\nwe would update the event version\nin the flight workflow meta so this\nactually goes\ninto the propellers custom resource\nso as you update the admin uh all the\nnew executions that you launch will have\nthe new event version\nso think of it this way so essentially\nwhat happens is that\neverything that you have run in the past\nthere you are that ui will not change\nbut the moment you update the new event\nversion\nuh every future executions will now\nstart\nshowing up uh in the correct format\nuh so i can actually show you some other\nchanges\nbut the main thing that we did was\nuh in the past\nthe biggest issue was that in the\ndatabase\nwe did not have a concept of a node\nbeing a parent node to another node\nso we had this concept in the in a\nworkflow structure in a static play but\nduring in an execution format when you\nrun it you cannot say that hey\ni actually am a child of another node\nwhen you are executing so for that what\nwe did was simply just add\na child relationship uh but once we did\nthat we actually found out that there\nwas another functionality that was that\nwent missing due to this change\nwhich is that within let's say if you\nare running a sub workflow\nthe entire sub workflow can have retries\nso so let's say you have a workflow and\nwithin the workflow you have a sub\nworkflow\nand you say that hey i want a retry of\nthree which means that let's say\nthe execution of the sub workflow phase\nthe first two times\nyou will run the third time and you can\nsucceed but\nin the ui you actually want to know\nwhich nodes were\npart of the first run which node\nexecutions are part of the second run\nand which node resolutions are part of\nthe third run\npreviously what we did was we used the\nconcept of task execution id to actually\ndo this arbitrary grouping\nbut in the new world what we have done\nis we introduce a concept called as\nretry groups and so on so essentially\neach execution of\nthe entire group we called as a retry\ngroup\nand we also passed this metadata from\nthe propeller\nso the whole change has been like\ngetting rid of\nfixing the bug and making sure that the\nfeature parity exists\nand also updating the ui so essentially\nnow the ui\nalso is able to fetch all the data\ncorrectly\nuh and so on so essentially if there is\na node execution\nuh admin sends to the ui that this node\nis a parent node\nand if it is a parent node uh the ui\nwill\ngo and fetch all the child executions\nand so on\nuh so yeah this is essentially the\nfeature\nbut you will have to upgrade to the\nlatest versions of flight uh\nto get the entire thing uh right now at\nleft we have rolled it out\nuh to our users but we just had to\nrevert it once\nmainly because one of our users actually\nhad written\ncustom code uh trying to fetch\nall the node executions and i know and\nuh\nmanually looking into the node id and so\non\nso we are actually trying to just fix\nthat particular issue\nbefore rolling it out for the uh\nyeah but everything is out here so you\ncan just try it out\nso feel free to try out the new even\nversion\nto get all the latest fixes another\nquick question can we just generally\nmake it\ntrue for everybody going forward you can\nlet's just bake it out for uh uh for a\nfew days maybe in the next\nrelease uh we can just make it uh add\nthe new event version as\nthe new v1 and so that everyone who is\nnew and uh\nwho wants to try it out can just get the\nnew version but the main issue is i want\nto make sure that we don't have we don't\nfind any bugs because rolling back is\nis not a happy place to be uh\nrolling back into you can roll back but\nthe only\nissue is that let's say if you have some\ncustom code you don't want those\nnew executions to fail that is the only\ntricky part so i just want to bake it\nout a little bit and then we can just\nmake it default\nfor everyone\nyeah yeah for uh\nfor people who just recently joined at\nleaf\nthere are far too many very very\ncritical workloads running on flight and\nso we are\nvery careful of rolling out changes\nanything that impacts even a small\npercentage is regarded as\ntough to roll out\nand anything else any questions from\nanybody\nand can you actually point a\nconfiguration\nas to where to enable that uh and can\nyou put that in the notes\nyes uh i'll do that so i didn't i didn't\nwant to announce the feature because uh\nthe\nso there is the small fix here is that\nuh\nwe want to expose the node metadata\nuh in the in flight kit\nso currently uh the node metadata is not\navailable in flight kit\nso like i'm working on a fix to just add\na node bit data field into the flight\nkit so that our users can use it\ninstead of hacking it their own way so\nonce i get that done\nuh essentially i just send you all the\nrelease notes and so on so\nbut the whole thing is very easy so if\nyou are\nrolling out your change uh just make\nsure that you are updating the database\nalso which if\ni believe your deployment should already\ndo it and all you have to do is just\nenable this\none event version and that's it but\nagain like i said uh\nmaybe in the next to next release you\nwill make this default event version so\nthat you don't even have to\ndo anything you just have to upgrade and\nall the newer executions\nwill be in the right format uh so that's\nthe target the\nthe new world would be that everything\nwill be in the newer event version\nuh because right now what we have is we\nhave both the event versions actively\nworking in that we have code\nfor both even version zero and even\nversion one and we don't want to be\nthere because even version zero is\nlike hacky and wrong so we actually want\nto delete\nall the code from propeller itself for\nthe older event version\nuh but we just want to bake it and you\nknow move it slowly before we do that\nthank you any questions from anybody\nprobably spotify okay\nand free gnome if you guys are running\nthe new one i don't know if\nit actually mainly impacts people who\nare running sub workflows\num launch plans and branch nodes\nso if you guys have like launch plan\ncaller within your execution sub\nworkflows or\nbranch node these are three cases where\nyou will hit it because essentially\nthink of it so you have branch node with\nthe same\nno oh yeah these are three cases where\nyou hit it exactly\ncool the next one is uh proful he's\nnew to our call so uh welcome profuse\nuh and he has a small demo he's been\nworking on flight for the last week or\nso uh\non and off and he wants to show\nhey hey thanks kaitlyn uh hey guys\nuh happy new year to everyone\nuh thanks for uh\ninviting me to this uh call so i've been\nworking\nlately uh in flight kit with katherine\nand trying to understand\nhow it's trying to solve the uh ml\npipeline problem so\num so with some of the open source\nthings that are happening so i\nwant to jump in and see how i can\ncontribute to this\nso one thing uh when i was reading about\nit like me and kate were discussing\nabout\nhow um uh jupiter text is\nkind of an environment where uh uh not\njupiter text but jupyter notebooks is\nthe one\nwhere most of the uh like academia\nand uh the ml workflows are being\nwritten\nand it's kind of an interactive\nenvironment uh\nwhich people can use it and\ncan test their workflows and it has been\nuh\nit will seem like it's being adapted uh\nquite a lot\nand one thing that we don't have\ncurrently in\nflight is actually supporting uh\nthe notebooks directly because\nuh most of this code is embedded inside\na json and so essentially when you say\nthis notebook is um basically just a\njson which contains like\nmetadata information along with certain\ncode blocks\nand i can just quickly show you how\nexactly the\nnotebook is structured so can i\nshare my screen you should be able to\nokay okay just a second\nokay\n[Music]\nokay okay\nlet me know if you guys can see my\nscreen\nokay great yeah so\nessentially you've been during this time\ni'll just go over some of the things\nthat i spoke about like why\nwhy we have the use of jupiter notebooks\nand\nthe one thing that i just went over is\nit has been\nused extensively for doing interactive\nuh\nworkflows in academia and machine\nlearning\nand we want to support this for flight\num so i'll go over\nhow what are the things that we'll need\nuh\nfor supporting this notebook\nin flight um what are the limitations\nand issues\nuh i'll go over a small demo and then\ni'll also this is my first week uh\ninstalling flight so\ni'll just go over some of the things\nthat i uh learned and\nuh how things we can help in\nonboarding uh new folks when they uh\nactually start using flight\num yeah that's my agenda so\num so as you know already that\nflight can't read python code in\nnotebooks i'll quickly show you how\nexactly\na notebook is structured so i've written\na sample notebook here\nuh i'll just go over that and\nthere's a the lab environment which\nactually shows you how to\nedit a particular notebook\nthis one just so jupy lab\nmost of the folks use this interactive\nenvironment for\nediting their notebooks\nokay so i have a sample jupyter notebook\nhere\nwhich is just essentially doing has some\nmeta information along with a very\nsimple\nfunctions which python functions which\nare just doing\na whole square of a plus b\nand this essentially is just calling two\nfunctions and\nuh returning uh the value now\nin a in a jupyter tab environment you\ncan actually\nrun this particular piece of code and\nif you have a main function you can\ndefine\nthe results of this and it can\nreturn you the results over here\nnow most of the jupiter notebooks are\nwritten this way now if you want to\nsupport these sorts of notebooks\nfor flight uh essentially we want uh\nthis is a very basic notebook but uh\nagain i will go over like what all\nthings that the\nnotebook author has to uh has to do\nin order to do this but i'll also show\nyou\nexactly how this notebook is structured\nso if\nso if you can see like it's mostly a\njson file with\ncertain cell types which are marked down\nand\nand certain code sections uh which\nactually\ni have an embedded code now in order to\nhave\nflight actually understand this uh\nit has to be able to pass this out now\nwe're\ntalking about like what all things we\ncan use to actually\nuh get this information out so one of\nthe\nuh\nopen source one which we found was\njupitex and\nwhat it supports is basically uh\nconversion to\nuh python and uh it can convert\na python code back to a notebook so\nthat's a\nquite flexible way for people who are\nwriting these notebooks to um have this\nuh direction support and they can um\nedit that\nuh within slide we know that it\nuh it it reads from the python file so\nit can it can generate uh the\nuh uh so the author can actually\ntry to convert it to um\na flight task and\nconvert it to have uh all the data\nthat currently is\nsupported through form from these\nfunctions so for flight to understand he\nhas to\nuh convert those to uh protobufs\nso those are some of the things that he\nneeds to additionally do\nuh in order to have a full flight\nsupport\nand uh one thing is with jupitex we\nfound it it was\na very easy to use command line\ninterface uh\nif there are any any additional ones we\nuh we can explore those as well this was\nthe very easiest for me to integrate in\nthe first\nversion um and\nlike i mentioned like what steps that\nthe author would need to do is basically\njust\nmodify the notebook to add the flight\ntask or\nworkflow annotation in the basic uh\nthing\nuh if any custom transformer that he\nneeds to add if he has\nany particular data class that is not\nnative then he has to\nadd transformers for those and then run\nserialize to\nconvert those to protocols and register\nthose\ntasks in flight uh and through the\nflight admin then he can\nrun those particular things um\nso so i'll just go over um\nlike a small demo of how exactly this\ncan be done\nand then i will go over the limitations\nof these\nso as i showed you from the flight task\nfrom\nthis is not a flight task as of now so\na simple way that i can do it\nis basically just import flight kit\nnow right now i'm running this file\nwithin a python environment which has\nflight kit so\nthat is the first thing that he will be\nrequired to do\nand from this you will be importing task\nand workflow right\nand also in order to have\nall the parameters actually be\nunderstood by\nuh flight you need to have uh the types\ndefined\nin this particular case uh this would be\nan output and an input and input peak\nand intend\noutput being on it so um\ni have not read much uh the\nwhatever i've read from uh that i'm\ninferring this is\nvery much required for flight to\nunderstand the data types that it needs\nto understand\nwhen it is generating the workflow and\nyeah so essentially this is sorry\nused to java\n[Music]\nyeah so this would be something i'm\ngonna change that\nthe previous import statement to from\nfrom flight kit import\nokay not in the previous sorry\nall the way around yeah again java\nor scala rather\nyeah and this is basically\nimporting the the workflow and you have\nit's essentially the same thing now you\ncan even run it\ninside\ni think you need a space\nor a blank line between square and sum\nokay yeah\nso\nessentially it's gonna so\nwith the conversion to the\nthe flight kit format it again runs the\nsame way in the\ninteractive lab now what we're gonna do\nis\nuse this in our flight environment\nand the way i have created one\nfolder inside our recipes\nthis just has one initial init file for\nthe python\nand we're just going to copy this\nnotebook over here and what we're going\nto do is now\nregister this particular so i'm going to\nserialize this\nso that's the first thing that we'll do\nis\nuh what this will do is basically pick\nup\nany uh any notebooks that we have\nfound in our cookbooks right now i'm\njust\nworking on the quickbooks and uh it will\ngenerate\na corresponding python file so you'll\nsee over here\nthere's like it found one notebook and\nit converted that\nto a python format currently it uses\nlike\nuh an option to convert it to a string\nspace\ni go over that it's just uh how\nthe documentations would be generated\neasier\nnow if you see at the end you have\nif i just grip for\n[Music]\nthe protobuf files wait i think\ni did something okay\nokay i didn't save it that's the one\nthing\ni missed so\nover there though it converted because i\ndidn't save it\nit wasn't able to find any flight\ntasks so just copy over the python\nnotebook\nagain over here\nand i'll just serialize it\nso for people who are new i think what's\nhappening is the container is being\nbuilt which gets attached to every\nsingle task\num there is some change happening in\nthis also\nwhere we are making this optional but\nthis is how it is today\nright okay\nso now if we try to grab\nfor the protocol files you'll see those\nprotocol\nfiles have been generated\nnow the next step is to basically\nuh register this\nand i'm running like a local flight\nadmin over here\nand essentially right now i don't have\nthat workflow here\nlet's see that pretty soon\nsorry this demo is taking too long i\nthink i should cut short this\nthe next time i do it i apologize for\nthat\ni think you're done though it's\nregistered yeah\ni think you can jump over okay\nyeah and\nso you see that the new workflow has\nbeen registered here\nand so this is the the whole sql\nworkflow that\nwe saw for which we had the notebook\nfile\nso just give some parameters that\nand yeah it should launch\n[Music]\ntakes a few minutes but i'll just go\nover uh\nso this this thing should complete and\nit should give the output similar way\nthe other recipes have been written so\nthe main agenda was to\nsee how exactly the uh how we can use\njupiter notebooks\nand can be used in flight\ncan be consumed over here some of the\nlimitations that i see\nis is currently which do not have\nautomated conversions to the flight\nformat so the author has to actually\ncome in and\nunderstand how through annotations he\nhas to annotate his workflow define how\nthe\nthe data is going to be used um if\nif there is any custom data classes he's\nusing he needs to define those\ncustom data transformers i've still\ngoing through those how exactly that can\nbe used so i'm trying to use\nanother example uh how exactly this can\nbe\nmade much simpler and\nother things are like i saw like issues\nwith\nuh once we generate the python currently\nuh in\nuh locally it doesn't allow you to\n[Music]\nrun uh the same uh register again\nbecause\nuh we have um a conditional\nthing that unless you have committed\nsomething uh\nyou won't be able to run uh because this\ngenerates a python file so if you want\nto\nfind a solution where we keep the\ngenerated file in some other folder or\nhave some github to solve this\nother is very small like sphinx\ngenerated files\nthere are certain formatting issues once\nwe convert\nfrom a notebook to a python file\num other than that i think this seems to\nwork\nfor most of the cases like basic cases\nuh okay going back so we'll see that\nthe two tasks have completed within that\nworkflow of the sum and square\nand if we go back to our workflow uh\nthis is also succeeded which is a\ncomposite workflow\nand if we look at our inputs and outputs\nyeah it's generated so it successfully\nshows that\nwe can convert any\na particular notebook and run it in\nflight\nuh using jupitex um\nsome so\nsome of the learnings i'll just quickly\ngo over that uh\none thing was\nfrom the documentation we have\nuses of mini cube and also\ndocker desktop for running it uh native\nkubernetes cluster\nso i found many cube has had certain\nissues when i\nstarted running with certain om\nissues uh and similar issues with docker\ndesktops so something that can clarify\nthat in our documentation would be very\nhelpful um\nalso with um with mini cube i think\ni was seeing uh certain times like\nuh the mount point that we are using uh\nfor\nthe serialized sandbox uh\nit used to uh not mount it correctly and\nso\nuh the local file system won't be\nuh reflected uh when the container was\nrun so\nuh the uh the resistor or the serialized\nwon't get any output files from the from\nthe protobuf output\ngenerated in the previous substage this\nwas very particular\nit was intermittently happening when i\nswitched back to docker desktop\nthis issue no longer happens and uh\nso so with all these i think i even like\nthe workflows\neven if it uh worked um even if it was\nable to register and generate the\nprotobuf files\ni found that there is the register will\ngo through but\nagain the workflow when we try to run it\nwould\ncomplain about pulling it would have\nissues with pulling the images again\nlike i couldn't debug\nuh deep into it but some networking\nissue over there but\nuh with docker desktop i was not having\nany of these issues\nso uh i mean if uh if docker desktop is\ngoing to be like a local environment\nwhere people will be using outsiders\nthat people\num clarify that one part\ninstead of mini cube so that people\ndon't run into this issue\num other things where slide get\nusing an older version with an\nincompatible flight admin so i think\nthat got resolved in a recent check-in\nuh converting uh pre-existing notebooks\nwhich use\ncustom data class i found it like a\nlittle\nnon-trivial to implement so i think i'll\nread more and work with you guys to\nunderstand how we can do this\nmuch simpler and probably add some\nexamples\nto help the community understand how\nhow to write these kinds of workflows as\nwell um\nand there were certain flights recipes\nuh which were failing i've worked with\ncaitlin on that\nthe um request i was looking for was\nrunning flight in debug mode and\nnavigating vlogs couldn't find some\ndocumentation around it\nuh and also like if you're using uh\npython environments uh\nprobably we can mention some place where\nall the versions of which python to use\nbecause i was jumping around with\ndifferent versions\nuh to make things work i had a latest\npython version which\ndoesn't have have support from numpy so\nnumber\nuh right now it doesn't compile with the\nthe new 3.9 version\nuh that is out there so i had to\ndowngrade python\n3.8 um yeah so\nuh so just click a small feedback uh\nif you can mention all uh like these\nsmall nets\naround in our documentation can help uh\nuh the the consumer of the talk\nmuch more that is awesome\nyeah i think that's it from me\ni think if you have any questions i can\nanswer now or\ni'm available on the slack also like we\ncan talk about that\nthank you truffle this is like the first\nweek it's awesome thank you\nyeah thanks for watching yeah thank you\nthank you the feedback i think\nthis is exactly sometimes when you're\nworking on it and you know the problems\nyou forget to add the description for it\nso\nthis really helps thank you\ni i wanted to add one more thing so what\nprofile demod is\nconverting an entire notebook which has\nworkflows and tasks\ninto python for equivalent file which\ngets registered with flight we do have\nanother\nmethod in flight which is you can write\nuh\nevery task can be a notebook which uses\nthis thing called as papermill it's an\nopen source library\nuh and it will be available in the\nthe new flight kit api probably in the\nnext alpha or beta release\nit's i have already implemented it it\nshould be out and that makes\nvery generalized it works for every task\ntype\nso but both are different paradigms\none is a paradigm where the user writes\nend-to-end\ntasks in one workflow and the other is a\nparadigm where you write end-to-end\nworkflows you know\nin a notebook and so\ni don't know which one is preferable\nthere are\ntwo camps outside in the world too so\nthis is an interesting another third\nsolution which is actually cloud pickle\nwhich we are\nleast in favor of because of brittleness\nso\nokay thank you i think\nfor the new people we definitely\nrecommend look at flight cookbook dot\nread the docs\nuh and i'll put a link in the notes\ntoday\nthat has the examples uh most of the\nexample that's what\nuh before using for running all the\nexamples\nand they all run locally and they also\nrun remote\nso please let us know if you find any\nproblems\nwe want to use a document example-based\ndocumentation approach going forward\nbecause that's probably much better for\nusers to follow\nthe last thing that we have uh two\nthings one thing is we are doing a\nplatform release today\n0.10.0 like we're back from the holidays\nand we've been working through the\nholidays\nyou will get a platform release that\nbrings some ui goodies and some back-end\nimprovements\nincluding uh squishing some bugs\nuh which uh you should see and then um\nthe other thing is uh i'll be sharing\nthe notes today later\nthat has a planning sheet attached with\nit\ncan you please add um\n[Music]\nthe planning like if if there are any\nthings that\nlet me share my screen and i'll show the\nplanning sheet\nquickly uh and then we can go from there\nso if you can see this is the planning\nsheet oh sorry this one this is the\nplanning\nsheet pretty empty of course if it loads\nuh what i would like if if you have any\ntask stories that you\nwant flight to have\nor you should think about definitely add\nthem at a description\nselect a priority that affects you or\nthe company that you work for\nor you know anywhere in the open source\nand please\nlet us know if you can help with them uh\ni would appreciate a reason of why you\nthink\nthis would this is required uh your\ngithub handle and a\nlink to a github issue if that exists or\ncreate one\nuh we will be also be filling this out\nin the next week\nor so so please fill this out um\nthis should uh basically hopefully we\ncan have something by the\nby next meeting and the idea is to get\nthis filled out so that we can plan out\nfor the year and we can have much more\num uh planned milestones for the year\njust like we did like in the later half\nof last year we want to continue doing\nthat\neven more planned and so that you know\nexactly when your feature is going to\nland\nuh and as we ramp up with more people\nuh this would be also helpful in you\nknow distributing\nworkload so that's the thing\nand and this is our meeting notes and\nmeeting\nuh agenda we'll share that\nany other questions uh let's we can do a\nq a\nany questions\ncomments\nokay i guess we're good um so definitely\ntry\naction item fill out the spreadsheet\nplease\nand um try out like cookbook\nand the new new flight alpha release\ncool everybody bye bye everybody a\ncouple weeks\ngood to see y'all bye\nbye\nbye"
    },
    {
        "title": "FlyteMeet 2020/12/15a",
        "transcript": "hi everybody i think first up is yi\nunless there any other do we have any\nnew people that i don't know before\nwell if you're new at this first time\nyou introduce yourself raise your hand\nsure\nhi um hi i'm thomas uh part of the\nfreedom engineering team\num and also the weekly user of flight\nand hoping to uh hopefully contribute\nback but also learn more from the\ncommunity\nnice to meet everyone\nwelcome thomas\nhey i just want to introduce myself um\nhappy to be here my name is neil\ni'm a ml engineer at talk space but i\ni'll i guess i'll talk in a little bit\nabout pandara which is a\nproject i've been maintaining\nokay thank you for doing this on such a\nshort notice\nnews\nhey so i guess i can get started\nuh we were hoping to\nlet me stop my video uh we were hoping\nto\ncut a release last night i think that\ngot delayed until this morning i think\nwe will try to do it\nright after or at least shortly after\nthis meeting\num this is going to be the\nsecond i guess alpha release\nof the new flight kit api\nthere's obviously less stuff than the\nfirst release and i think\nif people i don't think we're\nstuck to the strict\nmoniker of the the naming of the version\nso if we find bugs i think we're going\nto start\ncutting smaller patch releases as well\nbut in any case in this release\nthere are a few things that we wanted to\ngo over\nthe first of which is hive hive as you\nknow if you maybe\nif you don't know is uh strangely\nhandled currently\non the on the master branch this is how\nit works if you see on the screen\num that we we create a\nwe basically have a couple utilities\nthat create a\nthat wrap the user query in a template\nand uh that final query is actually what\nis executed\non the back end\nand there's a couple differences between\nhow presto and hive\nbehave when this new\nlatest model that i'm talking about now\nis what\neventually we'll be moving to which is\nkind of irrelevant because\nyou don't have a crystal client\ncurrently in open source but\nhopefully the near future um so instead\nof\nhaving pre-ordained wrapping templates\nlike you see here\nthis is basically a templating that will\nquery run a user select query but then\nwrite the outputs to a specific location\nthat is known to flight\nso instead of doing this we are\nwe've introduced you can take a look at\nall these later um\nwe've introduced things like this in the\npast\nthis was initially only for presto um we\nare now enabling this for hive or this\nhas been enabled as of the latest\nplugins and propeller um\nand users can construct whatever so if\nyou look here there's a\nthis is the insert one there's another\none for right\nuh and it never really made any sense\nfor us to do this\num so users can now write\nhive queries in the new way which will\nbe\nuh i wrote a little\nthis is not yet checked into the\ncookbook uh we will do that\nshortly as well but uh this is an\nexample that shows\nthe uh the\ntemplating fields that you get i guess\nso you can use this\nfor creation of a temp table uh this is\nwhere your output data should be\nand then input schemas you can refer to\nif you are before the this is applicable\nto the right case\nhere\ninput schemas will just get extrapolated\nand interpolated into\nlike with their uri field\nwe have also introduced this is a major\nsticking point in\nthe older flight kit multiple image\nsupport so if you specify a\nconfiguration like this you will be able\nto\ncontrol and the user is still\nresponsible for making sure that these\nimages exist\nbut you can run\ntasks with i think i have a\nbetter example of it um if you do this\nyou will now be well hang on\nthat's what's not exactly what i wanted\nwell in any case\num do you want me to explain this one\nsure\num so the problem\nuh statement here is that uh we\nit's currently felt by many users that\nflight only supports one container image\nbut that is not true\nuh the container image binding is per\ntask\nand that is by design because it\nwhen you're working locally it's very\neasy to install all your dependencies\ninto one virtual environment\nand uh work with your entire project if\nyou're using python if you're using java\nyou can do the same\nbut when you're running remote um\nin in the model of flight you want to\nactually package not only your\ndependencies but the runtime\nsystem while you're applying so for\nexample\nif you want to run an algorithm on\na deep learning model on gpus\nor if you want to run a spark job\nuh if you want to run something on gpu\nyou need media drivers if you want a\nspark job you need\nspark charge and an entry point that's\nuh\nthat's specific for running uh\nspark and like if you i don't know if\nyou've ever used sagemaker but if you\nuse sagemaker then you need another\nentry point for stage maker and\nit needs um if you're using distributed\ntraining then you might need hardware\nand open mpi setup\nso this setup which is actually not\nreally\nusually part of the user's journey when\nthey are\nyou know running it locally um when you\nare actually running it in production it\nbecomes complicated\nand if you you could get away with just\nbuilding one complicated image for\neverything\nuh now the problem with that is the\nbuild times increase the sizes of the\nimage increases which\nalso affects the runtime uh and\nyou know just generally large images are\nnot very useful\nalong with that they are the added\ncomplexity of making all of them work\ntogether in one image\nso to to avoid that problem\na simple solution is to use multiple\nimages one per task\nbut then they you the real problem with\nthat\nuntil now was the ux for this so we've\nnow proposed a new ux uh\nwhen you're working with it in flight\nkit um\nby default if you have a workflow with\nmultiple tasks then it will use the same\ndefault image\nif you don't specify anything but if you\nwant to overwrite that\nand create a new image then as\nhe is showing over there and task t2 you\ncan say\nhey instead of the regular container\nimage use this container image\nand the container image here is uh\nis parameterized using the same\ntemplating system that's been used\nthrough our flight kit so one of the uh\ndefault image is called image.default uh\nand\nthe fqn is the name of the image which\nincludes the you know the repository\nlike docker dot io slash\nflight.org xyz\nand then the version is actually the tag\non the image\nand so you can combine those two\ntogether to form a fully qualified image\nname with the version\num now if you don't specify both of them\nthen\nthis is as you see in t2 is the actual\nfinal version that will be applied to\nthe container image\nbut as you can see because these are\nparametrized you can actually\nchange any one of them for example in\nthe next one you just used\nsome you know docker dot i o like a d4\nyou're using a docker dot io image in d5\nyou are actually\nusing the fql from um\nwhich is constant but the version is\nsupplied at the run time and the version\nnow if your views flick it you what we\nare recommended\nand we recommend everybody do is use the\nversion as the git hash\nor the git sha and so in this case\nwhat what's essentially happening is\nwhen when we're building the\nimages for this repository instead of\nbuilding one image we're building\nmultiple images\nand tagging with those with them with\nthe same get share\nin the cookbook which is a better place\nto show all of these examples we have\num what we've done is we've used\nthe same image with different tags\nuh and the the difference is that the\ntags just have a\nhave a separate um uh\nappendix uh in the end like like so if\nyou for yeah that's park hyphen\nthat's it so basically the tag is the\nsame so the name of the image is the\nsame\nthe version is kind of the same except\nfor one prefix\nand so what this allows you to do is\nwrite the code locally\nhave all your dependencies okay make it\nrun locally as is\nbut when you're running it remote use\nthe benefits of having different images\nwith minimal overhead hopefully so and\nthen the\nexample for all of this exist\nhey thank you we've also added\non a completely for note tasks are now\nin the new api as well\num we will have\nwe i think we already do have um the\ncopic examples with us\nand the type engine has been extended to\nsupport\na subset of data classes namely data\nclasses that are also data\njson serializable so you have to\ndecorate with both\ndata class json and data and data class\nso there was some initial discussion\nabout\num i think from hong kong to use data\nclasses as\na mechanism by which users can specify\noutput uh like custom output names of\ntasks and workflows we decided to not to\ngo\nwith that approach because then we would\nhave to differentiate between\nwhen a user actually meant a custom data\nclass type\nor and when a user meant just to name\nthe output\nso for the sake of clarity we decided to\ngo with just\nletting users use this only for uh\ncustom types\nbut now tasks like this will work\num and kate then also want have made\nsome changes to\nspark data frame handling and also do\nyou want to talk about the new\nflight kit gallery i guess or flight\nsnacks gallery\ni guess you could i guess you can show\nthat but i'll talk\na little bit about so the data is the\ndata class json\nis a separate library that you that\ncomes with like it if you enable the\ndata class plugin what that essentially\nis trying to solve is handling\nuh you know support for arbitrary data\ntypes user defined data types\nand like flight supports\ndictionaries and flight supports uh json\nvalues being marshaled and\nsent through the system but the ux for\nit is basically what we've decided is to\nuse data class\none of the things that i wanted to add\nto what he said is why we did not decide\nto go with\nonyx's recommendation of using data\nclasses as a\nreturn variable naming system is because\nexploding a data class uh into a tuple\nbecause let's say on the left hand side\nyou are trying to\nreceive multiple parameters kind of um\nnot really idiomatic python\nso it would look odd when you're\nreturning a data class and you're\nexploring it into multiple\nvariables um instead returning a name\ntuple and exporting it into multiple\nvariables is very idiomatic\nand that's one of the reasons why we\ndecided to\ntake only named doubles and keep this\nthis way\num uh if you uh do you wanna\nactually scroll down a little bit so\nthis is the new the flight sex\nupdate essentially all the\nwritten in flight snacks is now written\nin the format of\nuh literate programming so code has\ncode and comments all in line and\nas we publish new examples and\nhopefully we document them very well um\nthey will automatically\ngenerate the stocks along with the code\nall of this code is runnable\nlocally with market you can also\ndownload\nall the code look at it\nyou can it also allows you to get a\npython notebook\nand we'll soon be adding binder support\nso then you should be able to\nrun everything in binder as well if you\ngo back\nyeah and of course this is all published\nto read the docs now in here\num there's an example in the\nintermediate section what we've done is\nwe've also laid out\nexamples in three sections so beginner\nintermediate and advanced\nso if you're getting started with flight\nit's better to start with\nbigner uh you can start it's it's laid\nout in a\nin a serial order in which if you read\nthe examples\nor try them out you should ramp up on\nflight pretty\nquickly uh and uh going to intermediate\nis once you have like you know\nunderstood all the beginner concepts and\ntried them out\nit's you might have questions and we try\nto answer them and intermediate\nfor example um one of the things is how\nto run conditions or\nhow to use schemas and those are all\ngiven as examples over here\nthe cool thing is as the documentation\nit generates if you\nmark them specifically it also captures\nthe output\nof that execution so we are constantly\nrunning those examples in docs and\ncapturing the outputs if you go back for\na minute\num yeah let's so the number\ni'm trying to understand which one so\nyou can open up number\nthree i guess\nso this example shows um how to use\nuh spark but in this case it's\neven more interesting uh what what's\nhappening is actually the task number\none\nis actually generating a spark data\nframe that matches\nthe schema construct uh or the schema\ndefinition declaration of flight\nand you just return the spark data frame\nand if you look at task number two it\nalso accepts a schema but you open it\nand the default open of the schema\nreturns a pandas data frame\nuh you know you can parameterize it to\nreturn like other types of data frames\nto currently only spark data frames\npandas data frames are supported\nthis is a loadable module where you can\nadd more plug-ins\nand we want to add different other types\nof data frame technologies like\nwakes and uh\nsorry i'm forgetting a couple of them\nbut there are a few\nof them available so the the interesting\npart to note over here is that you can\ntake\na spark data set or data frame\nand retrieve it as a pandas data frame\nin a subsequent task\nand this happens completely\ntransparently\nthis is possible because flight\nrepresentation of data frames is\nabstract\nit does not really care that the data is\nis uh uh a pandas data frame or a\nspark data frame it transforms from\nthose data for instance to a centralized\npresentation\nand then using that representation it\ncan transform to any other format so you\nshould be able to read it\nfrom a pi spark data frame into\ndefinitely into a spark\njava data frame as well as if there is a\njava data frame technology\nwhich is besides spark you should be\nable to read it into that\nas long as it can let's say read from\nparque or some other formats\nso that's one of the things uh and this\nis\nachieved by using um\ntype transformers that we talked about\nsome time ago and i think niels will\nprobably uh give a little more demo of\nhow you used it to\nto write pandera integration which\nactually makes this data frame system\neven better i think\nuh please go through this and ask\nquestions because it's a little\ncomplicated i think once you\ntry to use it it hopefully simplifies\neverything\nit's complicated to explain so i would\nlove feedback on this\nthat was it for um for the changes i\nthink\nsorry that's it for the the flight kit\nchanges\nyeah and we should have this alpha out\nsoon actually the alpha code is ready\nit's like we were\ntrying to do the examples and how to\nrepresent them out and so that's why we\nuse this new plugin\nin swings wait i i think i forgot to\nmention the conditionals\nhave been fixed on on all sides so they\nwill now fully work\npending one more pr from haytham\nperfect\num who's next meals\nyou're muted george\nchiang hong is next on the agenda but i\nthink we ought to go with neil's because\nwe're probably gonna run out of time\nbecause that went a little bit long\nand i'd like to see niels of stuff um\nsince he's new so i think neil\ngets pumped in before we hit our our\nlimit and then we'll deal with oh thank\nyou\nrebouncing the limit sorry taken\nsorry i might need permission to share\ni'm not\nseeing where i can\noh oops\ni should have figured this out before\nthe meeting but\nscreen like my computer needs\nto\noh yeah i need to restart my zoom to be\nable to actually share my screen\nit's weird so when you restart maybe\nchanging\ni think okay you can ignore that\nnotification\njust try to see it again it's a mac\nright\nyeah yeah\nokay i think it said something like i\nwon't be able to\nyeah yeah they won't be able to share\nlies a lot yes\nokay so everyone can see this this uh\njust my editor\nokay great um so\njust a little bit of a background on\npandara it's um\nbasically a runtime data validation\nlibrary for pandas\nalthough in the future we are planning\non supporting\nthings like desk and spark\nvia the koalas api\nbut um in any case the way\nit's kind of evolved but um sort of\nthe interface they're basically two ways\nof expressing schemas in pandara\num here i'll just show the newer\nshinier version of it so basically\ni have an example here of how you would\nuse pandara\nin concert with flight so at the top\nhere\ni've defined three schemas in sort of\nlike a a two-part pipeline\nso the first schema is the input schema\nwhich has two columns um and it\nwe can leverage inheritance in python\nhere to\nsay process that data\nto an intermediate schema that has total\npay and this is kind of\nmaybe inefficient or superfluous but\nthis is a custom\ncheck that pandara enables to just make\nsure that\ntotal pay is equals\nyou know hourly times hours worked and\nthen\nthis is also kind of a trivial um\nexample but you know the final output\nhas some kind of worker id\nattached to the to the data so the way\ni've threaded this together in flight\ntasks\nis um i have two tasks total pay and\nad id and the way in\nin pandara you check the types\nof of like the inputs is we're just\ngoing to use the type annotation\nof python\nand this is kind of a special data frame\nthat pandara has where you can\nyou can like specify schema with the\nsyntax\nand um it's all put together in a\nworkflow\nso i have some you know this is like a\ntoy example where i have some\nthe initial data here and i call total\npay and ad id\nand just to show you that everything\nworks here locally\ni'll just run this example and at the\nend of the workflow\ni get this data frame\nso if i for example mess up the\ncomputation so if i say\nthis is if i just add those two numbers\npandara should complain with informative\nerror messages saying what the failure\ncases are\nwhat the column and indexes were are so\nthis is like\none of the primary value ads of pandara\nis when things fail they fail early and\nthis is actually um with a few more\nlines of code you can actually\nget access to this entire data frame\nwith all the failure cases and so makes\nkind of debugging easier\nso i'll just i don't know i'll spend a\nlittle bit of time\ni guess on what the integration looks\nlike so\nin this panda schema\nmodule in flight i basically created\nthis pandara transformer here\num a lot of the stuff is duplicated from\nthe pandas or the flight schema\ntransformer\num so maybe not the most\ndry implementation but at the bottom i\njust register the\npandas transformer or pandara\ntransformer\num initially i was going for an\ninteraction like this where all\ni have to do is import pandara\nand like\nflight should be able to know about the\npen\nthe pandara data frame over here um\nbut that doesn't really work because it\ndoesn't know that the type is registered\noh\nhuh okay\nso that worked i don't i have no\nexplanation for that\num okay scratch that last comment\nuh i don't know why this is working\nbecause before\noh there there you go okay i didn't say\nthat i think it existed\nor if your fight code is older yeah\nyeah yeah great so this is what i was\ngoing for\nwhere you just kind of if you know about\npandara and flight kit\nyou can have both installed and it'll\nauto\nautomatically know that the pandara data\nframe type is\na thing um\nbut for now to get this working i just\nhave to import it from the types module\nin this way um so\ni guess i'll with that i'll end when\nopen up the discussion for\nyou know how to actually write these\nplug\nthese kind of third party plugins\nyeah i had a question about that nails\nactually so do you have a better way of\nloading those we would love any ideas\nthat\nanybody has of how we could you know\nlazily\nbut yet dynamically load um these type\nplugins\nthere's another place where we had the\nsame issue actually it's in the\nit's in our docs where we you have to\nimport something\njust to make the like the pandas data\nframe transformer work\nand you know you put an oq a check at\nthe end but\nif there is something we would love all\nyours\nyeah i i don't know\nuh the beauty of lazy import is that\nthis allows you to add plugins\ndynamically but the\nthe sad part of the import is that you\nhave to import it\nright so\nthis this looks awesome though i think\nuh just a\nshout out to neil basically he\ncompletely implemented this on his own\nlike in\nsome very little amount of time with\nzero help from us so thank you uh\nand i actually looked at pandera and i'm\nlike it's\nit's the the best ux essentially\naccording to me\nto to to define uh data quality checks\non data frames within\nin python it it probably is one of the\nbest\nuh and i'm hopeful that we will continue\nto you know work together and integrate\nit and make it even better for\nall the users great do we want to talk\nabout\nwhere uh yes thank you nielsen uh but\ndid you want to talk about where to put\nthis\ncaitlyn yeah should we open up that\nright now i'm sure\ndo you do you guys want to kick it off\nuh sure we we only have five minutes i\nthink remaining for this\nuh for this section of the meeting um\nbut basically\nthe it's the the issue is that it's um\nwhile having pandara is fantastic um\nhaving\nuh like 10 different uh pandara like\nthings\nall in flight kit increases the bloat of\nflight kit\num so we are thinking about a couple\noptions we can\nleave it there and just deal with it um\nwe can force\nflight kit into pandara which isn't\nideal either\num or more acceptable probably is\nuh creating either just a flight kit\npandera\nseparate repo entirely uh where this\nwould\nthat would get loaded and users would\nexplicitly have this\ndependency in their requirements or have\na flight kit plugins\nuh repo where all these plugins would go\nand you can just bracket select\nthe ones that you want i think those are\nthe the options that we were thinking\ni'm not sure if there are others but if\nuh\nneo4j then or anyone else has the\npinions\nyeah it's a panda of like it doesn't\nneed a separate repo it could be part of\neither of the repos just a separate\npackage if i find right that's what\nyou're saying\nsure yeah um\ni i actually think that the the\nseparation of modules is the best like\nwhen we started we just\ndecided to use the bracketing anything\nin setup\ntools but yeah it kind of bloats quickly\nso um but i am i'm open to suggestions\nof how what people would love the guy\nin my opinion it's the same right\nwhether you create one repo with all the\npackages or\nlet people have those packages published\nindependently\nit does not matter now the problem with\nindependent publishing\nin my opinion is is discovery like how\ndo you really find all the plugins how\ndo you\ninstall them um and\nand that might be solved using\ndocumentation or\nor having a central repository probably\nsolves that but it also\nincreases the you know\nlike the check-in commit problem where\nyou're like hey i have a comment and i'm\ncompletely different i'm pandering\ntensorflow and whatever so that's kind\nof\nmy two sides of the same coin\nessentially\nanybody has any ideas recommendations\ni guess nobody\nusually jeep would have an opinion on\nthis\nyeah i was going to say plus under the\nflight flight kit plugins\nuh repository just to kind of keep\nthings consistent i guess\nsince you already have like a flight\nplugins repository\nyeah and we can actually make it so that\nthere are multiple setup files right\nyeah i think that would be a great place\nto start i think\nand you would like publish multiple\npackages from that uh\nit could be a package per plugin from\nthat one viper\nokay i would i would actually even move\na flight spark and all those things yeah\nthat would be ideal yeah remove all of\nthose\ntensorflow flight tours all of these\ninternet\nyeah oh to that front i had uh i had\nsome\nnews i think we never talked about that\nin the agenda so flights moving to linux\nfoundation\nuh it's getting donated to the next\nfoundation\nwe still don't know the exact home\nwhether it's linux foundation any\nfoundation ai and data and all those\nthings but\nuh we are in talks and and i think\nuh there's all it's all approved from\nleft side it's just\nwe're just deciding the final\ndestination so some of this stuff might\nhappen\nafter the move just because all the\nrepos and everything making them it's so\nmuch easier\nin there\nall right it's less than a minute you're\ngoing to restart and let chang on\ncontinue\nand i don't know how to solve this 40\nminute problem with zoom\nyou solve it with a credit card i think\ni just solved it\nit says less than a minute still\nshouldn't your screen is still shared\nnews\njust fyi\nwell we'll see if it actually kicks us\nout because i just created to a paid\naccount so\nif it does we'll the only all you have\nto do is restart\nthe same meeting just re-click on the\nurl and we'll it'll come back to life\num give me one second\nshare screen\nall right uh can everyone see my screen\nokay cool"
    },
    {
        "title": "FlyteMeet 2020/12/15b",
        "transcript": "all right\num let's get started again\nhey george it says host disabled\nattendee screen sharing\nfor some reason uh yeah i understand\nhold on give me a sec\nokay you should be good to go cool\nall right um cool so\nuh so today i'm going to introduce a\nlittle\nuh idea about um what's called intra\ntask checkpointing and resumable\ntask execution on flight um\nso uh as we all know uh\nflight itself is designed to be a\ncheckpointing\nsystem uh already and it's working as a\nchecker point checkpointing system\nbut the checkpoint happens at the task\nboundary\ncurrently um so uh\nbut a lot of use cases on flight uh\nmandates on longer running tasks such as\nyou know some use cases such as ml model\ntraining or even\nyou know some other type of long-running\nprocesses to be completely\nyou know inside one single task\nso um the introduction so that this idea\nis\nto further um\ncomplete this type this direction or\nfeature\nto make uh to make you know\nuh to make it possible that\nuh the task can be um\nuh paused or interrupted but\nlater later on it can be resumed so that\nthe user don't lose any partial\nprogresses\nuh when they execute certain tasks\nso as you can imagine this is a very\ncommon feature\ninside the ml community uh checkpointing\nhas been used in\nall of the has been equipped with\nin all of the ml frameworks\num pytorch tensorflow keras all of them\nhave this checkpoint\nsupport to allow them to uh\nyou know store that partial progress and\nthen\nin in the case of uh computer crashes or\neven\num they just want to uh preserve\nthe current best uh model\nthey will use this checkpoint to achieve\nthat\npurpose so uh but beyond that\nyou can imagine that uh this\ncheckpointing thing\ncan also help us do some\num well\nit can also help us do some kind of cost\nsaving\nwhen we uh leverage those\ninterrupt interruptable interruptable\ninstances on each of the in each of the\ncloud providers\nso uh you can imagine like uh for\nexample aws\nspot instance is something that\nit doesn't guarantee the com the\ncompletion of the execution but if\nin exchange with uh for that it provides\na very low\nuh hourly rate so leveraging\nuh leveraging the checkpointing in this\ncase\ninfra task checkpointing in this case\nwill allow you to\nsave your partial progress and uh when\nyou are running when you are running\nthings on\nuh interruptable incident instances\nwhich\nwill allow well create a lot of cost\nsaving opportunities\nso uh this is a feature this is a ticket\ni created inside flight and then uh\nbasically what's\nthe important thing here is that it\nlinks\nto this design doc which is public now\num i believe if some people\nuh if any of you cannot access this\nplease let me know but i\nbelieve this is already public\nokay so uh this this project is\nuh at the very uh the beginning phase\nwhere we are still doing the design\num but um so\nplease feel free to give feedback on\nuh on this stock\nso uh let's briefly go through this this\ndocument\num so as we just said the goal of this\nproject is to focus on\nproviding the explicit interest task\ncheckpoint support\nwhat do i mean by explicit checkpoint\nit means that the user need to specify\nexplicitly\nwhat variables or states they want to\ncheckpoint\nand then and also with this current\nversion we\nwe want to first focus on\ncheckpoint that can be resumed\nfrom the same note execution\nonly so for example you can imagine\nif i have a checkpoint some people might\nwant to say hey\nin the later um in a different workflow\nexecution i want to you know jump\njump start with a checkpoint i saved\npreviously in another\nworkflow execution that might be a valid\nuse case but that's\nnot that might not be the uh best\ntarget to focus on uh at the very\nbeginning\nso we will decide whether we want to\nuh defer this or maybe\nmake maybe we will have another project\nuh\nfor us this purpose but so what i'm\ntrying to say is\nuh this project currently focuses only\non\nuh checkpoints that can be resumed from\nwithin the same\nuh node execution\nand as we said uh also we don't\nwe don't want to automatically\ncheckpoint the entire program state\nwe want the user to specify what they\nwant to checkpoint\nand what they want to save and we only\nsave those thing\nto the cloud or persist to somewhere\nand then the user should be able to just\nfigure out and reconstruct the state\nby looking at the content of these uh\nexplicitly checkpointed\nuh variables\nso um the background and motivation are\njust\nvery uh you know intuitive and we just\ndiscussed about it like the ml use cases\ncost saving opportunities all those\num so currently\nuh there's a there's a keyword in flight\ncalled interruptable\num i'm not sure if it's available in the\nopen source um open source version\nbut currently interruptable means that\nthis task can be scheduled\nonto a spot say interruptable instance\nlike spot instance um however\nwhen the execution is interrupted it\nwill not be able to\njumpstart from the partial progress it\nhas made\ninstead it will restart this execution\nfrom the very beginning\nuh but just so how do we guarantee that\nthe execution will finish\nso in this case the interrupted\nexecution\nuh in its in its\nnot next execution it will be scheduled\nonto a normal uh\ninstance so uh this is uh slightly\nso you can you can imagine that this is\na different\nuse case\nyou want to try to see if it can\ncomplete without\nwithin the time that the spot instance\nis alive\nuh if not it will you know use the\nhigher cost instance to ensure the\ncompletion\nof the execution so\nthat is a little bit different from what\nwe want to propose today\nuh we want to you know be able to keep\nusing spot instance\nuh and we make just even if we can just\nmake a little bit of progress\nevery time eventually we will get there\nwith our partial\nprogress being checkpoint every time\nyeah so so\ni don't want to go to too much details\non these\ndesign but i just want to talk briefly\nthat\nuh how do you uh so there are several\ncomponents\nyou need to uh you need to implement\ninside across the entire flight\narchitecture to make this happen\num so the most important components\ninclude uh data catalog flight propeller\nflight kit and co-pilot uh and these\nfour uh so i draw some of the data flow\nand timing diagram\nuh for different cases um\ni have a data flow and a time diagram\nfor the check pointing which is\nsaving like the persisting path\nand also a different set uh data flow\nand timing diagram for the resuming\npath so um\nlet's take a look at what actually\nhappens\nuh maybe in this\nlet me zoom in a little bit\noops\nokay cool um\nso uh as you can see here that\num oh sorry my laptop is\nlagging a little bit\nokay so um\nokay let's first look at the data flow\ngraph so you can see that\num these are the interaction between all\nthese different components inside flight\nuh so propeller will interact with data\ncatalog to register\nthe execution id which serves\nus the signature of that checkpoint\nto data network and then also the value\nof that key corresponding to that key\nwill be the\ncheckpoint data reference uh and also\npropeller will also provide copilot and\nflight kit\nuh the data the actual data reference\nthat\nthese two uh these two uh\ncomponents you use to do to you know\nactually\ndownload or upload the data to or\ndownload the data from\nand flight kit will write the actual\ndata\nyou know user will actually trigger that\nright of the data through flight kit to\na local value\nand the copilot will read from that\nlocal value\nand then upload the check the data to s3\nuh or uh any remote uh cloud storage\nuh so this is just uh basically it's\nvery\nintuitive uh and uh how this should work\nand then uh inside from uh\nfrom the timing perspective what we can\nsee uh\nin this graph is that so basically\nuh these several components uh uh\nwork together the co-pilot is basically\nuh responsible for\nuploading uh the data continuously\nso we want to do continuous\ncheckpointing to make sure that we don't\nmiss\nwe don't miss uh uh how to say we don't\nyou know when we we want to\nuh sorry let me talk about\nlet me talk about the user user side\nfirst\nso when the flight kit\nafter the flight can launch the user\nfunction the user function can start to\nsave uh data locally\nto a local value through flight kit but\nthen\nafter the user finished around and it\nhas a consistent snapshot\nof the stage they want to preserve\nit well the user need to want to call\nthe persist function inside flight kit\nand then the flight kit will then\ntell uh well then tell copilot\nto upload the current snapshot\nof the data and then um\nat the same time the user can still\nwrite to um\ncan can do can keep can keep making\nprogress\nand save other things but this uh the\ndifferent\nthis time it will be saved to a\ndifferent buffer\nand then uh the next time uh when the\ncompiler finished uploading the first\nbatch of data\nit will look at the second buffer and\nthen try to upload\nuh that second uh the data from that\nsecond buffer\nso this kind of create a com this\nallow user to specify hey\ni have done writing this batch of data\nyou can snapshot it and upload it i'm\nfine with it\nand the copilot can keep just keep\ncontinue uh\ndo this continuous uploading\nto make sure that we always keep a good\npartial progress of the user\nuser user function and then at the\ntime where the interrupt the interrupt\nfrom the\nsay the cloud provider comes in\nif the data is not written\nit's not has not been persisted then we\nwon't\nuh get the partial snapshot\nof that last batch of data so we are we\nby using this mechanism we ensure that\nthis uh this uploaded snapshot is\nconsistent within itself\num so yeah so this is the\ncheckpointing diagram and then\num yeah sorry i know this is a little\nbit\nthere are a lot of things going on in\nthis diagram so uh please feel free to\ntake a look\nuh yourself and uh we please leave\nfeedback\nand we can discuss more in details\nand resuming is uh simpler it's\nbasically\nuh you know just the co-pilot well just\nat the very beginning when after the\nco-pilot get the data url\nby uh you know when it derived the data\nurl\nfrom the execution id and all those\nit will just try to go up to the s\nthe cloud provider the cloud storage and\nfetch that data\nand put it into a local value uh\nand flight k will also note we'll also\nknow that\nthe local data reference of that local\nvolume\nuh inside that local value and it can\npass that local\nreference to the user function or things\nlike that\nso this create an abstraction\nand also the reason why we want to use\ncopilot is that we don't want to\ninterfere we don't want to take a lot of\nresource from the user\nfunction execution we want this to be\nresource-wise separated the upload of\nthe data and user function to be\nresource wise separated\nis there any questions so far\nokay um\nso uh we have so we have decided to\num uh to divide this project\ninto uh several phases uh the v0\nof this project is a simple a very\nsimple\nimplementation to support the sagemaker\ntraining job task\nbecause or um or you can you can support\nany kind of ml\nuse cases but in this v0 we specifically\nwant to focus\non sagemaker training job tasks because\nit has a slightly different\nimplementation under the hit\nbut basically we want to support\nwe want to let the sagemaker training\njob\nbeing able to use this checkpointing\nmechanism\nso that you can schedule your sage\nmaker training job onto spot engine\ninstances\nand here the\n[Music]\ninterface\nand then we use uh checkpoint assist\nto actually uh persist that uh\ncheckpoint\nonto the cloud uh and we want this to be\nexplicitly\nwritten by the user and then uh\nwhen we come to v1 we think that\num because this functionality is\napparently usable\ni think it is very generic and it should\nbe usable by all the different\nlike a lot of different type of flight\ntasks\nso v1 the target the the goal of v1 is\nto\nexecute this thing to implement this\nthing\nin a generic way so\nyou can like even in python tasks in\nhigh torch tasks in\ntensorflow tasks you can all use this\nuh checkpointing mechanism to checkpoint\nyour\ndata and resume from it uh\nand uh and resume from it later\nso that you can jumpstart to your\npartial progress\num yeah and\num so yeah here's just\nanother another different uh another\nexample of the user interface and here\nyou can see that it's\na python task and uh we got that we can\nget the checkpoint from the workflow\npower\nup parameters which is the older version\nof this uh which exists in\nthis uh conventional version of flight\nkit\nin the new fly kit it will be inside a\ncontext or something like that we\ni i haven't i dig deep into that but\num so yeah and the usage is also very\nsimple you can\nwrite to all these um look\nthis checkpoint locally by using the\nsave function\nuh and then once you are fine with\ntaking a snapshot and upload it\nuploading it to the cloud\nyou can call this persist and that uh\nuploading will happen\nyeah so that's pretty much it uh so\nuh please uh if you are interested in\ndoing uh in\nin this function in this feature please\ntake a look at this\ndocument and please provide feedback\nthank you so much\nthanks\num we're coming up on\non an hour and i want to be sensitive to\npeople's time and i think we're gonna\nhave to\num\npostpone one more agenda item right\nno i think we we talked about it already\noh yeah great okay\nokay yeah can you just share that doc\nmaybe or maybe george i guess in the\nmeeting notes or something\nso people can or it can be accessible\nfrom the uh yeah okay sure yeah just\nshoot your link\nand put in the meeting notes yeah\nbut um yeah awesome stuff\ngeorge should we take a quick poll that\nnext\ntime uh\nabout what uh whether or not we won the\nmeeting or caitlyn\noh oh next yeah there's a ho the next\nmeeting would normally fall\nlike right between just right before new\nyear's and the week before new year's\nso uh we were thinking to take that\nthat time off since many people are\ngoing to be traveling at least in north\namerica\num or at least off that week that's a\nsort of traditional\noff week in north america with that does\nanybody else\nwant to uh drive the agenda without us\nor should we just wait for\num four weeks from now\ni guess i will take silence as i said\nthat we should skip\nthe december 20 whatever\nmeeting and resume in january is that\ngood\nokay i'll take that as a good so we'll\nsend out\nmeeting notes and video as soon as\nthey're processed and then\na reminder that we won't be meeting next\nweek and we'll come back in we'll\nprobably change the zoom link so\ni'll you know that'll go out in slack\nand in the to users but we're gonna\nchange um to the paid plan of zoom so\nthe meeting id will change\num so if you just click on the old one\nyou probably\nwill be lonely\nall right all right okay i think\nanything else\nokay everyone thanks very much thank you\nand we'll see in about a month\nall right"
    },
    {
        "title": "FlyteMeet 2020/07/28",
        "transcript": "there we go\nokay we are recording\nand\nokay i just enabled screen sharing\ncadence see if that works\nanon thank you that was it for me\ni think\nyou should have a screen share button\nokay\nyeah cool um so\nthis is the flight snacks repo i'm just\ngoing to give a very quick\ntour uh the existing set of examples\nthat we had\nalready uh exist and that's\nuh so when you land on the home page of\nthe\nrepo there's a replay that shows you a\nfew options uh one of them is\nfor example you can use kit uh today\nwe only have like eight examples in here\nlike to python\nand else and if we do one we can we\nwould love to add\njava google so in this way that way it's\na very common\nlanguage of expressing ideas getting\npeople\nup to speed with new sdks\nbut uh so this is the existing examples\num but what we've added is a cookbook\nuh it's kind of a fun one like snacks i\nguess\nuh you know how to cook the snacks uh\none of\nthem here is uh how to write a task and\nthe\nway we've done it is you have a repeat\nactual file which is co-located\nuh you can go through various different\naspects\nlike an interesting one is the\ninteraction example in the interaction\nexample what we've done is instead of\njust\nuh showing the python code we're also\nshowing an example of how you use your\nnotebook and this is actually a\nrunning jupyter notebook it shows you\nhow to actually interact with\nexisting workflows and production or\ndevelopment\nand execute them or\nregister relaunch field execution and\nthere's a very interesting example in\nthis one\nwhich uh\nit also gives you in the same place how\nto use cli\nand once um i think austin and\nuh raj are working on like ctl once they\nget\nuh moving further ahead they can add\nexamples for\nctl in this one uh once\nonce you have that uh one of the\ninteresting examples is\nuh this is a very typical case that some\nuh some users have asked us about uh and\none of them is like let's say i have an\noverride the future introduction how do\ni\nrelaunch a backside institution or\ncreate a partial workflow for that\nand this is uh an example of doing just\nthat\num moreover\nif i go back here uh there's examples of\nusing raw containers dynamic workflows\ntasks and this is not by no means\ncomplete but\nwe are trying to complete this pattern\nis there something that is specific we\nwant uh definitely bring us in the\nchannel clear an issue\nand i think this should be a high\npriority task for honestly\ncomplete it on the other hand\nif you are if you are ever wanting to\nunderstand what are the different\nplugins and how to use them\nthey are actually listed separately\nso in this case it's not like a spark\njob there's an example of hardware\nstructure\nand here's an example of writing a smart\njob\nlater on this also tells you how to\ninstall spark\nfor your container same thing for five\ntouch training\nuh i guess once again instead you'll be\nadding\ntensorflow training uh and we have\npresto query with high quality\nuh statement all of these uh examples\nare coming in and i think\nthis is more of an interactive\ndocumentation that means we can actually\ngo and make all of these examples\nyou might find some hiccups and you're\nstill cleaning them up\nbut if you do find something issue park\nuh raise another channel either one it's\nfine\nyou'd love to know that so that's right\nsnacks um\ni think i should put a link to it\non the other hand uh is a quick quick\ndemo\nabout catalog so i think uh a lot of\npeople\nknow that flight comes with data catalog\nbuilt-in\nand that serves two purposes it's used\nin memorization\nand second and biggest tracking uh\nthe problem today is that our\nui doesn't really show any information\nwhether the cache was populated i think\nthere was a cache it or a cache\nand any links into the catalog\nand that was a true item and\nso that's what i completed but the ui\nstill\nis probably uh one step behind it will\nbe adding that\nsoon but i will show you a more ghetto\ndemo it will use\nan api to show all\nlet me show you an example\nin this case this\nall right um in this case this was a\nrun workflow and if you can see\nlet's go back so usually it takes about\nfive minutes and around in 33 seconds or\nseven seconds\nsystem and this is running on my local\nlaptop so something happens\nwhere i am running four workflows and it\ntakes longer but\nuh it ran very quickly uh but we don't\nknow why\nthat's not just permanent inputs it has\noutputs and if you look at the task\nit shows you that the task is\ndiscoverable\nwhich is you know catching is equal to\nthis is a\nlegacy name for catching uh and the\nwardrobe for discovery which is\nbut we don't know whether it was\ncaptured or catchments\nand so on uh same thing\nin this case is a sub workflow so this\nis the task and cycle software for\nthe same input output it is discoverable\nand you can see the discovery version\nbut yeah there is no information about\nthat\nlet's also catch it right now so let's\nrelaunch this again\njust just for kicks um\nand let's change the\nsize and i know i have not never run\nthis again so it's gonna be on all of\nthis\nso let's just run it while it's running\nyou know let's go back to that cached\nversion so the id\nis and that's the execution id\nwhat i have is uh actually i had already\npulled it off\nwhat i have is a flight admin api\nand i am actually going for that\nexecution id all the node executions\nand in this case there was a node called\nscaled\nlet's go back to the console let me see\nthat\nthere's a node called scale\nif you look at the cache status it says\ncache hit\nand if you look at the catalog key it\nalso provides something from this\ncategory\nwhich is essentially how to look up that\nitem in the catalog um\nproject domain and the name of the thing\nand the version\nuh it also provides you the ultimate\nidentifiers to look up\nthe item in the catalog in this case\nit's the artifact id\nand build the attack point of the name\n[Music]\nalong with this it also provides if this\nwas a cache hit\nwithin flight which was the originating\ntask that generated\ncaching information so it was generated\nfrom\na task called flight snacks\nunder which had this version\nand it was the execution id was uh some\nx y and z\nin this case i don't have unknown and\nthis is because this was a\nolder example uh\nthat means i i ran\num the caching information before\nor the cash flow populated before my\nchange was pushed in\nso right now we don't know the actual\nexecution name that's\ncausing the cash population but after my\nchanging you will actually start seeing\nuh the actual name of the cache\nand which is going to be which is what\ni'm uh which is what i'm doing\nso so that's the this is a subtask\nsomewhere flowing in rather so for that\nwe have to\nsee that it's a sub workflow and some\nworkflows\nopen that up once you open that up uh\nit has a rotate task that's the rotate\ntask\nagain that was the cashier\ntag notification\nso the eventual goal is in the ui\non the on the right panel uh maybe there\nwill be a small icon here that shows\nwhether it will catch it or\nmiss uh and then on the right hand side\nwe will show you the actual link to the\nexecution click on it\ntype more\ntool that somebody written on top of\ncatalog that shows you\nuh all the kind of entries and filter\nthem by\nsome some other thing that they are\ninterested in only\nlike for example yeah i don't know\nexactly how to use the tool but\nuh once you click on it it shows you\nmore details about all the artifacts\nthat were generated and the flight\nexecution id\nthat generated so\nyeah so then you can keep on you know\ngoing back and forth from\ncatalog to flight so eventually\nour goal is to take we are taking the\nexplorer that i just showed you and\nmerging it into our ui\nso then the users will be able to\nnavigate from their workflow execution\nto the originating workflow\nto the actual data set explore that data\nset and come back into the execution\nso that's that's the high levels here\nbut let's go back one minute and see\nwhat happened\nof course i'm running zoom and i'm\nrunning everything my\nlaptop is\nso sad but this one worked yeah actually\nwe can still get information\ni need to get a newer laptop my laptop's\nfour years old\nso uh yeah in this case\nuh let's see so it says cash populated\nnot\na cash because we actually\ngenerated data and then populated into\nthat\nso and then all the information that we\nneed to cut this information\nbut there is no overage source execution\nid because\nthis was the source and next time when\nyou run into source execution id\nso that's a quick look at\nthe changes that are coming in they are\nall\nready to go uh it's in one it's like\nacross\nall the recourse there's one one here\neach\nand my target to merge all of them\nis this week if not this week\nnext week and then when you deploy it\nthat should not cause any changes there\nis a small migration that will happen in\nthe database essentially at a new column\ncalled cache status in the database and\nthis is essentially for analytics\nso that we can quickly run the analytics\nof how many nodes\ngotta catch it what's the sketch miss\npopulator and so on\nuh but that should not change anything\nso those are the two things uh\nin my head kevin do you have anything to\nshare\nhey kitten i can also i i want to talk a\nlittle bit of the node\nas well yeah let's do that so\nokay can i go ahead yeah you can awesome\num i will write it in the notes sure\nuh hey guys uh for those of you who\nhaven't met me\nuh i'm anand i have been at forever\nbut this is my first meeting uh uh more\nrecently i\nuh i've been working on observability\nmostly on the internet\nand to keep flight accountable for any\nissues\nuh you know trying to track\ninfrastructure issues and trying to\naccount it\nsince last week what i have been working\non is um\nthe ability to add a\nparent-child relationship to a node\nexecution\nwhat i mean by that let me share my\nscreen\ncan you guys see my screen\nawesome so i am taking an example of\nbatch uh tasks workflow and\nthis is all open source so you can you\nknow try it out uh\nso here is a set of nodes and if you\nexpand the setup node\nuh there is a list of more nodes and so\non\nbut if you actually look at the\nuh they actually graph you this is the\ncorrect one where you only see a set of\nnodes\nthese nodes in turn produce a list of\nnodes\nand there is another example that i want\nto show you which is a sub workflow\nwhich is\nhere there is just one node called as\nidentity work\nexecution but what this node does is\nthis node\nis a sub workflow node and the sub\nworkflow can have more nodes\nbut ideally you want to have this node\nand other nodes\ninside the parent node\nwhich means that the any node if it\nproduces more nodes\nthis could be a dynamic task or a sub\nworkflow\nthey have to be that needs to apparent\nchild relationship between\nnode executions so today in our system\nwe do not have it\nbut you might ask me hey if that is not\nhappening\nhow is dynamic task alone working\ncorrectly\nthe reason is because what we have\nintroduced is we have introduced a\nconcept\nof a task being apparent to a list of\nnode executions what i mean by that is\nthat in terms of dynamic task a node\nresults in a task execution the task\nexecution\ncreates a list of nodes and we associate\nthe list of nodes produced\nto the task and not to the parent node\nso\ni am working in actually refactoring\nthis\nrelationship and making sure that node\nexecutions\ncan uh map to apparent node execution\nbut ideally that is slightly tricky\nbecause in our admin system today we did\nnot\nhave a concept of parent child uh\nat the node execution level so that's\nwhat i'm refactoring it\nbut one of the main things that i wanted\nto share in this aspect was\nuh we\nthis change is in some regards a\nbackward incompatible change\nso we are trying to be very careful in\nthe way we roll out\nuh so what i mean by that is technically\nthe way\nparent child uh relationship is today is\nthat uh all the node executions parent\nis a task\nand the task is mapped to a node we are\ncompletely going to remove\nuh this relationship of notes saying\nthat hey there are children to apparent\ntask\ninstead the the new world it will be\nlike all the node execution will say hey\ni am the children of a parent\nnode uh so as we move forward\nthe current code that we have is going\nto be removed\nbut how do we roll out so currently what\nwe are trying to\nintroduce is we are trying to introduce\nan even version concept\nwherein we will support\nboth parent tasks in parent node but the\nmoment you upgrade their event version\nyou will only see for any execution\nuh all parent charge relationship at a\nnode level\nso but if you go in the past and look at\ninto the ui\nfor your older workflow executions you\nstill\nyour ui will still work and mainly\nbecause they follow the old even\nuh old even version format so\nthe way we are trying to target is that\nwe will support both\nfor a significant amount of time and\nonce we do like a major version of grade\nwe will remove the old code but until\nthen uh we are planning to have both the\ncode\nexisting it's just that the partition of\nwhether an execution will be using a\nparent\nuh node parent-child relationship versus\na task parental relationship\nwill be at the execution level uh and so\non\nuh i also shared a document in the\nuh if you guys want to take a look uh do\ntake a look\nuh but yeah this is mainly what i have\ntoday\nuh yeah any questions\ndid you say windows is going out that's\na good question so\ni have made a change in the admin\nuh the next thing there are two more\nthings that needs to happen one is we're\ngoing to need to figure out a field\nwhich we want to use for event version\nso the propeller can use that field\nand then the last is ui so uh\nyeah so i i do not know the timeline for\nui but everything else should happen\nyou know in this week maybe mostly by\nthis week uh we i'll get\nuh randy's uh uh the person who's\nworking on ui\nand see when he can get to it and i can\nshare it in the chat\nyeah i think uh just also from my point\nof view this\nis uh one of the biggest problems in our\num in our data model at the moment\nand you're still sharing\num so this is a\nbiggest problem in our data model and uh\ni think before we can\nrelease 1.0 we wanted to have enough\ntime for this to take in\nuh we don't think this is a\ndestabilizing change in terms of\nuh the overall system but it's just uh\nincompatible with uh with the decision\nthat we've made\nin the past and we are fixing in a way\nthat\nshould have minimal impact but it's\nthere is some impact\num that you will see some some\ndifference in the ui\nuh but other than that i think this is\ngoing to help us\nyou know evolve even further i hope\neverybody's okay with that change uh of\ncourse\nbefore we roll this out into production\nand into the open source we will be\ntesting it thoroughly uh within our\nsystem\nso we're explaining it to be part of 0.7\nwhich is end of august\ni have a question uh you mentioned this\na problem in the\ndata model it's a data model of the\nadmin\nor yeah or something deeper in there\nno data model of the admin\nin fact\nyeah actually you can you can just show\nthe\nproblem and yeah the\nyou can still see my screen right yeah\nso uh yeah so here is what's happening\nso this is a\nnode execution every uh\nan entry of a node execution so whenever\npropeller sends a node\nexecution uh we store it in the node\nexecution table uh\nmain admin and each entry uh points to a\nsingle node execution\nso what happens today is we have\nintroduced this concept of apparent\ntasks which we have this is the older\nconcept\nlike we have this field parent task\nexecution id\nthis field essentially points to the id\nin the task execution table\nso what happens is that a node execution\nwill have a task execution\nand the task institution will have an id\nnow you have a list of nodes\nthese nodes are essentially saying that\nhey\nmy parent is that task execution in the\ntask association table\nwe want to deprecate this field and in\nturn\nhave a a recursive\na situation where you know each node\nexecution in turn will have a list of\nchild node execution\nand uh and vice versa which means that\neach new resolution which has a parent\nwill in turn have a parent\nnode execution id or a parent id which\nis what i'm naming it\nuh instead of having a parent task\nexecution id\nyeah this is the change in the data\nmodel\nuh that will be happening but what i'm\ntrying to say is that right now we are\nnot going to remove this field\nso uh in the in the near term you will\nhave both the columns\nuh and and you know at some point you\nwill have to get rid of it\nso that you can run uh you can just\ndeploy the new flight version and it\nwill still work in the older model\nunless you upgrade the event version\nwhich is what i want to come up with so\nsaying that hey\nadmin will now support both the versions\njust that that will slowly migrate\nand make sure that you know this field\nis completely unused\nat which time we can you know go and\ndelete it\nyeah i think uh we are probably\nat some point we're going to have a time\nwhen there are these deprecated fields\nand we'll do that clean up and maybe\nslot that for 1.0\nuh release so until then we'll support\nboth\nand else and i think they'll be\nsupported like\nat an epoch value so anything older it\nwill be the older\npropeller which is sending the events\nwith the right information in it\nand then which is not sending\ninformation\nright information in it then uh\nyou would continue to use the old model\notherwise you would start using that\nand the hopefully the impact is not much\nthis label uh if you can see the money\nhighlighting is actually\nnamed incorrectly this is not what i'm\nreferring to\ni'm going to introduce a new field\ncalled as parent id\nbut this is a different feature uh\njust that the label parrot node\nexecution id is now confusing\nso uh i i'll send you guys a br uh i'm\nalmost done it\nthat will clarify more uh yeah but yeah\ndon't get confused with this field this\nis com\nsomething completely different from\nwhatever we talked in this meeting\nit's just that the name is uh yeah\nconfusing\nsorry\nthen send any questions yeah\nyeah i think uh also for people who are\nnew right\nthere are three entities within an\nexecution\nof a flight graph uh the first top-level\nentity the workflow\nsecond entity is uh a workflow consists\nof\nnodes which are only meta entities they\nthey essentially exist\nto capture retries meta information\nabout a\ntask execution but they but we can have\nexecution like nodes of different types\nnode can have tasks in them\nit could be branches sub-workflows could\nbe dynamic\num and so that's what the node level\ncaptures\nand then there is a third entity which\nis only for task execution and a task is\nactual\nthe thing that actually doing the work\nfor example uh\nuh a container executed somewhere\nor a job executed in bigquery\nboth of those are tasks and they are\nmodeled as task executions\nso this change is at the load level uh\nthe central\nuh the method here that you are wanting\nto and\nthe natural hierarchy of\nexecution contains load executions\ncontain each node execution\nand so on what we had was one of the\nnode executions was\nwas linked to a parent education\nand so we had a circular relationship\nwhich we are\nyou know going forward\nokay uh all right i think i\ni had one more demo but i see gu has\njoined uh hey\njeeve welcome\nthank you it's good to be here hey yeah\nthanks for joining me\nand so nelson here is from spotify they\nare the other people who use it on gcp\nto our knowledge i know there are two\nother\nteams that are using it on gcp i\ni actually don't know which companies\nyeah they just ask questions that i\nanswer and then they go away but i know\nthese two few of you who\nofficially use ngcp\nso cool um\n[Music]\nyeah so g also we do this by weekly sync\ndefinitely join and bring your consoles\nthat you have\nokay one more demo i had was uh\nall the ui stuff that randy has been\ndoing\nhe's been doing a lot of work in the ui\nstuff so i just wanted to make sure that\nthat is uh there is a\nplace that we talk about it\n[Music]\nso this is not done yet but this is the\nprototype of the ui\nupdate that's happening um and if\nanybody is interested we can share the\nmark\ni'm uh not really good at navigating\nsigma\nhow do you do it\nyeah i have the same question i have\none of those yeah yes\nyeah why i don't know why it's so\ncomplicated but yeah okay\nuh maybe it's supposed to be so uh\ntoday we show all the workflow\nexpression is simple\nnode list uh what it's moving to\nis this waterfall pattern and it's not a\nwork it's a timing diagram\nessentially and the colors the different\nsaturation at times\ncolors indicate different states so for\nexample\nuh a node might have been queued for x\namount of time and then it actually ran\nand something like you and it's\nessentially the\nhigh level point of this is to provide a\nquick\nvisualization of what's taking time\nin the system or where did i spend most\ntime within my workflow\num how much eventually also to show how\nmuch\ncpu and every this part is using so\nkind of giving you a hint of the past\n[Music]\nand that lift this has been a very very\nhigh ask from our users um\nfrom open source i guess we've not\nreceived any of these apps\nbut at least people are using this on a\nday-to-day basis when they find\nthis sort of stuff really useful so\nuh these box essentially show\nwhat direction the ui is going in uh\nto make it it's more not\nmostly it's still an engineer focused ui\nbut it's\nuh it's made so that we can\nquickly crash problems uh otherwise\nso uh crosstalk system uh and\nthis will also form the basis of doing a\nlot of future work and that's why i\nthink\nrandy is probably going to work in this\nforum\nfor the first couple months in this ui\nuh and let me know if\nthe other thing that he's been doing is\ndeployed but uh for some reason we\ndidn't have this too basically\nuh\ni actually showed this a little bit more\nright so if if you are launching a blog\nyou can actually specify the uri and\neverything\nin line in the ui that didn't exist just\nsome time ago\num this is also going to change this is\nthe same source of that\nschema for uh blocks or multi-part\nblocks\ninstead of for single filing multiple\ndirectory\nand the uri will be\nand you can specify multiple a list of\nuris as well you can specify this for\ndirectories of level and so on\nand it's very useful to make sure that\nthe you can launch\nwith like accessory\nokay um that's i think\nfrom randy also he's done a lot of bug\nfixes and i think the ui\nis way more stable now so if you do find\nanything\nuse that as well um yeah\nwe have one ui engineer he is i think\nthe entire community always relies on\none ui engineer\nuh so these\nprecious resources so let us know if you\nfinally\ntry to try to get him to work and i\nthink if you know any ui\nleaders uh in your team or companies who\nwould love to help us\ni would really appreciate it\nwe have a lot of a lot of work to be\ndone in the ui\nand i just wanted to share that\ninformation with you what's happening in\nthe ui\nall right any questions comments\nsuggestions and\ni think i know a good judge\nthank you everybody um\ni yeah randy is um he's awesome but\nthere's only one of him\nand we definitely are looking for a\nsecond or third\nuh ui engineer with you know a strong\ntypescript background\nif you guys use your use your networks\nand let us know because we would love to\nhave some help\non that side especially because we have\na bunch of people\npiling up work behind him and randy has\nhe's the only person who\nexposes it to uh to the screen\nso um that's it\nanyone else or should we wrap this up\ntoday\nall right we'll call it thanks everybody\nfor joining um\nwe will see you in a couple weeks i'll\npost the video um\nuh shortly and uh anybody that wants to\ndemo\nnext week just dm me on slack and\nwe'll add you to the list yeah there is\na sign up street also you can just go\nand sign up\nuh yeah and kevin i hope you can level\nsomething for thank you for\ni'm putting some pressure all right\nall right thanks everybody\nyou"
    },
    {
        "title": "FlyteMeet 2020/06/30",
        "transcript": "um\ngood morning katrina\nhey good morning\nokay\nhello\ni'm afraid my dogs have spotted a deer\nso i'm gonna be unmute\nokay it's gonna get real noisy for a\nminute\nsorry\nhello\nso like the green orange\nwelcome everybody um i'm george from\nrunway from those that haven't met yet i\nthink we might have some new folks from\nintuit coming\num we are on zoom for the first time so\nwe're on the free plan which means\nthere's a 40 minute hard limit to the\nmeeting\nso keep that in mind which i think is a\ngood thing\num i think today we're going to go over\nwhat significant was in the last release\nand i don't know if we've got some\npresentation or demo\nabout um integration or not but maybe\nthat'll be for two weeks from now\nand any walk-on items are just you're\nwelcome to\nfire him up yeah i added to the agenda a\nfew things\nuh george okay yeah\noh chang was around he could have done\nthe demo\ni thought you are on vacation chiang\nyes\ni'm joking\nall right so\ngeorge do you want to start uh\nare the spotify folks on the call\nsonic's here the rest of the game\nis awesome okay um i\nthink the major announcement is that you\nguys are you know sort of officially on\nthe github page\nand are you know essentially co-sponsors\nof the project at this point\num so thank you for that\ndo you guys want to give us an update on\ninternal deployments\nat spotify or problems you're having or\nstuff that we could help with or\nany news at all\nit's still early stages uh we have some\npeople inside the company trying out\nthis will be like a a long project\nbecause we have\nlike uh uh thousands of uh\nwordpress running and luigi so\num and we have uh\nour scheduler uh it's open source but\ni think only us is are using it\nuh it's called sticks so\nuh right now we reached the point where\nwe have done enough\nproof of concept and evaluated\nwe would let us some order alternative\nuh no\nnow we are we know that we're going with\nflight but we\nstill need to build some\nintegration with our infra so\n[Music]\nright now we are in touch with a few\nteams inside the company to be early\nadopters and\nwe are doing the flicky java\nthat is uh we operate like some weeks\nago\num yeah it's still a long road to have\nuh\na production ready and migrate uh\nor the company to to fly but it's uh\nit's the first step in that direction\nthat's a huge step actually so thank you\nand just as a reminder anybody else so\nit's very hard for us to know when\npeople ask questions in the slack\nchannel\nand you know relate them to which\ncompanies\nthey're working at or what's like being\nused at it\nit would be really useful if if anybody\nif you are using\nthinking of using you know exploring\njust put your name\num either in the\npeople who are attending the meeting by\nweekly sync or\npreferably in the github repo that way\nwe know\nyou know how it's being adapted used and\nwhere we can\nchange and improve and if there are\ncomments and concerns when you're\ntrying to use it please raise them as\nwell that really helps us\nokay uh should we go over the next thing\ngeorge yeah do you want to go ahead and\nreview in the next release major fixes\nyeah so yesterday evening we just\nreleased v 0.5.0 again as promised\nend of the month every month uh\nwe've been doing this for five months\nnow uh and so\nthis one i think is probably one of the\nbiggest releases we've done\nuh till it it it includes a lot of new\nfeatures\nwhich should enable uh you know\ndifferent modes of interacting with\nflight as well as\nuh improvement within the the ci cd\ninfrastructure as well that\nhelps in contributions and testing and\nso on\nuh so we've been working hard done some\nof these things for the last few months\nand\nlet me quickly go over them maybe i'll\nshare my screen\noh george it seems you have disabled\nscreen sharing\nthat's okay if you can open up the\nchange log and it's in the\nmeeting notes there's a link to it just\nclick on it\nuh the basic infrastructure stuff that\nwe have improved is\nuh all the ci cd most of the cicd rather\nis on github actions now\nand we will be migrating eventually\nalmost everything\nall the bills and uh\ndocker builds checks will all be done on\ngithub actions\na few reasons for that one it's much\nnice much\nmore portable um so if you create a fork\nyou get all the cicd actions for free\nand that's really good so you can run\ntests and etc on your own\nforks\nend-to-end tests are now part of almost\nevery major component\npr so uh admin propeller and of course\nthe flight people so that runs a full\nend-to-end test and there's a way to add\nmore into and test tests and if uh\nclip or nelson you're interested in\nadding end-to-end intentions like kit\njava\nuh we can we can try we can you can use\nthe same pattern\nbrother and then\nuh for the core platform uh the first\nthing that we've\nadded is single task execution so you\ndon't have to\nregister a workflow when you start\niterating\non a on a set of tasks that you want to\nwork on\nas soon as you write one task you can\nstart executing it\nand with this also if you have tasks\nthat are containerless so cases in which\nyou don't need to build a container\nlike if you're running a query or if\nyou're running\nrock container which i'll just talk\nabout you can directly use a jupiter\nnotebook\nor any other interactive environment and\nlaunch the execution\nand the execution the ui is still a\nlittle wonky uh\nwhat we will what you get is at the end\nof the\nexecution launch you get the execution\nid if you drop that in the ui\nuh url you go to the execution page\nwe we have not yet built the list of\nexecutions\nfor single tasks and we i think katrina\ndemo did maybe about\nuh two meetings ago and and\nthat's exactly i think there are more\nimprovements and cleanup in the\ninterface itself\ni think should demo using a curl\ninterface there's a proper python flight\ndate integration\num a hugely requested uh\npiece or feature was\nrun to completion so\nif you remember if you've used flight uh\nevery time\none node within the graph fails we\nactually failed the workflow by default\nuh and this may not be preferable in\nsome cases in which you have a\nyou have a large workflow in it many\npeer nodes launch sub workflows uh\nand maybe you have bug and only one of\nthese workflows then\nthen we bought the uh entire execution\nuh and so if you're using memoization\nyou don't get the chance to memorize all\nthe\ninformation and rerun just for the work\nso\nwe've introduced three new flags or two\nnew flags\nyou can now tell flight that\ncontinue even on an error\nand it will try to execute as many nodes\nas it can\nwhich fall within the\nwithin the the okay set so that means if\na failed node\nhas if a node has failed then any\ndownstream node from that failure node\nwill not work but if if a node has\nsucceeded then downstream nodes that\nonly depend on the succeeded nodes will\ncontinue\nor the other option is to fade\nimmediately\nthere are lots of uh retries uh\nimprovements and time-out improvements\nwithin the system all throughout\nand uh the other big feature that we\nhave been wanting to get to eventually\nis to move data\nloading to be a side card and so we have\nmerged the pre-alpha support for raw\ncontainers with\ncopilot there's a set of sample\nnotebooks that are available in\nstrike kit and more examples will be\ncoming\nsoon and documentation will be added we\nhaven't had a documentation because we\nstill want it to be a little\nuh under the wraps and if you know if\nyou want to use it being me and we will\nstart\nexploring and using it there are a\ncouple caveats that we want to make\nsure that uh we cover all bases before\nwe\nrelease uh alpha and a beta and\neventually\nand improvements in the presto execution\nenvironment and that's about it that\nthose are the major features we've\nalways as usual we've done bug fixes\nand other improvements um\nand i think point 0.6.0 onwards\nwe there will be a huge set of\nimprovement that we're planning for\nthe user interface and the uh friction\nenvironment\nso keep keep looking on to the\nchange log and hopefully join this\nmeeting then we'll have more demos\ncoming soon\nthank you\ncharge your mutant\nthank you katherine um chiang hong do\nyou want to show anything today or do\nyou want to wait for two weeks\nuh i i would like to wait for two weeks\nthat's fine if that's possible\nyeah sure of course um\ni think that's all for the agenda has\nanybody got anything else they want to\nwalk on with\noh yeah there's one thing we have a talk\ntomorrow at\nopen source summit uh embedded linux\nconference\ntomorrow at 12 00 at 1pm pst\nuh if you i think it's free i'm not sure\ni can share the link i've shared the\nlink to the talk in the meeting notes um\nif you if you want to attend please to\nattend\nit and and i think it will be recorded\nand shared on youtube but probably in\ntwo or three weeks and we will i'll be\nposting\nand one one extra thing so kevin so\ni think we welcome him to the community\nhe is\nuh helping us with tensorflow operator\nintegration\num and he has\na prior background in tfx and keep flow\nand things like that so welcome kevin\nunless there any walk-ons i'm gonna call\nit a wrap anything else\nall right going going gone zooming\nworking for everyone i take it i pick\nthe people\nit's not working for are not on the call\nso we'll hear from them later\num all right so we are recording these\nuh we'll post the links to the\nrecordings in case anybody missed it you\nwant to show it to your friends\num but uh otherwise we're gonna stick\nwith this format going forward unless\nanybody wants to knock it off and\nand improve it some other way all right\neverybody\nthanks very much see in a couple weeks\nthank you bye"
    },
    {
        "title": "FlyteMeet 2020/08/11 - part 1 Flyte Extensions",
        "transcript": "all right cool thank you\nuh so it's a very plain presentation but\nit's mostly\ncontent oriented uh and i threw it up\ntogether very quickly\nthis is talking about uh two aspects of\nflight\nwhich uh one from the contributor side\none from the user side\nuh first we'll talk about the\ncontributor side\nand uh this is in\nin cases when flight is extremely\npowerful there are lots of things we can\ndo but there are lots of things that\nstill can't\nand uh many many uh\nof our contributors just trying to add\ndifferent features like basically if you\nfew people added supports for tensorflow\nuh distributed supports for one person\nadded distributed python support\nchannel is actually adding sagemaker\nsupport and so on so\nuh how does that mechanics like how do\nthose mechanics work\nuh and they're within the company with\nit at lift i mean\nsome people are uh using big query\nthey use in a different way so i just\nwanted to bring up like how what are the\ndifferent modalities of\nextending flight what are the different\nthings that you need to be careful about\nand why would you\nchoose one over the other and after that\nwe'll talk about dynamic tasks\nwhich is also another very misunderstood\nuh topic\nand i think a less advertised one but\nit's it's really powerful and we\nwill talk more about it today okay\nwhy extend flight right uh flight is an\norchestration platform that's what it is\nand if it is an orchestration platform\nit needs to orchestrate things\nand infrastructure and make things\nhappen for you\num but it doesn't want to solve any of\nthe\navailable open source uh solutions it\ndoesn't want to\nlike solve those problems on its own for\nexample it's not a mapreduce platform\nit's not a\nquery it's not a um\nit's not a a distributed\nuh machine learning platform it\nit enables all of these but it's not\nreally a platform on its own that allows\nyou to do one specific thing\nbut for those we have many many other\ntools in the open source\nand they do a great job for example we\nhave spark\nfor my produce we have presto big query\nretraction of a\nnavy pick and there are many more coming\nup uh\nto do querying on large amounts of data\nand they do a fantastic job with it\nuh and and with distributed\nuh training there are lots of new uh\nalgorithms and and frameworks coming up\none\nsome of them do crosstalk cluster all\nreduce some of them do a centralized\nreduced\nthey do a great job too\nwe found one thing missing and so we\nbuilt it and that's an array task and\nwe'll talk a little bit more about it\nthat rolls into the dynamic tasks it's\ninteresting that that's one thing that's\nnot really\npervasively available besides couple\ncloud environment\num and some cases users just want to\nwrite like\nsimple binaries right like run like a\nsmall\npython code or it's a binary in c plus\nclass or any other language\nand and those were the flight wants to\nconnect all of these pieces\nand it doesn't know all these pieces yet\nso we can teach it\nthat hey you can learn about this new\npiece or that device\nand it's still a nascent or\na young community we only know a few\nthings\nso people who have been offering and\nadding things\nthank you you're learning more things\nevery day so let's continue to answer\nokay um\nwhy let's talk about two types of\nextensions\nthat are possible in flight one of them\nis called the lightweight extension so\nyou know these are more like if you're\nfamiliar with airflow they're like\nairflow like operators\nuh you can basically write any code in\npython\nand try can execute it right for flight\nit's python and some people actually\nuh have asked me this in the past uh to\nextend\nthe flight do i need to do the back end\nextension and so on no you don't need to\nit's still running a python code you can\ncall any service\nuh do anything uh for from flight point\nof view it's executing a python\ncode or it could be java code right if\nyou're looking for java\nbut internally you may\ncall bitcoin or it may call emr or it\nmay do\nyou know call any other service or it\nmay\ncall another operator within kubernetes\nnow there are restrictions because that\nnamespace and the container that we're\nrunning should have permissions to do it\nbut that's about it it's like\nessentially as long as you can give\npermissions you should be able to do\nthat\nand there are two examples uh uh to do\nthis\nuh we already have in flight kick one of\nthem is a sensor task\nuh since it has big kumar i think i\nthink the slides might not be\nshowing because they're in full screen\nmode or something like that oh we just\nno we just see the oops it's our initial\nside\nthank you so much oh that's i'm sorry\nabout it i didn't know\nzoom works like that sorry\nuh what can i do better with this then i\nthink it's the share\nfull screen versus sharing window\nmaybe i don't know what this does i\nthink this is better right you can see\nthis now\nit's good enough\ndo you guys see the screen thumbs up we\njust see the title slide or at least i\ndon't\njust see the title slide okay uh and\nit's in the non-presenter mode so it's\nuh it's\nbasically like thank you for bringing it\nup though\nand i'm really sorry i'm trying to see\nmaybe i'll just share this\nhow about now yeah that's a lot better\nthank you\na lot better okay i don't know what\nhappens if i make it a full screen\nso yeah the previous slide when i talked\nabout was just a high level slide\nuh this slider is uh talking about\nwhat flight kit and raw containers can\nessentially execute any code that's up\nthere\nso if you uh\nsorry i've lost my train of thought so\num there are two examples available\nuh that we shipped with in flight kit\nbut that those are mostly examples right\nthe reason why we don't ship with like\ntons and tons of them because first of\nall nobody's question\nand even if people are writing they are\nwriting it within their\nown libraries and and they don't feel\nlike\nsharing them with other people and it's\nnot required to share right it's like\nthese are simple python\nextensions that you can write and if\nthey are essentially in my terms they\nare\ntheir decorations in python that you are\ndoing\nfor simplifying your tasks but we think\nwe thought sensor and notebook are\ngenerally applicable and useful so\nthat's why you shoot them\na sensor task is essentially when it\nruns it can wait for\nsomething to happen in this case it\nand the ones that we've shipped with are\ntwo things wait\nfor a file to land in one of the\ndata stores like s3 or gcas or actual\nfile system\num or it can be\na partition to land in a hive meta store\nuh if any of those things happen then it\nreturns a yes otherwise it just keeps on\npoking\nfrom flights point of view it's running\nup python container so it's just\nwaiting for that container to exit as a\nsuccess or a failure\nthe second example is a notebook task so\nwe\nrecently announced these we actually had\nthem for a while we just recently pushed\nthem out\num these allow you to run paper mail\nnotebooks uh with the slight\nmodification\nso we actually take the entire notebook\nand execute it\nby injecting inputs into it and reading\noutputs from the notebook\nso let's say if you're a data scientist\nand you prefer writing everything in\nnotebooks you can\nwrite everything in notebooks and just\ndrop the notebook\ninto flight you know wrap it with the\nmethod\noutside the notebook and which will\nexecute them\nbut uh again the reason why notebook\ntasks are not backing tasks is because\nfor us we are executing a container and\nit chooses to execute that\nokay uh internally at lyft actually\nthere's a team\nthat runs bigquery etls uh and what they\nhave done is they\nwrote it in in python uh except they\ndidn't they were mostly data engineers\nvery familiar with python did not want\nto extend the backend so they just wrote\nuh\na small etl stuff uh that run\nfrom bigquery and and they did it in\npython didn't even tell us they have not\nshared it\nit works fine for them\nthe mental model that i like to say that\nthis is like airflow\nyou could actually bring all of airflow\noperators to\nflight into the python world and just\nrun them\nas is uh the airflow operators are\nlimited where they don't take inputs and\noutputs but you could use the inputs as\nconfig and aggro\nand take the outputs as the xcom\nappropriate\nso that's how you know the lightweight\nextensions\nand how do you do that uh i don't know\nif everybody's looked at the\nflight kit python and i'm just talking\nabout right click python nelson or clip\nmight give a better\ndetail about java but in flight kit\npython\nthere is a base task called sdk task\nthis is essentially mirroring\nthe flight ideal or protocol concept of\ntask template\nuh it's just some decoration on top of\ntask template\num and uh\nyou can represent any task in sdk tests\nbut when you are extending\nflight kit only to perform new things\nyou should extend something called as\nthe sdk runnable task what the sdk\nrunnable task does is essentially\nallows you to um it automatically\ninjects the container\nthat you're in into the task context\nand it is the task that will get\nexecuted so all the input output\npartially and all that is handled\nin the sdk run about task\nan example for this like both the sensor\ntask and network tasks are run by tasks\nand i have links to them the\nthe highlighted portions are links to\nthe code themselves and so\ngo and check on it ask us questions uh\nif there is anything\nbut this is a good starting point\nand examples of this is the python task\nis\nhere's an example it's an sdk runable\ntask it's just a decorator on top of sdk\ntasks and it executes python function\nit's a misnomer it shouldn't have been\ncalled python\nit's a task it can be any language\ndoesn't really matter\nokay what why would you not choose to do\nthis always it's\nsuper fast really easy to go with you\nknow you can start today and probably\nhave\n10 tasks written today and go out the\nreason would be\nuh and at least this happened to us uh\npotential resource league let's say you\nlaunch\na data flow job and\nit runs fine but you forget\nyour user apart or maybe the container\ncrashes or you lose the machine\nso in any of these scenarios you've lost\nthe reference to the\njob you know unless uh a data file\nunless you have a an important mechanism\nto retry and recover that job it's very\nhard to reclaim\nthe job uh and and so what happened in\nthe past at least if we\nkept on running something and that cost\nus a lot of money\nuh in these cases it's a little hard to\nbasically the cleanup process second is\nthere is\nvery low visibility and debugging\ninformation directly available through\nthe platform you can you do get logs you\ndo get metrics\nbut you don't get specialized\nvisualization in the ui\nright yeah for example if you have if\nyou looked at the start\ntask you get a link in the ui that shows\nyou a link to the spark history server\nor the spark or log specifically or the\nsmart executor logs\nyou don't get all that when you're using\nuh python\nor extending only python tasks um\nand there is recoverability from crashes\nso that means partial state recovery so\nlet's say assume you launched a job\nuh but the container we lost the\ncontainer and we\nrecovered the container somewhere else\nuh flight would do that if you have a\nretry set\nand let's say you infinitely trace it so\nit will just keep on recovering but\nyou won't be able to just know which of\nyour launch\nunless you build that logic into the\nsystem\nuh into your task and that's quite a bit\nof work\nbut and it's not i've seen most\ncommonly that many users do not think\nabout failure scenarios\nand that's where it becomes right\nalso like let's say you want to share\nthis task to a with everybody\nyou could do you could write it in 5k\nand then other people will get it\nbut uh we have to write it to like it\nsecondly you won't have some common\nconfiguration\nuh let's take another example uh you\nhave spark\nand you want to always inject a\nmetastore configuration\nfor hype so that you can know what\ntables you have\nuh from i even use that it would be very\nhard\nto uh do that in a common platform\nspecific\nway for the entire for every user\nand the entire community itself um\nand then eventually resource pooling is\nalso very hard\nyou can only provide resource pooling\nwithin flight\nat a task type level so for example you\nsay i want only 10\n000 uh python tasks to run that's it\nbut you can't say now 10 000 by contrast\nand within that you know only\nyour specific task type to be limited\nthat's not easy\nwe actually can support it and we will\nbe working on it next year but yeah at\nthe moment\nall right so then what is the other\nalternative the other alternative is to\nextend flight packet uh and\ni always recommend starting with\nflightgate lightweight extensions\nbecause that's the best way to vet your\nidea\nmake sure that what you're thinking is\nright and\nthe interface is clean and correct um\nand then once you feel a little\nconfident maybe you have spent like a\nweek or something\ncome and extend the back end but\nin some cases you have to extend the\nbracket so we will talk about\nthose cases so the flight pack-end is\nextensible it's essentially what you're\ndoing is you're adding\nbits into the flight propeller\nwhich is the flat engine that runs\nflight\ntasks you're adding bits to it and\nyou can extend it we have created an\ninterface that allows you to extend it\nthe benefits of that are that you can\nactually teach flight to do\nanything create a new execution cluster\nand execute it\nuh call a service or run a container\nand you could just run a container in\nany container environment\none of the questions that i think you\nasked me recently is that\nwhy does the base container look\ndifferent from a part\nand the reason is because container is\nmore portable than a part\nwhat is a very kubernetes specific\nconcept where the container is you know\nyou can write it on mesos or\nand we have seen cases in which\nkubernetes doesn't scale enough\nand we find it uh better to actually\nuh schedule the container on to some\nother places\nand these are these have to be\nsimplistic containers because you cannot\nactually\nhave a volume and other things attached\nto it\nand that makes it heavy weight that\nmakes a part heavy\nwhen you write a backend plugin you do\nget automatic ui visualizations you get\nresource cooling\nuh it it's automatically available to\nall different languages\nin that like it is written in uh\nand and you can provide default\nconfigurations like recently we added\nblanket tolerations you can do that for\nbackhand task\nand then i talked about the ministry\nexamples of backhand tasks are spark\nit actually dynamically brings up a\ncluster and test it down\nit you know injects some common\nconfiguration automatically\nit monitors and makes sure that the task\nworks\nuh sagemaker allows you to launch\ntraining\nand inference jobs directly into aws\nstage maker\nuh so you could do that for cloudember\nif you want\npresto and hive we use this exclusive\na lot of lift uh so we can run directly\na query\nin cluster and return data frame\nand we do resource pooling on pressure\nand height\nand then array jobs we'll get into our\narray jobs a little bit more but uh\narray jobs that we run our nato dispatch\nactually at least you can also run them\non kubernetes and we are improving that\nsupport\nevery day but we run about\narray jobs of in the order of five to\nten thousand\ncontainers per job uh and\nfor that we have seen kubernetes doesn't\nscale as much so we are making\nkubernetes scale\nevery day all right so what's the\nanatomy\nof building a back-end touchscreen step\none is you actually define\nyour specification for the task and you\ncan do that in any language we\nwe choose photo buff for most of our\nexamples because that's the thing that\nwe are most comfortable with it's easy\nwe have tooling for it so you guys can\nchoose protobuf but if you don't\nwant to do that you can use open api\nspec you can use your strings\ntribal knowledge either ways right uh\nany of that is fine\nuh you then you go ahead and implement\nthe backend collin extension\nuh which is built on top of plugin\nmachinery\nand we can walk through plug-in\nmachinery but it's better to\nuh look at that it's available in the\nflight plug-in three-po\nyou will eventually move into flight\nidea we're keeping it there because we\nfind any issues and things we fix it\nimmediately we have not\nfound recently so as it gets more stable\nuh and then step three is actually\ndefining\nthe the thing that the users interact\nwith that's it's like kit\njava or python uh\nthis the reason why we just don't really\nwrite protobufs is because this is where\nthe user delight comes in right\nuser wants to interface with the system\nin a much nicer way\nlike by writing their python code and\nputting a small spark task\ndecorator on it is much nicer than like\nthinking about entire spark\nand doing clusters and so on so\ncome up with meaningful differentiation\nwithin\nthe interface and you can build really\npretty interfaces\nuh for for in fact an actual example\nimplementation\nis uh what i put here is i think pi\ntorch operator\nit gives you the entire gamut of all the\npieces that you have to implement\nand how did this user went about\nimplementing uh the backend plugin the\nspecification\nthe fighttic implementation the example\nand the docs so it's all of it\nand there is a if you want to really try\nit quickly there is a prototype repo\ncalled flight plugin example\nit's a standalone uh flight propeller\nback-end\nlike it everything together so you can\njust\nclone it and hack it and start getting\nrunning very quickly\nokay uh so the thing that makes this\nwork is plug-in machinery\nit's very simple interface it consists\nof three methods\num handle abort and finalize\nevery time a plugin runs we actually\ncall the handle method\nhandle method is called n number of\ntimes till\nhandle returns success uh once it\nreturns success\nwe can call a fine merge if some event\nhappens like another node dies or\nuh an externally a user clicks apart\nthen we call the abort method and after\nthe abort we also call the finalize\nmethod so\nthis is the core plug-in interface every\ntype of plug-in interface is defined on\ntop of this\num what we did is we also provide a very\nsimplified state handling system in the\nplugin interface so for example\nuh let's say you call some remote\nservice and you want to store\nthe id and a couple other information\nbits of information\nyou just write it as a co-struct and\nreturn it back\nto flight propeller fighter will\nautomatically store it\nnext time when it calls handle it will\nreturn that value so that you can\nreuse it um if you know anything about\nkubernetes operators they are not\nuh exactly once they are at least one so\nthey'll keep on calling the handle look\nmultiple times\nin flight propeller we have optimized\nthat we try to reduce the total number\nof calls\nand we kind of guarantee in many cases\nthat we only call you once\nif you want that and this prevents like\nlet's say you have called the remote\nservice and you want to\nquery you don't want to be called again\nand launch the query so you don't want\nto do all of that\ncrazy handling within yourself so we\nhave to\nuh but we still realize that there is a\nmuch nicer\nuh interface that we can create for\nkubernetes uh so we created a simpler\ninterface for kubernetes\nyou just implement even you know one\nmethod how to construct\nthe object and how to analyze the status\nand the third one is uh it's coming soon\nif you want to call any web service how\ndo you simply\ndo it okay if you want examples of\nwhere you can write one all of these are\nones that people have talked about asked\nabout\nso please help us right i will help you\nin any of these\nuh also a desk operator ray operator\nemr bigquery dataflow and they are\navailable as kubernetes operators so it\nshould be really straightforward to add\nall right like uh let's do a time check\nit's 30 minutes um\nso we get quickly i think it will take\nfive to ten minutes more\nto talk about dynamic tasks and then we\nwill open up for questions and other\nthings okay then we're still on\nwe're on a 40-minute list oh really\nremember\noh okay uh all right so then i can\nfinish in five minutes\nso dynamic tasks is a misnomer\nor it's not completely understood um\nthe dynamic tests allow you to alter the\nshape\nof a flight workflow right you can just\ndynamically generate a workflow or you\ncan dynamically generate generate a set\nof tasks that you want to run\nor you can dynamically launch other\nother workflows and\nit's simply constructed we are actually\nworking on improving\nthe construction itself like in fact it\nlike the usual delighting experience i\ndon't think we have the history like\nexperience here you're trying to work\nand improve on that this might be a\nlittle light\nbut what this is is essentially you\nwrite a python function and you put a\ndynamic task on it\nand you yield the tasks that are to be\ndone\nand the rotate task itself could be any\ntask pythagoras\nwe don't care but what essentially is\nhappening in the backend\nis uh we'll talk about it actually after\ngoing through all the examples\num the second case is selectively\nlaunching some launch plans\nuh it's the same dynamic task uh\nyou can it's exactly similar\nconstruction\nbut what in this case i've done is i've\ncreated a launch plan\nuh and yielded the launch plan uh and\nthis launch man was dynamically created\nthis launch panel was created\naesthetically and it was done so this\nallows you to selectively launch\nany launch plan and so you can put\ncomplex uh\nconditionals to decide what you want to\nexecute\nand this makes it really straightforward\nto execute\nthe third one is actually generating a\ndynamic workflow\nso workflows in flight as you know you\nhave to write them using the dsl and\nthey automatically get registered\nbut if there are some cases in which you\ndon't know the structure and you want to\nchange the structure or add new nodes or\nso on dynamic tasks\nallow you to do that and this is an\nexample of actually creating\na new workflow and\nall of these examples by the way don't\nworry they are in flight cookbook\nso if like snacks cookbooks cookbook has\nall of these examples\nplease go through them but uh\nthis allows you to generate a dynamic\nworkflow\nokay so i won't get more into detail\nabout the various use cases\nwhy you want to do them because what we\nhave learned is users have\namazing sets of use cases and i i can't\nreally document all those use cases but\ni'll tell you how it works so\nflight propeller when it's executing a\nnode and a node is a\nis a super concept of a task right\nit encapsulates all various execution\nretries and so on\nso when it's executing a node it\nlooks for one of three things it looks\nfor\na success uh with the results of that\nnote so when the\ntask execution completes it returns\nresults and it looks right\nor it looks for failure and the failure\nthat's how we capture the failure\ninformation in the ui so you get the\nlocked race in the ui\nthat's because we look for a failure or\nthe third option is\nwe look for a success but not results or\nan output but we look for a thing called\nas a future file\nuh and i should have put a link to the\nfuture file specification\nin flight idea but i will do that after\nthat so that allows you to\nessentially tell flight propeller by the\nway i had not done i\nthink i processed whatever i had to but\ndo this now\nand what flight propeller does is takes\nthat\nthat uh future specification\nand it it follows a format it's\nessentially a workflow specification\nuh and flight propeller dynamically\ncompiles that version\nif the compilation is successful then it\nstarts executing it like a\nnormal nested sub if the compilation is\nnot successful\nthen you get an error at runtime and\nthis is the caveat of using dynamic\ntests\nyou get errors much later in the game\nyou will get errors at runtime\nbecause we don't know what type of\nconstruction\nbut it's really powerful right with\ngreat power comes\ngreat responsibility so uh\nyou we have not advertised this feature\na lot but we will be arriving more and\nmore of this because we think we are\nabsolutely stable with this uh almost\n90 percent of workflows at lyft have\nsome sort of dynamic tasks in them\nuh which makes us look extremely\nconfident that this thing works its\nchange beautifully\nso if you have use cases let us know if\nyou're using it\num the interface is a little\nuh i still feel it's not perfect and we\nare trying to improve it\nuh and and that's where flight kit\nenhancement that he talked about few\nweeks ago\nis coming out soon at the proposal and\neverything he even has a prototype for\nit we'll be sharing that\nbut uh a quick overview of dynamic tests\nagain\nin our recap you use dynamic tasks when\nyou don't know\nthe structure of your workflow that\nmeans you can't define it ahead of time\ntoday you use dynamic tasks also if you\nwant to launch a large array job\nthat means you want to launch 10 000\ncopies of\ntask or and you want data partitioning\nbetween them\nand you use dynamic tasks if you want to\ndynamically generate a workflow\nso these are the three options right and\nand any\ntask can be a dynamic task and any tasks\ncan be the parent\ndynamic task that's my presentation\nany questions that's a lot of content\nit's it's all the slide deck is\nin there but you know\nhow does it work if dynamic task creates\na workflow\nthis workflow has a new input a new\ninput what do you mean\ni can you like something that doesn't\ndidn't exist before\nyeah yeah so it will fail compilation\nso the interface of the newly generated\nworkflow has to match the encapsulating\ntasks interface\nuh and that's like the enforcement that\nhappens\nso we compiled the dynamic task parent\nright the node that actually generated\nit with its interface\nand now whatever it generates has to\nabide by that rule\nand so if you generate something that\ngenerates one more\ninput and say oops that doesn't work\ni have a question um so we've used\ndynamic tasks\nuh prenome and you know we've used it\nmostly for\nuh running uh multiple sort of python\ntasks\nuh one thing that we really like is sort\nof the modularity of flight\nbeing able to even compile uh sorry uh\ncompose\nuh sub workflows into into larger\nworkflows or super workflows\num unfortunately i've noticed that sub\nworkflows don't play very well with\ndynamic tasks so you can't like\nyou know do a for loop over um for loop\nand launch multiple\nworkflows from multiple workflows of the\nsame kind using a dynamic task\num it fails with a with a node id error\nor something related\num is there is there is that something\nthat you guys are aware of and like uh\nis there a plan to support you know\nbeing able to launch multiple\nuh instances of a given workflow using\ndynamic tasks moving forward\nhey absolutely but i think he was\nnodding in there to let him answer this\nquestion\nbecause he's been digging oh no i think\nyou can you can answer but\nyeah go ahead no no this i i don't know\nif this problem exists i have actually\nuh i i think this will go away i'm\npretty sure this will go away with\nanand's\nnode to node relationship change oh the\nreason is because\nwhen you run the same workflow multiple\ntimes\nthe string id the node id which is\nsupposed to be unique\nends up being not unique and then that\nbecomes a problem\num but with the the node to node\nrelationship we're changing how we\ncompose or construct\nthis actual string yeah it's it's pretty\nclose to\ngetting done um i think we talked about\nit in the last meeting\nbut yeah alan's not here today he's\nworking on it\ni'm afraid our meeting is going to be\nending for us\nlike it or not um can we just restart\nanother meeting and just open up for\nquestions\nanybody who wants to check yeah i'm\nhappy to just rejoin right\nthis meeting can we do that i don't know\ni haven't tried it but i assume there's\nno reason why it shouldn't work\nyeah so all right i\nthen will preemptively end the meeting\nright now and then rejoin\nthe same link if um\nif you want to follow up with us okay so\ni'm\ntemporarily saying goodbye i hope to see\nyou all in about a minute"
    },
    {
        "title": "FlyteMeet 2020/08/11 - part 2 Flyte Extensions",
        "transcript": "looks like we're back\nwhere did caitlyn go\nhe's back\nyeah sorry i joined the wrong meeting i\ncame back here\ni have a few zoom links with me uh\nyeah any uh any more questions and we\nwere actually continuing the discussion\nso what you were saying jeev is that uh\nwe there\nthis is a known problem uh that we\nencountered recently and so we're fixing\nthat\nuh it's most of the work has been\ndone and there is one change that's\nyet to go out but we are trying to wrap\nit up in this\nmonth this month release so 0.7\nawesome sounds good thank you so much so\nafter that you should be able to and\nthen let us know if you find any bugs\nbut\nyou should be able to\num any other questions i think leora had\na question\n[Music]\ni i uh man\nthanks for picking on me i uh\ni love your version last question i was\na little curious about\nwhen uh in the example you showed for\nthe\nfor the dynamic tasks being yielded\nyeah and then appended to results was\nthat going to run them all seriously\nno it's all run them in parallel\nbut see the problem is because of the\npython interface it makes you feel that\nit's running right there and then it's\ngonna come the yield\nright you can come back and resume at\nthat point that's not what happens\nthis is the problem in the current\ninterface we think right it's\nan interface accordingly should be\nself-document it's not in this case what\nhappens when we yield we run\nwe we just collect the yields then this\ntask completes\nthen we run all the yielded plan right\nit's essentially what you're doing is\ngiving us a plan\ni want to run this this is ten thousand\ntimes it's five thousand\nbut not 10 000. we have a limitation of\n5000 in total on the candidates\nso uh we'll take that plan and we'll\nexecute it\nuh and it can be as many as you want and\nthey run in parallel but they are not as\nlightweight like they are running\ncontainers\nso they are not like running one\nmulti-processing like\nthat's the other confusion that some\nusers have\nnot as lightweight as using a back-end\nextension\nthey are not as lightweight as running a\npython thread\nokay next yeah or a process multi\nprocess right in python\nbecause it's actually spawning a new\ncontainer\nanybody else i had one good question on\nsensor tasks\num so i wanted to make sure that i was\nunderstanding this correctly if there is\none task that depends on\na particular file from a previous task\nbeing in\na google storage bucket is that like an\napplication of\nsensor tests or is that not something\nthat you use them for\nif one task is depending on a file from\na previous task within the same workflow\nthat's all automatically done this is\nfor cases in which\nuh you run another workflow which is\neven old actually by a different team\nmaybe\nand that's uh writing a file somewhere\nand you want to\nuse it okay yeah\nyeah and actually i i personally am not\na big fan of sensor uh\nwhat i think this needs to be done is\nusing some concept like what we've been\nsaying it's called reactive workforce\nbut\nwe don't have that today ethan has been\ndrug\nkicking and screaming into doing sensor\ntasks yeah like two years it was not\nthere are i mean when it's mainly useful\nfor things when you own when you don't\ncontrol\npart of the data flow of the application\nwhat's completely outside of the control\nof your\nthing and you have to you know you have\nto wait for an email to show up or\nsomething like that\num the problem obviously is it makes it\nrepeatable repeatability is sort of gone\nand deterministic outcome of that\nworkflow is gone so it's sort of a\nit's a dangerous gun and people shoot\nthemselves with it all the time but\nthere are legitimate tasks where it's\nthe only thing that you can do so we did\nadd it\nbut it was not after without a fight\nyeah\nthanks for the presentation um i'm one\nof the engineers of freedom it's first\ntime joining and\ni'm glad i did yeah awesome\nto have more freedom representation here\nthat's awesome\nyeah we need to get more of the other\npeople showing here i don't know why\nlike 828 is not showing up here today\nright a resource safety you mean like a\nclean-up clean like finally yeah finally\nbut\nalso workflows they have a finalizer or\nsome kind of finalizer\nbut there is a very good question can i\ncan i simulate it with the sub workflow\nyou can\nyou can if but you have to be careful\nlike what you know you'll have to\nbasically\nknow what you launched i actually don't\nknow if you would know what you launched\nlike if you do it deterministically yes\nyou can\nthat's that's a short answer but it's a\nlittle tricky\nbecause that's kind of what i wanted to\ndo yeah\nyeah yeah is it for data flow or what\nkind of jobs\nyeah to make up some workflows that\nwould launch it and this work will have\na\njob id as an example\njob identifier as an input yeah oh that\nyeah that if you do it that way that's\nthe right way to do it like create the\njob id ahead of time\nand then pass it to your finalizer\nbecause then you know you have to repeat\nit yeah that should work\nand then if i retry would finalize a\nretro as well\nwould finalizer retry yes uh there is\nthere is no workflow level we try right\nso what would happen if the node would\nretry three times\nand then once it failed it will go to\nthe final because like a try catch\nsemantic\nwithin the try block you are doing the\nretries and\nlet's say if you retry equal to three\nthen you get a failure then you say\naccept and then that actually just goes\nto the timeline\nokay thank you yeah in python we don't\nactually have the workflow finalizer\nimplemented i guess you in java did you\nimplement it already\nno but we can't because yes this is a\nnice\nfeature to have because then we can\nimplement finalizers without writing a\nback-end again\nyeah yep that's absolutely the case\nthat's awesome\num yeah but if if so\ncorrect me if i'm wrong but is there a\nkubernetes operator available for data\nflow and\nbigquery if we use uh\nit's called cloud config connector or\nkubernetes\nconfig connector i think there is a new\nname for it right now\nbut it's basically a set of operators\nthat google implemented\nwe need the kubernetes cluster like gt\ncluster to use it\nand there are many operators out of the\nbox including\ndata fill job and bigquery query um big\nvery clearly i'm not sure\nwhat data for job for sure exists and\nalso\nsome other kinds of if for some reason\nyou want to create a gcs bucket\nor other things you can do as well yeah\ni see aws also going in that direction\nso yeah that makes it very easy to add\nthose plugins\ncool um anybody else\nall right all right\nthank you everybody thanks a lot\neverybody and uh we'll see you in a\ncouple\nall right thanks is this meeting every\ntwo weeks is that it it's every two\nweeks\nokay yeah you should have gotten an\ninvite to your\nuh on your calendar as unless\noh gee you might have you might i did\nget this\nthis event i i can check to see if i\nhave the next should be i tried to make\nit a recurring event that had an\ninvitation everybody was in users so it\nshould appear on your calendar every two\nweeks from now\nyes it does clearly i am i've been\nchallenged with the permissions of\ngoogle groups so\num if it's not working as you expected\nfeel free to like let me know and i'll\nfix it\nsounds good thank you so much george\nappreciate it thanks all thanks\neverybody\nall right happy tuesday see ya\nyou"
    },
    {
        "title": "FlyteMeet 2020/12/01 - Choreographer, Java/Scala SDK, Version 10 update",
        "transcript": "i'll continue to drive\nuh jeeve do you want to go first uh sure\ni i don't really have any slides or a\npresentation i just have like\ni just have some code to share and just\nkind of want to talk about like our use\ncase and how we can\nyou know think about integration into\nflight so i'll share my screen\nreal quick here\nall right can you guys see this so so\nessentially\num we have a we have a particular use\ncase where um\nyou know we might have external sort of\nresearch collaborators or\nyou know uh just data providers\nessentially updating or uploading data\ninto our buckets\nthat we make available to them and then\nwhen these\nwhen this data lands we want to kick off\ningest pipelines that then you know like\ndoes some transformation\nuh and then loads this data into our\ninternal databases which is then\navailable to research\nso that is like one of the that is like\none of the use cases of\na event-based workflow trigger\nand another use case is also that uh in\nour production pipeline where\nwe have uh lab instruments so we're you\nknow we are uh\nwe are a biotech company and we've got a\nwet lab and\nand what i call a dry lab component to\nour company and dry lab being like\nbioinformatics and software engineering\nand all that\nand so when when data\nyou know when samples patient samples\npass through the wet lab\nyou know they get they go into these\ndifferent machines and these different\nmachines generate data\nand and all this data which is\nessentially in the form in the form of\nfiles gets uploaded into\nuh gcs and and that is what we use to\ntrigger off like our production pipeline\nbasically\nand so you know here's here's two\ndifferent cases that are\npretty relevant i think to freenom and\npotentially to other biotech companies\nas well where uh\nyou have you know external systems\nbasically dumping data into the cloud\nand then\nyou know triggering off like some sort\nof a transformation of this data right\nso so one thing that uh so so this is so\nthis is something that was like\nkind of uh inspired by argo events\nif you guys have heard of it and it's\njust like you know when when an\nevent comes in from um\nfrom gcs right so you can so in gcs or\ns3\nyou can do this thing where you uh\nlisten to object notifications\nessentially and so in gcs we're kind of\nlike doing the same and and we get\nobject notifications via\ngoogle pub sub um and so like you know\nit doesn't matter what the what the pipe\nis that feeds you these events but like\nyou know we have\nuh i just built this like very quick\nprototype of\nuh you know a router that can that can\nrespond to these events essentially so\nif you got an event that basically had\nthe object path in it you can define a\nrouter\num where uh you specify that you know\ncertain conditions like so in this case\ni've specified you know ham equals to\nthe fact that this object\nuh this particular object exists um for\nboth\nfor two different objects and when any\none of when\nwhen a path comes in that matches any\none of these patterns it extracts these\nattributes\nand then maps them into like both of\nthese right both of these different\npatterns and then checks to see if those\nfiles exist\nand those files exist it then runs this\ncallback\nand this callback you know can do a\nbunch of different things right it's\nbasically python so you can use it to\ntrigger off like flight workflows you\ncan\nyou can use it to like you know send\nmessages you can use it to do whatever\nand so um so that's kind of like what\nthis in essence is and i can uh\nfor the sake of like a live demo here um\ni don't have much and so like i've kind\nof mocked out\nmy object store and so my object store\nis essentially the set over here\nand my object exists is basically the\nfact that this path exists in the\nin this uh in this object store which is\npretty lame i admit but like you know\njust for the sake of uh\nfor for demo and so like if i if i go\nahead and i run this\num you can see that like uh when sorry\nwhen this event when this event drops uh\nit checks to see that this\nfile matches one of these patterns right\nso in this case it matches this ham file\ntext\nand then it extracts this category and\nuid and it applies it to this thing\nand that and it checks to see if like\nthose files exist and so that's\nbasically\nwhat this thing is doing here is object\nso these two files exist in the object\nstore and so\num the callback fires i can i can also\num\ncomment this out and so like that file\ndoesn't exist in the object store and if\ni run it again\nbasically nothing happens and so that's\nthat's kind of like the idea here\nso uh one when one of these file drops\nnothing happens when when this other\nfile drops and when both of these files\ndo exist now\nthis callback fires and so and so this\nis kind of like a very arbitrary\nsuper early stage prototype and so um\nwhat we're what we're basically thinking\nis that you know we could potentially\nuse it as just like a library right so\nwe can make this router\ngive it the ability to you know listen\nto like pop sub or s3 or whatever\nand then you can just like you know just\ndo something like a router start or\nroute listen and then it'll just respond\nto events using callbacks and you can\nand you can just define these callbacks\nfairly easily using python decorators\nso that would be one way to kind of do\nthis i think a really nice way to do it\nwould be to kind of define them as\nkubernetes objects like crds and have an\noperator that can like listen\nthat can basically like register these\nuh these triggers and listen to them\nand and respond to them as these events\ncome in and pumps up and so that's\nthat's basically what argo\nevents does right so like argo events\nlike all these registrations are\nbasically like crds\nand so i think that'd be a really nice\nthing to have uh for flight\num and and kaden mentioned like when i\nwhen i brought this idea up kate\nmentioned that\nwhy not just use argo events i think\nargo events for our purpose was really\nheavyweight it's got like a lot of stuff\nit requires like you know\na ton of basically the entire argo\necosystem to be up and running\nto be able to use argo events and i and\ni didn't want to\nuh go down that path of like setting\nthat up when we were using flight and\nyou know we just wanted something simple\nthat could like respond to\nevents coming from from cs\nand we didn't need like you know all\nthese other webhook sensors and like all\nthe other stuff\nbut you know it could be it could be\nreally nice if this was a this was\nsomething that we can you know add to\nflight\num and then it would make sense like\nwe're in the same ecosystem it makes\nsense to just use something that that\nworks really nicely and seamlessly with\nflight\num so that so that's basically what i\nhave i'm i'm happy to like share i can\nshare this code on flight general if\nanyone wants to kind of like take a look\nat it and see how i was messing around\nwith it\nno please do i think uh we've we've\ninternally also talked about this for a\nwhile\nlike um i think i have i personally have\nsaid that i do not like the sensors that\nyou know\nfor example airflow has we're running it\non a schedule\nand waiting for a file to arrive uh this\nis essentially a reaction\nright and the reaction is a much more\nnatural uh way of solving a problem\nuh like for where things are arriving\nsynchronously\nso we have uh we have\ntalked about this we have a few ideas um\nwe\nwe did see our event so i didn't know\nthe overlap like how much\ndoes it like what are the problems with\nit or so on so that's one of the reasons\nwhy we didn't\num solve it yet we might you know if\nargo events exist\nwhy not use it with them\n[Music]\nif it solves the problem if then we\nshould innovate and you know probably\nsolve the problem in a much more\nscalable and better way but\nuh i think this is definitely a problem\nthat\ni am interested in i would like to see\nif the\nrest of the folks are at some point i'll\nstart writing something down and\nwhatever you have achieved i would love\nto collaborate and\nsee if we can you know come up with uh\nthe right set of requirements uh\nthe base set of requirements for such a\nthing yeah\ni am definitely happy to share the uh uh\nthe requirements that we have as a bisex\ncompany but i'm sure like others\num yeah yeah yeah i think there are a\nlot of these requirements within\nuh lif2 especially with and\nand some of them are with data right and\nsome of them are with machine learning\nand they come they're kind of different\nin times uh but one of the requirements\nthat i've heard in the past is\nthat we want to fire a workflow when\nanother workflow finishes\nright yeah and so those are like you\nknow then the eventing system needs to\nbe really\nnicely decoupled and yet scalable\nit needs to scale to the same level as\nflight right and just having one\noperator as\nwe've seen is not enough at least for us\nso\ni would love to get those ideas and you\nknow we can work together on this\nyeah that makes sense um would love to\nin fact like i think the thing that you\nmentioned when we have one workflow\ntrigger another workflow is also one of\nour use\nyeah wow cases perfect yeah and i think\nsometimes it's hype partitions or\nwhatever right you know bigquery tables\ngetting updated i'm sure that's a\nthat's a requirement for spotify or at\nleast for us\nit is so yeah cool yeah so that's that's\nkind of all i have it's very\nunderwhelming but um i'm happy to like\ndiscuss more uh offline on slack\nhey dude nothing's underwhelming like\nit's like i didn't start\nlike really small and that's how they\nneed to you know germinate you need to\nwater them and they'll grow so this is\nawesome\nwe actually have something so i i wanted\nto that's why i have this and we can\nstart a\ncollaborative talk on this perfect yeah\ndo you guys have any uh dogs that you're\nwilling to share on this like\nkind of early thoughts or whatever i'd\nlove to kind of read and think about\nyeah\nwe'll talk about it internally and like\nit probably was very embedded in lyft\nwe are trying to get it out let's see\nmaybe i'll rewrite some\nparts of it\nyes\ncan you see my screen\nyep okay\nuh yeah i'm one of the developers for\nflight kit in java and scala and i will\npresent\nour recent work mainly i would present\nabout testing support but they will\nalso touch features that we have added\nrecently\ni will do like a quick refresher on java\nsdk\nitself and i will provide code examples\nin scala but you can do exactly the same\nin java\nif you like java so this is like a\nsimple workflow that\ncomputes fibonacci numbers it uses a\ntask that does\nsome and by composition it computes\ncomputes the fibonacci numbers\nwhere in fibonacci sequence a number is\nequal to the sum of previous two numbers\nso it'd be one one two et cetera\nthis is the most simple workflow i guess\nyou can write uh if you want\none don't want to do any side effects\nand we often use it as an examples\nand all the codes that i will present\ntoday you can find in\nopen source repository uh so\nif you want to declare a task you would\nnormally\nif you scour your declare case classes\nuse your inputs and outputs and you\nwrite the functions that takes an input\nand produces an output\nuh what we added recently is support for\nlaunchpads plans\nfor that you basically add the cost you\nextend\na base class and we have a special\nfunction if you want to register default\nlaunch points\nso default launch points and files they\njust have the name same name as the\nworkflow\nand they don't have any schedule but if\nyou want you can also add\nuh more launchpad so let's say in this\ncase\nwe will scale our fibonacci workflow to\nbe executed daily\nif you want to provide inputs so we want\nto always start with\nfibonacci sequences and then one and you\ncan also specify crown schedule\nso if you use would say aws you would\nuse cloud\nscheduler and it would trigger workforce\nin fight\nand this is truly decorative so the\nmoment you\nuh cause you call this function\nregistration doesn't actually happen\nit will happen during the time when you\nwith your coi and co register\nuh we did not present this feature but\nuh on the last meeting we have seen how\nyou can do fast registration in\npower is in sdk uh you can do exactly\nthe same\nin scala or java sdk it's actually the\nmost\ndeveloped method so it was the first\nmethod that existed\nthe way it works it will copy jars to\ncloud storage let's say s3 or gcs\nand it does rsync so it doesn't copy\njars if they're already copied so it\nwill basically append checksum\ninto the end of the file name and if\nfile already exists and check some\nsmudge it wouldn't upload it again so\nwhen i\ntried it on my machine it took me one\nsecond\nto do incremental compilation for my\nscour code\nit was a small project and then it took\nme\nfour seconds to do fast registration\nit was a pretty good devotion cycle\nuh something else we\nput our attention to this\nif you want to get started with it you\ndon't have to have a back end\nor you don't have to have a file service\nyou can just start using sdk\nand you can trigger workflows locally in\nthis case you don't have access to\noperators or you cannot use\npython tasks but if you workflow is\nself-contained\nand everything is simply manage it in\njava you can do that\nit's also pretty funny mainly\n[Music]\nuh\nsay you have a workforce does some\npolitical workload\nto compute the answer to life and\nuniverse and everything else\nuh so there is no way how local\nexecution agent\ncan run this task because it can use\nlike some operator or the code can be\nimplemented in gold\nuh to fix this we have a way how you can\nprovide\nbasically last time implementations for\nthis task\nso in this case we have a uh you can\nbasically mock\nyou can use a mock specific uh input\noutput or you can mock the whole\npermutation and then you can run it\nlocally\nand then you can test your workflow\nexists\nuh we also did a few other improvements\nif you use java\nyou would like it a lot because we\nbasically supported jackson annotations\nso you can rename you can map between\ndifferent types etc\nso it gives the same level of support we\nhave in scala right now with\ncase classes and implicits\nwell as we supported more uh five\ndigital features\nso for instance we have support for\ntaskbar retries\nyou can define it and to\nget better support for testing but as\nwell as a nice side effect now we have a\nsome subset of built in type checker\nso the reason for that\nthe moment type not much let's say you\nrefer to something doesn't exist\nor you're used to wrong type or you have\na cycle somehow\nit would throw an exception there is a\nstack based point into the\nexact line where the problem happened\nand it significantly improves\ndeveloper experience because you\nbasically can see\nthe moment of your code where things\nwent wrong\nand you don't have to search search the\nworks and then try to connect it to your\ncode and actually understand\nwhen something don't happen\nso that's it\nregularly\nor maybe just point people to the code\npath i guess\nyeah you can find all the from this\nprediction\non github yeah\nso in the notes uh grab if you can add\nyour slides and uh\nto the i'll add a link to flight to java\nso that way\nyou know people can go and start trying\nto produce it\nuh i i wanted to add side note it works\non um\non this as well um and so\nwe are we've tested it like i think i'll\ndo a round of test again\nand then we can say that if there are\npeople that have to want to use\nalso i i know there were some teams who\nwanted to use so\nyeah gleb is like you know amazing it's\njust\ncompletely build this like one day\nsome suddenly came as like i wrote this\nentire sdk scholar i'm like what the\nheck\nand it's been easy to see the progress\nand they run it in production\nright yes so this workflow\nproduces very important numbers\nso yeah it's reliable you can use it in\nproduction\nuh if you find anything it's likely\ngoing to be that some feature is missing\nit's not a bug an existing feature\nif you also don't use fight in\nproduction if you just have some trainer\norchestration engine you can\nget your easy way to fly just by using\nit in local mode\nand then get the full power of the whole\nsystem\ni think the local mode is very critical\ni think in python\nfor some reason that was missing and so\nwith the new sdk version right that's\ngoing to be that is the focus and i\nthink this\nprobably brings more parity to the java\nand the flight sdk\nuh and they kind of look similar uh\nlike they're testing this marking and so\nthat's\nthat's amazing that actually shows that\nthis probably isn't it's the right way\nto move ahead\nso that's awesome um\n[Music]\nanybody had questions uh\nglebe i had a question how many all the\ntypes are all the types included\nare implemented rather or only a subset\nright now\nso we have basically two layers of\nsupport\nthe first one is like sdk has nice\nfeatures to make it\ncomfortable to use something let's say\nit has a built-in support to open files\nlike if you read blobs etc so it doesn't\nexist\nfor blobs and for schemas but there is\nlike a second layer when you\nbasically have like lower level api\nand this layer exists so it might be not\nthe best user experience if you want to\nsupport\nthis but also it's pretty\nstraightforward to add we just don't\nhave a\nuse case for it yet if you don't want to\nadd something that\nis not used but if you have a use case\nfor it\nlet me know yeah\nuh yeah in the python so like\nwe should demo that thing again because\nit's progressed more but not\nnow maybe in the next one um\none of the things that we made and i\nthink i talked about this last time is\nwe\nrealized that we want to add new types\nin the user space\nthat map to the underlying type system\nin flight\nand i think that's because for example\nschema itself could be represented using\nthanos data frame or spark data frames\nor\nor you know some other data frames um as\nwell as uh\nusers usually and it's like in some\nother fields there are\nway more complicated types that we\nprobably don't even know but they\nusually map into like some binary\nprotocol or some\nsome files so that's one of the reasons\nwhy we added like\nan internal registration thing that just\nregisters a type\nwhich basically allows you to convert\nany type to an internal representation\nand it's just essentially a decorator\nso so you can write it as much or you\ncan use the base underlying\ncore concepts um to directly access and\nso i think\nit's similar to what you have done so\nthat's perfect\num anything\nelse any other questions anybody has\nuh yeah one question can you hear me my\nconnection was a little bit flaggy\num you can hear it so what i wanted to\nask\nokay great um about the scala versions\nbecause of you know binary compatibility\num so is it can i write\nworkflows and tasks and for which column\nversions are supported and\nhow how does it work when i when i\nswitch between like\n12 and 13 or in the future it is scalar\nthree\nso it's a great question so in general\ni will start with a more generic problem\nso in java there is a problem with the\ncharcoal that you cannot have to\nversions of the same classes on a class\nclass\nactually you can formulate it\ndifferently you cannot have it on the\nsame class folder so in java there is\nactually a concept of a class order\nthat is a unit of isolation\nso what we do we actually have multiple\nclass holders per\ndifferent task so in general and our sdk\ndoesn't have any dependencies\nat all so even like a jackson one it's\nlike an optional module you can pull in\nor you don't\nso we didn't even use guava for instance\nbecause he didn't want to have any\ndependency and guava is like super\npurple or java library everything\neveryone uses as a drawback if using\nguava because there are\nsometimes breaking changes and guavas\nand then everything just breaks\nso in our case uh basically each task\ncan\nuh have own class pass\nso let's say you use\nsomething like a spark or some like\napache beam\nor something else and you can basically\nhave different tasks that use different\nframeworks and let's say you also use\nspark and spark has could say scout 212\ndependency and then you want to have\nscour 213 or scower free let's say\nit's called three is coming in some\nother tasks you can actually mix it you\njust need to\nbe a bit careful with how you package it\nand\nuh also let's say you have a gc\nessentially\nuh gcs connector is needed because you\nneed to upload jars to gcs you need to\nread data from gcs etc right\nso it's extra crossbars they actually\nisolate this code that does\nios or gcs into separate class folder in\na way that there is no way how it can\ninteract with your code\nso our sdk is truly like zero dependency\nbasically\nso you can work around all these\ncrossbars issues it's awesome\nthat's really great for like long term\nstability i guess yeah\nthat's cool yeah there's a benefit that\nactually due to fast registration and\nit's like a default way to use it\n[Music]\nyou can swap containers so you don't\nhave to actually worry\nand package containers so you have your\ncode you have your jars\nlike let's say on some file system and\nthen you have a base image a container\none of the ways to use it you can just\nswap containers\nso an example would say you want to\ninstall security\nupdate for jdk so you don't really want\nto rebuild each\nuh docker image you published what you\ncan do you can take the cross path that\nalready\npublished and just swap this image\nso it also if you have many things you\nhave built historically it actually\nsimplifies\nuh maintenance so you don't have to\nrebuild a bunch of stuff and make sure\nit's\nupgraded etc\nso awesome yeah yeah yeah yeah\nthere's a new new uh there's a security\ndetection in the jrp or something like\nthat\nyeah even in the python side we are now\nwe added support for multiple\nimages one of the reasons was that you\ncould start off with just having some\nset of base images\nand both java and python you could write\nthose examples without even trying to\nbuild a docker container\nat least in the beginning i i still want\npeople to play docker containers in\nproduction\nbut uh yeah in the beginning it's like\nso much better\nand i think our inspiration came from\ngleb's work essentially\nso uh that's awesome thank you girl\nso uh uh sorry are you guys using the\nwould you be using the java sdk or the\npython\ni guess we we will be using both\nbecause we have when we actually\nprovide this to we have actually\ncustomers that are using this and\nthere's yeah there's just different use\ncases you know for\nmore for the for the\nengineering part there's just\nmore people using like jvm languages\nand for the more data science heavy\nthings of course\npython is the the choice\nfor most so i guess we we're interested\nin both\nwe will start with uh item\nbut we we really want to introduce some\nuh scala as well\nyeah i completely agree i think that's\nwhat's happening on our site too\nuh i think one of the updates that we\nwant to do in the documentation is like\nuh and i think we discussed this i guess\ncleve and i discussed about this some\ntime ago but\nour documentation's not good and we\ncompletely understand and so one of the\nthings that we are\nuh working on is updating\nlike in python and it's like a java\nright both of them\nare now kind of getting to a state where\nthey're ready\nthe way we think that they're getting\nstarted is essentially you keep\ninstalling\nyou know maybe install the dependencies\nand you\nlearn write code run them locally test\nit out\nget your hypothesis get happy with it\nright and then you install flight\nuh and hopefully that's also pretty\nstraightforward\nand then you install once you're happy\nwith your local flight then you'll still\nwrite it\nin a cluster and move on so that way we\ncan decouple the user behavior which is\ndon't you don't have to think about\nflight the back end stuff at\nall you just have to think about the\nuser land uh which is both the sdks\nand and and hopefully now that there's\nso much parity between both then we can\nactually\ngive them side by side examples and how\nthis isn't vital and this is in java\nchoose your language\nthat would be ideal i think there will\nremain some differences\nit's just the nature of those two\nlanguages but still\ni think we're pretty this is awesome\nreally excited\nyeah i think there could be room for\nfor even for showcases well with\nwith workflows using both you know where\nboth\ndifferent languages have their strengths\nand then in the flight you can use them\nuh together i think this is a great\nshowcase actually yeah yeah\ni think that's yeah like for example i\nactually think that\nsome things will be written in spark\nprobably in travel or scala\nand then other things can still continue\nto use python\nuh for doing their things and that will\nwork absolutely fine\nif we do this right and i think that the\npieces are there we\nwe probably need to rethink about the\nschema right\nif anybody has any ideas on how to\nrethink about schema please\nall yours um we will continue with the\ncurrent\nversion but i think we will put that on\nthe deprecation path and\nimprove or not application but an\nimprovement but further\nuh so if you have ideas how you think\nschema should evolve\none of the things that i would like to\ndo is multi-dimensional schemas\nwhich we don't support today right it's\nonly two-dimensional not very complex\nobjects are supported with\nschema which is i think a problem is\na tensor is a better uh thing than a\njust like a two-dimensional row uh but\nyeah so if you have ideas\nbring it up i would love to and\ncontribute to this people\nit's an old an idea that started off\nwhich is has taken its own life\nwe're gonna give it wings hopefully\nokay anybody else\nany other questions\nif not i'll give a quick update on\nmilestone 10 so we are in milestone 10\nthis\nlast two months we've become a little\nnot clear about like the end of the\nmonth and that's because we're still\nworking on tricky python\nwe did release\n[Music]\nversion nine it was\nthere are few very good features in the\nconsole or whatever but we did not\nlike not a huge ton of features i think\nthe milestone 10 is what we're really\ngetting towards uh which is hopefully\nthe end of the year\nwhich we want to uh add a bunch of\nlike you know include practical java\nimprove some documentation\nand have fractured python documentation\nwith the new version\nall of that is getting towards my extern\n10 which is\nthe one more month from now we will\nit says that it should be released like\nnow but we will\nwe will change that deadline to end of\nthis month\nand we're sorry that for the last two\nmonths we've been a little off but\ni think if everybody says the reason is\nbecause we think that there is a\nthere is a uh value add that we can do\nat every milestone and i think milestone\n10 can\nwe can celebrate the end of the year at\nwith my student\nso um\nand another thing is that we're working\non uh\nwith lyft on seeing how to donate flight\nto linux foundation uh and so there will\nbe updates on that in the\nfuture calls um and so if you\nyou might see some changes within\nnamespaces and so on and that might be\na little bit of a thing that you should\nbe cognizant about\nbut we think that going to the\nfoundation is the right way so that\neverybody can contribute easily and we\ncan create a proper\nproject name space for flight and you\ncan add all of these other projects that\nwe work on\nuh as in independent incubating entities\nwithin that project\nso\ndoes anybody have any concerns about\nthat\nwe think that's the right thing to do\nfor the community but we want to make\nsure that that's\nmakes sense to everybody else\ni think i think personally uh you know\nas a company that's building like a\nuh fda validatable product i think that\nwe that\nthat would be that would be pretty\namazing for us\nokay awesome great i mean we think it's\npretty obvious the right thing to do\nit worked for envoy for admins and two\nother big open source projects at\nlyft and they've been pretty happy with\nlinux foundation as a home\num and it's the governance\nis um we think it's pretty clear pretty\nstraightforward if anybody has concerns\nabout it they got a bunch of\ndocumentation about how\nprojects are governed but it's very much\nthe project sort of decides its own\ngovernance and the main rule is that\nit's written down and\nit's pure it's peer driven it's not\ncontrolled by any one company\nso that's that's what we're currently\nwhat kaithin is currently working on\nwith the folks at lyft\num so if anybody is not happy about that\nlet us know right away if you are happy\nabout it then um\nwe'll probably ask your support soon\nyeah yeah i think one thing is that we\nneed support because\nfor no reason it doesn't i think lyft is\n99\nsupportive but there could be that one\nperson case where they might be like oh\nwe don't want to move it we want to\ncontrol the project right and then\nwe'll have to decide an alternative\nin that path but at the moment i don't\nthink we have to recourse to that\nthought process at all but\nother than that we are going to move\nhopefully to the next foundation\nit's not going to be part of cncf but an\nalternate umbrella\nuh and we will be creating a steering\ncommittee\nconsisting of the various folks who are\npart of these calls regularly\num and then there will be different\nlayers like there will be maintainers\nand\nand contributors and committers\nso uh thank you for you know\neven joining in this is amazing we feel\nhumbled\nand yeah we would love to\ni think 2021 is going to be amazing so\nwe have one more\none more of these in this year let's\nconclude that with hopefully the good\nnews about moving into lf\nat with that timeline so\ncool all right if that's it\ni think we'll call it today um we'll be\nswitching all the videos to youtube\nunless somebody has any\nobjections to that instead of the mp4s\nanybody doesn't like that dm me but\notherwise\nyou know the links will all the links\nwill just go to youtube from here now\nall right thanks everybody\nyeah there's a channel in youtube you\ncan search for flight talk i think again\nand i think it shows up but uh\nall of the videos with posting that it's\neasier to discover okay\nall right so everybody in a couple weeks\nhave a good holiday\nthank you thanks everyone thanks bye\nbye\nyou"
    },
    {
        "title": "FlyteMeet 2020/08/11 - Flyte Extensibility part2",
        "transcript": "looks like we're back\nwhere did caitlyn go\nhe's back\nyeah sorry i joined the wrong meeting i\ncame back here\ni have a few zoom links with me uh\nyeah any uh any more questions and we\nwere actually continuing the discussion\nso what you were saying jeev is that we\nthere\nthis is a known problem that we\nencountered recently and so we're fixing\nthat\nuh it's most of the work has been\ndone and there is one change that's yet\nto go out\nbut we are trying to wrap it up in this\nmonth this month\nso 0.7\nawesome sounds good thank you so much\nyep so after that you should be able to\nand then let us know if you find any\nbugs but you should be able to\num any other questions i think leo had a\nquestion\n[Music]\ni i uh man\nthanks for picking on me i\nuh i love your version of this question\ni was a little curious about\nwhen uh in the example you showed for\nthe\nfor the dynamic tasks being yielded\nyeah and then appended to results was\nthat going to run them all seriously\nno it's all run them in parallel but\nsee the problem is because of the python\ninterface it makes you feel that\nit's running right there and then it's\ngonna come the yield\nright you can come back and resume at\nthat point that's not what happens\nthis is the problem in the current\ninterface we think right it's\nan interface accordingly should be\nself-document it's not in this case what\nhappens when we yield we\nrun we we just collect the yields\nthen this task completes then we run all\nthe yielded plan\nright it's essentially what you're doing\nis giving us a plan i want to run this\nthis is ten thousand times this five\nthousand times this 2010\nnot 10 000. we have a limitation of 5000\nin total or\ncanada so uh we'll take that plan and\nwe'll execute it\nuh and it can be as many as you want and\nthey run in parallel but they are not as\nlightweight like they are running\ncontainers\nso they are not like running one\nmulti-processor\nthat's the other confusion that some\nusers have\nnot as lightweight as using a back-end\nextension\nthey are not as lightweight as running a\npython thread\nokay next yeah or a pro multi process\nright in python\nbecause it's actually spawning a new\ncontainer\nanybody else i had one good question on\nsensor tasks\num so i wanted to make sure that i was\nunderstanding this correctly if there is\none task that depends on\na particular file from a previous task\nbeing in\na google storage bucket is that like an\napplication of\nsensor tests or is that not something\nthat you use them for\nif one task is depending on a file from\na previous task within the same workflow\nthat's all automatically done this is\nfor cases in which\nyou run another workflow which is even\nold actually by a different team maybe\nand that's uh writing a file somewhere\nand you want to\nuse it okay yeah\nyeah and actually i i personally am not\na big fan of sensor uh\nwhat i think this needs to be done is\nusing some concept like what we've been\nsaying it's called reactive workforce\nbut\nwe don't have that ethan has been drug\nkicking and screaming into doing sensor\ntasks yeah\nlike two years it was not there are\ni mean when it's mainly useful for\nthings when you when you don't control\npart of the data flow the application\nwas completely outside of the control of\nyour\nthing and you have to you know you have\nto wait for an email to show up or\nsomething like that\num the problem obviously is it makes it\nrepeatable repeatability is sort of gone\nand deterministic outcome of that\nworkflow is gone so it's sort of a\nit's a dangerous gun and people shoot\nthemselves with it all the time but\nthere are legitimate tasks where it's\nthe only thing that you can do so we did\nadd it\nbut it was not after without a fight\nyeah\nthanks for the presentation um i'm one\nof the engineers of freenom and his\nfirst time joining and i'm glad i did\nyeah awesome\nto have more freedom representation here\nthat's awesome\nyeah we need to get more of the other\npeople showing here i don't know why\nlike 828 is not showing up here today\nright a resource safety you mean like a\nclean-up clean like finally yeah finally\nbut\nalso workflows they have a finalizer or\nsome kind of finalizer\nbut there is a very good question can i\ncan i simulate it with a cell workflow\nyou can\nyou can if but you have to be careful\nlike what you know you'll have to\nbasically\nwhat you launched i actually don't know\nif you would\nknow what you launched like if you do it\ndeterministically yes you can\nyeah that's that's the short answer but\nit's a little tricky\nbecause that's kind of what i wanted to\ndo yeah\nyeah yeah is it for data flow or what\nkind of jobs\nyeah to make up some workflows that\nwould launch it and this work will have\na\njob id is an example of job identifier\nas an input\nyeah oh that yeah that if you do it that\nway that's the right way to do it like\ncreate the job id ahead of time and then\npass it to your finalizer\nbecause then you know you have to repeat\nit yeah that should work\nand then if i retry would finalize a\nretry as well\nwould finalizer retry yes uh there is\nthere is no workflow level we try right\nall right so what would happen if the\nnode would retry three times\nand then once it fails it will go to the\nfinal as it's like a try catch semantic\nwithin the try block you're doing the\nretries and\nlet's say if you retry equal to three\nthen you get a failure then you say\naccept and then that actually just goes\nto the timeline\nokay thank you yeah in python we don't\nactually have the workflow finalizer\nimplemented i guess you in java did you\nimplement it already\nno but we can't because yes it would be\na nice feature to have\nbecause then we can implement finalizers\nwithout writing a back-end again\nyeah that's absolutely the case that's\nawesome\num yeah but if if so\ncorrect me if i'm wrong but is there a\nkubernetes operator available for data\nflow and\nbigquery if we use uh\nit's called cloud config connector or\nkubernetes\nconfig connector i think there is a new\nname for it right now\nbut it's basically a set of operators\nthat google implemented\nwe need the kubernetes cluster like gta\ncluster to use it\nand there are many operators out of the\nbox including\ndata flow drop and bigquery query um\nthis is big very query i'm not sure\nwhat data for job for sure exists and\nalso\nsome other kinds of if for some reason\nyou want to create a gcs bucket or\nother things you can do as well yeah i\nsee aws also going in that direction so\nyeah that makes it very easy to add\nthose plugins\ncool um anybody else\nall right all right\nthank you everybody thanks a lot\neverybody and uh we'll see you in a\ncouple\nall right thanks is this meeting every\ntwo weeks is that it it's every two\nweeks\nokay yeah you should have gotten an\ninvite to your\nuh on your calendar as unless\noh gee you might have you might i did\nget this\nthis this event i i can check to see if\ni have the next should be i tried to\nmake it a recurring event that had an\ninvitation everybody was in users so it\nshould appear on your calendar every two\nweeks from now\nyes it does clearly i am i've been\nchallenged with the permissions of\ngoogle groups so\num if it's not working as you expected\nfeel free to like let me know and i'll\nfix it\nsounds good thank you so much george\nappreciate it thanks all right everybody\nall right happy tuesday see ya\nyou"
    },
    {
        "title": "FlyteMeet 2020/07/28 - Welcome Freenome, FlyteSnack menu",
        "transcript": "there we go\nokay we are recording\nand\nokay i just enabled screen sharing kate\nand see if that works\nanon thank you that was it for me\ni think\nyou should have a screen share button\nokay\nyeah cool um so\nthis is the flight snacks repo i'm just\ngoing to give a very quick\ntour uh the existing set of examples\nthat we had\nalready uh exist and that's\nuh so when you land on the homepage of\nthe\nrepo there's a repeat that shows you a\nfew options\nuh one of them is just for example you\ncan use\nlike it uh today we only have like\nexamples in here like if i can id and\nif we do one we can we would love to add\njava\ngoogle so in this way that way it's a\nvery common\nlanguage of expressing ideas getting\npeople\nup to speed with new sdks\nbut uh so this is the existing examples\num\nbut what we've added is a cookbook uh\nit's kind of a fun one\nlike snacks i guess uh you know how to\ncook the snacks\nuh one of them here is\nuh how to write a task and the way we've\ndone it is\nyou have a repeat actual file which is\ncollocated with the same\nuh you can go through various different\naspects\nlike an interesting one is the\ninteraction example in the interaction\nexample what you've done is instead of\njust\nuh showing the python code we're also\nshowing an example of how you use your\nnotebook and this is actually a\nrunning jupyter notebook it shows\nand there's a very interesting example\nin this one\nwhich uh\nit also gives you in the same place how\nto use cli\nand once um i think austin and\nuh raj are working on like ctl once they\nget\nuh moving further ahead they can add\nexamples for\nctrl uh once\nonce you have that one of the\ninteresting examples is\nuh this is a very typical case that some\nuh some users have asked us about uh and\none of them is like let's say i will\nforward the future introduction how do i\nre-launch a backstage institution or\ncreate a partial workflow for that\nand this is an example of doing just\nthat\nmoreover if i go back here\nuh there's examples of using raw\ncontainers dynamic workflows\ntasks and this is no by no means\ncomplete but\nwe are trying to complete this pattern\nis there something that is specific we\nwant uh definitely bring us in the\nchannel\nclear an issue and i think this should\nbe a high priority task\nhonestly\non the other hand if you are if you are\never wanting to\nunderstand what are the different\nplugins and how to use them they are\nactually listed separately\nso in this case it's going to write a\nspark job there's an example of hardware\na spark job\nand here's an example of writing a smart\njob\nexpand them further later on this also\ntells you how to install\nspark for your container same thing for\nfive touch training\nuh i guess one skill is done you'll be\nadding\ntensorflow training uh and we have\nrestore query with high quality\nstatement all of these uh examples are\ncoming in and i think\nthis is more of an interactive\ndocumentation that means we can actually\nclosely go and\nmake all of these examples\nyou might find some hiccups and you're\nstill cleaning them up\nbut if you do find something issue park\nuh either one it's fine\nyou'd love to know that so that's right\nsnacks um\ni think i should put a link to it\non the other hand uh there's a quick\nquick demo\nabout catalog so i think uh a lot of\npeople\nknow that flight comes with data catalog\nbuilt in\nand that serves two purposes it's used\nin memorization\nand second and biggest tracking\nthe problem today is that our\nui doesn't really show any information\nwhether the cache was populated whether\nthey were cached\nor cached uh and any links into the\ncatalog\nand that was a true item\nso that's what i completed but the ui\nstill\nis probably uh one step behind it we'll\nbe adding that\nsoon but i will show you a more ghetto\ndemo it will use\nan api to show all the information\nbut let us let me show you\nan example in this case\nthis all right um\nin this case this was a\nrun workflow if you can see\nlet's go back so usually it takes about\nfive minutes and around in 33 seconds or\nseven seconds\nsystem and this is running on my local\nlaptop so something happens\nwhere i am running for workflows and it\ntakes longer but\nuh it ran very quickly uh but we don't\nknow why\nthat's not just inputs it has outputs\nand if you look at the task\nit shows you that the task is\ndiscoverable\nwhich is you know catching is equal to\nthis is a\nlegacy name for catching uh and the\nversion for discovery features\nbut we don't know whether it will\ncapture captured or catchments\nand so on uh same thing\nin this case is a sub workflow so this\nis the task and cycle software for\nthe same input output it is discoverable\nand you can see the discovery version\nbut yeah there is no information about\nthat\ni don't want to catch it right now so\nlet's relaunch this again\njust just for kicks um\nand let's change the\nsize and i know i have not never run\nthis again so it's going to appear on\nall of this\nlet's just run it while it's running\nyou know let's go back to that cached\nversion so the id\nis and that's the execution id\nwhat i have is uh actually i've already\npulled it off\nwhat i have is a flight admin api\nand i am actually going for that\nexecution id all the node executions\nand in this case there was a node called\nscaled\nlet's go back to the console let me see\nthat\nthere's a node called scale\nif you look at the cache status for it\nit says cache hit\nand if you look at the catalog key it\nalso provides something called scatter\nkey\nwhich is essentially how to look up that\nitem in the catalog um\nproject domain and the name of the thing\nand the version\nuh it also provides the ultimate\nidentifiers to look up\nthe item in the catalog uh in this case\nit's the artifact id\nand build the attack caller\n[Music]\nalong with this it also provides if this\nwas a cache hit\nwithin flight which was the originating\ntask that generated\ncaching information so it was generated\nfrom\na task called flight snacks\nunder which at this version\nand it was the execution id was uh\nx y and z in this case i don't have\nunknown and this is because this was a\nolder example uh\nthat means i i ran\nthe caching information before or the\ncash flow populated before my change was\npushed in\nso right now we don't know the actual\nexecution name that's\ncausing the cash population but after my\nchanging you will actually start seeing\nuh the actual name of the cache\nand which is going to be which is which\nis what i'm doing\nso so that's the this is a subtask\nsomewhere flowing in rather so for that\nwe have to\nsee that it's a sub workflow and some\nworkflows\nopen that up once you open that up uh\nit has a rotate task that's the rotate\ntask\nagain that was the cache same stuff\ntag execution\nso the eventual goal is in the ui\non the on the right panel uh maybe there\nwill be a small icon here that shows\nwhether it will catch it or\nmiss uh and then on the right hand side\nwe will show you the actual link to the\nexecution click on it\ntype and get more information about it\nuh\neventually we have an internal thing\ncalled\nexplorer uh and i'm not showing any of\nthe data\ni'll show you how it's a very simple\ntool that somebody's written on top of\ncatalog\nthat shows you uh all the catalog\nentries and\nfilter them by some some other thing\nthat they are interested in\nlike for example yeah i don't know\nexactly how to use the tool but\nuh once you click on it it shows you\nmore details about all the artifacts\nthat were generated and the flight\nexecution id\nthat generated so\nyeah so then you can keep on you know\ngoing back and forth from\ncatalog to flag so eventually\nour goal is we are taking the explorer\nthat i just showed you and merging it\ninto our ui\nso then the users will be able to\nnavigate it from their workflow\nexecution to the originating workflow\nto the actual data set explore that data\nset and come back into the execution\nuh so that's that's the high levels here\nbut let's go back one minute and see\nwhat happened\nof course i'm running zoom and i'm\nrunning everything my\nlaptop doesn't have memory um\nso sad but this one worked\nyeah actually we can still get\ninformation\ni need to get a newer laptop my laptop's\nfour years old\nso uh yeah in this case\nlet's see so it says cash populated not\na cash rate because we actually\ngenerated data and then\npopulated into that so and then all the\ninformation that we need to cut this\ninformation\nbut there is no overage source execution\nid because\nthis was the source and next time when\nyou run into source execution id\nso that's a quick look at\nthe changes that are coming in they are\nall\nready to go uh it's in one it's like\nacross\nall the reports there's one one there\neach\nand my target to merge all of them is\nthis week if not this week\nnext week and then when you deploy it\nthat should not cause any changes there\nis a small migration that will happen in\nthe database essentially at a new column\ncalled cache status in the database and\nthis is essentially for analytics\nso that we can quickly run an analytics\nof how many nodes gotta catch it what's\nthe sketch miss\npopulator and so on uh but that should\nnot change anything\nso those are the two things uh in my\nhead\nkevin do you have anything to share\nhey katherine i can also i want to talk\na little bit of the node\nas well yeah let's do that so\nokay can i go ahead yeah you can awesome\num i will write it in the notes sure\nuh hey guys uh for those of you who\nhaven't met me\nuh i'm anand i have been at forever\nbut this is my first meeting uh uh\nmore recently i uh i've been working on\nobservability mostly on the internal\nand to keep flight accountable for any\nissues\nuh you know trying to track\ninfrastructure issues and trying to\naccount it\nsince last week what i have been working\non is um\nthe ability to add a\nparent child relationship to a node\nexecution\nwhat i mean by that let me share my\nscreen\ncan you guys see my screen\nawesome so i am taking example of\nbatch uh tasks workflow and this is all\nopen source so you can you know try it\nout uh\nso here is a set of nodes and if you\nexpand the setup node\nuh there is a list of more nodes and so\non\nbut if you actually look at the\nuh they actually graph you this is the\ncorrect one\nwhere you only see a set of nodes these\nnodes in turn produce a list of nodes\nand there is another example that i want\nto show you which is a sub workflow\nwhich is\nhere there is just one node called as\nidentity verb\nexecution but what this node does is\nthis node\nis a sub workflow node and the sub\nworkflow can have more nodes\nbut ideally you want to have this node\nand other nodes\ninside the parent node\nwhich means that any node if it produces\nmore nodes\nthis could be a dynamic task or a sub\nworkflow\nthey have to be that needs to be a\nparent-child relationship between\nnode executions so today in our system\nwe do not have it\nbut you might ask me hey if that is not\nhappening\nhow is dynamic task alone working\ncorrectly\nthe reason is because what we have\nintroduced is we have introduced a\nconcept\nof a task being apparent to a list of\nnode executions what i mean by that\nis that in terms of dynamic task a node\nresults in a task execution the task\nexecution\ncreates a list of nodes and we associate\nthe list of nodes produced\nto the task and not to the parent node\nso\ni am working in actually refactoring\nthis\nrelationship and making sure that node\nexecutions\ncan map to a parent node execution\nbut ideal that is slightly tricky\nbecause in our admin system today we did\nnot\nhave a concept of parent child uh\nat the node execution level so that's\nwhat i'm refactoring it\nbut one of the main things that i wanted\nto share in this aspect was\nuh we\nthis change is in some regards a\nbackward incompatible change\nso we are trying to be very careful in\nthe way we roll out\nuh so what i mean by that is technically\nthe way\nparent child uh relationship is today is\nthat uh all the node executions parent\nis a task\nand the task is mapped to a node we are\ncompletely going to remove\nuh this relationship of notes saying\nthat hey there are children to apparent\ntask\ninstead the in the new world it will be\nlike all the node execution will say hey\ni\nam the children of a parent node uh\nso as we move forward the current code\nthat we have is going to be removed\nbut how do we roll out so currently what\nwe are trying to\nintroduce is we are trying to introduce\nan event version concept\nwherein we will support both\nparent tasks in parent node but the\nmoment you upgrade their event\nversion you will only see for any\nexecution\nuh all parent chess relationship at a\nnode level\nso but if you go in the past and look at\ninto the ui\nfor your older workflow executions you\nstill\nyour ui will still work and mainly\nbecause they follow the old even\nold even version format so the way we\nare trying to target is that\nwe will support both for a significant\namount of time\nand once we do like a major version of\ngrade we will remove the old code but\nuntil then\nuh we are planning to have both the code\nexisting\nit's just that the partition of whether\nan execution will be using a parent\nuh node parent-child relationship versus\na task parental relationship\nwill be at the execution level uh and so\non\nuh i also shared a document in the\nuh if you guys want to take a look uh do\ntake a look uh\nbut yeah this is mainly what i have\ntoday uh\nyeah any questions\ndid you say windows is going out that's\na good question so\ni have made a change uh in the admin\nuh the next thing there are two more\nthings that needs to happen one is we're\ngoing to need to figure out a\nwhich we want to use for event version\nso the propeller can use that field\nand then the last is ui so uh\nyeah so i i do not know the timeline for\nui but everything else should happen\nyou know in this week maybe mostly by\nthis week\nwe i'll get randy's uh\nthe person who's working on ui and see\nwhen he can get to it and i can share it\nin the channel\nyeah i think uh just also from my point\nof view this\nis uh one of the biggest problems in our\num in our data model at the moment\nand you're still sharing the\num so this is a\nbiggest problem in our data model and uh\ni think before we can\nrelease 1.0 we wanted to have enough\ntime for this to take\nwe don't think this is a destabilizing\nchange in terms of\nuh the overall system but it's just uh\nincompatible with uh with the decision\nthat we've made\nin the past and we're fixing in a way\nthat\nshould have minimal impact but it's\nthere is some impact uh\nthen you will see some some difference\nin the ui\nuh but other than that i think this is\ngoing to help us\nyou know evolve even further i hope\neverybody's okay with that change\nuh of course before we roll this out\ninto production\nand into the open source we will be\ntesting it thoroughly\nuh within our system so\nwe are waiting it to be part of 0.7\nwhich is end of august\ni have a question you mentioned is a\nproblem in the\ndata model it's a data model of the\nadmin or yeah or it's something deeper\nin there\nno data model of the admin\nin fact\nyeah actually you can you can just show\nthe\nproblem and yeah like the\nyou can still see my screen right\nso uh yeah so here is what's happening\nso this is a\nnode execution every uh\nan entry of a node execution so whenever\npropeller sends a node\nexecution uh we store it in the node\nexecution table uh\nmain admin and each entry uh points to a\nsingle node execution\nso what happens today is we have\nintroduced this concept of apparent\ntasks which we have this is the older\nconcept\nlike we have this field parent task\nexecution id\nthis field essentially points to the id\nin the task execution table\nso what happens is that a node execution\nwill have a task execution\nand the task institution will have an id\nnow you have a list of nodes\nthese nodes are essentially saying that\nhey\nmy parent is that task execution in the\ntask execution table we want to\ndeprecate this\nfield and in turn have a\na recursive a situation where you know\neach node execution\nin turn will have a list of child node\nexecution and uh\nand vice versa which means that uh each\nnew resolution which has a parent\nwill in turn have a parent node\nexecution id or a\nparent id which is what i'm naming it uh\ninstead of having a parent task\nexecution id\nyeah this is the change in the data\nmodel\nuh that will be happening but what i'm\ntrying to say is that right now we are\nnot going to remove this field\nso uh in the s in the near term you will\nhave both the columns\nuh and and you know at some point you\nwill have to get rid of it\nso that you can run uh you can just\ndeploy the new flight version and it\nwill still work in the older model\nunless you upgrade the event version\nwhich is what i want to come up with so\nit's saying that hey\nadmin will now support both the versions\njust that that we slowly migrate\nand make sure that you know this field\nis completely unused\nat which time we can you know go and\ndelete it\nyeah i think uh we are probably\nat some point we're going to have a time\nwhen there are these deprecated fields\nuh\nand we do that cleanup and maybe slot\nthat for 1.0\nrelease so until then we'll support both\nand else and i think they'll be\nsupported like\nat an epoch value so anything older if\nthe older\npropeller which is sending the events\nwith the right information in it\nwhich is not telling that information in\nit\nthen uh you would continue to use the\nold model\notherwise you would start using that and\nthe\nhopefully the impact is not much\nthis label if you can see the money\nhighlighting is actually\nnamed incorrectly this is not what i'm\nreferring to\ni'm going to introduce a new field\ncalled as parent id\nbut this is a different feature uh\njust that the label parrot node\nexecution id is now confusing\nso i i'll send you guys a br uh i'm\nalmost done it\nthat will clarify more uh yeah but yeah\ndon't get confused with this film this\nis com\nsomething completely different from\nwhatever we talked in this meeting\nit's just that the name is uh yeah\nconfusing\nsorry\nthen send any questions yeah\nyeah i think uh also for people who are\nnew right\nthere are three entities within an\nexecution\nof a flight graph uh the first top-level\nentity the workflow execution\nsecond entity is a workflow consists of\nnodes which are only meta entities they\nthey essentially exist\nto capture retries meta information\nabout a\ntask execution but they but we can have\nexecution like nodes of different types\nnode can have tasks in them\nit could be branches sub workflows could\nbe dynamic\num and so that's what the node level\ncaptures\nand then there is a third entity which\nis only for task execution and a task is\nactual\nthe thing that actually doing the work\nfor example\nuh a container executed\non kubernetes somewhere or a job\nexecuted in bigquery\nboth of those are tasks and they are\nmodeled as task executions\nso this change is at the node level uh\nthe central\nuh the method here that you are wanting\nto\nand the natural hierarchy\nof execution contains node executions\ncontain each node execution\ntask institutions and so on what we had\nwas one of the node executions was\nwas linked to a\nuh all right i think i i had one more\ndemo\nbut i see geu has joined hey jeeve\nwelcome\nthank you it's good to be here hey yeah\nthanks for joining me\nuh and so nelson here is from spotify\nthey are the other people who use it on\ngcp to our knowledge i know there are\ntwo other\nteams that are using it on gcp i\ni actually don't know which companies\nyeah they just ask questions that i\nanswer\nand then they go away but i know these\ntwo few of you who\nofficially use ngcp\nso cool um\n[Music]\nyeah so uh g also we do this by vtsync\ndefinitely join and bring your consoles\nokay one more demo i had was uh\nall the ui stuff that randy has been\ndoing\nhe's been doing a lot of work in the ui\nstuff so i just wanted to make sure that\nthat is uh there is a\nplace that we talk about it\n[Music]\nso this is not done yet but this is\nthe prototype\nof the ui update that's happening um\nand if anybody's interested we can try\nthe mark\ni uh am not really good at navigating\nsigma\nall right how do you do\ni it one of those yeah yes\nyeah why i don't know why it's so\ncomplicated but yeah okay\nuh maybe it's supposed to be so uh\ntoday we show all the workflow\nexpression is simple node list\nuh what it's moving to is this waterfall\npattern\nand it's not a work it's a timing\ndiagram essentially\nand the colors the different saturation\nat times\ncolors indicate different states so for\nexample\nuh a node might have been queued for x\namount of time and then it actually ran\nand something like you and it's\nessentially the\nhigh-level point of this is to provide a\nquick\nvisualization of what's taking time in\nthe system\nor where did i spend most time within my\nworkflow\num how much eventually also to show\nhow much cpu and every response is using\nso\nkind of giving you a hint of the past um\nand that lift this has been a very very\nhigh ask from our users um\nfrom open source i guess we've not\nreceived any of these asks\nbut uh unless people are using this on a\nday-to-day basis when they find\nthis sort of stuff very useful so\nuh these box essentially show\nwhat direction the ui is going in uh\nto make it it's more not\nmostly it's still an engineer focused ui\nbut it's\nuh it's made so that we can\nquickly crash problems uh otherwise\nperforming\nso fast\nuh and this will also form the basis of\ndoing a lot of future work and that's\nwhy i think\nrandy is probably gonna work in this\nforum\nfor the first couple months in this ui\nuh and let me know if\nthe other thing that he's been doing is\ndeployed but uh for some reason we\ndidn't have this too basically\nuh\ni actually showed this a little bit more\nright so if if you are launching a blog\nyou can actually specify the uri and\neverything right in line in the ui\nthat didn't exist until some time ago um\nthis is also going to change this is the\nsame\nsource of that schema for blocks or\nmulti-part blocks\ninstead of for single filing multiple\ndirectory\nand then the uri will be\njust\nokay um that's i think\nfrom randy also he's done a lot of bug\nfixes and i think the ui\nis way more stable now so if you do find\nanything\nuse that as well um yeah\nwe have one ui engineer he's i think the\nentire community always relies on one ui\nengineer\nso these precious resources so let us\nknow\nif you finally try to\ntry to get into work and i think if you\nknow any ui\nleaders uh in your team or companies who\nwould love to help us\ni would really appreciate it\nwe have a lot of lot of work to be done\nin the ui\nand i just wanted to share that\ninformation with you what's happening in\nthe ui\nall right any questions comments\nsuggestions and\ni think i'll handle the judge\nthank you everybody um\ni yeah randy is um he's awesome but\nthere's only one of him\nand we definitely are looking for a\nsecond or third\nuh ui engineer with you know a strong\ntypescript background\nif you guys use your use your networks\nand let us know because we would love to\nhave some help\non that side especially because we have\na bunch of people\npiling up work behind him and randy has\nhe's the only person who\nexposes it to uh to the screen\nso um that's it\nanyone else or should we wrap this up\ntoday\nall right we'll call it thanks everybody\nfor joining um\nwe will see you in a couple weeks i'll\npost the video um\nuh shortly and uh anybody that wants to\ndemo\nnext week just dm me on slack and\nwe'll add you to the list yeah there is\na signup sheet also you can just go and\nsign up\nand kevin i hope you can level something\nfor tensorflow\nand putting some pressure all right\nall right thanks everybody\nyou"
    },
    {
        "title": "FlyteMeet 2020/09/08 - Sagemaker part 1, Lineage and Job Cloning in Console",
        "transcript": "foreign\ngood morning caitlin good morning\njosh should we get started or maybe wait\nfor a little bit\nmaybe till 905\n[Music]\nmorning everybody do we want to get\nstarted\ni'm going to give it a few more minutes\nmaybe there are some\ninitial agenda items so i created the\nagenda items in the same plots\num but they are like\nuh i don't think we need everybody there\nbut as people join into the demo\nso should we start\nyeah i think so okay\num good morning everybody or\ngood evening in some cases i guess but\nuh\none thing i am restructuring the notes a\nlittle bit\nso that as we are maturing as a\ncommunity\njust wanted to make sure that we we keep\nthis meeting\nfruitful meaningful and useful to\neverybody right so\num if you have any suggestions please\neither add them to the notes as a\ncomment or\nwhatever or we can you can dm or put it\nin the general channel\nin slack but the new structure is we\nwill try to see some of the\nmajor versus contribution highlights\nfrom the community\nwe try to do a deep dive or a demo\nor a proposal review so\nif anybody has new proposals\nand if you think that you are just\nwriting\nuh just writing and sharing them is not\nenough you can of course come and talk\nabout them here we can have a 10-minute\ndebrief like everybody should come\nahead of time read it and you know we\ncan just decrease them\num and then we'll we'll try to do a more\nof what the current milestone we are\nworking on what's the progress on that\nand have links to it and then we'll try\nto keep about five\nfive or ten minutes depending on the\ntime of that day uh for q a\nuh so that's the new slightly more\nclearer proposal of agenda uh going\nforward for this meeting\num and yeah yeah i will open up for the\nq a in the end\nso this week uh\ni just wanted to bring up like thank you\nboth hanukks and\nyuraj on ebal circuit business handle\nthey've both been really contributing\nreconyx has\nmade it possible to run flight on gcp\nreliably\nhe's been adding core to all the\ncomponents and i keep on saying pr from\nit\nthank you so much um and same thing with\nyuraji is working on\nlike ttl almost one man's show is he's\nuh fighting through it he has a pretty\ncool\nproposal um to do\nuh more likely a very flexible way of\nof uh showcasing the entities and that's\ncoming\nsurely so for those of you\nor you guys who don't know flightcheck\nspecifically is essentially\nintended cli for\nflight um and it should be available\nuh hopefully by 0.8 is when we are\ntargeting at least a\nlike a small radius that has a few\nfunctions a few functions available it's\ngoing to be a standalone library that\nyou download\non whatever platform you want and it's\nusable\num okay we can go into the demos and\nproposals if anybody has them\nuh we have a couple demos and i think i\nwant to stop saying that i'm\ngoing to do all the demos i want other\npeople who are doing cool stuff to do\nthe demos\nbut we'll do it for a little bit more as\nlong as i can do them\nand and if you guys want any specific\ndeep dives\nto specific areas or topics please\npropose i would love to\ngo into details of the deep type of\nthose topics\nokay so the demo for this week so we\nuh we closed out on last\nweek a little later than usual like a\nday or two\nbut we wanted to make sure we have\nuh one um\nyou know reliably tested it and do\n[Music]\nuh enough coverage of the like bugs and\nthings like that that we wanted to get\nin before\nand that's why it took a couple more\ndays but so\nuh as we closed out i'll basically be\ndemoing a couple of features these are\nnot done by me\nso i'm just doing the mantle of the demo\nbut\nuh sagemaker integration is completely\ndone by chiang hal\nuh and the ui work is done by randy cart\nand and i'll just be demoing and taking\nthe credit today\nthey both cannot wait to the meeting so\num let me stop right\nshare my screen\nall right so\nuh sagemaker custom training job so it's\ninteresting i don't i know\nmost of you or some of you guys are on\nyour gcp but\nin aws there is a hosted machine\nlearning\nservice called sagemaker it offers\nmultiple\n[Music]\ndifferent ways of of training your model\nuh deploying your model reprocessing\nsome data but they are all like hosted\nservices\nuh without a common orchestration stuff\nand so flights\nalso intends to be one orchestration\nlayer on top of\nsage maker um so one of the there are\nmultiple ways of doing things in city\nmaker\nthere's a concept of built-in algorithms\nthese are black box algorithms that i\nthink channel developed a few weeks ago\nthese are built-in algorithms you don't\nhave to really uh even build a container\nyou just have to run\nthe code um and those are\nprepared to just give you the inputs and\nproduces them all\nbut oftentimes we have models especially\nin deep learning that\nwhere you want to provide the internet\narchitecture\nor you want to run a script as part of\nthe training job\nand in that case sagemaker has a thing\ncalled as custom models\nit's a little more hairy to use custom\nmodels as compared to\nthe built-in stuff um it's essentially\nwriting a container in a specific way\nuh the inputs without it from specific\ndirectories\n[Music]\nand then and playing around with\nenvironment variables and command line\nparameters\nbut what we decided to do is we wanted\nto make it really simple for\nanybody who's writing a model that runs\non changemaker\nand if you're familiar with how you\nwrite python functions\nuh in flight then writing a\nsagewaker model is no different uh\ncustom signature models\nyou essentially write a python function\nand you put an annotation on it\nit's called custom training job and\nin there you specify the algorithm\nspecification and so\nthe interesting thing over here is it\ndoesn't mean there is any algorithm it's\nnot like actually boost or\nby dodge based some\nuh resonant 50 or anything it is just\nessentially saying it's a custom model\num and you can specify the type of\ninputs there are a couple of different\ntypes here\nyou can also specify if you want\nspecific resources\nso we decided to not abstract the\nresources\nfrom sagemaker but to directly expose it\nand that's because\nsagemaker has does not really support\npartial cpus or gpus or whatever what\nthey do is they give you an entire\ninstance\nto run a training job um so we decided\nto expose that because that has cost\nimplications\nso if you decide to use ml m4 extra\nlarge\nyou will pay for that entire machine\nit's not that if you just use one cpu\nyou will only pay for one simply looking\nfor the entire machine and that's why we\nwanted to expose\nthe instance type instead of like\nabstracting that into cpu gpu\nbut other than that it's just writing a\nfunction uh with all the inputs and\noutputs as\nis and this automatically this\nthis computation will automatically flow\ntoward into stage maker\nwill run it will set it up in the right\nway and\nfasten the input and the output will be\nfed back out\nand here's an example of actually an\nmnist\ntraining algorithm it's the same code\nbut that's the algorithm written as it\nis and i actually\nwe just you know copy pasted it from uh\ntensorflow or\nin this case yeah tensorflow is uh keras\nimplementation\nand so this is just copy wasted it's\nnothing different it's a custom training\njob\nand you run it you can put it into\noverflow as is and\nexecute it i actually have it registered\nat the moment\nthe one caveat with um sagemaker is that\nthe\nthe trained or the the model code\nhas to be the doctor container and the\ndoctor container has to be eci registry\nthey cannot pull from at least not to my\nknowledge they cannot pull from software\nor from gc gcr or whatever you have to\nbe in easier\nso that's why i didn't want to push it\nbecause i don't have permissions to push\nthe container from my local laptop\nbut uh we can\njust relaunch this thing and this is an\nexample this is the same example that i\nhave previously launched you can just\nrelaunch it\ni'm re-launching it so that i don't have\nto\nremove this little thing okay\nthat's it so you launch it and the\ninteresting thing happens is the code\nwas as we\nsaw we are running i'm running this in\nour staging cluster at least\nuh that's also the reason you cannot\nreally learn sagemaker\nfrom loco unless you have all the roles\nand everything set up\non your local cluster and that's a\nlittle\ncomplicated and we're trying to simplify\nthat as well\nbut as we see this stuff running and\nit takes a bunch to initialize that's\nbecause it's actually bringing up the\nmachine\nso uh that's one of the places where\ncustom training it's not as useful on\ncity maker if you're just going to run\nsomething really small where kubernetes\npart is\nmuch more powerful or quicker to\ninitialize\nyeah while this is going to run let's go\nand\nshow the other parts of the demo uh\noops the other demo i had was\njust going to show the two new features\nthat\nrandy created in the ui uh both of them\none of them is actually exposing the\nlineage information\nor one part of it and the other one is\nhow to clone so\nif you have uh if you have used the\ndiabetes boost trainer workflow\nand this is the example workflow now\nyou'll start seeing\ni don't know how to resize this\nokay so uh for every task\nuh the node and the task you see the\nstatus but you also see this nice cool\nsimple let's do it\nwhat that says is that this output for\nthis execution was that from cache\nand this this was a missing thing we\nshould have had this at day one but\nuh that is a quick way of knowing that\nit was getting ready for cache\nso if you do a relaunch um this should\njust quickly\nit's running on my local laptop so\npardon if it's a little slow\nbut essentially to start\nsaying that okay ready for cash right\nfor cash and just like\nthe other interesting thing is if it's\nread from cache you can go to the source\nexecution\nso you can hit that button and it will\ntake you to that it was a different\nexecution that\nthat actually populated the cache uh and\nif you see the result of this execution\nwas returned to cache it's always noted\nover there\nyou click on it and you'll see more\ndetail\nnow if you go to some other examples\nwhere you did not use caching\nactually\n[Music]\nyeah so this one doesn't do any caching\nso you will start seeing caching with\ndisabled surface execution so all of\nthis shows up in the ui\nuh much as you expected and uh\nand this is like i think one of the\nthings i was missing for a while\nthe backend actually supported this all\nthe time the bracket exposes all this\ninformation just the ui was not\nthat we've gone up now um\nanother interesting thing is let's go\nand\nlaunch an execution let's do this\nlet's put some image here okay once it\nstarts running\nmany times what users want to do is\nrelaunch clone the execution and launch\nanother one so on the top and\ntop right there is a three dots uh\nlike material design you can click on it\nand you can say clone execution\nand it will get in all the parameters\nyou can modify the parameter could be\nsomething else right\nso that's the idea of creating these ui\nfeatures\nall right um which one was the one\nwhere did the example go oh this one\nfailed\ni don't know it got um killed so i will\nhave to debunk this sorry this is a live\ndemo but\nthis was supposed to be uh the way it\nworks is\ni'll show you previous one i think some\nconfiguration got changed but the way it\nworks is you get\nall the logs you get the link to the\nstagemaker training job\nand the stagemaker training job is\nuh as if all the inputs are passing\ncorrectly\nall the values are set uh and\nthe algorithm metrics and instance\nmetrics all them are available\nbut we also pull in all the interesting\nstuff out into here\nso you can go to the log from here and\nso on so that's\nuh stage with the custom training and\nall the ui features\none of the future things coming in\nsagemaker is\nthis still does not support distributed\ntraining so\nif you want to run something like uh\nhorror world\nor something this doesn't directly\nsupport it\nwe are working on making it very easy to\nbe supported\nand that's coming in china will continue\nworking on that\nand uh this\nwe've still not completed our\nintegration with sagemaker in terms of\nuh their batch predictive point and uh\ntheir endpoint creation uh\nairport creation api we have not\nintegrated with that but that that's\nlike a\nlonger fold at the moment and keep on\ndoing that\nthat's uh that's the demo from my side\noh i actually wanted to talk about\nmilestone 8\nand then we can open up for questions\nall right so for the milestone 8 um the\nnew milestone is called my sonate what\nwe've\nstarted doing is the best way to know\nwhat's our immediate work items that\nthe team is working on and if you or any\nof your works wanted please mark them\nas part of the milestone that we can you\nknow we should be automating some of\nthis using bots\nit's not very easy to put parts in it\nlifts uh organization uh because of\nsecurity reasons so\nthat's another item that we should all\ndiscuss whether we want to move\nflight out of lyft organization that has\nsome\ncaveats where you know if you are\ndepending on some of the reports they\nmay operate\ntemporarily but we should discuss that\nbut other than that 0.8.0 is a milestone\nthat we are\nslating for september 30th uh and\nmostly working on uh\nsmaller improvements uh bug fixes\nuh like ui there are lots of ui\nimprovements coming in including the\nwaterfall ui\nuh and flight ctl and changemaker\nwe are also uh\nsome flight admin stuff also that we\nhave been wanting to do\nuh which is actually which should help a\nlot and performance\ntuning of uh presto couple\nthings that are not noted as part of\nhere yet\nbut they will be our flight kit\nenhancement\nuh at least parts of it and a faster\niteration model that we are\nwe are thinking of prototyping and they\nmight they'll come as proposals probably\nin 0.8 and from the implemented 0.9\nso in the next oss meeting uh we should\nhave some proposals\nand so if there is anything that you\nguys would like to be prioritized\nuh in here that's not here look at it\nlet us let me know and if you would love\nto have been contributing something\nplease let us know\nwe love that so thank you that's\nit for my\nanybody q a this meeting\nhey austin you tried to say something\nyeah hey no questions oh we're good\nthank you for showing up here today oh\nyeah i usually\nam uh watching it after the fact um you\nknow yeah\nso yeah i know\nyou you've told us about the videos and\nthat's really helped because i i hope\nthe recent\nall the you know the setup and such that\nyou can easily access the videos\noh yeah i think i think things are\nsorted out and it's it's good to see\nthese developments\nuh in general positive directions\nshould we be putting all these videos\ninto a youtube channel or something\nwould that be more\nuseful\ni don't know i mean probably\nright but like i think this is so not\nwell\nknown that the people that are gonna\nfind it on a youtube channel are the\nsame ones that are gonna find this\nuh you know wherever it is right\nright oh yeah i'm just more like a\ncatalog right\noh yeah yeah but i mean if if you're\nthinking for discoverability i don't\nthink\nthe president is at a point where you\nknow\na very passing layman is gonna you know\nhappen across this on a youtube channel\nand you know be so eager to dive in it's\nyou know\npretty specialized though yeah\nany other comments questions\ni don't have questions either\nbut i just wanted to say\nuh thanks for for the cookbook\nbecause i i love the the overhaul\nthere's a lot\na lot of stuff in there now and\num this was really really helpful for us\nfor for actually getting started\nso yeah just to thank thanks oh thank\nyou\nsir and for that yeah we've got that\nsetting being the focus\nand we'll continue to improve it as and\nwhen i get time like it's\nit's the extra effort right it's outside\nof business our\nwork but yeah thank you for the comment\nvideos\nokay nelson uh so you guys are going to\ndemocratic java next time is that right\num uh i will need to check with clap\nit's a but\nprobably yeah yeah but uh\nmaybe you have talked to him directly\nyeah he said he should be in a state\nwhere it can\nin this in the next couple one of the\nnext couple meetings he'll be in a state\nwhere it can\nbe remembered it's being used in\nproduction is what i understand right as\nfar as\nyeah actually um\nis the lefty i mean\nwe have one team that's running\nin production uh and we are\nboarding a second team they're\nasking some features are not available\nyet in the\nin their flight\nright now we are focusing on the\nintegration with our schedule\nwith a flight and as soon as we finish\nthat then we will\ngo back the attention to the flight kit\nyeah and\nmake this extra feature to import this\nthing\nyeah yeah one of the things that i'm\nalso trying to help with\nwhenever i get a chance is to test right\nto java on aws\nand it should work there was one problem\nlast time we're working through that but\nafter that it should work and then\nif people want to use it on aws they\nshould be able to\nokay all right everybody\nthanks for coming i'll have the video\nposted as soon as zoom has finished\ngrinding through it and feel free to\nshare it with your colleagues\nand anyone else on youtube who might be\nlooking for cat videos\nno\nand we'll see in a couple weeks and\nhopefully everyone is\nuh avoiding the fires and the coven\nand and everybody's staying safe\nall right bye everybody all right\nbye bye"
    },
    {
        "title": "FlyteMeet 2020/11/17 - Usability Roadmap & Flytekit native typing alpha",
        "transcript": "one thing uh some of the\noss contributions thank you\ngive the archival archive of the project\nwork\nlike merged in uh\nso that that basic solves the problem\nwhere\nyou have the project right there in the\nui screen and\nyou want to hide that then from the ui\nor from the cli\nso thank you very much\nso there is a more structured uh demo\nand condition around\nthe features that we are going to talk\nabout today\nhey you've got an echo going on kevin\nall right okay one minute\nis it better now probably noisy but\nbetter\ni don't know what's happening\nso maybe\nso i do have a few slides\nbasically i think after it's been\ni think next month will be almost close\nto a year\nsince we open source flight uh\nwe have uh at the company we have\nspecially worked at lift within lift we\nhave worked on\nyou know reliability and getting things\nup to date uh make sure things work at\nscale\nand and\nwould reduce cost for operating flight\nat scale\nbut uh one of the areas that we think\nthat we have not really paid attention\nuh after opening\nwas uh usability uh of the\nof the platform itself but how you get\nstarted quickly and so on\nso i wanted to touch base on how we are\ntrying to tackle that problem\nand what's the current roadmap look\nright because i think\ni have heard this from a few users like\nthey would like\nsome newer features in the usability\npart and i just wanted to touch base\nlike what we are doing and what's the\ngrand plan that we\nfeed at least for the next few months\num so we basically\nuh have a capability in the open source\nwhere you can\ncreate a cluster on your laptop\nor in eks or in gke\nit's not as straightforward as just\nsaying here's a button when you could\ncreate an eks cluster\nthat working at scale uh everybody knows\nthat\nit's a little hard but we've tried to\nmake it as simple as possible to create\na sandbox cluster\nand you can create that you can use some\nexamples use containers\nlocally to actually register those\nexamples and get everything running\nso that's what what we open source to it\nand we've\ntried to define it a little bit\nthen we realize that there is a need for\nsome users who do not want\nuh to use the likes like it\npython reflected java they want to just\nrun some other processes without\nactually\nneeding to uh needing to\ninstall either of those components and\nso we we launched this thing called as\na rocket header tasks you can run\narbitrary containers and\nflight will essentially mount volumes of\ndata into the\ncontainer and read data back out of the\ncontainer\nuh one of the goals of that is like that\nwill become our data medium\nfor most of uh you know data accesses\nthat\nany of the tricky libraries do\nspecifically for cloud scenarios that we\ndon't\ndirectly support in those libraries for\nexample today we in\nin 5k java and python we support gcp and\naws but if you want to run this on azure\nthen then the raw container does\nactually support you in azure\nuh from then on we we realized that the\nusers\nuh the way the users actually well\nwith flight we always want to match the\nuser's workflow\nand one of the things with the user's\nworkflow is that they\ndon't really start with an entire\npipeline they start with one task or one\nthing to do\nlike usually a spark job or a training\njob or you know python script or so on\num and we wanted to allow users to scale\nthat job out\nto multiple machines um and flight does\nvery well at that and so we exposed a\nfunction uh which\nwas always a platform i think that came\nprobably in last three months ago we\nstarted\nwe launched the capability of doing\nsingle task executions\nsingle task executions allow you to run\none task at a time track it\nuh and then eventually you can take each\nof the tasks and put them together into\na pipeline\nand execute a pipeline itself\nand today uh katrina will talk about\nfast register so actually\nuh one of the biggest pain points uh\nwith flight as it is uh in the open\nsource today's you always have to start\nwith\ndinner you have to go to the container\nand then go from there right and we\nwanted to\nreduce the uh\nproblems associated with building a\ncontainer where where some users may not\nhave complete grasp on using docker\nfiles and so on so we want to\ntake that away and and reduce this\nreduce the time it takes to iterate\nbetween\nbetween changes in the code to testing\nit\nin remote and this is specifically uh\nyou know taking your code from local\nmachine to testing it in robot\nthe other and then the next two\nsections are the upcoming things that\nwe're working on one of them is\na major rehaul in flight date itself\nthat\ntalks about that that essentially makes\nhopefully makes it easy\nfor users to uh ramp up with flightgate\num\nyou should be able to write code in your\npython and flight will just\nshould just work uh or even flight to\njava\nuh it's kind of similar enough like your\njava and python\nmore similar than not\nalong with that we want to make it\nreally easy to extend\ntasks and add custom\ndata type support within within the\nflight kit\necosystem and we will talk about a\ncouple use cases when we\nlook at the side deck further\nuh and i think this is one of the\nbiggest uh\n[Music]\npieces of work that we feel will help\nour users a lot\nbecause this gives them full local\niteration\nwhenever possible and so the the user\nstory would be that the user starts with\nan idea\nwrites code in python tests locally\nto whatever extent they can and then\nhits a button\ntested remote now there is one task\nfrom one task you can create multiple\ntasks test it locally and then test it\nremote\nusing fast register change the iteration\ntime between\nlocal and remote to be less than uh in\nin order of millisecond instead of uh\nminutes\nand then and then the eventual goal is\nonce we have all of this\nyou want to provide a set of\ntools or templates that you can deploy\nflight to any\ncloud environment easily we already have\nyou've seen the working customize that\nwe've already done that\nallows you to make it easier to deploy\nto various environments but i think\nuh there's a debate going on in the\ncommunity about\nwhether to use helm or whether to use uh\ncustomize or whether to provide our own\ntemplating system so\nuh we'll be talking about that like\nearlier next year\nand making sure that we can provide the\nbest way to deploy flight\ninto any of the hosted environments\nall right with that\nsorry um and just i wanted to i think i\ntouched most of this but uh i wanted to\nhighlight a couple points of why we\nare doing some of these things right\nfast register and that's what\ni will talk about in a minute is\nessentially to improve velocity\niteration velocity\nfor the developers um while the flight\nteam\npython overall is to actually reduce the\nverbosity\nin um in writing flight workflows and\ntasks and\nand make the code more pythonic so that\nit's very\neasy for new users to ramp up and have\nminimum\nset of questions that they ask about how\ndo i do this or\nit should flow more naturally if you are\na python if you have ever written any\npython code\nwrite almost\nuh equivalent to that and then the\nother reason why we wanted to do it is\nflight kit today is extensible but we\nwanted to make it even\nsimpler to extend and uh\nwe think we can expose an api that that\nmakes this extremely\nsimple and we'll talk about it and the\nlast bit uh for the simplified\ndevelopment i think\none thing that we want to focus on is\nwhen you start a local flight cluster we\nwant that\nto be minimal in overhead uh today it\nstarts a\ndarker or kubernetes environment\nthere are some great developments that\nhave happened in kubernetes itself to\nmake it\na very low footprint to start a new\nkubernetes cluster like micro kits and\nkind\nand so on so we want to leverage one of\nthose and\nhave a out of box capability of running\nuh flight locally\nyeah and then uh the 154 as i said\ni will hand this over to katrina i don't\nknow if you change the diagram i had\nsome diagrams here\nkept it like that oh\nthat's fine um work on the diagram in\nthe future\num so thanks for bearing with us with\nthis uh really elaborate diagram thank\nyou catherine for\ndropping this together um so yeah fast\nregister\nuh what is it and what is it good for um\nso i lift uh one of the things we've\nobserved is that a lot of the tasks that\nuh require building a container in order\nto\nupdate the task definition this includes\nmost tasks like python\num spark uh hive etc\nbut it does exclude some like um the\npresto task for all containers has those\nobviously don't need to build a\ncontainer\num however this process uh ended up\nbeing really really slow\num so what happened was you know your\nordinary user would um\nmake some code changes commit them and\ncreate a new container definition\num and because of the way lyft has base\ncontainers that um\nuse um uh inside of our flight\ncontainers\nsometimes this process would take about\nlike 20 to like 30 minutes\njust to build for a single change um\nthis is a horrible iteration cycle for\nour users and people were kind of like\nincredibly frustrated by this obviously\num so\nfast registration kind of steps into\nfill in the gap here\nwhen it turns out that you don't\nnecessarily need to rebuild your\ncontainer because let's say you're not\nadding new codependencies um you're not\nadding new imports etc\num there's no reason you need to rebuild\nyour container fast registration\nactually\nso you can take a look here at this\ndiagram allows you to upload your code\nchanges to s3\nand in the process of registration we'll\nuse the base container image\nbut during execution we'll go and fetch\nthose live code changes\nand apply that code artifact inside of\nyour container\nso here we have our happy user who's\nbuilt his docker image and his base\ncontainer\num that he uh to begin working with\nand as he's iterating and working on his\ncode changes he can use this fast\nregister process to kind of align\num the entire uh container build\num if yeah and so here we can see at\nexecution time we're pulling the base\ncontainer and then we also pull and\nfetch the\nthe code artifact um so maybe this will\nmake a little more sense with a demo\num so keith do you mind if i share my\nscreen\nthanks\ncool um so let's take a look at some\ncode we have here\num so here i have a really trivial\nworkflow um\nbasically it grates whoever you wanted\nto greet um\nprints out hi um not very sophisticated\nbut you can use your imagination for\nwhat\nregister can accomplish um so let's say\nin this case\ni want to change the greeting here and\ni'm feeling a little funky so i'm going\nto make it howdy instead\nso ordinarily if i wanted to you might\nwant to you might want to jack your\nphone up a little bit\noh yes apologies sorry\nhow is that is that more visible\nokay i don't hear complaints so i'll go\nfrom roll with it um\nyeah so sorry about that so here we oops\nthat is another greeting so here we have\nour single fast workflow it has a single\ntask uh which greets whoever\num the workflow input it happens to be\nand in this case we've gone ahead and\nmodified\nour our dummy task in order to change\nthe greeting to say howdy\num so our next step is um instead of\ngoing through the typical registration\nprocess of rebuilding this container\num and then registering this updated\ntask definition\nwe can switch over to our next tab over\nhere\num so i've entered the my original\ncontainer um that i built\nwhich has the old definition of the task\nin it\num so we can go ahead and run\ndocker run that and then once we're in\nhere we can\ngo ahead and use pi flight to run the\nfast register command this is actually\ngoing to change soon so you don't need\nto be within a container\nbut for the purpose of this demo we need\nto um\nso here we can take a look this is the\nhigh flight fast register command\nuh takes in the computer yeah sorry i\njust realized that apology is a\ndifferent tab okay hopefully is this\nmore visible again\num okay i guess if you clear screen it\nmight be\neasier oh even better yeah cool\nall right so uh simple and big now\num so we have our pi flight register\ncommand um\ntakes in a config file like uh normal\nproject domain\nand then the command in this case is\ngoing to be fast registered it'll pass\nin the source directory as well to\nspecifying the code that you want to\nupload\num in this case i'm using the flight\nsnacks container and we're just\ngreat so\nlet's go ahead and fast register our new\ncode changes\ntake a look here and see what's\nhappening\nusing our config file um in this case\nwe're checking to see if that code\ndistribution already exists\num on s3 which is our blob store in this\ncase\num we hash the entire directory so every\ntime you make a code change um we're\nable to notice that we need to upload a\nnew\nuh code distribution but if there are no\nchanges then we just skip that process\nall together\num so we've gone ahead and compressed\nthe directory um\nuploaded a test three to a a specific\num hash directory for this version of\nthe code\nwe've registered our workflow tasks and\nlaunch plan\nawesome so let's go ahead and take a\nlook at launching that\num so i'm going to go in here\nlaunch our workflow see this workflow\nversion 866 af\nthat corresponds with the latest version\nthat we just uploaded\num let's go ahead and launch that\nit's doing its thing this should\nhopefully be pretty fast during so it's\nnot exactly the most sophisticated task\num running\nawesome\nnormally it's a little faster there we\ngo 10 seconds cool and then we can take\na look at our outputs here\nand that is wrong um awesome of course\nthis demo is not working\nwas just working this morning so that is\ngreat um\nlovely um\nyou can take a look at some of the\nprevious versions that we had here\nso this one i had an extra high that i\nadded as a test literally just this\nmorning\nnot sure what happened now um oh\nactually i know what happened\ndid i not save my file\nokay sorry about that let's try that one\nmore time\num but benefits of fast register this\nshould be a fast process so\nhopefully this demo won't take too much\nlonger um\ncool so again we have a new version\nof um the hash code because i changed\nthe the code\num cc the version has changed and again\nwe upload that compressed version\nso let's go on back here launch our\nworkflow with our latest latest version\num\nsweet okay give it another 10 or so\nseconds\nlet the suspense build up cute\nrunning\nactually this seems like a more of a\nplanned demo this way\nalso katrina maybe we should uh prefix\nthe\nversion with like something like fast or\noh sure\nyeah so one of the things i've been\nsuggesting is you just have a remote\ndirectory where you upload your code\nthat's specific to your distributions um\nso that way you can organize them\num but yeah that's also another option\nlet's take a look here hey\nhowdy flight crew look at that didn't\nhave to wait for a whole container built\nthis time so a totally intentional demo\nall the way through there\num yeah awesome okay cool\num so i will stop sharing\num actually we can take a look um would\nit be useful to kind of go\nover what you need to do in order to\nenable fast registration um it should be\npretty simple changes\num so i we haven't quite built the docs\nyet here but they're available on github\num so there's a few prerequisites that\nyou need um you obviously need to bump\nyour flight kit version\nuh in order to a version that supports\nfast registration\nand there's a new parameter that you\nneed to add to your\nflight kit config which is this fast\nregistration directory that i was\ntalking about and this is the\nintermediate directory\nwhere we upload those code distributions\nthat we end up pulling in execution time\num so you can either have a specific\ndirectory for distributions in order to\norganize them or we can\ngo ahead and make a configurable prefix\nas well too so you don't have those\nrandom\nhashes uh for your compressed uh your\ntar files\nand that's all you have to do um you do\nneed to make sure that the role that\nyou're using\ncan write to this directory and also i\ncan read from it as well too\nbut after that's done uh how to um\nso merge your changes from above make\nyour code changes like we saw\nand then go ahead and run the fast\nregister command\num and then once you um had done once\nthat succeeded like we saw\num you can go ahead and create a new\nexecution\nand what this is doing under the hood is\num\nexecution time we actually run a\nseparate fast execute command\nwhich goes ahead and pulls the code\ndistribution installs it\nand then creates a new process to go\nahead and run the regular\npi flight execute command so that it can\npick up\nwith the latest code changes so pretty\nstraightforward pretty simple\nbut um a lot faster to iterate on as you\nhopefully saw just now\num so yeah that's that\num hand it over to kaithin to talk about\na new flight kit changes\nyeah that's awesome actually\nthis eventually will also the aim is\nthat\nthe users don't need to have docker\ncontainers to iterate on\nexamples and things like that locally\nyou can get started\njust go ahead and modify the code and\nmove on\nall right thank you for the demo\ntrina uh\nagain i am mostly just talking because\nhe did not want to talk so i'm talking\nbut this is all his work um\n[Music]\nso flight kit uh we've talked about it\nin the last thing but i just wanted to\ngive a little more deeper dive into all\nthe features because they are now\nfleshed out\nin code and they are available in alpha\nand i'll quickly give you the\ninstructions of how to\naccess the alpha at the end of the slide\ndeck but\nbefore that what is the change so\nimportant you just need one import right\nfor task or workflow so that from flight\nkit\nimport task for workflow is always\nconstant\nthat's the import that you need to do to\ncreate a task or a workflow\nin the new flight kit uh if there are\nplugins that you want to use\nthen they are always standardized so\nthey might be in different packages like\ninstead of from\nflightcheck.taskplugin.spark it could be\nfrom my thing dot\nx import a type and the type is what\nqualifies the\nplugin and we'll see an example of how\nthat is used\nuh whenever and then you go about\nwriting your task and when you write the\ntask you just write a python function\nyou write a python function you specify\ninputs in python native types and you\nspecify outputs and python data types\nand there's always one decorator there\nare no\nsite card tasks and you know you don't\nhave to come up with funky names you\njust come up with one name\nwhich is called task and if you if you\ndrop that task decorator then it becomes\na pure python function\nand that is by design we wanted it so\nthat\nyou can start writing python functions\nand then add tasks where\nthey're applicable\nfrom here uh you go to defining a\nworkflow and in this example if you see\nuh this is the diabetes workflow that we\nhad in our one of our first examples\nstill the same um but it looks very\ndifferent because you\njust have a function and you take a data\nset which is a csv\nand if you notice over there we have a\nflight file type we'll talk about it in\na minute\nand type csv and we take a bunch of\nother values\nthe interesting thing is defaults now\nare also very natural over here\nuh you just specify defaults as if they\nwere the default values for a specific\nuh thing uh input type\nuh and and then they then they just work\nuh and then if you look at the\nway the workflow is structured it's\nessentially calling a bunch of tasks\nand each task returns inputs and outputs\nand those inputs and output gets\npassed around as you expect in a normal\npython function\nand that gets converted to a\nflight workflow and there are a couple\ncaveats we'll talk about it\nand the last part is you can execute all\nof this locally and this is like this is\nactually from that file\nif your name is equal to main just run\nit and it'll just run\nthat thing locally so you don't need to\neven go to fast register or build a\ncontainer or do any of that\nif your code cannot look but\nnot everything can be run locally you\nhave a sql task how can you run that\nthing locally you probably need to call\nthe query or hive\nor whatever presto so in that case you\ncan mock\nso the bottom of this example over here\nshows that you can mock things i know\ni'm mocking it to return a value string\nuh this will change to return a type of\nschema but in this case i've written\nanother\nstring and you can execute it and\ninstead of actually calling\nthe function itself it will call your\nfunction or your mock or it\nalways written your marked value so you\ncan also have a side effect and this is\na full magic marker object so you can do\neverything that you do with magic mark\nwith this\nall right um dynamic tasks uh was one of\nthe most uh\nprobably very powerful but ill-designed\nuh primitive in the system and we we\nhave gone in and rethought about it\nbecause it's essentially is a dynamic\nworkflow instead of dynamic tasks like\nwe are writing arbitrary workflows that\ncomb\nat runtime we're generating arbitrary\nworkflows at runtime and then excluding\nthem\nso that's what it looks like now a\nworkflow and a dynamic ad dynamic is the\nnew\nuh decorator that you use but it's the\nsame as that workflow\nuh you can write everything that you can\nwrite in network flow\nthe only difference is uh the input are\nmaterialized at this point that means\nyou can do\ninteresting things like for i in range a\nand then do things that you want that's\nbecause the value of\na is known because this runs at runtime\nwhereas in a workflow if you do for i in\nrange a\non this value over here that will not\nwork because workflow is a pure\ndeclaration\nand there is no value right it will run\nat runtime\nand for loops are not understood by\nflight\nso when you do that but\nthe interesting part is when you do that\nlocally itself will fail you will say\nlike oh you can't use for loops inside\nyou might not say that i will say like\nit's it's impossible to get a range\nvalue from\na promise all right um the next bit that\nwas is it's a powerful feature in flight\nbut\nsometimes could be hard to use is a\nschema and i think\ni don't know if some other new members\nof\nthe community are here who are who are\ninterested in helping us with\nyou know redesigning the schema but\nhere's the existing schema and we are\njust making it bringing into the new\nworld\na schema is essentially a columnar data\nstructure you can model it using columns\nand here's an example uh also we've\nreplaced blob\nand multi-part blog in the name\nlike we didn't like the name for the\nusers because for for\nusers of python that really doesn't make\nsense it's a file or a directory\nand that's what it is so you call it a\nfile order directory from the user point\nof view\nuh one of the things to note over here\nis\nthis is a generic python type and you\ncan\ncreate a format in inside there we are\nstill debating as to how\nto really represent um the format but\nthis is the current\nincarnation uh and you can say you know\nflight file\ntakes a csv or if this is a csv file\nand we will upload a csv file the\nbenefit of using formats is if you have\ntwo tasks one that produces the csv\nuh that consume the csv we will assert\nand we will\ndo a static type checking on the type of\nthe file also\nif you specified if you don't then it's\nan option\nsimilarly uh flight schema is now a\ngeneric\ninstead of uh the object you have and\nyou have\nall the column names over here the\ncolumn names is essentially an ordered\ndictionary of key value pairs where the\nkeys are the names of the columns and\nvalues of the types of\nthe column and you just define them\none of the cool things that happens is\npandas data frame is now natively\nunderstood\nthat's right so if you return a pandas\ndata frame but you are\nyou've qualified the function to return\na\nflight schema then we will auto up\nconvert from pandas data frame to like\nschema\nuh and we'll quickly talk about how the\ntype extensions work so you can add\nthese type extensions\nthis auto converting for spark frames\nother types of data frames you know\nanything\nthat conforms to what the schema is\nall right so extending task\num we\nwe know current flight kit allows you to\nextend things easily but we wanted to\nmake it even simpler\num and so in this case uh as an example\ni wanted to just add new python function\ntask type called my python function task\nand now it can give two types we always\nsaid that we you can\nhave a flight kit only task extension\nand that's useful in providing\ndifferent types of interfaces uh you\nknow\nfor the users for example um\ni may want to wrap up a function\nand make it so that it runs\non one element at a time and\ni want to provide that as a common\nfunctionality to all my users you can\nwrite that as a pure python\nfunction task for flight is still\nexecuting one container but you\ninternally are executing\nmultiple functions and so\nthat can be done using uh just extend\nthis\nuh just implement the method execute and\nyou're good if you have a backend plugin\nyou can optionally implement\nthe method called get custom information\nwhich returns a\ndictionary or a cheek sound\nand this json gets converted and\ntransported to the actual packing plugin\nwhich should know how to uh decipher it\nand use it for\nuh for the runtime execution\nof the plugin the other part is these\nplugins are all lazily\nuh embedded or lazily registered\nsorry i should have updated spark but um\nyou can basically register um python\ntask plugin\nwith the type in this case that type is\nthe\ntype of the generic object here so my\nobject type\nand the name of the class and that's it\nonce you do that\nthis this um\nfunction type or task type is now\nregistered with the system\nuh so this ad allows you to now register\nnew task types and user land like you\ncan just like create your task type\nregister it only for yourself and use\nthem\nthe other thing is the types types are\nalso like just like\ntasks types type system itself can be\nextended\nso for example if you're using primitive\ntyping integer date times they are all\nhandled\nalready but let's say i want to write\nmy own exotic type um let's say\nthat type is i am in bioinformatics and\ni\nhave a specific uh type for\nyou know showing or modeling dnas\ni don't know maybe i'm doing saying\nsomething completely wrong but\ni have a specific type it's essentially\na binary format and i want all my users\nto use that type\nthen you can create a type for modeling\na dna and register\nit with a floodgate's type engine in the\nback end\nyou can provide an implementation and\nthat's the transformer\nand that transformer actually just\nconverts from\nthe user type to the act\nto flight understood type uh in this\ncase let's say if it's just a binary\nfile\nof some sort you could convert it to a\nfile\nand in the file you would probably add\nthe format information to the\ndna file or something um and\nyou have to to do that you have to\nimplement three methods what is the type\nof that thing and when i say type here\nit's actually converting to the flight\ntype uh and when i give you a value\nhow do i convert it to the flight type\nso you give in that method\nand when i have a flight type value how\ndo i convert it to\nyour value if you just implement these\nthree methods\nyou can extend flight kits type engine\nto\nencompass any type that you want and\nthis is\nespecially useful for domain specific\ntypes\nall right so we've got about four\nminutes before they're going to kick us\nout of the meeting and we can restart it\nbut if you want to\nyeah no i think i'm almost done okay so\nthe current state we've installed\nuh alpha alpha one is available now um\nto to get started you can just say click\ninstall flight kit\n0.16.0 alpha zero uh it is\nuh in a separate branch in flightgate it\nwill eventually get merged into master\nuh and all the examples we have tried to\nput it together in a separate branch and\nfly snacks\num again there are not many examples and\na lot of documentation that's coming\nsoon\nbut hopefully this quick primer will\ngive you\na way to get started uh\nand i wanted to give you a quick update\nof what's coming up next um\nso branching support\nand i'll show you an example of\nbasically we have native branching\nwithin workflows\nuh multi-image support\nso flight backhand does not care about\nwhat\nimage you use for a task right each task\ncan be a separate image but that's\nthere's no\nvery easy clean way of really uh\ndoing this for the user and so we we\nthink we have one way and\nthat'll be part of alpha two um lots of\nunit tests and you're going to port or\nmost of the task types we are hoping\nthat the\nthe alpha 3 will where we'll try to\nallow you\nto use tasks from the older sdk or the\nolder style\nwith and mix them within your side and\nthat would be the backwards\ncompatibility model that we go ahead\nwith you won't be able to use new tasks\nin the old workforce\nwhile testing a new workflow um\nand then we are hoping that the ga\nshould be in the video\na quick look into how the multi-image\nsupport is going to work\nso if you have a task you can have a\ncontainer image in which you can\nspecify instead of always having the\ndefault image\nyou can say that i actually want an\nimage whose fql remains the same\nas a different word and in this case for\nspark i'm building a different\nimage i'm reading two images one called\nspark hyphen the version name\nand another one just with the version so\nin this case that both the examples will\nhave\ndifferent images uh all both the tasks\nwill have different images and they are\nused in the same workflow\nnow to use it in the same workflow you\nhave to have all dependencies we are\nstarting\nwe are out of time i'm gonna so i'm\ngonna have to end the meeting\nand then i'll restart the meeting and\neverybody can rejoin if they want to\npick it up\nbut that's zoom is gonna do that for us\nin about 30 seconds yeah so i would like\nto answer any questions if people want\nuh so please\njoin back in uh how do we what would be\nthe uri\nsame with it's exactly the same url i\njust have to end the meeting but you can\nclick back on the same url in about a\nminute and it'll all come back in\nokay all right all right we're going to\nend this right now everybody and then\nclick back on the same link that you\njoined us with in about a minute and we\nshould be ready to go again\nyou're muted\nyeah i think when you come back in it\nmutes you by default\nyeah yeah sorry about that i'll repeat\nwhat i was saying was uh\n[Music]\ni'll quickly talk about multi-image\nsupport again\nand specifically from the use case point\nof view um\nthe use case over here is if you're if\nyou want to use\ngpus for some specific task uh and could\nbe machine learning\nor could be image processing or any\nother\ncase or rendering right but\nin another task you don't want to use\ngpus you want to\ndo some machine learning using\nscikit-learn on cpus right\ndoesn't matter and in another task you\nwant to do spark\nit doesn't care about gpus in this case\nand you just want to do some data\nprocessing\nwhat you do typically today um because\nof\nthe way flight kit was built is that you\nhave to build one image\nthat has gpus and spark and uh second\none and that's\nthat's that's nuts it's crazy to build\nthat image because our image size\nbecomes\nmany gigabytes uh with this change\nwhat we are hoping is that users will\nbuild multiple\nimages uh one specific for that use case\nuh we still think that the versioning\nshould\nhappen in sync so that you know you can\nmake sense of what's happening\nand like oh i built four images and all\ndependencies that are directly involved\nin that module\nuh so that's the multi-image support uh\nfor conditionals this\nis the syntax we are going ahead with um\nwe\nlove feedback if there are somebody\nabsolutely hates it we would love to\nknow\nand please come up with ideas also uh\nthe syntax over here is um\nthere is an assignment that's happening\nso\nby default and this is flight sway is\nlike every\nuh conditional has it's more like a\nternary operator in\nlanguages so it returns a value\nright so so the so the output of every\nexecution within the conditional\nreturns um the same set of\nor same type of value and that's the\ninterface\nof a conditional and that is\nautomatically derived from the\ntasks that you invoke within your\ncondition and then you can do if then\nelse and of course you can do else of\na function like t2 or a task or you can\nfail\nand failure is okay because it's not\nreturning an error it's actually raising\nan exception\nuh from frank's point of view so that's\nfine\nso that's how the conditionals uh are\ngonna be they'll also get merged soon\nand all of this is needed for alpha 2.\nand that's it from my side i i don't\nreally need to do a demo\ni think um all of this runs locally\nall of the examples in native typing\nflight snacks\nand i think i can show things running\nlocally if that's\npeople are interested because i think\nthere were\nusers who were interested um\nso in this case we do have some work for\nsupport we have dynamic support and so\non\nso this is a task it's returning a you\ncan't really name\noutput returns so we\nif you want to name them you can use the\nname tuple and then\nthe variables in the tuple become the\nnames of the output\nbut otherwise the output name is\nro created we will create a default name\nlike o0 or\nout zero but you don't even need to know\nbecause you're\nin your workflow you're just working\nwith the outputs themselves\nuh and then you can just run this okay\nthey should just run about\nmy environment set up correctly um\nsimilarly as i said you can have simple\ntasks but you can mark it\nand you can once you do that you can run\nthe mark locally\nsame thing for conditionals and and\nspark even the spark stuff if you use by\nspark it runs locally\ncompletely so you will be able to write\nregular python code and write spark code\nand all of that\nlocally uh it uses patchwork local mode\nand it'll set it up correctly so that it\nworks\nuh and\nyeah of course like like things like\nwhere you are\napplying constants to a value\nare supported so you can bind constant\nvalues to a\nuh through a function um\nfunction invocation so because this is\nnot declared as an input\nanother constant we will automatically\nconvert that to a constant for\nthe next variation of the workflow\nuh what else expressions can be complex\nright uh\nthese if else expressions can be complex\none of the weird things that you have to\ndo\nis instead of saying and in python\nbecause the hands\nis not exposed on an overloadable object\nwe have to use\nbinary and uh our\nbitwise operator and the problem with\nbitwise operator and is the precedence\nis higher than comparisons so\nif you don't put brackets it will try to\ndo an\nend of these two things thankfully the\nrejects\ni think the compiler will fail saying\nthat oh you can't\nand two objects because there's no\nbinary\nuh sorry i'll be twice and between them\nbut that's like a\nquirk of python um\nbesides that uh oh yeah the flight file\ntype\nuh vs the idea over here is that we want\nusers to start thinking about\nfiles and local objects and persistent\nobjects so once you start thinking about\nthem being as persistent you just work\nwith firearms\nand flight will take care of the rest of\nor\nuploading it uploading it now it's part\nyou you don't want to work with files in\nthe file system so that's okay you can\nwork with rdds\nbut they are persistent uh automatically\nbecause we may outpersist on them\ndepends on the implementation and yes\nyou will support osf path like\nmore of the natural pythonic syntax\nuh i'm trying to find if there is\nsomething special or that missed out\nyeah you can invoke launch plans i think\nworkflows all of that\nthat existed with old exists with the\nnew just more natural\naccording to what we think should be\nuh also we are moving to a more\nfunctional paradigm\nwhere you don't say a workflow should\ncreate a launcher and you say hey create\na launch plan from this workflow with\nthis type\ninformation um\nand if you if you've noticed we have\nif you have used the new flight kit\nalpha there is no\nat least we recommend not to use the\nregister directly\nbecause register is not supported as we\nthink we are going to split the\nregistration step into two steps\none is serialized followed by register\nand serialize is where you need\nsome context information with code and\nso on register can be done anywhere\nuh completely independently and and the\nreason for that is you want to share\nthe examples as a downloadable entities\nthat you can just register from wherever\nthat's the goal that we're\ngoing to get to uh and i think our\ncurrent\nsetup for that having one combined\nregister doesn't really work\nvery well for open source and we think\nthis is better\nyep that's it uh anything questions any\nquestions let's\ntalk about it\nhey kevin i had a question around uh you\nmentioned this\nlike running the spark test locally\nit'll just use a\npi spark locomotive does it users still\nhave to kind of set up\npi spark uh i guess locally and install\nuh spark and everything locally you have\nto pip install\n[Music]\ni don't know if you also need to have\njvm\nprobably you need to have java running\num i i have to check because maybe i\nhave it running\nalready or i have it installed yeah\nhe probably knows the answer to that so\nno okay any other questions\nno questions what i have one question um\nso the this\nfeature of choosing a different\ndocker container for each or docker\nimage for\nfor each task and the stuff katrina\npresented earlier where you can actually\nuh um change code and\num just it will just be uploaded to\nto s3 so you can freely\nmix that right so you could have a bunch\nof\nbase docker images\nand as long as you don't have to like\ninstall a very specific dependency of\nsome python library\nyou could change the docker image and\nalso change the code and then run it on\na different docker image\nwith without having to rebuild\nyou are a student that is right you can\ndo it\na recommended way is questionable um\nso i i don't want to advertise it but\nyes there is\nso for example we might have a set of\nexamples with just essentially like\nright whatever you feel like\nwithin this realm of things and here are\na bunch of base containers\nokay okay well that's it you can have\nabsolutely all right that you can just\nno need to build a container ever in\nthat scenario now\nwhether that's a good or yeah yeah if if\nyou really want it it's another question\njust just\nyeah i think it's more like reducing our\nopinion over here right i think\nas a as a platform you should probably\nnot have\ni have an opinion on this um we still\nrecommend\nusing containers when you're\nproductionizing things\nbecause they're just more it's a natural\npackage\nbut yeah it's you are free to do it\nokay thanks yes so actually one thing\nthat\ni uh messaged katrina about uh with that\nis\nsince like we use bazel um\nwell quite a bit of users use bazel so\nit already includes all the dependencies\nas part of\nthe code so you actually we actually\ndon't have to\nrebuild an image at all just use the\nfast register\nyou can yeah that'll be awesome yeah so\nthat goes\nyeah i'm going to incorporate that in\nlike the bazel rules that i wrote and\nsee how\nusers like that yeah you might speed up\nyou like you make it even faster i think\nthese basis anyways faster i think this\nmight be even faster\nprobably a second or so to get it\nrunning\nbut what happens is when you make reduce\nthe friction so much\nfrom going from local to remote people\njust start using remote\nall the time and then you your\ncost spikes up so you have to be kind of\ncareful\nbut yeah what users want users always\nright so they they want it right\nquestion uh kitten so we're using some\nlike local\ndocker build optimizations where um we\nare you know kind of like installing\nrequirements like an earlier stage of\nthe\nearlier step i guess early layer of the\ndocker build how much faster will this\nbe compared to that you think\nis it just like something that we have\nto test um\nso so for instance for us when a user\nmakes a change it'll just like you know\nit'll cache it you lose all the cash for\neverything and then just like\ndo like maybe a two meg or something\nlayer um\npush yeah it's like that's how it should\nbe\nthat is the right way of doing it\naccording to me\nnot everybody does it that way right i\nsee\nokay it's like it's like you look at\nsome of the docker files and you\n99 people don't know how to optimize\nsoftware it's\nsad right um and then there are some\nweird things that people do\nwhere the code will be copied first and\nthen the requirements will be run and\nit's just\nright right it becomes slower and slower\nso if you are if you're doing everything\nright according to docker probably this\nis\nstill going to be faster but then why\nchange that docker behavior which is so\nmuch\nso much good yeah still i'd like to test\nit because we have we have a\nwe have a sort of a situation where um\nyou know all of our development actually\nalready happens in docker containers\num and so you know like flight kit and\nstuff is already installed in this\ndocker container and like you know when\num and then users can just kind of like\nuh push so what we what we're currently\ndoing is like users just like push the\nbuild and push the image using google\ncloud build which leverages the cache\nfrom like you know gcr or whatever\num but then like they register from like\ntheir local environment\nso they're not actually like you know uh\nthey're not exactly like executing into\nthe container or like you know running\nin a container to like run\nthe register command so but but i'd be\ni'd be curious to see like how fast\nregister works with this sort of a setup\num oh i think with that setup right\nyou you don't even need to do a google\ncloud build again and again you do it\nonce\nwhen you modify your requirements you\njust start eating fast register\nyeah that's what i'm thinking yeah okay\ni will let you decide\nwhen is the right time to switch from\nfast that's why we created two set of\ncommands\none for fast register and one for\nregister and the reason is we\nstill think when your production writing\ninterviews register just got it\nyeah good to know okay\nmaybe at some point probably we should\nhave a feature in admin that says lock\nlike staging and production environments\nto or give the capability to lock any\nenvironments that you want\nshould not get fast register bills\nbut again opinions not it won't be\nopinions it'll be an opt-in feature if\nthat is the case\nwe should all discuss it okay thank you\nanything else\ni think sam might hey\nsam thank you for joining do you have\nany questions you've probably\nlooked at the api\nso i'm sleeping\nall right so if there are no more\nquestions um\nthe only request i have is please try\nout\nmy kit alpha there are bugs\ni'm not even going to say there may be\nbugs there are bugs probably i just\ndon't know of them\num let us know it's it's really early\nalpha\nand we we will uh keep on improving it\nokay cool thanks everybody for joining\num\n[Music]\nyou kept it under an hour which is good\ni'll post the recordings as soon as i\nzoom finishes grinding through it's um i\ndon't exactly know what's going to\nhappen\nbecause on the split meeting because it\ni don't know if we'll get one recording\nor two but i'll post whatever i have\num and see everybody in a couple of\nweeks and we are always\nanybody who wants to demo their fun uh\nenvironment setups or how you guys have\nsolved some of these problems in\nuh it sounds like people have been\nworking on work around the registration\nproblem on their own which makes sense\nuh anybody wants to show off their\nconfig particularly the gcp folks\nwho've been using some of the gcp tools\nthat the flight team is not that\nfamiliar with that would be useful to\nsee\num uh and anybody just dm keith and let\nhim know if you want to\nyou know you want to show one of these\noff i had a i had a request from jeeve\nactually\nwe just talked you know briefly about\nthe reaction\nevents from gcs that you guys monitor\nwould you would you like to share about\nthat next\nmeeting or something it would be great\nfor i think i'm sure most of us\num yeah so so it's it's it's very young\num we basically like wrote it in like a\nhalf a day or something uh we're still\nkind of iterating on it\nbut i'm happy to show like how we're\nsort of like thinking about parsing\nthese things and like using it in flight\nworkflows\num but it won't be like uh you know\nit'll just be like maybe 10 minutes or\nsomething if that's okay\noh that's more than enough so we have uh\nwe've thought a lot about\nreactive pipelines in different ways and\nwe would love to\ncollaborate or coordinate any of our\nwork in any way\nthat you guys are open to okay i think\nit could be generally\ngenerally useful to do something like\nthat\ni have my reservations about doing that\non kubernetes but that's like the only\nresolution\nyeah um you know like we're again we're\njust like\nwe're just kind of starting to think\nabout this now and so um these are just\nlike very early prototypes but\ndefinitely happy to share\nyeah yeah sure i think i'm also going to\nask spotify to talk about sticks and\nlight integration and how they are\nworking through that\nand i think there is also a backstage\nand flight integration in progress\ncool cool all right all right everybody\nsee everybody in a couple weeks see ya\nright\nbye bye"
    },
    {
        "title": "FlyteMeet 2020/07/28 - Extensibility",
        "transcript": "all right cool thank you\nuh so it's a very plain presentation but\nit's mostly\ncontent oriented uh and i threw it up\ntogether very quickly\nthis is talking about uh two aspects of\nflight\nwhich uh one from the contributor side\none from the user side\nuh first we'll talk about the\ncontributor side\nand this is in\nin cases when flight is extremely\npowerful there are lots of things we can\ndo but there are lots of things that\nstill can't\nand uh many many uh\nof our contributors just trying to add\ndifferent features like basically\nif you few people added supports for\ntensorflow\nuh distributed tensorflow support one\nperson added distributed python support\nchannel is actually adding sagemaker\nsupport and so on so\nuh how does that mechanics like how do\nthose mechanics work\nuh and they're within the company with\nat lift i mean\nsome people are uh using bigquery\nthey use in a different way so i just\nwanted to bring up like how what are the\ndifferent modalities of\nextending flight what are the different\nthings that you need to be careful about\nand why would you do\nchoose one over the other and after that\nwe'll talk about dynamic tasks\nwhich is also another very misunderstood\nuh topic and i think\na less advertised one but it's it's\nreally powerful and we will talk\nmore about it okay\nwhy extend flight right uh flight is an\norchestration platform\nthat's what it is and if it is an\norchestration platform it needs to\norchestrate things and infrastructure\nand\nmake things happen for you um\nbut it doesn't want to solve any of the\navailable open source\nuh solutions it doesn't want to like\nsolve those problems on its own for\nexample\nit's not a mapreduce platform it's not a\nquery it's not a um\nit's not a a dis distributed uh\nmachine learning platform it it enables\nall of these but it's not really\na platform on its own that allows you to\ndo one specific thing\nbut for those we have many many other\ntools in the open source\nand they do a great job for example we\nhave spark for my produce we have presto\nbig query directoration of\na navier pic and there are many more\ncoming up\nuh to do querying on large amounts of\ndata and they do a fantastic job with it\nuh and and with distributed\nuh training there are lots of new uh\nalgorithms and and frameworks coming up\none\nsome of them do crosstalk cluster on\nreduce some of them do a centralized\nreduced\nthey do a great job too\nwe found one thing missing and so we\nbuilt it and that's an array task and\nwe'll talk a little bit more about it\nthat rolls into the dynamic tasks it's\ninteresting that that's one thing that's\nnot really\npervasively available besides couple\ncloud environment\num in some cases users just want to\nwrite like\nsimple binaries right like run like a\nsmall\npython code or it's a binary in c plus\nclass or any other language\nand and those were the flight wants to\nconnect\nall of these pieces and it doesn't know\nall these pieces yet\nso we can teach it that hey you can\nlearn about this new piece or that\ndevice\nand it's still a nascent or\na young community we only know a few\nthings\nso people who have been offering and\nadding things thank you\nyou're learning more things every day so\nlet's continue to answer\nokay um\nwhy let's talk about two types of\nextensions\nthat are possible in flight one of them\nis called the lightweight extension so\nyou know these are more like if you're\nfamiliar with airflow they're like\nairflow like operators\nuh you can basically write any code in\npython\nand flight can execute it right for\nflight it's python and some people\nactually\nuh have asked me this in the past uh to\nextend\nthe flight do i need to do the back end\nextension and so on no you don't need to\nit's still running a python code you can\ncall\nany service uh do anything uh\nfor from flight point of view it's\nexecuting a python code\nor it could be java code right if you're\nlooking for java\nbut internally you may\ncall bigquery or it may call emr or it\nmay do\nyou know call any other service or it\nmay\ncall another operator within kubernetes\nnow there are restrictions because that\nname space and the container that we're\nrunning should have permissions to do it\nbut that's about it it's like\nessentially as long as you can give\npermissions you should be able to do\nthat\nuh and there are two examples uh uh to\ndo this\nuh we already have a flight kit one of\nthem is a sensor task\nuh sensor tasks i think i think the\nslides might not be\nshowing because they're in full screen\nmode or something like that oh we just\nno we just see the oops it's our initial\nside\nthank you so much oh that's i'm sorry\nabout it i didn't know zoom works like\nthat\nsorry uh what can i do better with this\nthen\ni think it's the share full screen\nversus sharing window\nmaybe i don't know what this does i\nthink this is better right you can see\nthis now\nit's good enough\ndo you guys see the screen thumbs up we\njust see the title slide or at least i\ndon't\njust see the title slide okay uh and\nit's in the non-presenter mode so it's\nuh it's\nbasically like thank you for bringing it\nup though\nand i'm really sorry i'm trying to see\nmaybe i'll just share this\nhow about now yeah that's that's a lot\nbetter thank you\na lot better okay um i don't know what\nhappens if i make it a full screen\nso yeah the previous slide where i\ntalked about was just a\nhigh level slide uh this slider is uh\ntalking about\nwhat flight kit and raw containers can\nessentially execute any code that's up\nthere\nso if you uh\nsorry i lost my train of thought so um\nthere are two examples available\nuh that we ship with inside kit\nbut that those are mostly examples right\nthe reason why we don't ship with like\ntons and tons of them because first of\nall nobody's watching\nand even if people are writing they are\nwriting it within their\nown libraries and and they don't feel\nlike\nsharing them with other people and it's\nnot required to share right it's like\nthese are simple python\nextensions that you can write and if\nthey are essentially in my terms they\nare\ntheir decorations in python that you are\ndoing\nfor simplifying your tasks but we think\nwe thought sensor and notebook\nare generally applicable and useful so\nthat's why you ship them\na sensor task is essentially when it\nruns it can wait for\nsomething to happen in this case it\nand the ones that we've shipped with are\ntwo things wait\nfor a file to land in one of the\nuh data stores like s3 or gcs or actual\nfile system\num or it can be\na partition to land in a hive meta store\nuh if any of those things happen then it\nreturns a yes otherwise it just keeps on\npoking\nfrom flights point of view it's running\na python container so it's just\nwaiting for that container to exit as a\nsuccess or a failure\nthe second example is a notebook task uh\nso we recently announced these\nwe actually had them for a while you\njust recently pushed them out\nthese allow you to run paper mail\nnotebooks\nwith a slight modification so we\nactually take the entire notebook and\nexecute it\nby injecting inputs into it and reading\noutputs from the notebook\nso let's say if you're a data scientist\nand you prefer writing everything in\nnotebooks you can write everything in\nnotebooks and just\ndrop the notebook into flight\nyou know wrap it with the method outside\nthe notebook and which will execute them\nbut uh again the reason why notebook\ntasks are not backing tasks is because\nfor us we are executing a container and\nit chooses to execute that\nokay uh internally at lyft actually\nthere's a team\nthat runs bigquery etls uh and what they\nhave done is they wrote it in\nin python uh except they didn't\nthey were mostly data engineers very\nfamiliar with python did not want to\nextend the back end\nso they just wrote a small etl stuff\nuh that run from bigquery and\nand they did in python didn't even tell\nus they have not shared it\nit works fine for them right\nthe mental model that i like to say that\nthis is like airflow\nyou could actually bring all of airflow\noperators to\nflight into the python world and just\nrun them\nas is uh the airflow operators are\nlimited where they don't take inputs and\noutputs but you could use the inputs as\nconfig\nyou know and take the outputs as the\nxcom\nso uh that's how you know the\nlightweight extensions\nand how do you do that uh i don't know\nif everybody's looked at the\nflight kit python and i'm just talking\nabout like if python nelson or clip\nmight give a better detail about java\nbut in flight kit python there is a\nbase task called sdk task this is\nessentially mirroring\nthe flight ideal or protobuf concept of\ntask template\nuh it's just some decoration on top of\ntask template\num and uh\nyou can represent any task in sdk tests\nbut when you are extending\nflight kit only to perform new things\nyou should extend something called as\nthe sdk runnable task what the sdk\nrunnable task does is essentially allows\nyou to\num it automatically injects the\ncontainer that you're in\ninto the task context and\nit is the task that will get executed so\nall the input output partially and all\nthat is handled in the sdk\nwhatever task an example for this like\nboth the sensor task and network tasks\nare running about tasks\nand i have links to them the\nhighlighted portions are links to the\ncode themselves and so\ngo and check on it ask us questions uh\nif there is anything\nbut this is a good starting point\nand examples of this is the python task\nis an example it's an sdk runable task\nright it's just a decorator on top of\nsdk enabled\nand it executes python function it's a\nmisnomer it shouldn't have been called\npython\nit's a task it can be any language\ndoesn't really matter\nokay why would you not choose to do this\nalways it's super fast really easy to go\nwith you know you can start today and\nprobably have\n10 tasks written today and go out\nthe reason would be uh and at least this\nhappened to us\nuh potential of resource leak let's say\nyou launch\na data flow job and\nit runs fine but you forget when your\nuser apart\nor maybe the container crashes or you\nlose the machine\nso in any of these scenarios you've lost\nthe reference to the\njob you know unless uh a data phone\nunless you have a an important mechanism\nto retry and recover that job it's very\nhard to reclaim\nthe job uh and and so what happened in\nthe past it lived is we\nkept on running something and that cost\nus a lot of money\nuh in these cases it's a little hard to\nbasically the cleanup process second is\nthere is very low visibility and\ndebugging information directly available\nthrough the platform you can you do get\nlogs you do get metrics\nbut you don't get specialized\nvisualization in the ui\nright yeah for example if you have if\nyou've looked at the start\ntask you get a link in the ui that shows\nyou a link to the spark history server\nor the spark driver log specifically on\nthe spark executor logs\nyou don't get all that when you're using\nuh python\nor extending only python tasks um\nand there is recoverability from crashes\nso that means partial state recovery so\nlet's say assume you launched a job\nuh but the container we lost the\ncontainer and then we\nrecovered the container somewhere else\nuh flight would do that if you have a\ndry set and that's even\ninfinitely trace it so it will just keep\non recovering but\nyou won't be able to just know which of\nyour launch\nunless you build that logic into the\nsystem\nuh into your task and that's quite a bit\nof work\nbut and it's not i've seen most\ncommonly that many users do not think\nabout failure scenarios this way\nand that's where it becomes right\nalso like let's say you want to share\nthis task to a with everybody\nyou could do you could write it in 5k\nand then other people will get it\nbut uh we have to write it to like it\nsecondly you won't have some common\nconfiguration\nuh let's take another example you have\nspark and you want to\nalways inject a metastore configuration\nfor hype so that you can know what\ntables\nyou have uh from i even use them\nit would be very hard to do that in a\ncommon\nplatform-specific way for the entire\nyou know for every user and the entire\ncommunity itself\num and then eventually resource pooling\nis also very hard\nyou can only provide resource pooling\nwithin flight\nat a task type level so for example you\nsay i want only 10 000\nuh python tasks to run that's it but you\ncan't say now\n10 000 by contrast and within that you\nknow\nonly your specific task type to be\nlimited\nthat's not easy we actually can support\nit and we will be working on it\nnext year but yeah at the moment\nall right so then what is the other\nalternative the other alternative is to\nextend flight packet uh and\ni always recommend starting with\nflightgate lightweight extensions\nbecause that's the best way to vet your\nidea\nmake sure that what you're thinking is\nright and\nthe interface is clean and correct um\nand then once you feel a little\nconfident maybe you've spent like a week\nor something\ncome and extend the back end but\nin some cases you have to extend the\nbracket so we will talk about\nuh those cases so the flight package is\nextensible\nit's essentially what you're doing is\nyou're adding\nbits into the flight propeller uh which\nis the first\nengine that runs flight\ntasks you're adding bits to it and\nyou can explain it we have created an\ninterface that allows you to extend it\nthe benefits of that are that you can\nactually teach flight to do anything\ncreate a new\nexecution cluster and execute it uh call\na service\nor run a container and you could just\nrun a container in any\ncontainer environment one of the\nquestions that i think you asked me\nrecently is that\nwhy does the base container look\ndifferent from a part\nand the reason is because container is\nmore portable than a part\nwhat is a very kubernetes specific\nconcept well a container is you know you\ncan print it on mesos or\ndatabase patch or whatever and we have\nseen cases in which kubernetes doesn't\nscale\nenough and we find it uh better to\nactually\nuh schedule the container on to some\nother places\nand these are these have to be\nsimplistic containers because you cannot\nactually\nhave a volume and other things attached\nto it\nand that makes it heavy weight that\nmakes a part heavy\nwhen you write a backend plug-in you do\nget automatic ui visualizations you get\nresource cooling\nuh it it's automatically available to\nall different languages\nin that right kit is written in uh\nand and you can provide default\nconfigurations like recently added\nblanket tolerations you can do that for\nback-end tasks\nand then i talked about the ministry\nexamples of backend tasks are spark\nit actually dynamically brings up a\ncluster and turns it down\nit injects some common configuration\nautomatically it monitors\nand makes sure that the task works uh\nsagemaker\nallows you to launch training and\ninference jobs directly into aws stage\nmaker\nuh so you could do that for cloud ml if\nyou want\npresto and hive we use this exclusive\na lot of lift so we can run directly a\nquery\nin presto and return data frame um\nand we do resource pooling on pressure\nand height\nand then array jobs we'll get into our\narray jobs a little bit more but uh\narray jobs that we run our native\ndispatch actually at least you can also\nrun them on kubernetes and we are\nimproving that support\nevery day but uh we run about\narray jobs of in the order of five to\nten thousand\ncontainers per job uh and\nfor that we have seen kubernetes doesn't\nscale as much so we are making\nkubernetes scale\nevery day all right so what's the\nanatomy\nof building a back-end touchscreen step\none is you actually define\nyour specification for the task and you\ncan do that in any language\nwe we choose protobuf for most of our\nexample because that's the thing that\nwe're most comfortable with it's easy\nwe have to link for it so you guys can\nchoose protobuf but if you don't\nwant to do that you can use open api\nspec you can use your strings\ntribal knowledge either ways right any\nof that is fine\nuh you then you go ahead and implement\nthe back end call line extension\nuh which is built on top of plugin\nmachinery\nand we can walk through plug-in\nmachinery but it's better to\nuh look at that it's available in the\nflight plug-in frequent\nyou will eventually move into flight\nidea we're keeping it there because we\nfind any issues and things we fix it\nimmediately\nwe have not found recently so as it gets\nmore stable\nuh and then step three is actually\ndefining\nthe the thing that the users interact\nwith that's it's like kit\njava or python uh\nthis the reason why we just don't really\nwrite protobufs is because this is where\nthe user delight comes in right\nuser wants to interface with the system\nin a much nicer way\nlike by writing their python code and\nputting a small spark task\ndecorator on it is much nicer than like\nthinking about entire spark and\ndoing clusters and so on so come up with\nmeaningful differentiation within\nthe interface and you can build really\npretty interfaces\nuh for inviting an actual example\nimplementation\nis what i put here is i think by torch\noperator\nit gives you the entire gamut of all the\npieces that you have to implement\nand how did this user went about\nimplementing\nuh the back-end plug-in the\nspecifications the fighttic\nimplementation\nthe example and the docs so it's all of\nit uh and there is a\nif you want to really try it quickly\nthere is a prototype repo called flight\nplug-in example\nit's a standalone uh flight propeller\nback-end\nlike get everything together so you can\njust\nclone it and hack it and start getting\nrunning very quickly\nokay uh so the thing that makes this\nwork is plug-in machinery uh\nit's very simple uh interface it\nconsists of three methods\num handle abort and finalize\nevery time a plugin runs we actually\ncall the handle method handle method is\ncalled\nn number of times till hand will return\nsuccess\nonce it returns success we can call the\nfinance\nif some event happens like another node\ndies or\nuh an externally a user clicks apart\nthen we call the abort method and after\nthe abort we also call the finalize\nmethod so\nthis is the core plug-in interface every\ntype of plug-in interface is defined on\ntop of this\ninterface um what we did is we also\nprovide a very\nsimplified state handling system in the\nplugin interface so for example\nlet's say you call some remote service\nand you want to store\nthe id and a couple other information\nbits of information\nyou just write it as a co-struct and\nreturn it back\nto flight propeller fighter will\nautomatically store it\nnext time when it calls handle it will\nreturn that value so that you can\nreuse it um if you know anything about\nkubernetes operators they are not\nuh exactly once they are at least one so\nthey'll keep on calling the handle look\nmultiple times\nin flight propeller we have optimized\nthat we try to reduce the total number\nof calls\nand we kind of guarantee in many cases\nthat we only call you once\nif you want that and this prevents like\nlet's say you have called the remote\nservice and you've gone to\nquery you don't want to be called again\nand launch the query so you don't want\nto do all of that\ncrazy handling within yourself so we\nhave to\nuh but we still realize that there is a\nmuch nicer\nuh interface that we can create for\nkubernetes uh so we created a simpler\ninterface for kubernetes\nwe just implement even you know one\nmethod how to construct\nthe object and how to analyze the status\nand the third one is uh it's coming soon\nif you want to call any web service how\ndo you simply\ndo it okay if you want examples of\nwhere you can write one all of these are\nones that people have talked about asked\nabout\nso please help us right i will help you\nin any of these\nuh also a desk operator ray operator\nemr big query data flow and they are\navailable as kubernetes operators so it\nshould be really straightforward to add\nall right like uh let's do a time check\nit's 30 minutes um\nso we get quickly i think it will take\nfive to ten minutes more\nto talk about dynamic tasks and then we\nwill open up for questions and other\nthings okay then we're still on\nwe're on a 40 minute level oh really\nremember\noh okay uh all right so then i can\nfinish in five minutes\nso dynamic tasks is a misnomer\nor it's not completely understood um\nthe dynamic tests allow you to alter the\nshape\nof a flight workflow right you can just\ndynamically generate a workflow or you\ncan dynamically generate\ngenerate a set of tasks that you want to\nrun or\nyou can dynamically launch other\nother workflows uh and\nit's simply constructed we are actually\nworking on improving\nthe construction itself like in fact it\nlike there's a delighting experience i\ndon't think we have the history\nlike experience here you're trying to\nwork and improve on that\nthis might be a little light but what\nthis is is essentially you write a\npython function and you put a dynamic\ntask on it\nand you yield the tasks that are to be\ndone\nand the rotate task itself could be any\ntasks\nwe don't care but what essentially is\nhappening in the back end\nis uh we'll talk about it actually after\ngoing through all the examples\num the second case is selectively\nlaunching some launch plans\nuh it's the same dynamic task uh\nyou can it's exactly similar\nconstruction\nbut what in this case i've done is i've\ncreated a launch plan\nuh and yielded the launch plan uh and\nthis launch man was dynamically created\nthis launch panel was created\naesthetically\nso this allows you to selectively launch\nany launch plan and so you can put\ncomplex uh\nconditionals to decide what you want to\nexecute\nand this makes it really straightforward\nto execute\nthe third one is actually generating a\ndynamic workflow\nso workflows in flight as you know you\nhave to write them using the dsl\nand they automatically get registered\nbut if there are some cases in which you\ndon't know the structure and you want to\nchange the structure or add new nodes or\nso on\ndynamic tasks allow you to do that and\nthis is an example of actually creating\na new workflow and\nall of these examples by the way don't\nworry they are in flight cookbook\nso if like snacks cookbook slash\ncookbook has all of these examples\nplease go through them\nbut uh this allows you to generate a\ndynamic workflow\nokay i won't get more into detail about\nthe various use cases why you want to do\nthem because\nwhat we've learned is users have amazing\nsets of use cases and i i can't really\ndocument all those use cases but i'll\ntell you how it works\nso flight propeller when it's executing\na node\nand a node is a is a super concept of a\ntask\nright uh it it encapsulates all\nvarious execution retries and so on so\nwhen it's executing a node\nit it looks for one of three things\nit looks for a success uh with results\nof that note so when the\ntask execution completes it returns\nresults and it looks good\nor it looks for failure and the failure\nthat's how we capture the failure\ninformation in the ui so you get the\nlock trace in the ui\nthat's because we look for a failure or\nthe third option is\nwe look for a success but not results or\nan output but we look for a thing called\nas a future file\nuh and i should have put a link to the\nfuture file specification\nin flight idea but i will do that after\nthat so that allows you to\nessentially tell flight propeller by the\nway i had not done i think i processed\nwhatever i had to but do this now\num and what flight propeller does is\ntakes that\nthat future specification\nand it it follows a format it's\nessentially a workflow specification\nuh and flight propeller dynamically\ncompiles that version\nif the compilation is successful then it\nstarts executing it like a\nnormal nested sub if the compilation is\nnot successful\nthen you get an error at runtime and\nthis is the caveat of using dynamic\ntests\nyou get errors much later in the game\nyou will get errors at runtime\nbecause we don't know what type of\nconstruction\nbut it's really powerful right with\ngreat power comes\ngreat responsibility so uh\nyou we have not advertised this feature\na lot but we will be advertising more\nand more of this because we think we are\nabsolutely stable with this uh almost\n90 percent of workflows at lyft have\nsome sort of dynamic tasks in them uh\nwhich\nmakes us extremely confident that this\nthing works and scales beautifully\nso if you have use cases let us know if\nyou're using it\nthe interface is a little uh i still\nfeel it's not perfect and we are trying\nto improve it\nuh and and that's where flight kit\nenhancement that we talked about few\nweeks ago\nis coming out soon at the proposal and\neverything he even has a prototype for\nit we'll be sharing that\nbut uh a quick overview of dynamic tests\nagain\nor a recap you use dynamic tasks when\nyou don't know\nthe structure of your workflow you can't\ndefine it ahead of time\ntoday use dynamic tasks also if you want\nto launch a large\narray job that means you want to launch\n10 000\ncopies of task or and you want\ndata partitioning between them and you\nuse dynamic tasks if you want to\ndynamically generate a workflow\nall right so these are the three options\nright and and any\ntasks can be a dynamic task and any taps\ncan be the parent\ndynamic task that's my presentation\nany questions that's a lot of content\nit's it's all the slide deck is\nin there but you know\nhow does it work if dynamic task creates\na workflow\nand this workflow has a new input\na new input what do you read like in you\nit's something that doesn't\ndidn't exist before can you happen\nyeah so it will fail compilation so the\ninterface of the newly generated\nworkflow has to match the\nencapsulating tasks interface uh\nand that's like the enforcement that\nhappens at time\nso we compiled the dynamic task parent\nright the node that actually generated\nit with its interface\nand now whatever it generates has to\nabide by that rule\nand so if you generate something that\ngenerates one more\ninput and say oops that doesn't work\ni have a question um so we've used\ndynamic tasks uh\nprenome and you know we've used it\nmostly for\nrunning uh multiple sort of python tasks\nuh one thing that we really like is sort\nof the modularity of like being able to\neven compile\nuh sorry uh compose uh sub workflows\ninto into larger workflows or super\nworkflows\num unfortunately i've noticed that some\nworkflows don't play very well with\ndynamic tasks so you can't like\nyou know do a for loop over um for loop\nand launch multiple workflows from\nmultiple workflows of the same kind\nusing a dynamic task\num it fails with a with a node id error\nor something related um\nis there is there is that something that\nyou guys are aware of and like uh is\nthere a plan to support\nyou know being able to launch multiple\ninstances of a given workflow using\ndynamic tasks\nmoving forward absolutely but i think he\nwas nodding in there to let him answer\nthis question\nbecause he's been digging oh no i think\nyou can you can answer but\nyeah go ahead no no you you this i i\ndon't know if this problem exists i\nhave actually\ni i think this will go away i'm pretty\nsure this will go away with anan's\nnode to node relationship change oh the\nreason is because\nwhen you run the same workflow multiple\ntimes\nthe string id the node id which is\nsupposed to be unique\nends up being not unique and then that\nbecomes a problem um\nbut with the the node to node\nrelationship we're changing how we\ncompose or construct the actual string\nyeah it's it's pretty close to getting\ndone um\ni think we talked about it in the last\nmeeting but yeah alan's\nnot here today he's working on it i'm\nafraid our meeting is going to be ending\nfor us\nlike it or not um can we just restart\nanother meeting and just open up for\nquestions to anybody who wants to change\nyeah i'm happy to just rejoin right this\nmeeting can we do that i don't know\nhaven't tried it but i assume there's no\nreason why it shouldn't work\nyeah so all right i\nthen will preemptively end the meeting\nright now and then rejoin\nthe same link if um\nif you want to follow up with us okay so\ni'm\ntemporarily saying goodbye i hope to see\nyou all in about a minute"
    },
    {
        "title": "FlyteMeet 2020/08/25 - A sneak peak on the new Flytekit",
        "transcript": "yeah this is the major project oh yeah\nuh we are all the startup floor includes\nuh getting data\ndirectly into your jupiter notebooks if\nyou're using jupiter all of that is part\nof this release like basically improving\nthis data flow\nthis doesn't mean we are done with this\nit's just improvements\ni guess in the next we will be doing it\nfurther\num and um\nthat's it from my side do you want to go\nyeah and i'm really sorry i'm not really\nvery prepared today\nuh me yeah i can share my screen\nsure sure um\n[Music]\nokay let's stop too sure\nopen system\n[Music]\num please hold\nyou should be able to share at least\nfrom the permission\num give me 10 seconds\nwe can also of course open up questions\nanything that anybody else wants to\nshare are\nuh just in general questions so that's\nthat's the intention of this meeting\nwe've been trying to provide them those\ntypes uh when we don't ask questions\nlike this\nokay everyone can see right\nsorry i have to restart because um\nit was not set up properly um so we are\nas club uh has pinged me people\nask why the current flight kit looks so\npython to\nie antiquated and the reason is because\nwe were still supporting python 2\ninternally\num when the the project was underway but\nwe are hoping to modernize it\num that brings in with it we have a dock\nthat is like\npartly filled out partly internally\ninconsistent\num so we're kind of going through a poc\neffort\num to kind of flesh out\nexactly what uh what the new experience\nshould be\nand ensure that we don't write something\nthat is not implementable\num so one of so we're still actually\ndeciding some of the\nmore fundamental interactions so if\npeople have\nopinions about this matter they should\ndefinitely feel free to speak up\nping us on the channel and offer\nsuggestions and whatnot\nand point out flaws um so for for\ninstance one of the things that we're\nthinking through is this right here\nwhich is how\nshould people use um\n[Music]\nhow should people declare workflows so\nfor instance\nyou can see i don't really have the\ncurrent model on here but\nuh it's something akin to\nuh this model in the middle\nexcept that it's a\ncurrently it is a class not a function\nwe're toying around the idea of turning\nworkflows into functions themselves\num the benefit of that being you can\nchoose when to evaluate it and it's not\nnecessarily evaluated\nat module load time so uh the current\nmodel\nand downstream here is any either a task\nor a um\nor a just a python function\ngenerally currently you say you\nwhen you call a task you assign it to a\nvariable that variable represents a node\nand then you access\nthe dot outputs on it and then you can\nretrieve\nthe variously named outputs\nwe are seeing if um something like\nuh if this would be more clear it is\nvery\nlike explicitly assigning a\ni guess this is uh\nuh like you can extract the\nactual python native outputs\num in assignment if you want to but you\nhave to call that\ndot outputs on it um and then we can\nalso\npotentially do away with everything and\njust\nignore node the concept of node naming\nat all\nand just let people write native python\nwhich is assigning it to variables and\nthen using them downstream\nthis looks cleaner uh not entirely sure\nif we can actually\nimplement this um and there is also the\nquestion of\nhow do you uh how do you name\nhow do you inspect the intermediate\nsteps i.e extract the inputs and outputs\nof a given node\nif they are not explicitly named so\nthese are the kind of questions that\nwe're trying to resolve\nbefore we dive deeper um as for tasks\nthemselves they're going to look like\nthis\nbasically uh pretty simple like\nextracting from annotations flight\ntypes uh what a task uh tasks interface\nshould be are\nis pretty simple i think we're going\nwith um\nif you we're not going to allow people\nto name outputs\nor rather if you want to name outputs\nyou have to use a typing named tuple\nif you don't name outputs them which is\nassigned an auto incrementing\noutput name so this would be some\n[Music]\noutput zero call it the type system\nbetween python and\nflight idl is mostly consistent one of\nthe things\nit lacks in some areas like we don't\nflight idle doesn't actually currently\nsupport\nnative tuples uh we support lists but\nthere's no\ntuple it can be rendered as a uh\ncustom struct but that seems not\ngreat so we're we're debating from\nthings around that\num one of the things it does do that\nisn't quite doesn't quite have a\ngood parallel in um in python typing is\nfiles so if you think about it if you\nare\nif you return a local file to\nas the output of a task something has to\nbe responsible for\num uploading the contents of that file\ninto the flight durable store which is\ns3\nand something has to create a\nthe the literal version the literal\nversion of a file\nin python is just a file the literal\nversion of a file in flight\nis the location of s3 stored as a what\nwe call a literal blob\nso that brings into it some interesting\nquestions about\nhow and why we're effectively like\nwe have to like tie the typing system to\nsomething that does like file uploading\nand file management so\nwe're we're working these are the kinds\nof things that we're working through\nand ideas that once we have answers to\nthese we\ncan implement hopefully fairly quickly\nbut yeah that's it\nany questions hey uh yeah i think this\nwould be a good time to share with\neverybody if you're not\nif you're not already seen there's a\nflight kit enhancements proposal\nuh which is pretty fairly big it's\nit becomes it has become a little\nchaotic because the amount of uh\nthing that we want to improve and the\nideas around it we have tried to\ndocument\neverything in there um and so\nwe'll share them uh we are looking for\npeople who\nreally are passionate about how to\ndesign a\npretty interface really that's what we\nwant to do we want to design a pretty\nusable\nuh user-friendly interface and that\nthat is also\nif you're about like the intention of\ndoing this is that if you're a python\nuh developer this is natural doing but\nthis is like oh yeah this just\nworks naturally as i expect because we\ncompletely realize that fly kit\nis not natural uh it is very\nit's very forced on and and partially as\neventually we said four years ago it was\nlike it just grew on\nand you're like we should take a step\nback python 2.7 instead\nlet's just get uh python 3 working and\nlet's just improve the interface\nso anybody who has you know\nideas i mean this is the best time to\ntalk about\nhe has done a phenomenal job actually\nhe's he's not\ntelling you as much as he's done he's\ngotten all of these\npocs to work which is uh pretty\ninteresting\num except for the workflow one but the\ntasks and everything are working\nwe're trying to simplify the the\nthe api landscape essentially just to\nlike\none or four four concepts that the user\nneeds to\nuh look at and quickly wrap up on and\nstart using\nuh so uh any any ideas about that\ndoes anybody have any thoughts\nany interfaces that they've seen they\nreally like\nlaughter\nso if you're an opinionated python\nperson now is a particularly leveraged\ntime to make your\nopinions known because once we get\nthrough this\nwe probably won't revisit it it'll be\nbaked into the backer compatibility\ncontract\nyou know going forward so if you're if\nyou care a lot about the\nyou know the aesthetic mouth feel of\nyour python code\nreach out directly to you and make your\nopinions known\num because the you know in big projects\nyou don't do redesigns like this very\noften so it's a very leveraged time\nto to to care and we'll do the best we\ncan\nwith the feedback we have but now if you\nwanna if you wanna jump in and\nmake your opinions known do it soon and\nyou can just talk you can\ncomment however you feel comfortable in\nthe paper or directly with you\nbut he's driving\nthere is a chat right i'll share the\ndocument just in this chat itself so\nthat people have it but\notherwise we should clean it in the\ngeneral conversation okay\njust a disclaimer also like in the uh\npoc process we've already noticed some\nthings that are internally inconsistent\nso\nit's not quite up to date but it's\nmostly updated\nthat's all we have for the agenda\nanybody else have a walk-on i want to\ntalk about\nproblems confusions\nupdates all right\nuh saran i think sorry not muted do you\nhave anything\noh hi i'm leor uh sorry\ndid you want to say something i also\nwould like to\nafter you\noh i'm just gone yeah i put him on the\nspot\nsorry yeah okay\ni'm just i'm just on vacation but\ntoday it's rainy so i'm not on the beach\nand uh\ni can also join that's what it is yeah i\nwasn't here you're supposed to be on\nvacation\nanyways\num one thing that's\nthat's uh that is on our mind is\nthat sometimes the startup time can be\nlong and we talked about how it's not\nappropriate for launching maybe\nthousands or tens of thousands of of\nsimultaneous tasks\num and that it's there are other ways to\ndo that\ni think you mentioned as us now\ni wonder if there's uh is that something\nthat would be interesting to include\nsupport for that some way where you can\ndo things where you want to launch 10\n000 jobs maybe just\nmake a an easy pathway so that's\nto add some built-in support for that\ngreat question i think\noh i should have covered that that's the\nthat's the other part that we're working\non uh\nkatrina was not on that call was working\non that so i said\ni'll tell you what happened so um with\nflight\nfew tenants are we don't want to rebuild\nthings that exist\nin the world use the things as they are\nso for example\nif spark is right for you you spark\nright um\nbut we found and then in some cases you\nwant to fill in the gaps\nthat don't exist for example i think\nwe weren't able to find a very good\nsimple\nparallel map and that's what i think\nyou're referring to like i have a\ncorpus of data i want to run 10 000\ncontainers\ntake this data and push them through the\ncontainers and get a result out\nand that's why we build something\ninternally\nwhich is available in open source uh\nthis is where the concept of dynamic\ntasks started\nit has evolved and it blossomed into its\nown thing uh we're not talking about it\nfor a minute but\nthe idea was that we want to do a map\nover a\nbig list or corpus of theta and\nthousands of containers\nnow we realized kubernetes doesn't\nreally scale\nfor that very well so we started doing\nit in two ways\none for for uh\nuh for on aws if you're on aqueous it's\navailable using\necs which is not eks like the uncool\nversion of\neks uh the ccs uh the good part is it's\nso simple\nthat it scales really well right it's\nlike it doesn't allow you to have parts\nand volumes and all of that funky stuff\nso it makes it\nmuch easier to stay much light weight to\nstart up um\nand we actually use a thing called the\nkeyboard which patch which is on top of\nxcs\nwe get us cues and\nthey work okay right they work pretty\nwell we run about\nmany of our users run jobs in the uh 10\n000 plus\norder and they don't find that but\nuh our l5 team which is the autonomous\nthing does not use\nit as much they use mostly use\naks which is stupid uh and so we've been\nworking with them\nto get the same functionality working on\nkubernetes it's it's not as easy it's\nway more challenging uh even the the\nstate of the art\nin\nsome other scheduler systems are not\nreally\nthey don't we have been we have actually\na lot of internal research on\nhow we try to test them and scale them\nand we fail\nand with flight one thing you'll realize\nwe don't release something that we don't\ni'm not comfortable with it's getting\nright then it needs to scale to our\nneeds and then to your teeth also\nbut we found an alternate mechanism so\nflight comes with a resource manager\ninternally that has queueing semantics\nso it automatically\ncontrols how many resources are used\nfrom\nkubernetes um and then we are\nusing something like cube patch so we\nare aggregating both of these together\nto achieve the end result and this is\none of the things that's\nin focus for this quarter at lift\nso we are hoping that we have something\ni want to say end of september maybe\nearly october\num is basically the\nthe dynamic tasks with part support\nrunning at scale um for tens of\nthousands of\njobs um uh again not time bound right so\nyou have to realize that when you have a\narray of ten thousand jobs there's not\ntime down where you\nwill get you will get essentially quota\nmanagement\nand a slow release and so on so all of\nthat will happen but\nthat's the piece that you're working on\nif there is specific needs on that\nlet us know but i can share that\ndocument i think people\nshare but maybe i'll share it again does\nthat answer your question yeah\ncool cool thanks yeah yeah i'll look in\nthe\ni'll look in the slack channels for the\ntalks yeah\ndon't look in like i just i just think\nagain that's easier\num yeah but that's exactly that's a we\nwe thought that that's a great value\naddition\nto non-aws folks uh because i don't\nthink there is anything in gcp\nthat does something like aws does uh and\naws one is one's\nbatch itself is not great it's kind of\nthere are problems with\nuh at some scale so uh\nwe will introduce that concept\nit is available today you can actually\nrun it it's just don't try to run it for\n10 000 jobs it will not run you could\nrun for like 100 200 we have tried it\nwith 2 000\nand then it works fine but\ncool thanks\nanything else can i\nask what is the use case for running so\nmany things in\nparallel uh that's not how we do\nspotify so i'm curious yeah i can tell\nyou about our use cases but i'll let\nyour\ntalk about his use cases that'll be\ninteresting for me to learn as well\nyeah i'm hopefully uh i'm not\nuh i might i might make a mistake in\nwhat i'm describing so i'll try\nhopefully i don't uh but i think we are\nusing dynamic tasks\nand running maybe on the order of one or\ntwo hundred right now\nand when we compare that with the\nruntime of that same work\nthis is uh processing\nof of things that are in the blood so\nprocessing of\nof the wet labs uh that are come back\nfrom the\nthe results pardon me from the the\nassays that we do\nright so we have to process them and and\nand featurize them and things like that\nfor\nuh for the uh\nthe classification yep so we're\nuh we're looking at doing one or two\nhundred right now and\nuh those are maybe tasks that should\nshould take maybe eight to ten minutes\neach\nso this is uh if i summarize this is\nlike a matching learning\nfeatures thing mac\nyeah it's pretty it's pretty basic data\nmunching with python\nand like i said it's uh you know it's on\nsome of these are\nlong sequences so it there might be a\ni don't know if it's gigabytes involved\nbut there's uh like\ntens of thousands you know a list of ten\nthousand things or something like that\nand you're\nyou're going through it and you're doing\nlots of comparison with other lists\nand looking for differences and it it\ntypically runs eight to ten minutes you\nknow\non a you know i think it's already using\na few\nand there that's specifically the data\nis\nis nicely partitionable right that's a\nthat's a thing that you need so because\nyou could use\ndataproc or dataflow on gcp to achieve\nsome of this\nbut one is cost and the second one is\nit's like a big hammer to solve a\nsmaller problem is that right\nyeah and those might just i think we\nmight just not have looked into those as\noptions i think there are things we like\nabout owning\nthe workflow engine having having more\ncontrol and visibility in\nover it yeah yeah no i think uh you\ncould\nlike launch a job from flight into data\nproc i i\ni but i understand your data so i'll\ntell you where we use it\num\nso there are two cases uh one is an\nunstructured data uh and unstructured\ndata\nmeans basically there is no there is no\nschema\nassociated with that data for example\nimages\nit just block it just fights and if you\nyou could use spark for example\nto run it but spark really doesn't work\nthat well\nwith these kind of data types\nat least in our experience and\nwe don't have a reduced step there is no\nreduce there's just a\nmap you take the data you send it\nthrough a bunch of algorithms\nand you get a transformation so you're\ntransforming the data really\nand that transformation can be done very\neffectively if you just partition\nlet's say a million images run\n100 copies of algorithm\n10 000 each and you're good right that's\nthat's really what the\nsolution is um and so\nbut each one of them may use different\nsets of memory\nand maybe using gpus in some cases\nuh using like maybe more cores or using\nalgorithms that are\nin the c plus plus and go that's where\num at least that lift for autonomous\nand for uh or for some of our\nmarketplace algorithm we use\ndynamic tasks a lot uh and actually\nit's so much use that some people\nexploit it\nright like for example the tiny amounts\nof data could fit on one machine they\nstill run\nhundreds uh it's just that one machine\nmight be slightly slower\nbut our parallel thing runs faster\nuh or the second reason is that they\ndon't know spark\nthat's another thing and we don't want\nto solve those use cases but\nusers find the actual flight api much\nsimpler than sparx and they are like oh\nyou just use it it's so much easier to\nlearn so that's like\nthe abuse but there are many valid use\ncases for which we actually built it uh\nbut you know if there are users who are\nable to get smaller views and\nmake advantage of it and it works for\nthem that's great\nwe are not going to stop that so does\nthat answer your question nelson\nyeah yeah thanks for asking and uh i\nhope that was interesting to hear about\nand i'll check out uh google data proc\nand\ndata flow um and i don't know if we're\ninterested in spark right now i think\nwe've been using\ndas so maybe we would uh do something\nwith desk\nyeah yeah oh yeah we actually were\nthinking of adding dash as a plugin into\nflight and if you guys are interested we\nwould love to\nuh work with you guys there it's\nactually an issue\nso about it cool people keep in touch on\nthat and i'll look for a chance\nto to do that i don't think yeah i think\ni'm not sure what our timeline will be\nfor for solving that problem\nbut yeah no we we might get to it it's\nnot even that much work i've looked at\nit's pretty straightforward\nand then it will be natively available\nwithin flight kit so you just install\nflightgate plus task\nand then it just works\nokay\nnelson\nhey do you have any updates on your\nscala sdk maybe you should do a demo\non your demo yeah i think we should do\nthem again\nbut in two months i guess we would add a\ndynamic tasks and some workflows so then\nthey are going to the demo\nah that's yeah that would be\nlike dynamic tasks i want to see how you\nguys do it don't do a dynamic task we\nshould\ntalk a little bit more about it look at\nflight hit enhancement\ndon't get into the mistake we did uh\nyeah the dynamic task it would just look\nlike a task\nthat can build a workflow but also it\nhas access to inputs from\napi perspective i think in our case it\nfits very nicely\nyeah and that's actually the right way\nto do it so at dynamic tasks we are\nchanging a little bit where\nthis is the like lior also said right\nnow right the dynamic task is the array\nwhich is not what it is dynamic task is\njust a way to spin off another workflow\narray is a different thing itself\nand today they are kind of mixed things\nlike it so\nthere's a that's one of the reasons why\nthe announcement is coming\nwe started with that\nright nothing from my side all right\nclip i'm going to hold you that demo in\ntwo months\ndone it's being recorded it's like a\ndeadline\non development\n[Music]\nno i would just love to see a scala sdk\nthat'd be awesome there is one\nit works also this is just like an\nadvanced version now\nall right all right so\nif anybody watching this recording wants\nto try out\nuh java zk is tried out it works\nyou can run real workforce with it like\ndata for jobs uh\nwhatever you want and if you have\nproblems uh ask me on\nlike yeah i've been trying to run it on\naws as well so uh\ni have had one issue but i will have an\nupdate by this weekend\ni had a quick question um so this is\nabout dynamic tasks and how i've been\nusing them and i'm not sure if this is\ncorrect um but\nright now like if i have a task that\nwant like needs to launch another one\ni've been making the parent task a\ndynamic task so that i can\nactually go through and launch them um\nis that functionality going to change at\nall when we start using\nlike the mapping or is that okay and\nalso\nsecond question have you like right now\nit doesn't add\nthe new task to the workflow graph\num am i like do i have to do something\nelse for that to be included or is that\nsomething you guys have thought about\nadding\noh yeah if you have ui engineers help us\nthat's\nwhat yeah i've never experienced that so\ni'm just\npushing that out there uh yeah so the\ngraph\nupdates uh so there are two views uh\nyou've seen the two views hopefully uh\nthere's a graph view and there's a list\nview\nin the list view all of them will\npopulate right uh in the graph view they\ndon't because\nthe ui part is not done uh but that that\nwill continue\nright what you are doing is to launch\ntasks dynamically is the right intention\nof dynamics okay great\nokay not map is uh is should be\navailable even like static maps will be\navailable after\nthe 5k enhancement that means you should\nbe able to just\nknowing the size of the array you can\njust launch an array as a\nas a task itself okay yeah so that's\nthat's the difference\nbut otherwise you can also launch a map\ndynamically a sparks or dynamically a\nsagemaker job dynamic you can do\nanything dynamic\nbasically that's what a dynamic task is\nuh again and\nand if you saw actually another code\nfunctionality where the\nparent touch can be also anything now so\nthat's weird\ncool yeah i've been using his um dynamic\nsidecar task\nuh functionality so that's been really\nhelpful but yeah yeah we would love to\nunderstand how what are you guys doing\nwith it so that you know we make sure\nthat\nwe do the right things when we\nyeah definitely\ncool so we can\nall right okay we'll we'll send release\nnotes when\nwe cut the september release um and then\nif anybody has any topics they want to\ndiscuss next week or they want to\npresent\nwe'd love to have somebody else present\nsomething that you're working on and\nyou're\nexcited about even if it's buggy um\nyou can just dm me or sign up for the\ndemo sheet otherwise\nwe will we'll talk in a couple weeks\nsound good okay all right\nthank you have a good summer bye"
    },
    {
        "title": "FlyteMeet 2020/09/22 - New kustomize templates coming your way!",
        "transcript": "let me know if you guys can see my\nscreen that happens sometimes you just\ncannot see my screen\nis that the right one okay can you see\nmy screen\nokay cool so again bare bones um\nread me this is something that i did\nwith flight snacks and some of you guys\nappreciated it so that\nhelps me you're probably building better\ndocumentation for deployment\num so first of all what is customized\ncustomize is essentially\na very low\nconfiguration way and very native way of\ndeploying\napplications to kubernetes uh you\nessentially write\neverything as kubernetes ammos and then\nyou compose them\ntogether to form your application so\num so i'm basically you know\nhaving uh there will be a review it's\ngonna be filled out more i'm just\ntrying to write the documentation as we\ngo along but uh\nan example of this is customization uh\ncustomization\nuh or customize in customized the\nmost important component is\ncustomization.amo\nand the customization yaml is\nessentially like a\ncomposer class in any programming\nlanguage it allows you to compose\nmultiple different\nand yamos together to create the final\nartifact\nso it's a pretty long documentation but\nthe cool thing is it allows you to\nreally work with config maps\nand create config maps it allows you to\nspecify\nbase deployments and then modify those\ndeployments like\nlet's say in our base we actually start\noff because we are always deploying to a\nsandbox environment which\nmany times or often times as a laptop\nwe start with propeller having like 0.1\ncpu\nand 100 megabytes of ram and so on and\ni've seen that some of you guys are\nactually trying to deploy that to\nproduction\nit's amazing it works but i'm sure it\nwill fail after\nuh a bit as it won't fail but you'll see\nperformance problems and you will see\nmany problems and so on uh so customize\nactually makes it very easy\nto modify the cpu and memory uh for a\ndeployment\nand i'll just uh show how that is done\ni also realized that um that for example\nif you're using a cloud-based\nsql engine then you want to have you\nknow password\npassed to it and that should be taking\ncare of the deployment itself so i'm\nfixing all of those small little things\nand creating a much\ncleaner deployment system and\nwe mostly be using customize it also is\nthe first class\nsupported uh deployment or configuration\nsystem in\nin kubernetes um the other one is called\nhelm but helm is very heavyweight tell\nme it's like a server to\nrequire and are trying to be required\nand so on while with\ncustomize you just need keep ctl or\nanother command line called customize\nall right so that's a quick overview of\ncustomers\nso in customize initially if you uh\nif you remember the previous setup\nactually maybe i have to change my\nsharing settings\nsorry\nall right so i think my entire screen is\nshared\ncan you still see my screen\nokay and they can silence us\nas a yes all right so the current\ncustomize so there's a folder called\ncustomize under flight\nuh the current plus customize has this\nthing called as base\ndependencies and overlays um\nand i will go through all of them uh\nthere's a slight difference now i've\ngotten rid of dependencies\nwhat dependencies really is is\nessentially all\nthe things that you need to get a flight\ncluster started up so flight when it\nsteps up\nit needs in the back end needs a\ndatabase\nsome sort of spreadsheet database um\nred is is optional but if you have it it\ngives you resource control and so on\num it needs a storage uh\nsystem and by storage here we need a\nblock store\nlike one of the cloud block stores like\ns3 or\ngoogle cloud system or azure file system\nand so on\nand i think we support about five\ndifferent\nstores and\nonce you have all of this you need to be\nable to expose\nthe end points to your user song so\nin the default deployment we use address\ncontroller\nor contour uh it's based on it's a very\nlightweight\nuh layer on top of onward\nwhich is also this project all right so\nthat's what we had\nuh i got rid of it because i think these\ndependencies were mostly for sandbox and\nso i moved them to a sandbox environment\nand i'll show you that so now the new\nthing looks like there's a base\nand then there is an overlay what are\nthe base components uh\nso base components are uh the admin\nservice\nso flight preferences if you remember or\nif you if you've seen the documentation\nflight consists of\nuh primarily four components uh actually\nthere are five components but in the\nback end it's four components\none of them is admin uh which is\nactually the control plane\nthe console which is the ui it's like\npropeller which is actually the engine\nthat\nruns your workflows and tasks and so on\num and data catalog now data catalog is\noptional but\nif you deployed you get memorization and\nyou get artifact\nlinking so when\nthese the base one actually has all of\nthese components individually\nconfigured and you can go into let's say\nadmin deployment\nand you'll see and i'll be adding vbs\neverywhere and i'll be explaining\nwhat each one is doing but this is a\ntypical deployment of flight act\nit says deploy flight time and take the\nimage that we publish every month\nand it's always at the end of the month\nthat we update this image\nand uh there are some volume mods and so\non and\nthat's about it right along with that we\nalso deploy\nuh a doc server uh with flight admin so\nflight admins uh entire api is available\nas a\ndocumentation everything you've ever\ndone\nbut it's available so you guys should\nlook at it\nso that's the deployment same thing we\nactually start a service\nthe service is very simple it's\nbasically saying that\nin in kubernetes you have to use some\nsort of item\nnow you have to but this is the pattern\nthat most of the people are following\nwhen we use some sort of annotation to\nindicate what type of service and how to\nhandle it\nand in this case we are indicating it to\ncontour that this is how you handle the\nservice\nuh and actually we should just just get\nrid of this because this should only be\nfor the sandbox environment\nbut that's admin deployment um if you\njust deploy admin\nwhat's going to happen is you'll get a\nservice and you'll get everything\nrunning but\nonce you create a project we need to\ncreate a bunch of resources for that\nproject\nand that is handled by the cluster sim\nutility and\nin the default case it's run like a\ncompetitor's chrome\num and we we actually at least we don't\nrun it like a kubernetes spawn we run it\nusing a more\nuh stable prong system which is\nmulti-cluster\naware but that's not open sourceable\nbecause that's using something turned on\ntechnology\nbut uh kubernetes chrome works fine for\nthis\nthe other things in base are console\ndata catalog\nyou know ingress namespaces any\noperators\nso flight can be extended as you guys\nknow with a bunch of operators\nor by writing files um\nand and some of you have contributed\nsome operators like\nduplo operators like my culture\ntensorflow\nthank you for the contributions um\ni think channel has been working on the\nsagemaker operator and\nthese are also deployed or available as\ncomponents within the system\nand you have to mark them for deployment\nif you want to enable this\nthis plugin system all right\nokay so you have all of this um\ndifferent parts like propeller is\nanother part\nright yeah how do you really put them\ntogether\nso in each one of them let's open\ntogether\nthere is a customization diamond in the\ncustomization.aml you say\nwhat are the resources that you should\nthis module exports\nin this case we are exporting\ndeployment.aml\nafter that you eventually\ngo into this thing called as i've\ncreated two\nuh composed systems on top of the base\nsystem\nuh one of them is called flight headless\ncluster\nand why it's not headless because it\ndoesn't have console and it doesn't have\nany of the syncing stuff\nthe reason why uh we have this is\nbecause we run all our integration tests\nusing the same system that's how we make\nsure that things are working\nokay\n[Music]\nso in the headless cluster there is a\ncustomizationary ammo\nand what we do is we pick out which\nresources\nand these are not the actual\ncustomization or demo these are the\nfolders which have a customization.yaml\nso you said these are the resources i\nwant to pick up and then i'm going to\nbuild the configs\nfor all of my flight and we'll talk\nabout this in a minute\nand we also now support uh\na secret uh as the db path in this case\nit's just awesome for us it's\nit's plain text don't use this you can\nuse the file based primitive\nread through customization or ammo and\ncreate your own\npassword uh for production deployment\nbut for the\nsandbox deployment awesome source works\ngreat um\ncool uh so\nand this was one of the biggest changes\nuh i've made is all the config is now in\none place so when you want to configure\nflight it's extremely configurable and i\ndon't know how to really document all of\nthe configuration knobs that we have\num and so we are we are actually working\non a utility that will generate the\nentire configuration\nautomatically um once you get a little\nbit of time we will\nget into that but for now this forms as\na very\ngood base starting configuration\nand even decently advanced and i'll show\nyou how i've made it\nso for example if you go to admin admin\nhas\nx number of configuration files all of\nthem\ncome together to make admin work uh so\nfor example\ndbriamo tells you how to connect to a\ndatabase\num then we have uh\nremote data dot dmos how do i deal with\nremote data\ndoing signing and so on uh and how do\nyou start the server\ndo you wanna authenticate cars and so on\nokay uh the more complicated one is\nactually propeller\nit's propeller on its own has a bunch of\nplug-ins\nand the most common one that you might\nwant to use is enabled plug-ins\nuh by default you will enable some\nplug-ins\nand you can add more plug-ins to be\nenabled and these are string names of\nthe plugins themselves\nuh and discovery of this is interesting\nbecause\nit's either in the documentation or in\nthe code in both places so\nthat's how we have to find it and the\nplugins themselves may have\nconfiguration and so you add on the\nconfiguration so for example if you want\nto change the how you log\nhow the logs are linked in the ui you\ncan change it in the configuration here\nand and we are actually thinking of\nchanging this to be a much more flexible\nsystem that's coming\nall right so once you have all of these\nconfigs and you know there are a lot of\nthem i won't go through all of them\nbut you can put them together using a\nconfig map generator\nso we have already created such that\nflight admin expects\ntridiagonal fade to exist and a flight\nadministrative phase is actually a\ncomposition of all of these\nconfigurations\nand if you notice all of these are under\nadmin which i just showed you\nbut couple of them are common so for\nexample if you want to change the logger\nacross the entire system you go and\nupdate the longer dirty ammo\nto be level five and you'll start\ngetting more wordpress logging if you\nwant less wordpress vlogging change the\nlogo\nand the entire system will change same\nthing storage.aml is the\ngcs or s3 configuration in one place\nand if you notice flight admin flight\npropeller\ndata catalog all of them are using the\nsame configuration now you don't need to\nyou can just copy paste and you know\nchange one thing at a time\nif you want to but just to simplify\neverything you use the same\ncommon configuration and each one of\nthem has their own special\nconfigurations\num and data catalogs pretty we like to\nactually\nconfiguration all right so this is how\nyou\nuh generate the headless faster uh on\ntop of it now\nnow i can build something on top of it\nso we can build the single cluster mode\non top of it\nand the single cluster mode is\nessentially\nuses the base as a endless cluster\nand adds cluster sync and console tool\nthat's about it\nit doesn't do anything with the configs\ntoday because we just use the same\nconfigs in both cases\nnow uh you don't really go and deploy\nflight single cluster you have to\nmodify some things in it because flight\nsingle cluster doesn't\nhave a storage primitive today the\nstorage primitive that it uses\nwhich is not going to work um so then\nyou decide whether you want to deploy it\non sandbox gcp eks and more\nas you build them so if you want to\ndeploy\nsandbox what is sandbox sandbox is\nessentially\nall the dependencies of flight and all\nof the flight components put together\ninto one kubernetes\ncluster package as one uh it's great for\ntrying out testing getting started but\nit is not good for running in production\num it is we are running like for example\ni'll tell you\nwe are running a database on one machine\nand\nyou don't want to run that in production\nbecause if that database goes down you\nwill\nlose data and that's not the expected\nright\nso you should replace that with clocks\nequal and see how but other than that\nall the dependencies are in here now\num for sandbox and the way we compose\nthis\nif you open up customization ammo again\nand you say\ni depend on my bases right single\ncluster\nbut i want to add other resources to it\ni want to add\nuh i touch operator and add spark\nand then i want to\n[Music]\ni want to also add these dependencies i\nwant to add redis\nand i want to add a database i want a\nstorage contour and so on\nand then i want to actually patch my\nadmin deployment now we're going to look\nat what\nthe patch is doing before we jump\ntogether\nsorry\nthe patch is essentially taking flight\nadmin\nand adding this one line to wait for the\ndatabase to be ready\nbecause uh we are starting a database\nsometimes it takes a little bit longer\nto be ready so we don't want to\nget into a crash loop we want to have a\nclean startup\nand that's why we have this setup\nbut if you add a init container you have\nto replace all the containers\nright that's how it is the other thing\nwe also do is we set the resources to be\nvery low\nbecause for sandbox environment we want\nthat resources to be very low\nand that's about it so that's the uh\nmodification i've done on the admin\ndeployment i have not modified the\nentire deployment because\ni've not really created how the service\nstarts up and so on i've just modified\nthe\nintentions i want the way it does it it\nfinds\nthe deployment with the name and the\nnamespace\nand the you know the api version that's\nhow it tries to match\nthe base and then overwrites it\nokay the other cool thing that you can\ndo with customize now\nis you can actually create\nyour config and what in this case what\nyou're saying is\nbecause i dependent on the dependent\nalready on the base\nflight single cluster i'm getting a\nbunch of config from that base\nbut i want to add some of my own\nconfiguration so what we what i did is i\nmodified the db.aml in this case\nto use the local database uh i also\nmodified the logger to be more verbose\nin sandbox and i added some like uh\ncluster sync utilities\nfor uh you know spark so that means\nwhenever somebody creates a new project\nincluding export roles service account\nand so on\num and i enabled a couple more plugins\nand i added that utilities and so on\nso once you do this what it's doing it's\nfor each one of them uh it's trying to\nfind\nthe name of the config element\nand overriding the same name\nand the behavior is called moderate in\nthis case so for example\nin the previous uh example or the\nsingle customer i showed you that there\nis a file called any good plugin i\nrewrote the same file\nand it will come and i edited the\ncontent\nnow let me just go and find that file\nand replace it and create the new\nconfiguration\nall right the other thing is we will now\nbe releasing all the images\nin the stock layer uh this gives us\ncontrol in the future to\nslowly release out uh like probably\nonce every month or maybe once every two\nmonths for gcp\nand bks while keeping sandbox the most\nfresh\num i don't we don't have plans to do\nthat yet because we are always trying to\nbe stable every month but\nmaybe there is something that we might\nhave to roll out slowly\nuh and also if you push five time into\nyour own\ninternal registry you can just overwrite\nthe images here\nand it'll work all right so\nuh one last thing that i want to\nshow is yeah i actually\ni actually did this uh i\nhanik actually wrote a wonderful gcp\noverlay and i modify\nit to work with this new system\nand it looks much cleaner uh so the gcp\noverlay i have to get to modify all the\ntext but\nessentially there is a customization on\nammo as explained which actually\nrelies on like single cluster because\nit's a single cluster board\nand then adds data catalog and our\nservices\nfor the monitoring stuff and patches\nadmin and data catalog and propeller it\nmostly updates the memory and cpu for\nall of them\nthe other thing that i've done is also\npropeller i've enabled all configuration\nand propeller\nto be the high performance configuration\nthat we have within lyft\nbecause i realized that we didn't have\nthat enabled and it was running on\nlike four workers and\nthere was no cash the cash size was very\nlimited and so on\nso all of that has been modified um\nwhat this will so if you build\nthis customized configuration will it\nwork for any gcp environment it won't\nyou still need to modify it to connect\nto your gcs\nyour database and so what onyx has done\nis essentially added\nto do's everywhere in the configuration\nuntil you can go into\nfig and you can go and modify let's see\nthe comment\nand open up the storage.aml and he's\nwritten up\nto here say replace this with your\nbucket or replace this with the project\ntype\nbut you just have to modify this and it\nshould be able to apply to any gcp\nenvironment\nuh and all of these are the new things\nthat i've added recently\nto the performance okay\nuh and so is for the database uh\nlet's quickly open up the database i\nthink in the database\nwhat he's done is said okay let's find\nfind the password here\nand instead of using the default\ndatabase we drop c code box\nhow do you build all of this um so\nin uh if you clone flight repo there is\na make customized command\nthat will build all of the customized\noverlays uh\nother option is just to brew install\ncustomize or\napt-get install customize any of your\ncustomize and just\ngo to the folder the overlay that you\nwant in your chrome repo\nuh and find like it should have a\ncustomization.ammo\nso in this case maybe the sandbox right\nand you can just run\ncustomization uh customize build\ndot and then we'll find the local\ncustomization.dml and build the entire\norder and the final build looks\nsomething like this this is the final\nbuild that it looks like it's one\nbig gigantic file that's putting\ntogether everything\nso let's say you find a bug you did\nsomething wrong you could just always go\nand look up this file\nlike for example this is a configuration\nand this configuration is for i think\nfor admin\nso it's putting all of your configs\ntogether and you\nlet's say you modify the logger for the\nammo and you wanted to check that just\nopen this up and check it\nand that's like an easy way to do it uh\n[Music]\nand so yeah so all the config is in one\nplace everything is\njust in one place that's what's\nbeautiful about customization\nit is kind of limiting though you cannot\nreally\nremove things from the base\nthere's no deletion primitive available\nin customization so if you want to do\nsomething funky\nlike if you want to deploy add me to a\ndifferent cluster and\nuh then it's better to start off with\nthe base component\nand you use and build your own overlay\nand for that customization actually\nallows you to reference\na remote git repository um\ncustomization on ammo as as a base\nand and it will automatically i don't\nknow how it really does it\nit probably uses the goget module in\ngolang\nso yeah it works pretty well that way\nand you can paint to a specific version\ni think freedom has already done that\nfor example um and and so any mutations\nover here won't really affect you\nbut so that's a hopefully that's a quick\noverview\ni'm hoping that in the next month or so\ni'll have this or not next time next\nweek i will have this out\num with the documentation and you should\nbe able to\ndeploy it to your environments i would\nneed volunteers who can help me test it\nin gcp\nthat would be great i don't really have\ni have a gcp account now so i might test\nit but if you guys can\ni'll be i'll be setting up a gcp cluster\nthis week\noh that's awesome that's good good\ntiming yes\nthis looks great uh the to do's make\nsense yeah thanks for the the overview\nof the system\nthere's so many details yeah it's it's\nuh i realized that you know\nuh not a lot of people uh probably have\never used customization\nuh or customized so it's a\nsimple tool but it can get hairy so i\nrealize\nevery configuration\nyep that's let me stop sharing my screen\ni lost it somewhere\nhoney's uh i saw a pull request from\nhoney today\nit says like use later overlay for gcp\nso uh\ntoday we deployed like uh our\nour flight cluster with the latest uh\ngcp overlay\nyeah yeah but it would be good to\nget more texting from oh yeah i think\nyou guys have your own right internal\ndeployment but i think this the\nconfiguration from here you should\ndefinitely look at an\nupdate uh and that the configuration\ntool is going to be one of the things\nthat we work on\nso that you can just modify that pretty\nquickly\nyour flight has a lot of configuration\njust a lot\nthat's what makes it flexible and you\nknow scalable but\nit also makes it really hard to keep on\nimproving it\num unless you read through the code at\nsome point you have to look at the\nconfiguration options available\nand then update that configuration but i\nhave now started off with a very\npretty high quality configuration\nokay it mimics our production\nconfiguration\nall right so we can do q a or if george\nhad one agenda item before we\ndo the q a\num we wanted to see if we if you guys\nlike this bi-weekly cadence or do you\nwant to move to a\nonce a month cadence and either way it\nworks with us\nthat's actually a ball\nyeah i'm i'll bi-weekly seems nice to me\nsame all right\nso do you get value from it right it's\nnot like hey these guys are just showing\nup and uh\nno i think it's really important to like\nhear what you have been working on\num and we get like context on different\nthings every two weeks which is cool\nthat's awesome yeah uh yeah i would like\nthat if you guys have\nuse cases and things like that would\nlove to you should showcase them\nuh in this and that really helps because\nnot a lot of people join these meetings\nthere are many people who are just\nseeing the video after the fact and\nprobably use cases help them a lot\nuh sure then we'll continue with this\nlet's do q a any questions\ncomments\ni also see new people i i don't think we\nsaid hi to everybody\nso uh i can introduce for freedom uh\ni'm leora i'm one of the software\nengineers at fremont frenome\nas is brenda uh and stephen\nand will uh was a tpm on our team\nand greg d greg junga is one of our new\nteam members of reno\nwow so it's awesome we are\nwe are in force today that is great that\njust\nis amazing actually lyft has very little\nrepresentation today somehow\none thing that that's on our agenda for\nthe the next quarter or so\nis um some of the\nfda regulation and we know that's\nsomething that's that's\ninteresting to the flight team and so\ni'll be\nthat might be something that i can i can\nhave some show and tell on\nor or just something to share on\neventually\nprobably not the next meeting but uh at\none of these meetings\nyeah uh yeah i think please bring it up\nthis is\njudgement yeah i was just saying that we\nwent through\na similar process at lyft with hipaa\ncompliance which is not exactly the same\nbut has some of the same\nas a bunch of overlap so we'd be super\ninterested in what fda\nsay and particularly in differences that\nthey had between hipaa compliance\nso that would be super useful from our\npoint of view\nyeah like our flight internal deployment\nis\ngdpr and hipaa compliant so we can\nshare all that information\ncool um yeah i'll uh i will reach out\nwhen i'm ready to\ntalk further i think this week i'll\nfocus on the setup but it will be it is\non the\nit is on the agenda awesome\nyeah um any other q a\ncomments\nuh hey nelson yeah are you talking\nyeah it's uh can you hear me yeah\nokay uh we\nin spotify captain pc integrate we have\na\nopen source scheduler um\nwe our data engineers\nuh are used to our stack\nso some relying something like a chrome\nschedule is\nnot enough for\nour use case or what our users\nare used to do so\nuh we are close together our\nscheduler running with flight\nwe are already triggering launch plans\nuh\nwe are our schedulers have their own\nstate machines so we there are some\npieces of the state machine that we\nneed to for example when we hold\nin in the sticks how to call the system\nwe need to propagate that to\nto fly there to terminate the flight\nexecution so\nwe are facing that and\nonce i've done i can show here\nwhy what the value that we see\nin this system and why uh\nhow or people use it yeah i think\nthere are a lot of gcp users so you\nshould present it for sure\nyeah now this is like a a very good\nobservation this is\nheavily based on gcp so\nuse data source something like we plan\nto\nmove away in the future but so right now\nis using datastore and bigtable\nnelson is it open source of bull it is\nopen source\nit's already open source\ncontributions you guys are making are\nthe contributions you're making are you\nplanning to open sources\n[Music]\nall the development is happening\nso this just to give a little uh an\noverview to everybody this\nprovides backfill and all that\nfunctionality\nout of the box won't be with flight with\nflight plus sticks\nessentially will give you backfills\nscheduling\nuh observability of the schedules like\nwhen we say schedules here it's quran\nlike schedules and and more things\nso this is awesome and i think it works\nwith backstage as well right\nyeah we have uh backstage is our\nweb page for where we integrate via\nplugins uh different part of our stack\nso that's something that we still we\nhave our cli\nand we have a backstage plugin but that\nbut say plugin is not\nopen source because it depends a lot of\nuh\nspotify specific infrared so it's like a\nbut these things uh you can use it with\nthe cli\nand\nthat's awesome yeah and uh running it\nyou you will see we're gonna hit our zoo\nmeeting\nokay but i was saying\nyeah zoom is all the developing is\nhappening in the wild\nokay i think yeah zoom has this\narbitrary illustration of 40 minutes\nuh any other questions if you want we\ncan log back in onto the same thing or\nwe can call it today\nyeah i guess not that i\nalex yeah i have to run so i'll say\ngoodbye thank you\nbye um bye everybody thank you for\njoining and\nyou know stay safe uh hopefully get more\npresentations next time from you guys\nthat will be awesome sticks and\njava sdk and free now so cool thank you\nokay guys we'll see you all next five\nweeks all right\nbye bye everybody bye guys"
    },
    {
        "title": "FlyteMeet 2020/10/06 - Deep SageMaker training integration on Flyte",
        "transcript": "hi uh give me one second\nsorry i'm trying to figure out how do i\nshare my screen with\nzoom\non the main window there's a green share\nscreen icon\nyeah um\nbut i cannot select my chrome\nfor some reason\nuh\nis that\ncan you go first sorry i need to it\nseems like my um\nprivacy setting is blocking me uh let me\nreconnect\nuh real quick\nsorry no worries i already went we can\ndo a q a till\nyou're ready any questions any\nsuggestions on\nadvice on stuff or anybody wanting to\nhelp\nwith some of the things\noops i think trying works nobody has any\nquestions\nyou can unmute and ask by the way\ncan people see my screen\nyep yep cool all right\nuh let me see\nall right um so hi\nmy name is chang hong today i'm\ndemonstrating\nhow our recent addition\nto flight and flight kit\nso basically uh we recently added the\nfunctionality to\nenable users to\nwrite machine learning jobs\nand run it on sagemaker\nso we call it sagemaker training job\nin the previous so i have actually\ndemoed a similar functionality\nuh a couple uh meetings ago\nuh but at that time uh we were doing\nwe were leveraging we can we could only\nleveraging sagemaker's\nuh building algorithms uh\nso the recent edition adds some\nfunctionality\nabout writing custom training jobs\nand run it on sagemaker\nso as you can see in my uh in this\nbrowser\nin this github page let me do larger\nyeah so this is the\nuh our new support for uh composing\na custom training job on sagemaker\nwhat you need to do is very similar to\nthe normal python task other type of\ntask\nyou you define the input you\ndefine the output and you define you\njust add a custom training job task\ndecorator to your function definition\nand then in the function you can just\nsimply\nuh you know just write normal\nuh say tensorflow code or anything like\nthat and then\nyou only need to just uh\njust the interfacing between the tasks\nuh that\nyou only need to take care about take\nextra not instructive but just\na little bit more care about uh your uh\ninterfacing from the flight interface\nand\nyour own original tensorflow functions\nso what i mean is something like setting\nthe output\nor something like that these are very\njust very small pieces\nso you can see that it's very intuitive\nto define this kind of code\nuh and then to run it on stage maker\nwhat you need to do\nis to just launch it through our ui\nor your flight cli or\nanything like command line uh\nand we can say hey my\ni want this input i want this training\ndata\non this validation data and then i\nlaunch it so it will take a while\nand also i'm experiencing some\ninfrastructure error so that's just take\na look at the previous\nexecution so this\nin this execution you can see that the\ncustom training job finishes correctly\nand then it will provide you a link to\nthe\nactual training job on sagemaker\nso on this stage maker you can see we\nare\nuh we are using some some tricks to\npassing the\nuh the input and uh\nall sorts of information that flight\nneeds\nbut uh it's very uh\nintuitive from here and then you can\njust kick the log\nand you know take a look at what's\nhappening\nuh by just looking at the log and\nwe can see that everything like here's\nthe\nlet me enlarge the font you can see that\nthis is the actual tensorflow training\nhappening\nand then uh it finishes successfully\nand output uh the uh\nfinal output uh multiple outputs\nanyway whatever however you want to do\nit\nyou can do it so that's one thing\nthat's uh just uh that's just\ndemonstrating uh it is\nuh uh also very intuitive to write this\nkind of job\ninside flight and you this saves you a\nlot of time\nuh say building um your\nown uh docker container your docker\nimage\nor like launching uh using you know hey\nyou don't need to use\nuh sagemaker's um api\nwhich is sometimes a little bit\nconfusing but\nuh so with this you can have also good\nintegration with other type of tasks\nso uh yeah this is custom\ntraining uh one more thing\nuh what we want to show is we can\nactually\ndo distributed training\nuh so this is also this is the newest\nedition\nthis is the newest feature and then um\nso you can as you can see the difference\nhere so this\nis uh this is a different uh\nthis is a different file this is for\ndistributed\ntraining and then we are also still\nusing the same decorator\nsame input same outputs and you know\njust the same function uh definition\nbut what would the the only thing you\nneed to do here well\nyou have to uh we'll talk about that\nlater but uh\nfrom flight side the only thing you need\nto do is\nuh to increase your instance count\nand change your instance type if you\nwant to use gpu for instance\nand also you add this distributed\ntraining\nprotocol to your function to your\nfunction decorator\nand inside this custom training task\nuh the the code is basically\njust like it's the code that\nyou write for use\noh sorry just to uh at uh\nsomething that we are supporting\ncurrently we can support\nuh mpi based message passing interface\nbased\nuh distributed training and the\nframework that we are relying on is\nuber's\nuh horror vote so uh here you can see\nthat you can directly take\na horrible a horrible enabled code\nand put it inside this task\n[Music]\nand so basically you are just adding\nsomething like\nhpd.init and\nyou just wrap your optimizer into\nthe hvd horror volt optimizer\nand then adding some extra\ncallback and also when you are\ntuning your uh say uh\nhorrible your your training process you\nalso\nneed to do something like this but this\nis entirely horrible this is nothing\nthe the changes here are just you can\njust take\nyour horrible tutorial and just do it\nyou don't need to worry about flight\nhere and then you just\nyou know you just take this code put it\ninto this task\ndecorate it with our decorator\nand do some interfacing and you should\nbe able to just\nlaunch your task with distribute uh just\ndistributedly\ntrain your uh your task and\njust to demonstrate uh the uh\nhow quick like how what's uh\nhow effective this is so uh\ncurrently we can see that the same task\nrunning for one thousand epoch\nuh it's just an mnist training training\nan m\nnext uh using uh one thousand epoch\nand so this one this one gives you\nthe result uh it does the trending\non two uh p38 16x\nlarge machines\nand you can see that\nuh that the training time only takes 23\nminutes\nand if you take uh if you uh\nchange your instance type to a p3\n2 to p3 2x\nsorry p32x large machine\nthen it becomes an hour\nbut originally when you use only two cpu\nbased machine which is this\nexecution uh you can see that it takes\n17 hours to finish it needs 17 hours to\nfinish this job\nso enabling this uh distributed training\nwe think we can give\na lot of benefit and a lot of\nyou know uh efficiency improvement\nto the user and and\none thing to note that is this\nthe difference between these three uh\nexecutions is just you know uh\nyou only change the instance type you\ndon't do any other thing like the same\njob just apply\nyou can just try it on this instance\ntry it on that instance just with very\nuh it's very easy process\nso yeah so uh and if you want to take a\nlook\nat the the log and the metric\nchangemaker gives you that uh it gives\nyou the gpu utilization\nthe cpu utilization the memory\nutilization and also\nin the log we can see that\n[Music]\nso it provides a log stream from uh one\neither of the uh the instances but most\nof the log will be sent to the master\nlog\nmaster node and it will be output here\nand uh you can see that\nuh each of these so the the uh\nthe beginning this uh this\nthe square bracket shows uh which\nno which rank which in this case which\ngpu\nuh this log log line comes from\nso which yeah so basically\nuh you can see all of the information\ninside the\ncloud watch and uh sagemaker ui\nand then uh yep\nit should be very fairly intuitive to\nmigrate your job to sagemaker\nthat's it for me\nthat's awesome chang i think uh just to\ntell everybody\nsagemaker is an alternative service you\ncan always use five dodged operator\ntensorflow operator and they will look\nthe same way\ncurrently flight doesn't do the\ndistributed data management\nthat's like a longer term plan of how to\ndo it but\nuh just making it easier for users to\nlaunch\nand have an intuitive interface what's\nchannel has been working on it's really\nhard\nthank you channel\nthank you\ni think unless we have any questions or\nany updates from the freedom folks or\nthe spotify folks we might be through\ntoday\nanybody glad you have any news\nso sorry you have any uh how's the poc\ngoing so soren works for usu\ndoing poc for flight how's it going\noh we had to uh postpone\na little bit so that's why i don't\nreally need anything to\nreport but we're still still planning to\nstart to start uh yeah maybe\ni don't know october hopefully at least\nnovember so yeah it's just\na few other things that come up in\nbetween but we're still\nall right yeah i guess\nnext time uh he and i probably we can go\nover\nflight kit enhancement and i think i'm\nputting v on the spot but\nwe'll go over like the how it looks the\nnew stuff\nit won't be done but you haven't go over\nokay\nthat's exciting um i just want to say\nthat uh so we we\nmigrated to using all the new customized\ntemplates that was pushed to lift flight\nand it's\nlike it's reduced our code like quite a\nbit so i'm pretty excited about that and\nthank you for doing all the work on that\nkid oh thank you chief for trying it out\nif you can if you find any bugs in the\ngcp template please\nuh you know push it back upstream\nthat'll really help us we'll do we're\nnot exactly using the the gcp\nuh we have some like uh a few slight\ndifferences so we're starting from\nbasically\nuh uh single single yep single\ncorrect yeah yeah that's that's actually\nthe intended idea but\nuh i would love to understand what's the\ndifference between the gcp setup that\nyou have and what we have\nbecause probably what you're doing is\nmuch more\nlike what it should be so let us know we\ncan update it\nwe'll do all right\ncool if no other questions and i'm good\nfor my site\nokay all right everyone\nthanks for coming we'll see you in two\nweeks and as always everybody wants to\ndemo anything they're working on\nwe'd love to see it thanks for the demo\nthank you all\nokay thank you thank you goodbye thank\nyou bye\nyou"
    },
    {
        "title": "FlyteMeet 2020/10/20 - Flyetkit improvements, UI Updates, Array Jobs",
        "transcript": "i'm going to start recording now sure\nall right\ni will start sharing\n[Music]\neveryone can see\ninteresting uh okay so um i wasn't going\nto do slides but then\nsomeone made me do slides yesterday so\nthis is all this is as good as it gets\nin\na couple hours um so\nuh i think everyone here is familiar\nwith flight kit and its\nconcept actually so i'm not going to go\ntoo much into the background but\nbasically this is what it looks like\ncurrently up top\num i didn't include launch plans in here\nbut it\nshouldn't be too much different um so\ngoing over tasks\nand workflows and then just showing some\nof the uh\ndifferences and why we didn't like the\nthe old style i guess so the old style\nof stuff of uh declaring tasks if you\nlook at it like it\nit's a lot more heavy-handed so this is\nnot i don't want to\nindict the the original flight kit the\ncurrent flight kit it's\nit was written for a different era and\nby a different era i mean python 2 which\nis not typed\num and so it necessarily has a lot more\ncrux to it when you bring in native\npython 3 typing obviously things will\nsimplify a lot so this is our effort to\ndo that\nso this is the old one some of the\nthings we didn't like about it lots of\nweird imports\nyou have to have a you had to take a\nparameter in the front\nto get access to flight\nkit utilities like logging and sets\nand the unit test was a little bit\nstrange because you had to call\na specific unit test function that ended\nup just calling the original function\nso a new one we've made our best effort\nto\nkeep the task as a task\none of our guiding principles was that\nwe should be able to\nstep completely out of the way\nand let the user just have at his or her\noriginal python function so if you just\nrun t\n2 here like it just runs the function\nwe have also we're also auto naming a\nlot of things\nso inputs you have to name outputs will\njust be named\naccording to some pattern that\nincrements the generator\nso out underscore zero up to\nn minus one or whatever um or\nif you want to i don't have an example\non this slide but um\nif you want to name it you can use a\ntyping dot named\ntuple which will allow you to both\nspecify the names and as well as the\nenter types\n[Music]\nand yeah that should be it differences\nin the workflows\nuh the biggest change is that previously\nworkflows were classes\nnow we have made them functions we've\nmade them\nfunctions um primarily because\nwe wanted a way to unit test\nworkflows themselves and\nthe loading of classes tends to happen\nonce when the module is loaded you can't\nreally control\nhow often it is rerun effectively\nwith different inputs these inputs are\nalso a little bit because they have to\nbe\nclass static variables they're a little\nbit\nthey look a little bit strange they're\ndeclared like this in line\num and one of the other downsides i\nguess i didn't mention is\nyou can't have like\nthis you have to assign it to a variable\notherwise it won't get picked up in the\nclass dictionary\nso in the new one we've made it a\nuh okay then out of this we've made it a\nfunction functions can take\ndefaults and the return type it follows\nthe same thing if you you can use the\nname\ntuple or you cannot use a name tuple and\nwe'll just auto\ngenerate a default name for you um\ninstead of handling with\nhandling nodes so tk here\nis tk1 here is a quote unquote node\num the outputs of which you access like\nso\num and the new style\nwhat you get back are they actually\nlike in python the actual values\nthemselves\nand local execution is not quite\nactually the values\num uh because we want to support\npatterns like this\nequals whatever right\num stuff like that but\nthey are when you just run\nthe my wf as a python function\num everything should still just work\nwhich is again one of our goals um\nand yeah so this is this just means\npython can be\nour engine because python will do things\nin the correct order naturally\nrather than having to understand node\nstructure\nand write like a mini propeller inside\npython\nuh this is something that we're still\nworking on um\nso this is not yet implemented in the\nsense that we don't have an end-to-end\nexample running um but we're also still\ndebating\nthe merits of this and maybe a couple\nother\num user experiences like syntax\neffectively\nso uh the old style dynamic tasks you\nyield things\nand then you set the outputs in the in\nthe same format\num the new one hopefully it's a little\nbit easier to reason about\num it still generates effectively a\nin a workflow a sub workflow but we've\ni guess kind of hidden that from the\nuser and\nwe suspect this is a cleaner interface\nfor\nusers to reason about and again this\nworks\nin local execution\num\nso just uh roughly as\nthese were the goals that we're kind of\nworking with um\nwell a subset of the goals is just what\ni came up with\num local workflow execution we've\ncovered that\nuh reduced barriers to entry so like\nyour if\na properly annotated python 3\nmodule should be very very easy to\nconvert\nto flight tasks somewhere close\num tasks at least workflows will take\nsome but you have to know what a\nwhat a workflow is and some limitations\naround it\nextensibility so we found that the\nplug-in\ndevelopment cycle for the existing\nflight kit was a little bit\ntoo onerous for new contributors and we\nwanted to make sure it was\neasy to extend and um\nmore broadly we wanted to make sure that\nit was easy for contributors to\ncontribute to the code base at large um\ni for one found\nthe current flight kit to be very\nconfusing\num partly again just because it has to\nbe\nto compensate for the lack of typing in\npython 2.\nand the goal with all of this is\nhopefully that the new\ntasks will work with the old style\nclasses\num that's going to be like a shim layer\nin between i\nsomething i think um like two levels of\ndecorating effectively\num you decorate like you have like the\nnormal\ni think a new style of python\ntask and then you decorate that again\nwith just to convert it to like the old\nstyle that uses that can uh\nthat uses the old sdk types and values\nand can interact with the old style\nclasses\nand then there's some questions there\nabout whether or not\nsubworkflows and stuff would be able to\nbe translated as well\npossibly um we have some of the simple\nexamples running\non um\nlet me show you first so if you\ncan take a look at one of the goals also\nwith our alpha release which i'm not\nsure if one is coming but\nwe are going to rewrite or right\nalongside the existing cookbook\nall the examples in the new style just\nso users have a reference for how to\ntranslate it and how things might work\nin the new style\nand these are just currently some\nexamples\nuh basic tasks we have running\nuh basic workflows\nslightly more complicated workflows to\nuse\ni mean i think glove this is something\nthat you were complaining about\nuh not quite seeing um\nso this does the binding collection\nthing\num and self workflows so\ni think the the ui is still a little bit\nfinicky on\non sub workflows um which is where\nuh yes okay so\nthis is one of the things that we had to\ndo because we still haven't resolved the\num\n[Music]\nthe the node name conflict so because\nall our nodes are already generated\nthis would produce a node dash zero and\nthis would also produce a node dash zero\nso those two were in conflict\nso you had to override the name um but\ni think the graph doesn't quite look\ngreat because one of the notes is\nmissing but\nuh uh the the subwoofers are here so\nthis is where we're at right now and\ni think i'll let kaithin give a\ntimeline estimate on when when we think\nan alpha might be\nready\nyeah i think\nso our goal is as you mentioned\nis to most importantly simplify\nusers development cycle and we are doing\nthat in\nin three ways one of them is\nessentially reducing the cognitive\noverhead that you\nhave when you're using flight in the sdk\nitself so that's\none of the things that you showed that\nyou know you just import one thing\nevery task type is the same at task\ndecorator\nthat's it we automatically\nunderstand using strings or types of\nparameters that you pass to the\ndecorator based on that we will figure\nout the type of task and so on\num very like\nso from the user's point of view it\nshould be very straightforward to get\nstarted with\nsecondly the user should be able to run\nit completely locally\nnow there are caveats over there but if\nyou are interacting with a\nhive or a presto or a bigquery table\nrunning that locally may mean you have\nto have the capability of actually\nquerying those tables\nso in those cases we want to support\ncomplete marketability so\nflight the default application just\nprints the query\nand says hey this is the expected query\nhow it will look but it doesn't do the\noutput\nautomatically so it would be\nthe users the users will have a hook\nwhere they can just say hey\nin case you call this method or this\ntask here's the output\nnow they could also write actually a\nplugin on their side only that allows\nthem to query bigquery\nuh for example but we don't want to take\non to that because\nthere are security considerations there\nare\nmaintenance overhead for such things\nbecause these are libraries right like\nwe have to fix them all the time make\nsure things work\nwell so it doesn't work really well with\nthe flight model of like having\ncontinuous\nextensive grid and flexibility in the\nback end uh\nbut it does give the user the capability\nof having sample data sets of running\nthe end-to-end flow\nthe third thing that we intend to\nachieve with this\nis that um the\ncontributors and these are contributors\nwho probably don't know go\ndon't want to extend the back end just\nwant to write things in python\nuh allow the many of the existing\nframeworks out there\nthey should be able to do it like a very\nairflow-esque style\nsimply extend the class write one method\nand you're done\num and that's the capability that this\nsdk already broke so these are the three\nprime goals that we want to get to um\ni want to say we are very close to\nachieving those goals in the\nstyle that we have implemented the code\nin\num one of the uh things that we have to\ndo is\nfeature complete but for alpha we may\nnot be 100 feature completely\nor 80 feature complete um\nbut along with that we will have to make\nsure that you are able to interact with\nthe older\nworkflows like older tasks and new tasks\ntogether so\ntogether i think that might take up to a\nmonth or more\nfrom now to get that alpha out uh it\nmight be less but i would\nsay a month um\nand besides that this next part that we\nare tackling which is\nindependent of this track is to\nreduce the total iteration time from the\ntime of writing code\nto getting it right running on robot and\nhopefully katrina will demo that next\nweek the next time\nwhen we meet and that basically\nthe idea is to reduce that type less\nthan five seconds\num or even lower so\nbut that's the goal that we are going\nwith uh\nso together i think that one might land\nearlier so keep\nkeep your lookout for that and\ndefinitely attend the next\nmeeting to understand how that's going\nto work this one will follow up\nand together we feel that they will\nempower the python developer\nto to essentially just use flight kit\nand not even use flight bracket and\nstill get benefits out of it\nlike use automatic kind of data\nconversion and so on like without even\nuh needing anything from flight packet\nand then flight bracket is the\nadditional stuff\nso that's uh the goal um\nrough timeline alpha a month maybe\nand we believe that we'll have this\nstable and ready to go by end of the\nyear\num and does that sound okay\nyeah and um hopefully once like dude\nit's not the cleanest code right now\nthere's definitely some spots\nthat we just in the process of iterating\nwe've\nlet get out of hand once we clean up\neverything and have an established\nframework i think we can also cut\nsmaller issues\nfor people in the open source community\nto work on if they want to\nyeah i think we'll start a project in\ngithub\nwhich will be managed with multiple\ntasks\nand the layout will be straightforward\nwith unit tests and so on so\ncontributions welcome and this i think\nuh\nin my opinion this is one of the most uh\neffective changes that we can do\nto help flight and our users um and so\nwe are heavily focused on this and we\nwill we will continue working on this\nand sorry one last comment if anyone\nsees anything that is\nobviously wrong or we've gotten\nsomething terribly wrong please type up\nchief had a couple of questions in the\nchat here you want to take a look at\nthat\nyeah there you use the chats yeah um oh\njeeve has a question what about the\naccess of\nto the oh oh no the w of params are not\nphased out i removed it from yi slide\nuh i just wanted to show the brevity of\nthe new syntax\nbut essentially it is like it that get\ncontext\nlike spark or other things it's a global\nsingleton that you get access to\nin the process chief does that answer\nyour question\nyeah that's awesome thank you yeah and\nthen the yield generators for dynamic\noh no more yield most likely because\nthe problem with yield is this in python\nthe\nthe idea is that you will return back to\nthat point\nonce you know the yield has completed\nwhich is not the case with dynamic\ndynamic yields in the traditional sense\nwhere it\ncompletely gives up like a sense of\nthings to execute it's like a planner\nthat gives out a plan and that's it and\nthen the plan executes and\nflight propeller essentially tries to\nassemble the results back\nso we think it's actually a break in the\nmental model\nthe yield and that's why we've\nwe're getting rid of it no more yield\nthen we'll figure out a nicer way but\nright now it's like writing a workflow\nessentially and that's what it is\nin in the runtime a dynamic\ntask is essentially a workflow\ncompilation\nwith inputs that are bound because we\nknow the inputs at runtime\nuh and that's what it's it's becoming\nwhere you bind the inputs to the\nworkflow and it generates the actual\nworkflow so you can interact with the\ninputs\nbut you can't interact with the\nintermidgets because the intermediates\nare still promises they are not complete\nwhat do sidecar tasks look like would\nthat just be updated\nyep yes sidecar\nshould be no different it should be\nexactly how it is today\nwith of course addition of part uh spec\nthat you can do\nin the yammer but yeah the dot with\noverrides is going to be a generic\nstandardized thing for every single task\ntype\num so i i wanted to have a brief comment\non the with overwrite\nso the idea is that you write a task and\nyou think that it needs\none gpu but maybe based on the data set\nyou think that you need two gpus\ni don't think cheap users are good good\nexample here but two cpus\num you can just save it override only\nwithin the context of the\nof the workflow as a follow-up we are\nworking on and this actually needs a ui\nupdate\nis uh overriding\nthese parameters like cpu gpu at run\ntime for an execution because we've seen\nour users essentially have that problem\nwhere they're like hey i'm running an\nexecution and i want to update the cache\ni don't want to use the cache or\ni want to increase the cpu for this\nspecific task for this run\nbecause my previous task failed because\nof overriding memory or because of\nmemory pressure or something\nso you can change the memory for this\nexecution so that's like a\nthat's a separate track too but\nthey are all going in the same direction\nthe width override semantics and the\nconfig overrides eventually we let you\nhopefully share that also in a bit\nthey're all going in that same direction\nbut any other questions\njust a comment i think i think this is\ngreat it's so much cleaner\num and much easier to even read\nexactly trust us\nwhen we started when we look at it\nsometimes you're like oh what were we\nthinking but you know\num and contributions worked out if\nanybody i think\ni don't want to say we are the best\npython programmers there are many better\npython programmers out there who can\nreally really help us\nyou know make this even better so please\ncontribute\nuh our at least ideas um if you see\nsomething\nterribly wrong what we'll do is we'll\nprobably share out these examples\num in the email list and so on\nplease contribute and say like hey this\ncould be proof and maybe in alpha we can\ndo that\num i have a question um about\nthe types now that we use that\nyou have like proper python types\nwhat about things like\nlet's say you have a schema for instance\nhow this now represented yes uh\ngreat question i think we we kind of\nskipped that in the presentation right\nso uh there are these are the types that\nwe are handling\nuh all the python native types will be\neach are easy to represent\num binary string even um\nthen the next level of types are uh i o\ntypes like string i o buffer\nand things like that right and those are\nrepresented as blobs\nuh in the back end but\nwe also have a file like object\nuh and that we will have to create like\nit probably we will be exposing\nsomething like typester\nfile or type stock directory file\nbecomes again a blob\ndirectory becomes a multi-part blog\nbecause that's what it is\nit's not a recursive directory single\nlayer directory but we could\neventually start supporting recursive\ndirectories\num and schema is essentially\na pandas data frame a spark data set\nor a wakes data frame any of these\nyou know data frames that are out there\nbut uh\nthe schema is loose at the moment right\nit's not very easy to extract the schema\nfrom the data frames themselves\nso at least i don't know of and so we\nwere thinking we'd actually keep on\nsupporting\nuh flight that schema which translates\nto one of these data frames and the way\nthe translation happens is\nnot out of magic like the way it is\ntoday where you say schema or download\nand\nschema.get data frame instead of that\nyou will say\ni have a helper method that says from\nschema to\npanda therefore from schema to uh\nspark rdd or you know not rdp data set\nor from schema to wake's data frame any\nof those\ndoes that answer your question sorry\nyeah you have to represent schema\nthe the the container types no now now\nthat\nwe have like proper typing um\ni i guess it would be possible to\nto actually support like list of\nstring whatever and things like that and\nreally\nuh put that into the into the type\nsystem right\noh yeah we are so actually that's an\ninteresting actually gleb\nwe clever had this interesting question\nsome time ago\nwhere our type system is univariate\nbut the actual protobuf seems to be like\nmultivariate\nbut you know we we don't expose\nbecause i think that kind of runtime\ncomplexity eventually you might want to\noptimize things right\nso it's all universal higher level and i\nthink\nso today we already support\ntypes.list of a specific type but with\npython it's going to be much simpler you\nwill just say typing.list and indoor\ntyping and there will be a list of lists\nof integers\nand we will continue yep cool\nthanks yeah and uh\ngleb and nelson in the in the java sdk\nare you guys\nalso supporting univariate lists and\nthings like that\nwhat so right now we are working on\nextending serialization code\nbut uh yeah it will generalize it will\nserialize it to structs and protocol\nokay oh so\nyou're just using struct for lists and\nthings like that okay\nyeah eventually you but you could\nsupport because the benefit of this\nis that the ui can be generated to be\njust lists\nlike that that's really the benefit and\ni think yesterday somebody in the\ngeneral channel was asking\nyeah i want to pass in a list of lists\nin the ui and we don't support that\nbecause\nthe ui is a little behind as usual\nit will support it\nanything else great question sorry like\ni think schema is not gone\nit's one of the heavily used types that\nlift and we cannot get rid of it even if\nyou want to\nany other question\nif i i can give a quick update on my\nstoryline if there's nothing else but\nyeah i charge you at something\nnope i'm just gonna wrap up if there's\nnothing else to do but\nif you have one more gift for it yeah\nit's just an update on the milestone\nline i think we decided to\nkeep on talking about uh what we're\nworking on\nuh so i as we we already showed flight\nkit improvement\nthat's in progress um dynamic array jobs\nuh so if i think uh some of you are\nusing\narray jobs on kubernetes and if you see\nactually the area job and kubernetes\nhave a\nhave a problem uh where if you launch\ntens of thousands of them they affect\nkubernetes\nuh so we've been working on a way to\nreally launch large numbers of arrays in\nkubernetes and that's\nmerged we are testing it at scale\ninternally at lyft\num so we are ours we ourselves use aws\nbatch to do the array jobs but we are\nmigrating away from\nthis batch because we've seen stability\nissues with patch\num and so we are testing it for our\ninternal\nusers um it's merged in mainline\num but uh as we find any stabilities\nwe will fix them we're hoping at the end\nof this month or\nuh already next month or mid next month\nwe will have a stable version for\nthis ui improvements uh\nyeah single task launch ui is in\nso you can now launch a single task\nright from the ui and the eventual goal\nis when something fails\nin a workflow right you\ncan launch that specific failing task\nwith modified parameters if you want to\nto quickly test out your hypothesis or\nso on so\nthe end story the flow in the ui is\ngetting more\nstabilized blob rendering support is not\ndone yet and waterfall ui is in progress\num brenda is working on lock plugins\nand and thank you again for it\nit's been a little slow i'm sorry but\ni'm this is my first\ntime with go too so it's awesome fun to\ndo yeah\nyeah go is a fun language it's like i\nstarted off\nnot liking it much but i'm yeah it's\npretty clean\nit's very simple it's like that's what\nthe cookbook is\nuh and then katrina is working on the\nfast iteration i think we'll have a demo\nfor it next time\nand yeah and then flight ctl uh\nuh uraj or eval socket has been chugging\nalong on it\nand i have also been helping with it\nit's coming along uh\nit's a side project so whenever we get a\nchance we work on it\nand one last thing is i have a talk\ntomorrow at\nuh global ai conference\nbut it's more of a business talk about\nlike things why we should consider\nthings like flight in an organization\nso yeah if you guys i can share a\nspeaker called that you guys can come to\nand if you guys want to please go ahead\nand talk\nabout right and if you're liking it\ndefinitely\npost it you'll appreciate that thank you\nthat's it from mine\nhey nelson and clapton you guys want to\ngive an update if you want\nso four weeks left before we demo\ndynamic workflows\nand also we have some uh directional\nbusiness support like a testing for\nworkforce\nso you can compare it from not only\ntasks but workforce\nvocally yeah that's awesome yeah same\nthing\ni think both of the python and java\nflight gate are now kind of hopefully\naligning on the same path so it's\nawesome\nalso with the faster operation the it'll\nlook even\nmore similar\nthat's awesome four weeks for the\ndynamic i'm excited\ni think that's it then unless there any\nmore questions\nwe'll wrap it up and uh we'll try to get\nthe video in the notes posted\na little more timely today and then see\neverybody again in two weeks and maybe\nkatrina will have something to show us\nif we're lucky\nhow about faster iteration time\nis that it all right going going on\neverybody nice to see y'all\nplease stay safe thanks everyone see ya"
    }
]