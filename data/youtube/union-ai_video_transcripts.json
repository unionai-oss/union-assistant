[
    {
        "title": "Large Language Models for Enterprise Success: Challenges and Approaches Panel",
        "transcript": "all right I think we're live\num yeah looks like we're live we're\nready to go so look if you're here to\nlearn about large language models in the\nEnterprise you're probably in the right\nplace you're supposed to be working turn\nthe volume down so your managers don't\nhear you listening to this instead now\nI'm going to be joined today by a lot of\npanelists that we're going to be talking\nabout maybe the depths of large language\nmodels in general but also like how do\nyou adopt this stuff in the average\nEnterprise so if you don't know what a\nlarge language model is don't worry our\npanel of experts will explain that to\nyou and I'll be representing you in\nterms of asking more Curious questions\nabout how does this stuff work in the\nreal world you also have the ability to\nask your own questions in the comments\nsection so if you got a question drop it\nin that comment area I'll pay attention\nto those and raise them to the audience\nand also I think you can do some\ncall-ins I don't know how that works but\nif you remember your registration email\nyou should be able to request the\nability to come on stage and we'll let\nyou ask your question in person keep in\nmind this is being recorded So if you\ndon't want to be seen in the recording\ndo not raise your hand to come on stage\nyou will be in the recording all right\nso to get it started we're going to have\neach of the panelists introduce\nourselves so I'm just going to go from\ntop to bottom based on what I see on my\nscreen so Harrison if you could unmute\nyourself introduce yourself and tell\npeople a little bit about your\nbackground\nabsolutely and I think I should be\nunmuted already I was not a good\npanelist and I had not made myself to\nstart so I'm not I'm already not\nfollowing instructions um but my name is\nHarrison great great to meet everyone\num I am CEO and co-founder of link chain\num link chain is uh well LinkedIn is now\na company but it started off as an open\nsource framework for building llm\napplications\num started last October end of October\nearly November of last year\num started as a company in January we've\ngot python typescript package and before\nthat my background's in ml and ml Ops\num so uh worked worked at a few\ncompanies doing that so that's kind of\nthe perspective that I'll bring\nI got a quick question on that so you're\na developer you got your app\nyou probably got some app where you're\nselling ice cream over iPhone who knows\nand you're like I need to use Lane chain\nwhat would they be using it for and what\ndoes that developer experience look like\nin terms of\nlayering this into an existing\napplication\nyeah absolutely so the\num\nthe core of it is basically connecting\nllms to your application and kind of\nlike acting as a middleware between the\nunderlying llm and then your application\nand so why is that necessary it's\nnecessary in the cases where you're\ntrying to build\nmore interesting applications that\naren't just kind of like a simple call\nto a language model so if you're if\nyou're just calling like the API you\ndon't need LinkedIn where LinkedIn kind\nof excels in helping you build uh\ncontext aware kind of like reasoning\napplications and what I mean by that is\nlike albums are great but they don't\nknow everything and so when you need to\nbring external context to the llm\nwhether it be uh whether it be data\nwhether it be kind of like instructions\non how it should behave in your\ncompany's context\num\nwhether it be a kind of like\nspecifications of apis that it can call\nand then have it reason about those\nthings and then there's there's also the\nconcept of of chains uh hence the name\nwhich is basically sequence of these\nthings so as a simple example you can\nkind of like reason about uh through an\nAPI spec in some context what parameters\nyou should send to an API you can then\nget back the response and then decide\nwhat to do next and sometimes you can\nreturn that to the user sometimes you\ncan realize that the response uh you got\nlike a 404 or something and so this the\nthe request that you sent was bad and\nmaybe you should try again with some\nslightly different parameters and that's\nwhere like using the language model as\nthis reasoning engine to decide like\nthere's so much human ambiguity in how\nwe speak right and so having the\nlanguage model take that and reason\nabout specific actions of what to do\nthose types of actions are where link\nchain helps Excel\nI promise I'm going to move on coming\nfrom the symbolic programming world if\nthen else\nand some people think about that as like\ndecision trees and decision making and\nit sounds like all of this is like one\nlevel up from that this idea of\nreasoning some would say maybe you're\nusing the word reasoning a little too\nearly in this stage\nbut is this kind of the way developers\nshould be thinking about a smarter way\nof making decisions\nyeah the the way and so there's also\nlike this distinction in Lang chain\nbetween like chains and and agents um\nand agents are a bit of an overloaded\nterm and so specifically what I mean by\nagents is when you're using the language\nmodel to determine what sequence of\nsteps to take and in what order and then\nchains are when those are kind of like\nhard-coded in code so like if and that\nand stuff like that and so I think like\nthere's there's pros and cons to both\nright chains much more controlled um you\nhave you they don't go off the rails you\ncan kind of like control them much more\neasily the benefit of where Agents come\ninto play is that they can handle a\nreally long tale of events and so if you\nstart having like really complicated\nlike if this then this if this then this\nif this other thing\num\nthe idea of an agent is basically okay\ndon't write kind of like don't write all\nthat complicated code have a language\nmodel kind of like decide what to do and\nkind of simplify the logic there and so\nagain pros and cons to both and I'm sure\nas we talk about how Enterprises are\nadopting this we'll talk about the pros\nand cons of some of these more out there\nthings compared with the stuff that you\ncan reliably do now\num but yeah there's that idea of using\nthe language model to\ntake away this like complicated uh\nbranching logic that starts to develop\ndope thank you for that next up on my\nlist I have step step if you could mute\nyourself introduce yourself\nhey I'm uh Steph uh I live in Berkeley\nCalifornia and I've got a background in\nLMS kind of going back to the Facebook\ndays\num I was working in Translation and\nlanguage understanding uh for many years\nat Facebook\num and sort of developed in\nproductionized in the first large\nlanguage models\num\nmodels like Roberta and XOM in like 2018\num and then later I've uh applied lens\nat stripe in kind of the merchant fraud\nspace so\num\nthe one question I have for you is you\ntalked about like language translation\nand so when I think about that computers\nare getting better at this understanding\nof language thing but if you have to\ncompare it to you know I meet some\npeople that know like seven languages\nlike fluently where would you compare in\n2023 where the machines are and people\nthat are like Linguistics by by training\nwhat would you say they cut a rank in\nterms of their ability to comprehend and\nunderstand language\n[Music]\num\nin Translation specifically or like\nlanguage understanding I I think\nlanguage understanding right yeah\nchildren start understanding language\nbefore they can talk right so like where\ndo we compare\nyeah I think uh there's a great article\nin the New York Times on this like five\nyears ago about just how far language\nunderstanding is from like a real sort\nof linguistic translation\num like it's really shocking how\npowerful these modern language models\nare from the perspective of the\ntechnologist and and like where language\nunderstanding was five years ago but\nit's still like you know it it still\nfeels almost infinitely far away from\nlike a real person in a lot of Senses\num like understanding like basic context\nabout like who the speaker is is\nsomething that language models like\nreally you know like you can kind of\nlike Supply that with prompting to some\ndegree and like you can in some you can\nfind some like really compelling\nexamples if you like cherry pick them\nbut in terms of like an individual's\nability to like a reason about context I\nthink we're just like we're like still\nas far away as we were almost uh you\nknow five years ago\nI appreciate that additional context\nespecially as someone that works at an\nEnterprise trying to figure out are we\nStar Trek\nor are we just kind of making progress\nover like things like Siri right so I\nthink it just really helps ground the\nconversation so I really appreciate the\nadditional context next to my list is\nthe nylon or nalen if you could unmute\nyourself introduce yourself and uh\nwelcome\nyeah see like we can't even stay\nconnected to it again\nall right with that we're gonna keep\nmoving let's go with the Caitlin if you\nif you could mute yourself introduce\nyourself\nuh\nhello thank you for having me on this\npanel uh my name is cathan I'm the I\ncreated flight and um I'm the open\nsource chair for flight but I'm also the\nCEO and co-founder at Union at Union we\nare building AI infrastructure it's a\ncompany where we think the core of it is\nan orchestrator which is powered by\nflight but I am AI infrastructure it can\npasses larger area than just\norchestration\nI think you're the first person you know\nI became an advisor I think last year\nyou're like the first person who\nintroduced me to the term ml Ops\nfirst of all what the hell is ml Ops and\nwhy is it important\nespecially for the Enterprise that's\ntrying to get into this for the first\ntime\nyeah it's interesting because I actually\nand honestly on this panel I do not have\ncomplete background in llms what I have\na background is getting machine learning\nmodels into production uh and that's\nwhere all the challenges come to play\nwhat happens if you start with an idea\nand oftentimes I've seen machine\nlearning scientists come up with cool\nideas\nbut very few things I've seen many of\nthem make the production I think what\nends up happening is they get lost in\nthat in the sea of\nhere's an idea and how do I I finally\nput it in production and the reason why\nthat happens is\ninfrastructure management of the\npipelines themselves management of in\nproduction right you are now reliant on\nyour in the model or service to be up to\nbe providing real values quality answers\nright\nand that if you take the pure software\nterminology\nwe have been pioneering how to deliver\nsoftware consistently reliably and I\nwould still say it's not like 100 done\nright\nnow here in the world of America and\nWorld of ml is completely different like\nan idea that starts off today works well\nin practice today May absolutely not\nwork in six months and that's because\nassumptions have changed underneath and\nand now what people have been trying to\ndo for some time is take the software\nengineering principles and apply them to\nit well\nuh but but the fundamental problem\nitself is different ml is constantly\nmoving this variable thing and now\nyou're trying to put it into this strict\nforced way of thinking about like the\nway we think about software and I'll\ngive you an example right let's think\nabout a database three years ago if\nthere existed a database today it's\nprobably better they started off with an\nidea that I will build indexes\nand they keep on improving they keep on\nimproving performance characteristics\nand so on with machine learning models\nthat's not how you do it machine\nlearning models you start with an\nassumption for example it's not covered\n2019 you want to deliver\num like let's say you want to do some\ncoupons and you want to get a lot of\npeople onto your platform either\ndoordash or your Lyft or your Uber uh so\nyou have different assumptions and you\ngo paste on those\nand pandemic hits all your assumptions\nhave changed there's nobody on the\nstreets Uber and Lyft cannot get people\ninto the into the cars how do they now\nuse machine learning to incentivize\nthose people the incentivizing process\nhas to change so in this kind of a\nchanging environment you need a\ndifferent process a different set of\ntooling that takes you into production\nreliably make it let you do this\nconsistently repeatedly\nand reproducibly right which which\nthings are so this is\nwhat is in general term the ml Ops was\ncoined for now because it was not very\nwell crispy defined everybody is free to\ntake their own spin on envelopes but the\nidea is to take whatever you've tried in\nin this pure research environment into\nproduction so that customers can get\nvalue from it and\num even if they're alone World some of\nit for a while remained in that research\nenvironment and we actually saw some\nreally good interesting ideas put into\nproduction and that's where caught\neverybody is you know we we almost got\neverybody's imagination saying that oh\nwow chat GPT can do XYZ and that's where\nyou know that's the difference like you\ntake something from idea to production\nthat's in my opinion that entire field\nis\nyeah I think it helps people really\nthink about\ntraditional software you almost has a\nsix foot of rules if this didn't do this\nif you put this in your cart it costs\nfive dollars you buy it that's it either\nwe have it in stock we don't very easy\nto program in that way but I think when\nwe think about ml in general we're\nmoving closer to people\nyes and when you're dealing with people\nyou got to be able to read the room and\nso one model May no longer work in a\ndifferent context and you're just going\nto make the wrong decision the right\ndecision driving a car during the\ndaytime could be very much the wrong\ndecision in the evening or after major\nconstruction work has been performed\nyour model may now be obsolete because\nwe've never made roads that way before\nand so I think this this need to be\ndynamic in the ability to read the room\nintroduces a whole new challenge in\nterms of maintaining software that most\npeople may not be thinking about all\nright all right I think our last\npanelists that we have online uh Amish\nif you could admit yourself introduce\nyourself and walk into the stage\nyeah thanks for having me here and glad\nto be with you all I'm currently at\nLinkedIn uh executive director for our\nAI platform efforts which include our\ndistributed training platform fine\ntuning platform machine learning\npipelines uh AI metadata and other\nrelated efforts so essentially you know\na lot of the core uh products you need\nto build for realizing ml Ops and no\nllmops right so that's another term and\nbefore that I was part of IBM Watson\num distinguishment engineer there\nleading a lot of efforts around creating\nthe Watson platform around distributed\ntraining pipelines Etc working heavily\nalso with the open source communities I\nwas one of the co-creators of k-serv\nwhich is part of the Q flow serving\nplatform one of the co-creators of qflow\npipelines on tecton all these projects\nare now part of the different arms of\nLinux foundation so work very heavy with\nLinux foundations in terms of also\nlaunching trusted AI projects around\nyeah foreignty\nEtc\nWatson was probably\nthe moment that people are experiencing\ntoday with like chat GPT I think Watson\nfor me was that moment especially the\nwhole Jeopardy thing like beating\nsomeone in chess ah okay\nbut watching it play Jeopardy is the\nfirst time I felt like okay we are now\nentering the Star Trek era and the\nreason why I thought that\nyou can call it a presentation a demo\nbut it was real what's so interesting is\nbecause the machine was next to the\nhumans in a context we understand a game\nshow and I think for a lot of people it\nwas seen like the tech actually meet\nwhat people would consider normal people\nand\noutclassing the best consistently and so\nto me that was that aha moment and so\nI'm definitely like a big fan of Watson\nbut I also think Watson people thought\nit was going to become I don't know the\nserenity like people thought this was\nthe end game\nand then it went away\nit felt like that was it like we had\nreached this peak\nI guess I have two questions number one\nkind of what happened to Watson and then\nhow do we be careful right now not to\nover hype llms as like this thing that\nwas going to bring AGI to the world\num very good question Kelsey I think uh\ndefinitely you know it was back in 2011\nright when when Jeff Hardy came in and\nthat was quite a bit of aha moments for\na lot of folks right yeah I can actually\nyou know go and beat uh the chapati\nplayers and I think uh you know it was a\nsystem which was heavily customized and\ntrained on that data set right and over\nthe period of time right definitely that\nwas something which was done in terms of\nyou know establishing a moment showing\nthe capability but beyond that the\ncommercialization aspect okay how can we\ntake this idea that you know AI can\nactually uh do natural language\nprocessing uh and you know apply it in\nreal world and that's where you know\nWatson Evolution has gone in the last\nfew years so a lot of the things which\nwe are seeing now for example you know\nchair GPD coming and uh democratizing\nthis whole NLP space overall uh but if\nyou see in general like you know back uh\nit was around you know seven to eight\nyears ago when Watson launched a lot of\nthese services around natural image\nprocessing speech to text text-to-speech\nsentiment analysis they're running uh\nstill in public Cloud definitely you\nknow catering to Enterprises and being\nused as chat Bots across a lot of the\nEnterprises so a lot of the Enterprises\nwe interact with\nand I don't want to take names right but\nthey're essentially when you are\nactually uh going and using their\nchatbots uh you'll find what's in behind\na lot of them so it's still there\nthriving a lot of the ready-made\nservices are are being sold and also the\nplatform right which powered that right\nin terms of distributed training model\ninferencing uh machine learning\npipelines that's another set of product\nofferings right compared to something\nlike which you see in Google's vertex AI\nportfolio or Sage maker right so it's a\nsimilar portfolio what's an X products\nright which has been launched both on\nthe private Cloud as well as the public\ncloud and doing good right I mean if you\nsee the overall Gartner and other\nratings it's almost always there in the\nleaders quadrant in in terms of that so\nI think yes definitely uh the the party\nmoment had a lot of promise but I think\nthe thing which probably was missed was\nthat you know it was curated customized\non that one particular data set and and\nyou know that's why we saw the results\nwe saw\nfast forward now what has happened with\nchat GPT is okay now it's not that one\nparticular data set right it's a data\nset right which is so huge these\nFoundation models are being trained on\nyou know the amount of data sets which\nis mostly accessible to these web scale\ncompanies Etc right and that's why they\nare essentially you know uh more\ngeneralized able to answer those things\nnow instead of you know being\nconstrained by Enterprise settings and\nbeing able to answer in reason uh for a\nparticular Enterprise they are able to\nbe the general purpose models they are\nthere right but it's a democratization\nof that idea right which which has been\nfloating out for the last decade\nthat's perfect so look we have a lot of\ntopics to cover\nwhen I I spent some time in the average\nEnterprise myself I've worked at\nfinancial institutions not the fancy\nones with the fancy technology not those\nthe ones where you gonna really spend\nmonths to release anything right where\neveryone is afraid of Regulation no one\nwants to make a decision about anything\nso when I think about Enterprise that's\nwhat I'm thinking about I'm not thinking\nabout people that are just looking for\nthat last 10 percent to outpast their\ncompetitors not not that I'm thinking\nabout those companies that really need\nthe most help they're really focused on\na core business and they hope that\ntechnology will somehow a system and the\nsame Enterprise when I think about the\npeople who work there right you know\nthat person's been there for 10 years\nplus they have a pretty good mental\nmodel of the business right like they\nwalk around in the very natural sense of\na model they know what's being sold they\nknow how it works they know what breaks\nthey know how the system performs and\nthey can just look at a graph\nand tell you something is out of stock\nhey what is that a stock in Home Depot\nthat's what that graph means like how\nit's like yeah I've seen enough I've\nbeen around enough and I think a lot of\nEnterprises are trying to take that type\nof thing\nthat that idea of a model and put it in\nthe machine so the machines can be\nmaking these type of decisions because\nwe have no idea the type of input that\ngoes into a seasoned veteran with 10\nyears of experience building up a real\nworld mental model and so I think a lot\nof Enterprises are there they have\npeople that are walking around in that\ncompany they have pretty strong models\nof how things should go and maybe they\ncan make decisions if they're pulled in\nand given enough context but they want\nto scale that\nright they want to scale that to bring\nthis capability to their end users\neveryone in the organization so when I\nthink about llms this ability to take\nthis concept and layer it in top of\ntheir data where do people start so\nanyone want to raise their hand unmute\nyourself and go from there\nI think given that Foundation they're\nalready doing data analytics they know\nhow to run SQL queries they got a lot of\nExcel spreadsheets open right now\nthey're probably watching this with 50\nspreadsheet tabs open right now\nwhat is their step into this world\nwell I can I can take a stab at this\num I think I mean first of all I think\nlike part of the part of the beauty of\nlarge language models is that they're so\nGeneral there's so many different\napplications so I think choosing kind of\nlike one entry point\num\nis is a bit tough\num that being said like I think there\nare a few kind of like natural places to\nstart one of the biggest unlocks here is\nthat you can use unstructured data now\nin some way like the by far the biggest\napplication we're seeing is some sort of\nlike chat with your data like chat GPT\nbut it knows your data and I think like\nthe easiest version of that\num is if you have documents\num they can there's a pretty standard\nkind of like formula for kind of like\ninserting them into context and\nanswering questions about it and and\nit's not perfect\num and this is why especially in larger\nEnterprises the main things we see\npeople working on are actually internal\ntools because the cost of an error there\nis much lower than if it's kind of like\ncustomer facing\num but but chat over your unstructured\ndata is something that previously was\nkind of like\nnot really doable now is doable it's\npretty easy to get started with weirdly\nenough like\num the the structured data where there's\nprobably more analytics before in terms\nof like yeah Excel and SQL queries\nthat's actually harder and we're seeing\nlike there's there's a lot of startups\nthat are trying to solve this there's\num it's actually a harder problem and so\nI think like in terms of chat your data\nwe're definitely seeing like chat over\nunstructured just like raw pieces of\ntext\num be one big application another one\nwhich I'm very bullish on and I think is\na good place to start is also extraction\nof information from these documents um\nand so this isn't this maybe isn't\nsomething that's done live like you\nmight not ask it a question but it's\nrather you have some pre-processing step\nwhere you extract information and then\nput them in some type of SQL query where\npeople can analyze it those are two of\nlike the larger\num kind of like use cases that that we\nsee people getting getting started with\nand and the formulas there are\nrelatively standard and it's kind of I\nthink those are two great starting\npoints I got two quick follow-ups and\nthen we'll get to you uh cathan\nwe talk about chat I'm I actually don't\nlike chat I like chat for support like I\nlike chat with like I need to change a\nflight with Delta and the thing that I\nlike about chat I actually like chat\nwith the real human\nnumber one I can say something once like\nmy address is copy and paste\nand if you need my address again\nthroughout this entire session you can\nscroll up and we got share context I\nlove chat for that one reason\nbut I don't like chat when I feel like\nthis thing is guessing and I can just do\na better job for someone with someone\nthat had a better mental model what I'm\ngoing through I am literally stuck on\nvacation no I don't want to leave four\ndays from now that is not you you're\nlacking context in the discussion\nchatbot\nso this llm thing is chat like the\nnumber one\nI guess facade that people will interact\nwith these models because I would think\nas an Enterprise chat is one thing but\nis chat kind of the Core Essence of what\npeople will interface with these things\nI mean I think it's still I think it's\nstill so early but I think so far the\nanswer has to be yes like I think like\nchat is the dominant thing that we're\nseeing by far I think um into like I\nthink I think there's a few separate\nthings there like you said chat isn't\ngood when the thing is just kind of like\nguessing like the big problem here is\nmaking these things context aware like\nregardless of whether it's chat or just\na question that's a big kind of like\nunsolved thing so assuming you have\nsomething that's like reasonably good at\nthat I think the reason chat is a a\npretty good interface is also kind of\nlike exactly what you said you can ask\nfollow-up questions you have this chat\nhistory and you have that as context as\nopposed to so if you're asking like a\nsingle question and then if you want to\nlike edit something\nyou kind of need to be able to reference\nthat thing before and so that chat\nhistory provides kind of like a good\nbuffer there I I'll also say that I\nthink like\nagain it's just like so early I'm I'm\nvery bullish like a lot of the things\nthat you might ask a thing to do\nbasically again there's so many\napplications I'm bullish on like\nlong-running things I think a good\nexample of this is like you know when\nyou're asking questions maybe you don't\nactually want to be chatting with\ndocuments you want to be like writing a\nreport and so maybe we'll see\napplications kind of like transition\nover to some interface where they're\nwriting a report based on a prompt but\nthat's there's a lot more moving pieces\nthere I don't think we're at that level\nof reliability yet so chat is also kind\nof like a playground almost and provides\nthat kind of ux I'm also not a ux or a\nUI expert so there's lots of other\npeople who have more informed opinions I\nkind of feel like Chad\nis the easiest thing we can perform\nright now right because a lot of the\nthings that listen to natural language\ndepending on your accent the way you\npronounce things\nit can change the whole query but it\nsounds like we're going from a world\nwhere you have this rigid structured\nthing called SQL and running that on\nstructured data you can get some amazing\nhigh quality results because you have to\ndo so much work when you gather the data\nstructure the tables all these things to\na world where we say hey maybe you're\ndealing with unstructured data\nhow would you query it I don't I don't\nthink we want to write SQL queries for\nthat so using natural language via chat\nmeaning I can spell check I can get my\ngrammar correct and then post it and\nthen I have way more time to kind of\nanalyze the text versus trying to decode\nthe sound to make sure that the query is\nas accurate as possible is is that maybe\na fair way to think about the reason why\ntext based chat Bots are so prevalent\nright now during this phase of llms\nyeah I think it's uh\nprobably pretty fair chat's very\naccessible to a lot of folks and yeah\nand that's probably a good all right\nwe're going to Kate then and then we go\nto Steph\nyeah so I think I just wanted to add a\nlittle bit to both of your advice I can\nsee you asked and very very important\nquestion which\nhidden over there which is like\ndecisions like people make decisions\ntoday I would not rely at the moment\nand maybe this is a very big statement\nand people are going to come and\nprobably kill me for it uh I wouldn't\nrely on nlms to make decisions what I\nwould rely on llms is to help me in\nmaking decisions that's a slight\ndifference it's a very very critical and\nimportant difference right uh when you\nwant decisions you want precision\nand by the nature of it when I say\nsomething to you that I went to the\nhospital yesterday\nI have other random contacts in my brain\nthat I did not even communicate to you\nright because Language by nature is\nimprecise and I've seen a lot of\napplications and again another one that\nprobably pitchforks will come out is\nthat people are saying let's use natural\nlanguage to query structured data right\nyou know so like SQL I think it is not\nit is not right and the reason why this\nis is like you know when you say when\nyou say certain things in English\ntwo people are finding it hard to\ncommunicate with each other how do you\ncommunicate with uh\na database\nthat has rules right but your rules in\nyour natural language were not precisely\nkibble so you will lose that in the\ncontext that's why we created SQL the\nreason why SQL exists it gives you this\na syntactical validator that allows you\nto precisely follow the rules and you\ncannot disobey it you cannot just say\njust randomly query something you have\nto say the primary key you're aware and\nthe word and blah blah so I think it is\nreally important to understand these two\nthings like one is language is imprecise\nhence I think every application which is\nunstructured data where imprecision is\nokay as a response\nelements are fantastic at it\nbut that also leads to a point that you\nshouldn't use it to make decisions at\nthe moment maybe at some point we will\nget there by combining multiple\nknowledges as well as chains Etc and I\nknow you come from the ml Ops and I\nremember I was doing an interview around\nvery similar topic around people in the\nobservability space and I think\nyou talked about infrastructure if you\nare going to try to have this thing help\nyou make decisions or cross fingers make\ndecisions for end users you want that\naudit Trail because you need to be able\nto reproduce that decision what was the\nmodel what was the data what was the\nsituation when you made that decision so\nthat way humans can go back and say yo\nthat's a bad decision that we're making\nand we either need to go fix the model\nor ask ourselves do we are we even on\nthe right track with this and I think\nthis is where the infrastructure part\nbecomes a huge piece of it I think you\neven told me once back in the day that\nit's just not the model the code alone\nisn't the model the infrastructure the\ndata and the model all together equal\nthe output and how you need to version\nthese things together I'll give you an\nexample on that we work with a company\ncalled absalera\num and they they were the people who\nfound antibodies for the code vaccine\nuh so the process of uh vaccine\nDiscovery Works where you go and\nactually\nuh a company like moderna will say like\nI want to just put the wax let me go\nfirst find what antibody should I Target\nor whatever right like I think this is\nfor the Activision vaccine so don't\nquote me if I'm wrong here but\nthey go to a company like absura and\nthey they go through\na huge research process to find these\nantibodies and these are like including\nblood cells and you know wet labs and\nelectron microscopes and finally there\nare hundreds of thousands of\npermutations that they go through and\nfinally decide on one and say like let's\ntarget this one\nand then comes the clinical trial\nprocess\nand after that you actually we had to\nFast Track all of this for covert so we\nwent through and delivered a vaccine\nlet's say five years later you find a\nproblem that this vaccine was impacting\nyou you know and it had Converse effect\nFDA will come knocking\nand to the company to say what were the\nexamples you did and you have to have a\ntrace of everything and this is they are\nml driven in this case I call this ml\nright it could be called AI the higher\nfield okay uh\nthis is true in any kind of a place\nwhere it's a human making it we work\nwith autonomous companies like\nyour time they do the same thing\nwhenever there's a left turn or right\nturn to be made why did you make that\ndecision what was the set of inputs in\nmy simulation what did I see and how was\nthe observation done and how was you\naccepted like we have to have a trace\nbecause this is not a human making a\ndecision this is a machine making a\ndecision the the number of times or the\nposition requirement increases probably\nartificially but it does increase right\nso and and every time it makes an\nimprecise decision\ntrust reduces which is which is a very\nvery big factor in my opinion\nyou're giving wonderful answers just\ngive them a little louder because no one\nwants to miss them and we're going to go\nover to Steph I think you had your hand\nup next and then uh oh and Mish we'll go\nnext\nyeah\num\nI really agree with everything Kevin\nsaid I'm gonna go ahead and take a\nslightly stronger stance and maybe say\nthe opposite of what Harrison was saying\nand say I think that chat is possibly\nthe kind of like least useful\napplication of llms and the last one\nthat is going to see wide adoption\num I think uh like\nLMS have been in production at at in\nkind of like Enterprise scenarios for a\nlong time and I think chat gbt really\nwas\nuh only it was only really revolutionary\nin like two ways like one is the uh uh\nreinforcement learning through human\nfeedback kind of mechanism I think is\nreally powerful\num and and we're like chat gbt is like a\ntestament to that but it really captured\npeople's imagination by showing them\nwhat these large language models could\ndo like it's a very interpersonal thing\nthat you can like chat with this thing\nit'll treat you it'll like answer your\nquestions for you but in fact like the\nthe underlying models have existed and\nbeen in production Enterprise for a long\ntime for a lot of tasks and I think like\nin like you were saying I think kind of\nCaitlyn was talking about like there's\nthere's like real consequences right to\nexposing these like chats chat models to\nusers and we really don't have the\nability to control them that well\nespecially in this sort of like\nunregulated text output uh sort of\nenvironment\nso it's really kind of like dangerous\nlike imagine it so like one one example\nkind of getting back to your original\npoint of like experts and like things\nthat experts do that we can train\nlanguage wants to do so one example\napplication of large language models is\nuh\nlike toss violation for merchants at\nstripe\num so this is a problem that like\nexisted before language models were\ninvolved but you basically want to say\nlike you know Merchants are only allowed\nto sell certain things according to the\nterms of service of stripe and we want\nto like look at a merchant and decide\nwhether we think that they're violating\nthe terms of service and then\npotentially like disc you know\ndiscontinue their relationship with them\num and large language models can do uh\nlike really interesting things with that\nright they can not only say like pull in\ntext and use the text like before that\nwe're just using like a bunch of random\nsignals you know like uh potentially\nlike just like charges or something like\nthat but being able to really pull in\nthe text is really valuable because you\nknow if they're selling something that's\nagainst the terms of service they're\nprobably going to say what they're\nselling at some point and so that\nunderstanding tax is really valuable but\nwith large language models you can go\nfurther and say like hey what what like\ncan you point to me on the website and\ntell me like uh what that reason was and\nthat's that's suddenly like a really\npowerful thing to give to like an expert\nand really reduce the amount of time\nthat they need to spend doing this and\nsay like okay we think not only do we\nthink here's here's a merchant that we\nthink is by the way in terms of service\nhere's we've done a bunch of the legwork\nfor you to make that decision and then\nwe like have all those records available\nfor you to like store in case there's\nlike a lawsuit about this later right\num versus like\nI don't know like directly sort of\ninteracting with the user uh without a\nperson involved uh would be I think so\nmuch harder of a lift right you need to\nbe both very confident and like you\ncan't really control necessarily what\nthe bot's gonna say to them\nso I think there's there's actually a\nhuge amount of use cases already that\npeople are using large language models\nfor and I think that we'll what we'll\nsee is that like a lot of the use cases\nlonger term\num are going to be these like more\nspecific structured problems\num and and maybe there will be some\ninteresting generative ones as well but\nI would say those are going to be kind\nof the minority of cases\nuh\nall all good points right like uh the\nonly thing like in general in in terms\nof models making decisions they have\nbeen doing for quite a while now right\nthey are uh not necessarily llms right\nbut lpms and others right uh from the\npoint of you know when we are applying\nto a company your application is going\nto universities uh submitting alone\napplications models are making that\ndecisions right and and so far a large\nnumber of Enterprises have developed\nstrong enough you know governance\ncapabilities to be able to capture the\nthe lineage and the metadata so that you\nknow yes in case of a compliance or\naudit right you are able to go back and\nTrace I think llms as such you know the\nfact that it's a language model catered\ntowards language processing uh more uh\ngeared towards you know augmenting and\ngiving you knowledge to be able to make\nthose decisions right and and that's\nwhere uh the benefit shines right it's\nable to summarize tons and tons of data\nright and give you a very precise\nanalysis of you know what that data\nmeans and then you can make based on\nthat can make decisions right now the\nthe other point you know which service\nthe interface I think it's in general\nlike you know the way you talk to a\nmachine whether you are going to use a\nvoice or you are going to send some form\nof text right I mean these are the only\ntwo modes of interaction which exist\num so so in general like I mean llms are\nbecoming that voice of machines right so\neither you are going to send some text\nor you have voice enabled interfaces and\nin a lot of cases like you know there\nare use cases emerging where people are\ninteracting\nare not knowing that they are actually\ntalking to a machine behind the scenes\nright so they're able to uh reason they\nare able to contextualize they are able\nto pull in more information so I think\nthe early generation chatbots they were\ndefinitely very clunky uh you could\ndefinitely tell you know right at the\noutset that hey you're talking to a\nchatbot right there is not a human\nbehind that more and more what we are\nseeing in the next generation of chat\nmods which are coming right it takes a\nwhile even if you are able to determine\nthat you're actually you know talking to\na chatbot right so they're becoming\nbetter and better so I think Chad is an\ninterface and then you know voice as an\ninterface to talk to these different\nsoftware systems is is going to to\nproliferate overall\num and some of the use cases like even\nwithin LinkedIn right which we are\nseeing\num personalization is a is a great use\ncase which llns have been avered right\nso for example you know recruiters need\nto send emails to to different\ncandidates uh what was happening was you\nknow you have a lot of these genetic\nemails being sent out right now what\nllms are able to do look at you know the\nthe company context the recruiter\ncontext the applicant context uh their\nprofile data their activity and they're\nable to create a very you know targeted\npersonalized emails right which you can\nactually send out other set of use cases\nwhich we have enabled using llms right\nuh within the context of bringing in for\nexample you know collaborative articles\nright so a lot of uh Professional\nNetwork which exists on LinkedIn right\nhas knowledge about certain things so\nbeing able to generate these\ncollaborative articles around a certain\ntopic and then seek inputs from\ndifferent humans to augment that machine\ngenerated article right we are seeing a\nhuge uptick on on those right so uh I\nthink in these kind of cases llms are\nproving you know uh very very beneficial\nwhere the context is more around\nqualitative data generation\nsummarization analysis right and and\nthen you have the other set of models\nright which are right now dominant in\nthe decision-making world yeah\nyou know what's interesting I think for\na lot of people they're first\ninteracting with the machine maybe\nthrough touch like you touch something\nand it does something like no very\nsimple machine like a thermometer right\nwhat's the temperature of me you put it\nin your mouth and it does something very\nsimple machine does one particular thing\nwell but it's a really nice feedback\nloop with human interaction and a\nmachine and it feels to me like the\nmachines and the software we interact\nwith is so complex that now you can't\njust use a dial a knob or figure out all\nthe commands or the API in order to\ndiscover all of its capabilities now we\nwant to go into the most powerful\ninterface that humans tend to know like\nif I walk up to a person I have my\nentire language and vocabulary at my\ndisposal to query this person's\ncapabilities ask them to do something\nand if that person has the skill they\ncan take my query and turn into some\nactionable outcome and I think now\npeople are saying is it time now that\nthis thing we interact with so much in\nour lives almost as much for some people\nlistening you probably interact with the\ncomputer more than you interact with\nhumans and now I think you want to\nupgrade the interface and the protocol\nyou use to issue and receive answers and\nI think that might be why I think a lot\nof the hype is there and I think what\nchat GPT did for everyone\nit gave people the ability to experience\nthe checkpoint of where we are\nis it great is it something I think\nwe'll do forever probably not but I do\nthink it's that checkpoint that the\nwhole world millions of people\nsimultaneously could experience and say\noh\nthis is where we are because before I\nused to use those you know speech to\ntext to try to write docs like it wasn't\ngood I just went back to my keyboard now\nthis thing is like yo not only can we do\nspeech to Tech we can generate what\nyou're probably going to write anyway\ngiven that I'm going to Pivot to the\nnext part of the conversation which is\none thing that I think I heard come up a\nlittle bit is around this stuff is\npowered by data\nyou know someone asked me a question\nwhat I thought about llms and I said\nlisten llms highlight to me how valuable\nhumans are instead of sitting in front\nof Chad GPT all day I want to be the\ntraining set I want to go live a world\nthat generates the new model the data\nthat I do from my activities in the real\nworld feed into some model for someone\nthat wants to sit behind a computer\nbecause I don't now the thing that makes\nmost of these Enterprises interesting to\nme is their unique data set\nI kind of feel like\nthese models will race to open source\nthe model itself will be generically\navailable but I don't think the data\nwill anyone want to jump in and like\nwhat impact will a unique data set\nwill that give an unpresent advantage do\nyou all see data sets also becoming a\ngreat commodity as well because that\nseems to be a huge part of this whole\nmovement right now\num\nI think in in general there are\ndefinitely the the advantages of you\nknow having your own data set comes in\nthe context of fine tuning right so one\nof the things which we are typically\nseeing right I mean models general\npurpose models are very good at you know\ngeneralizing uh and looking at you know\ngiving at more generic answers but when\nyou are talking within the context of an\nEnterprise you need to have that\nproprietary data set on which it is\nfine-tuned on otherwise you know the\nanswers are probably you know full of\nhallucination then you are also you know\ngetting a lot of the biases and other\nthings on which the the genetic data set\non which the model was trained on so a\nlot of fine tuning which is happening\nwithin Enterprises is a make it domain\nready for my Enterprise right so okay uh\nwe we have for example you know just\nfrom the context of within language\nthese are different profiles of members\nright so let's you know train and\nfine-tune on some of that data the\nsecond thing is you know checking a lot\nfor hallucination and uh trust and bias\nbecause you know these are going to\nbecome the voice of the Enterprises the\nway these uh llms are being positioned\nare you know these are your Chief of\nStaff or or you know co-pilots which are\nessentially you know speaking on your\nbehalfs taking on on your uh for your\nbrand and the more the distinction\nvanishes between you know whether a\nhuman is speaking or a chatbot is\nspeaking the higher the guard rails\nwhich need to be developed so that you\nknow you are very very strongly checking\nfor all these different things\nhallucination and balances so I think\ndefinitely it's a need of the hour right\nyou cannot uh for most of the Enterprise\nsettings you cannot take a proprietary\nLang I mean sorry a general purpose\nlanguage model which exists and then you\nknow create a brand offering or a\nproduct offering within an Enterprise\nout of it you will need to clear curate\ncreate your own data sets unique data\nsets on which you need to fine tune\nrefine uh make sure you are getting rid\nof all all the other toxicity\nhallucination and other things before\nyou can actually you know launch a\nproduct and this is a process I see any\nbrand worth you know uh which wants to\ngo and launch their own llms and chat\nmods Etc they will need to follow so\nmore than an advantage I think this is a\nnecessity at this point that you've got\nto have uh techniques and platforms and\nyou know proprietary data sets to\nfine-tune these models yeah\nbefore I think you had your hand up\nthere I have a follow-up question\nbecause when we say data sets I think\nsome Enterprises may be thinking like\njust random data dumps just just random\ndata sprinkled throughout the Enterprise\nput in the data Lake we got our data now\nit almost sounds like you're saying is\nthat that expertise that we talked about\nearlier like the people who kind of know\nit's almost like they need to serialize\na lot of that context right those\nvarious data points because some people\nare just not capturing this unique data\nright they're just like well we don't\nget it from our sales register and we\njust know it's there we we make\ndecisions by Intuition or we don't even\nsave that data we just look at it and\nevery 10 days we throw it away and it\nsounds like a lot of this data that\nfeeds into that expertise needs to be\nserialized and treated as almost this\nversioned thing that is almost as\ncritical as the model that you're trying\nto generate so is that also one of these\nsteps that you need to go from I think I\nremember that hierarchy is like data\num information knowledge and then wisdom\nand a lot of people have a bunch of data\nthe last 15 20 years of big data but\nthen that data needs to be curated I\nthink using your word that curation\nturns it into like information and\nideally combined with ML it becomes\nknowledge and then how it's being used I\nthink that's where the wisdom comes in\nso hopefully that's a good way for\npeople to think about going from just\nraw buckets of data hoping for the best\nto really thinking about moving up that\nknowledge pyramid into something that's\nactually usable nylon I think you\ndropped early if you could introduce\nyourself and then maybe jump in on the\ntopic\nyeah yeah yeah so I think I was actually\nonline but I think maybe for some reason\nit wasn't visible to the to you guys uh\nyeah so yeah just quickly you know\ngiving my introduction so yeah my name\nis Alan and I'm working at Nvidia as a\nsenior software engineer and we have\nbeen actually building you know some of\nthese conversational AI based\nintelligent kiosks which actually kinds\nof you know amalgamates the\nconversationally it's a natural language\nunderstanding recommendations Vision AI\nyou know all of them clubbed into a\nsingle thing that you know you come in\nfront of the kiosks you have an\ninteraction with it and it understands\nyou and it understands even your emotion\nusing the vision and it can also you\nknow do some recommendations or certain\nthings like that uh so yeah I mean\nthat's kind of you know the background\nthat I have and uh just kind of jumping\nback to you know some of the points that\nanimesh was making about hallucination\nright and how they're actually\naugmenting it together with the profile\nof the user so uh so there are actually\ntwo things there right first one is uh\nwhat animation already mentioned that\nyou have a set of these documents and\nyou\nuh basically do some sort of a retrieval\nin order to make sure that you know uh\nyou have the versioning correct for\nexample Kelsey you're mentioning that if\nif let's say you have version one of the\ndocument and then you have a version two\nand you want to make sure that now given\nthat you have an AI which can do a\nquestion answering on top of it you want\nversion two to be used instead of\nversion one so that is where you know\nthe the whole information retrieval\nbased llm models comes into picture and\nthat is something that I mean chat CPT\nor some of these usual llm models only\ndon't support that but uh given that you\nknow what animation was mentioning about\nthese uh LinkedIn documents then you can\nbasically Club in those uh you know\ncontents that you actually retrieved\nfrom the query first and then you\nbasically to use that together with the\nchat GPT or in llm and that kind of\ngives you an edge that you know every\ntime you have a new version of the\ndocument that comes in you really don't\nhave to worry about fine-tuning the llm\nyou can just keep using the same llm but\nbasically change the information\nretrieval on this initial stage of the\npipeline so that you get the up-to-date\ninformation from\nyou know it sounds like you know if I\nwere thinking about someone being a\nlawyer and they're about to give an\nanswer that can determine my fate I\nwould like them to check their notes\nactually look at the legal president\nbefore so you have a good model you've\nbeen a lawyer for 20 years but it would\nbe nice if you actually had the current\nset of laws on hand before you give your\nrecommendation just could you just\ndouble check\nthe actual known facts could you look at\nthe elements of my particular case like\noh I defend people who run stop signs\nall the time you'll be fine what what\nmake sure you understand what state I\nwas in what city I was in what the\nactual laws were before you give that\nrecommendation so I think that\nadditional context if I had to try to\nput in analogy form but I do have one\nquestion for you now I'm pretty sure any\nEnterprise that's watching this right\nnow you being from Nvidia there's only\none question\nwhere did gpu's at\nand like why'd it cost so much where are\nthey and why is the GPU so important to\nthis whole process right A lot of people\nare familiar with Intel Inside most\npeople became aware of Nvidia probably\nthrough its strike the stock price or a\nvideo game they try to play and then\nHere Comes This World of llms and AI ml\nin general and it seems like Nvidia is\nat the center of that it can't just be\ngpus so maybe give people a little bit\nof peek into how the hardware plays into\nall of this\nso\nit's it's basically an amalgamation of a\nlot of different things that come with\nwith the GPU itself uh I mean talking in\na a very limited um you know what what a\nGPU ends up doing is that you know you\nhave\nyou have a let's say a model that is\nrunning\nwhat you try to do is that instead of\nyou just having maybe a single processor\nwhich is doing the whole you know\nanalysis of the data you now have\nbasically multiple threads uh so instead\nof a single CPU you can think of it as\nlike a having a 10 10 000 CPUs all of\nthem working in parallel and doing the\nsame thing at the very same time so what\nends up happening is that you are\nactually reducing the latency uh instead\nof it taking 10 000 seconds now you're\njust taking just a single second to do\nthe same thing and given that you know\nall of these large language models are\nactually quite compute heavy uh so you\ncan't really have an interns if you can\nand ask it to do the same thing as what\nmay be a GPU can do uh so it's it's all\nabout parallelizing you know the compute\nefforts of ingesting the data and coming\nup with the right side of the model\nand\nalso on top of the GPU now you know we\nhave this whole ecosystem where you\nwould also need you know the the\nlibraries and the software tools on top\nof these individual gpus to be\nessentially able to be able to you know\nmake it easy for the people to come up\nand use it for example any any you know\nperson coming outside of the background\nof gpus they really don't have to worry\nabout you know coding the actual GPU\ncode because that needs a lot of\nexpertise so basically you create all of\nthese abstractions basically create\nthese tools which makes it easy for an\nend user to be able to use these gpus\nand essentially come up with a uh with a\nset of requirements that that they have\nand be able to you know do that\nefficiently\nall right well look we got seven minutes\nleft I think caitan you wanted to jump\nin on this one so you'll do that and\nthen I'll go pick our next topic\nyeah I'll do a quick addition over here\nso I think this has been it you know\nnvidia's\npresence in this has been in progress\nsince 2006 and five like\num at that time there was a GP GPU\nprogramming Trend like general purpose\nGPU programming in fact 2007 my I wrote\npapers on gpg and I was like working in\na lab that was trying to make sense of\nwhat to do with this Hardware because if\nyou think about the hardware it's a mini\nsuper computer which is not really very\nsmart so you know in a general\nProcessing Unit you can do one operation\nwhere you can do badge predictions and\nso on this thing has thousands of\ncomputers or processors in it but they\nare very dumb they're all controlled by\nthis unsimplified processor so they can\nonly do one thing really well and\nactually the one thing turns out to be\nreally really important for gaming for\nAI for ML for\ncrypto for a bunch of things and that is\nmatrix multiplication because that's\nlike the core of all of this and and\nbecause you know you can have integrals\naccountants and that's where this came\non\num I think\nit is being tuned to do this really\nreally well by a lot of grad students to\nNvidia company to all kinds of people\nand this is just um\nand like when I was working in 2007 and\nGP GPU I would cry saying that this\nthing just goes offline all the time I\ndidn't know in 2011 that somebody will\nbring the vision model that beats every\nmodel that has happened uh till now uh\nyeah so it's it's been happening for 15\nyears just us not knowing it\nall right so what these last five\nminutes I think\nif you're an Enterprise user there's so\nmuch to take away and I'll summarize at\nthe end\nbut people are pretty curious you know\nsome people they need to have an answer\nfor their next board meeting but the\nBoard needs to hear something\nwhat are you doing about llms even if\nthey don't know what it means they gotta\nhave something and so I'm curious to\nknow have you seen any Enterprises right\nif maybe you can't mention the name of\nsaid Enterprise but it'd be good to\nunderstand what they do\nand how\nspecifically for this topic of\ndiscussion today llms has made a\nmaterial difference and how's that\nmaterial difference either experienced\nby their customers or their internal\nteams that are making decisions if\nanyone has any real world example like\nthis is the number one question that I\nget from a lot of Enterprises when any\nnew technologies is new introduced who\nis using it for something real and that\nhelps people gauge the sense of maturity\nor is this just going to be a fad and\nand fade out eventually anyone want to\njump in\nforeign\nI mean I talked essentially you know I\ncannot talk about others right but I\ngave couple of examples right within the\ncontext how we are leveraging it for\nLinkedIn and definitely you know we are\nseeing a great overall you know\nengagement from the users and some of\nthese capabilities and you may not you\nknow even realize while you're using it\nbut uh we have seen you know uh whole\nlot of uh email acceptances when\nrecruiters are sending these more\npersonalized emails which is then you\nknow behind the scenes powered by\nelements generating uh these emails\nwhich are contextually targeted to the\nparticular applicant right based on that\nso that's a great uptick we are seeing\ndefinitely a lot of the premium members\nwho essentially are generating headlines\nand profiles summaries for different uh\nLinkedIn profiles they are actually you\nknow we are seeing a lot of usage behind\nthat\num in in terms of you know being able to\ncreate more catching worthwhile profiles\nwhich are you know being looked at more\nso I think in general there is material\ndifference which is happening uh the I\ntalked about the collaborative article\nspace right where we are actually able\nto generate a lot of these collaborative\narticles based on the content which is\nbeing posted on LinkedIn and then you\nknow uh select different uh experts\nagain you know being powered by llms to\nactually then come and uh give their\nmore human view on those collaborative\narticles so a lot of these great use\ncases which are being powered right uh\nright there on LinkedIn so from our\nperspective I think it's real it's\ngetting great user engagement and you\nknow there are benefits which are being\nrealized\nname it\nyeah and and adding just one more point\nto it you know uh so we have been\ntalking about elements for now only but\nuh I mean in the in the near future and\neven what is happening right now is that\nyou are having basically multimodal uh\nsystems so you're not just talking about\nchat but you're talking about Vision\ntogether with language you're talking\nabout speech together with language for\nexample meta recently you know came up\nwith a model where you can just type in\nand say uh okay give me a sound of you\nknow a train which is whistling a lot\nbut it just passed me by and it gives\nyou basically a speech out of out of it\nso in in terms of the application it is\nnot just limited to you know just just\nthe language part of things but as as\nthings go you would see that there'll be\nlike tons and tons of different\napplications we have things where you\nknow people are even creating you know a\ncomplete episode out of just nothing you\njust type in a prompt and it gives you a\nstoryline it gives you uh you know\ndialogues it gives your speech input it\ngives you Vision so you can basically\ncreate you know anything out of it in\nthe near future so yeah applications are\npretty Broad\nwe'll probably have to do a part two of\nthis at some point we're almost at the\nend of the whole hour and I'm going to\ntry to summarize because I definitely\nthink I've learned a little bit and also\ngave me a lot to reason about and think\nabout when we think about humans we are\nvery very Advanced complex machines if\nyou appreciate machine learning you\nshould really appreciate human learning\nand context and experience and that\ncontext and experience that we have that\nwe walk around with well that's our\nlittle personal models and sometimes we\nchoose to share those models with other\npeople it could be experience in the\nmovies that we make we're really\nelaborate but one thing that we do\nis we use language to communicate verbal\nphysical every interaction we typically\nhave starts with some form of\ncommunication and that's how we do it in\nthe very Advanced society and now what\nwe want to do is kind of raise computers\nas advances we think they are we've been\ninteracting with them in mostly\nprimitive ways typing on the keyboard\nhaving to read all the documentation\nbefore you can do anything useful but\nmost of these Computing systems that we\ninteract with today I think we're moving\nto a world where now we're starting to\nunderstand the human interfaces we use\nVision speech all of these things we\nwant to now see being applied to our\nmachines but also we're now asking these\nmachines to make even more complex\ndecisions than before not just selling\nstuff at a known price but making the\ndecision on where we should go on\nvacation and decisions come with\nresponsibility and so I think\nwe heard some really good advice around\nyou just don't want to go get a generic\nlanguage model and start giving advice\nto your customer base what's the point\nthat's the race to zero the reason why\nyou're in business is because of the\nmodel you actually have of the world and\nyour data should reflect that that\nunique data set is what makes your\ncompany a lot more interesting than\nanother company and it's only when you\ncombine this new technology it's still\ntechnology large language models are\njust another example of Technology but\nwhen you combine it with this know-how\nthat's when you get something special so\nif you're Enterprise thinking about what\nyour approach has to be it's not a copy\nand paste just go and pay open AI a\nbunch of credits to access their API you\ngot to go deeper than that you have to\nspend as much time curating the machine\nas you can curate the experts that serve\nyour customers so with that I hope you\nenjoyed this panel I want to thank all\nthe speakers for hanging out with us\ntoday so much interesting context and\nyou've given people like me a new way to\nthink about this stuff end to end we'll\ncatch y'all next time\nthank you"
    },
    {
        "title": "Flyte School: Deploy Flyte on GCP - A live walkthrough",
        "transcript": "right welcome\neveryone we are live already\ncool so let's get straight to the point\nwelcome again to a new episode of flight\nschool this is\na kind of monthly or periodic let's say\nlive stream where we try to cover\ncontent that was a requested by the\ncommunity or we've seen patterns in the\ninteraction with the community and we\nfeel that it's important to dedicate a\nspace to to dive a little deeper on\nspecific\ntopics I'm David SPO um part of the\ndeveloper relations team at Union AI who\nis\nuh one of the largest contributors to\nflight and today we'll be covering the\nprocess to deploy flight on\ngcp cool so let me share my\nscreen and we'll kick it\noff\ncool right so we'll take a brief look\nit's kind of a recap on the components\nor the flight architecture components\nand\nalso an introduction on how the\nreference implementation for gcp is\nstructur uh finally how to use it and uh\nin the process of of covering how to use\nit we will use it live so fingers cross\num we be covering a bit on how to run\nworkflows especially workflows that\nrequire custom\nimages and some Q&A at the end if if you\nhave questions if you're watching us on\nLinkedIn or YouTube feel free to drop\nquestions in the\nchat uh I think streamyard will help us\nsurface this and uh we'll try to cover\nthem by the end of the session unless\nthere's something urgent we need to\ntouch right on\nOkay cool so yeah let me start with uh\num mentioning\nbriefly Union in spef specifically Union\nCloud product um Union is the main\nsponsor for this um sessions and it's\ninteresting because you bring your\ncompute basically and uh this is the\nflight managed version with a number of\nadditional features that are not\navailable in OSS like role based access\ncontrol and task level monitoring of\nresources among others so feel free to\ncheck it out it's a very interesting\noption to run flight or to let uh\nspecialized in run flight for\nyou cool so a recap on the flight\narchitecture this is straight out from\nthe documentation there's a section\ncalled component architecture the\ndiagram is not perfect but it kind\nof describes what you end up getting at\nleast At The Logical from a logical\npoint of view when you deploy\nflight uh we don't show here the user\nplane in the user plane you will find\nthe CLI and the\nUI and in the control plan you will find\ndifferent components U for for a deeper\ndive on the\narchitecture please check out the a\nprevious flight school session where we\ndedicated entire 60 Minutes to cover\nflight architecture piure uh the\nresources that are marked with a star\nhere are external dependencies for a\nflight\ndeployment and it's part of what we will\ncover here today so we will cover the\nIngress the database blob storage\ncornetes cluster and uh by the end of\nthe session we will have everything\nincluding flight itself deployed on a\ngcp environment\nhopefully cool so this is the overall\nIDE IDE with the reference\nimplementation\nin what what we mean by reference\nimplementation is uh considering\nlearnings from the interactions with the\ncommunity flight Community who happen to\nbe also gcp users uh we try to\nincorporate this in a in an automated\nimplementation using terap firm and uh\nwhen you run these modules you will end\nup with an opinionated environment that\nworks uh as is describing this diagram\nso you will have an Ingress resource\nthat is managed by an enginex\ncontroller um that will be your main\npoint of contact from the user\nperspective be the CLI or or the eii\nyour first touch point will be the\nIngress and then it will will connect to\nthe control in data plane services from\nflight and all of these Services run as\ncoordinates workloads on a GK\ncluster and for this to properly access\ngcp resources we are making use of\nworkload identity as the recommended\napproach to do this and in Cloud IM\nroles and service accounts and and that\nis the same for also for the actual\nworkflow executions they need to\nauthenticate before they can access be\nit cloud storage or blob\nstorage or uh before the flight control\nplane can con connect or contact or use\nthe database the relational\ndatabase uh it will need to first um get\na proper\nauthorization so this this is the\noverview of the uh of this reference\nimplementations a bunch of decisions\nhave been made beforehand and that is\nthe case with any other automation\nespecially for reference\nimplementation this means that you can\nadapt this extend for your particular\nenvironment but here we're trying to um\nuse this as an educational\nresource and also to help you bootstrap\nenvironment real\nquick cool so I'll briefly touch on the\nmodules but first I want to go to the\nrepo\nitself uh let me see Floy flight there\nyou\ngo so we will find here flight on\ngcp and I'll be basically following the\ninstructions here right so the summary\nis that you the first thing you need to\ndo is to create a project gcp and to log\nin uh in your gcloud CLI uh session to\nmake sure that that that session is\nauthenticated and authorized to access\nall the\nresources uh already did the first\nportion already have a project fight tf8\nit's already connected to bilan count\nand that's what that's all I have and uh\nthe next thing you need to have\navailable is a\nbucket\num simple bucket instructions are in the\nrmy but this bucket will be used by\nterraform to store\nState uh this is essential for terraform\nand these are the the two main things\nyou need to provide to the module a Sy\nproject and a GCS bucket that's\nit okay so the next piece is uh to\nactually authenticate your session so I\nwill do this right away it will trigger\nan oidc\nflow uh allowing me to\nauthenticate the session and uh there\nyou go um I'm here it's already already\nrecognize my my credentials cool so the\nnext piece according to the instructions\nis to go to the locals file I mean you\nneed to clone the repo Etc uh we go\nright away there and uh NOP this one so\nlocals um is where you will Define again\nthe the particular configuration or\nparticular identity of your environment\nso for example the application and\nenvironment you can change this um and\nthe modules will use this to build the\nprefix for the names of the majority of\nthe the\nresources uh I will change this but what\nI will change is the project ID I mean\nthe this is the what you will see in the\nin the repo so you need to make sure\nthat you change\nuh for the project ID and the project\nnumber uh you can get the project number\nuh from the\nCLI and we use the project number yeah\nwe have a lot of them and here's the\nproject unique project number so uh we\nuse this to build um unique names for\nthe GCF or a unique name for the GCS\nbucket that the modules create\nand this is because the the way GCS\nworks is that all the buckets from all\ngcp users are in a single name space so\nyou need to make sure that your bucket\nhas a unique globally unique name so\nwe're using the project number for that\nand a DNS name that could be hosted on\nGoogle cloud or any other provider and\nwe will use this to also to um configure\nfqdn to access flight and the gcp\nregion also an email this one is is\nimportant because as as you will see\nthis modules also configure SSL so the\nmodules use let's encrypt and ser\nmanager to\nautomatically create a ass signning\ncertificate for the Ingress and this\nparticular email address will be used by\nlet encrypt to send you warnings when a\ncertificate is about about to expire Etc\nso you should consider changing this and\nalso by default the projects and domains\nthat are Ena by default in the\nimplementation so I will leave it uh as\nis so these are these are the uh main\nthings you need to change and also you\nneed to go to terraform to the terraform\nmodule and here in the back end you need\nto specify the bucket name uh that you\nwill use for terraform state remember\nthe one that I I just described here is\nuh this one right you just need to you\njust need to specify the the name all\nright if if it's the first time you run\nthis um the first thing you will do will\nbe a terraform in it um right and then a\nterraform plan uh probably I will run a\nterraform plan I I'll already form the\ninit so I I w't do it again but probably\nwe can do it uh just for the sake of\ndemonstration nothing changed so\neverything was already installed all the\nproviders were were already install but\nif it's your first time it will install\nall the\ndependencies uh then I will um run and\nplan so this is kind of a dry run it\nwill it will inform you hey this these\nare the plan changes I won't I won't do\nanything so I'm just informing you these\nare the changes I will do I will do on\nyour environment right so it's also a\ngood way to catch um probably syntax\nerrors if in the process of customizing\nyour local file probably there's a I\ndon't I don't know missing H colon or\nEtc something like that uh it will be\ncaptured here so for now it's it's all\ngreen school so we will go straight to\nterraform teron\napply it will again compare the state I\nmean the the the current state of the uh\nenvironment versus the intended state in\nthe modules and uh yeah it needs to\ndeploy everything 59 resources to create\nso I will say yes yes it usually takes\nfrom 15 to 20 minutes if no\nerrors so uh in the\nmeantime I will keep describing the\nmodules so uh first one in the list is\nthe gy module uh this is how the end\nresult looks like it will deploy a g\ncluster using this table release CH\nchannel so probably you will see here a\ncoronated version that is 3 to six\nmonths years old and month yeah to six\nmonths\nold uh right now it's\n1.27 which is uh fairly recent I mean\n1.29 was released yesterday so uh this\nis pretty cool and also it will deploy\nthe note pools all automatically\nuh let me give you a look at the code so\nhere is uh again it will use the name\nprefix uh to build the name of the\ncluster so it will be uh for example in\nthis case flight dgcp will be the name\nof the G\ncluster and um yeah the release channel\nis stable it will be a regional type of\ndeployment and uh the note Pools by\ndefault in this reference implementation\nis\nthis type of in instance type uh I think\nthis is 4 vcpus and 16 gigs of RAM if\nI'm not\nwrong and uh but you can change this you\ncan adapt this to to your environment\nthis is a a compact let's say a compact\nor midsized uh in incense type that can\nrun the majority of the uh um basic\nworkflows without issues but if you need\nif you have heavier workloads probably\nyou will need to change the machine type\nwith the instance type right cool um\nyeah I think it's pretty straightforward\nnext up the GCS module this one will\ncreate an additional bucket and this\nbucket by default by default we use\nbefore storing the\noutputs from tasks from executions and\nalso to store metadata for example if\nyou eventually enable\ncatching uh it will all be stored in\nthat bucket right now we're working on\nenabling flight core the flight core\nHelm chart and I I I make a parenthesis\nhere there are two main hel charts Helm\ncharts to deploy flight flight binary\nthat chips everything a single binary\nright and flight core that that it it's\nmore designed for multi coronar clusters\nenvironments where you will see uh\nmultiple pots for for the different\ncomponents you will see I don't know two\npots for flight admin pots for flight\npropeller Etc and flight bner you will\nsee a single PO for\neverything and in Flight core there's a\ncurrent limitation to specify uh a\nseparate bucket for metadata and for\nstoring task outputs that's about to\nchange but that's the current status so\nthat's why uh this reference\nimplementation which uses flight core\nwill only deploy a single bucket right\nbut that will change soon and but even\nso I mean specifying an additional\nbucket is optional and we are aware that\nhaving a a bucket just created and doing\nnothing cost money so these modules only\ncreate a single bucket and configure Li\nto store everything there okay let me\nsee yeah it's making progress creating\nthe\ndatabase now the Ingress module this is\ninteresting um because well we first\ncreate the flight and engine X name\nspaces I know that a Helm chart or a\nHelm install command can also\nautomatically create the name space but\nfor the increas module we are not using\nhel\ncharts um the first step is deploying\nthe engine x controller I will show you\nthe code in a bit um remember that when\nyou define an Ingress resource this is\nonly aspirational goal because uh you\nneed a third party controller to\nactually reconcile that uh configuration\ninto an actual Ingress resource uh there\nare multiple Ingress controllers out\nthere available in this case we're using\nengine X we are aware that many gcp\nusers require uh the identity of our\nproxy and the the native GC uh\ncontroller to manage this this type of\nuh\nconnection there's already support in in\nflight for for the I app but it's not\nall it's not yet available as an\nautomated um terraform module there's an\nissue for that that and if you want to\nhelp with this that will be awesome uh\nit's in the kind of the in the road map\nto have also a an additional reference\nimplementation that uses the I app the\nIdentity or proxy but for now it's the\nengine x controller next step is that\nthe modules will install the serve\nmanager here this and this is important\nbecause I mean it's kind of explained in\nthe code and there's a link um for the\nSer manager Docks but in summary there's\na limitation on how Helm handles crds so\nthe the recommended approach for\nproduction environments is that the Ser\nmanager here this should be\ninstalled just using a manifest and and\nthat's what you will see in the code\nthen Ser manager controller is deployed\nand then a Ser manager issuer is created\nso this is all the structure to ensure\nthat whenever flight is created or\nflight is deployed also by the modules\nautomatically automatically the Ingress\nis\nenabled and there will be a controller\navailable to a um create an Ingress\nresource and V create a signed\ncertificate SSL certificate and\nassociated with the um Ingress uh so\nwhenever you perform a connection via\nthe UI or or the CLI\nuh it's it it follows an SSL it's an SSL\ntunnel let's say and the SSL connection\nis terminated in the\nIngress uh it doesn't include\nauthentication and that's that's an\nimportant thing to mention I right now\nit's a recommendation in docs the next\nthing you should do is to configure o\nwith your identity provider of choice\none thing we want to do and there's an\nissue for with that is to also add\nautomated authentication to these\nmodules\nusing I will say using key clock as IDP\nbecause it's kind of the easiest to\nembed into a cluster make it a workload\nand and use it as an\nIDP uh but but for now it it will handle\nall the SSL configuration for you right\nso let me show you the code right\naway uh this is what I mentioned name\nspaces the engine x\ncontroller uh it downloads and process\nthe Manifest for the Ser manager sir\nthis and install the here\nthis uh this secret is used by Sir\nmanager to configure\nTLS and install Ser manager and an\nissuer right this issuer is running in\nthe flight name space and here you can\nsee that it's created with the email\nthat you provide in the local\nfile\ncool then yeah it's live it's it's it it\nit's not finished but it's uh creating\nthe engine x controller or deploying the\nengx controller as we speak\ncool uh next up this one is pretty\ninteresting I spent a bunch of time with\nthis and I really want to give it a\nshout out to multiple um community\nmembers including fabior grats member of\nthe steering committee of flight for for\nhis support on this on refactoring the I\nam module uh to make it more align with\nGoogle's security\nrecommendations and kind of making it\nmore Gran\ngranular so this is the overall\nstructure again the it uses the\nrecommended approach of for for enabling\nGK workloads to access gcp resources in\nthis case it's it's using workload\nidentity so you will have a number of\npots here running G ke be it be it\nflight itself the flight control plane\nor your task executions so there will be\nkubernetes service accounts for for this\ntype of workloads there are service\naccounts for for the control plane and\nthere's there service accounts for the\ntask executions they will be\nautomatically annotated with an a\nspecific Google service account so the\ncoret service account will\nimpersonate or will use the Google\nservice account also with a custom role\nwith just specific permissions and this\ncombination plus the workload identity\nuser\nrole uh will\nenable um the GK workloads to get an\naccess token\nand uh be able to consume gcp resources\nsecurely right so in the past and I I\nstill see this um eventually even in\nother clouds users for example using the\nuh kubernetes note uh I IM roll to do\neverything um that's not good because\nthat gives a broad that gives broad\npermissions for anything running in a\nnote uh this is is as granular as\npossible at the PO level it will give\nyou this uh level of permissions it's\nall automated you don't need to do\nanything so here you can\nsee the IM\nroll so the first thing that it does\nkind of the process explained there but\nit will create this Google service\naccount for for the different components\nthis is also in line with Google's\nrecommendation of U making the\npermissions as tight as possible or as\nspecific as possible so we have a\nservice account for flight admin\npropeller scheduler data catalog and the\nworkers what we mean here by flight\nworkers are the when you execute a\nworkflow in Flight every task will be a\npoth and those pots will use a service\naccount and in turn that service account\nwill impersonate that the post will use\na coronary service account\nand\nuh and that service account will be\nassociated with a Google service account\nand so on so forth right so first step\nis to configure what we will be what\nwhat will be the Google service account\nthere's a special one this one it's only\nneeded if you will be using artifact\nregistry as your repo for for container\nimages if you don't need it you can just\nremove it uh but it's there because many\ntimes\nusers the next thing they do is to try\nto push C custom images to artifact\nregistry so will it will do two things\nthe first thing is that will it will\ncreate a service account with right\npermissions right now it's only created\nservice account and uh also it will\nenable the workers to have read\npermissions to be able to pull images I\nI will cover this by by the end of the\nsession next space is to create the the\ncustom R so here you can see that the\npermissions are are as specific as\npossible we are not configuring admins\nstorage admins nothing like that so uh\nflight admin just the stuff that it\nneeds in the blob storage propeller uh\nthe scheduler data catalog and the\nworkers right and uh we are um basically\nassigning this permission to each custom\nrole then we perform an an I am binding\nat the project level and we bind\ntogether uh the custom Ro we just\ncreated with the corresponding Google\nservice account so this is what we have\nhere custom roll with the service\naccount right it's right\nthere and the yeah this one here it's\nit's what I was mentioning it binds the\nartifa registry service account that we\njust created uh with a role this is a\ndefault role it's not a custom one a\ndefault role uh to be able to push\nimages to artifact\nregistry uh next step in the overall\nprocess is to do now an A binding at the\nservice account level so this is where\nyou bind the actual kubernetes service\naccount see it's in the it's mentioned\nin the names space the FL flight name\nspace in the flight admin service\naccount coordinated service account and\nbinding this to a Google service account\nwith the only required role to be\nworkload identity user basically giving\nit access to impersonate a Google\nservice account with a specific uh set\nof permissions right and we and the next\nthing uh we do is to do the same but for\nthe workers the process is a bit\ndifferent\nbecause we um I mean in every name Space\nby default you will have three M spaces\nto run workloads uh the flight snacks\nlet me show you uh the flight snacks Das\nDash development flight snack staging\nflight snack production will be three\nNami spaces and each of them will have a\ndefault service account that by default\nwill be used by the BS to do everything\nso uh we needed a different structure to\ngo recursively throughout all the name\nspaces to configure this uh binding and\nand enable them to be workload identity\nusers for the flight workers service\naccount\nright cool so that's the I am portion\nand it's\ndone I I haven't I haven't even finished\nthis destion in and the process is\nalready complete cool great right on\ntime how to use it so once the modules\nare done running the only output you\nwill see is this the gky cluster name\nand the reason for this is that we will\nuse this to um configure Cube cuddle to\nbe able to connect to the cor cluster\nfrom the CLI you can do the same from\nthe Google console but I wanted to avoid\nany kind of context um switching so we\nwill do most of the stuff here in the\nCLI so the next piece according to\ndocs uh will be to let me show you\nhere uh will be to do the following and\nis basically to connect I will do it the\ncloud container clusters get credentials\nthe name of your cluster in this case\nflight\nDas G gcp it's here this name the region\nand the project name and it will create\na new entry in in the cube config your\nlocal Cube config file it will\nautomattic automatically change the\ncontext there so you can see here that\nfor example if I get notes uh this is\nrunning on GK right it was deployed\nseven minutes ago so this is your new\nenvironment so we need the CLI because\nwe need to know what was the actual\naddress uh IP address configured for the\nIngress um the fact that there's an\naddress configured means that the engine\nx controller was able to do its job\nreconciling request and uh actually\nfulfilling this and and configuring\nIngress\nresource So the instructions from the\nrme is to go to your DNS provider and\nmake sure that your uh your host or your\nfqn now points to this uh specific IP I\nhave my DNS\non Route 53 so I guess that makes me a\nmulticloud\nuser but um I'll go\nhere uh it's pretty simple it's an\na see there you go it's a type a record\nso it's pointing to an IP address in\nthis case this\none cool it takes some time to propagate\nin the meantime what I will do this is\nall in the r me what I will do is to uh\nmake sure that my local config file\npoints to my fqdn to my domain name or\nmy DNS name Why is that because every\ntime you use the uh flight\nCLI uh it will it will query the\nconfiguration here and that's what it\nwill use uh to try to connect to a\nflight cluster so right now uh yeah I\nhave here A bunch of things so every\ntime I need to move between environments\nI I uncommon stuff so I have this end\npoint it will be fly that you know demo\nnot run you can use the the host you\nneed um where you will Define that host\nhere right the DNS domain by default the\nfqn will be formed by the\napplication uh what whatever you use\nhere in application. your DNS domain so\nyou can change this\naccordingly I'm using this here uh will\nmake sure that this is insecure false so\nit will it will force the CLI to use an\nSSL uh connection I will save changes\nhere right so if I do something like\nflight CTL get\nprojects uh it's great because the\ncertificate is not not thrusted yeah it\ntakes some\ntime remember that we um well there's a\nmodule that I didn't cover is the light\nmodule this\nmodule uh thank\nyou uh yeah this module actually\ninstalls flight for you so if if if you\ntake a look here get pods in the flight\nname space everything is already\ninstalled for you I mean flight admin\npropeller console everything is there\nand\nrunning and we didn't touch Helm charts\nor or whatever so um this module takes\nthe outputs from all the previous\nprocess in your uh terraform execution\nand it uses that information to actually\ninstall uh the flight call flight core\nchart in the name space in the flight\nname space right so everything is\ninstalled but the thing with that is\nthat it uses this file and if you go to\nthe\nIngress\nportion right everything is automated\nit's configured to use TLS\nright but uh at at the first moment it\ndeployed everything configured\neverything but the actual flight. Union\ndemo that run of your domain name wasn't\nactually resolving because we just\ncreated the a or or updated the the a\nrecord right so for Ser manager to be\nable to actually issue a certificate the\nDNS uh record needs to resolve the host\nneeds to resol and that takes some time\nI've seen anything from minutes to hours\nmultiple hours for this to propagate it\ndepends on on how your DNS configuration\nworks but but eventually we'll have here\na certificate let me\nsee uh seems like it's ready but let me\ntry again otherwise we can try with no\nwe can try with\nADD see we can try with\nuh insecure true the\nmeantime just\nto make sure\nthat uh\nconnection what is happening here uh\nthat's that's not cool\num okay let me see with this\nfinish\n[Music]\nokay there you\ngo yeah it seems to be pointing in the\nright\ndirection\num one last\nthing yeah I\nmean we can use this\nand use this as the\nlast um\nResort and otherwise I'll be yeah there\nyou go so what I did I I instructed yes\nit's an SSL connection but you cannot\nyet verify that this\nis that the certificate is trusted right\nbecause right now it's using the default\nthis is default for every Ser manager\nimplementation it will use this\nuh default certificate um one to zero um\nbut yeah it's working the the projects\nare there if you go to\nthe sl. union demo. runc\nconsole uh it should be there but with a\nwarning on the security side yeah\ncertificate is\nnot there\nyet and I'm not sure why is not letting\noh yeah it's\njusts yeah this is actually interesting\nbut yeah should be there uh visible for\nyou there are ways to travel shoot this\num\nCertificate\nrequest uh it's approved but it's not\nready and probably the reason is\nbecause the\nactual um BNS name is not resoled\njust yet right see\nhere\num here we\ngo um yeah it's\npending um yeah most typically it's due\nto the reason that I just mentioned is I\nmean the request is is right but um it's\ncorrect but it it's spending DNS\nresolution take some\ntime right but that's that's a kind of a\ndependency uh not exactly a flight thing\nat least flight is alive and running and\nuh try to use uh different browser just\nto make sure that I can overwrite\nthis uh if I can show\ndetails Bas it yeah Bas it no matter\nwhat um so it's there right cool so I\ncan start running workloads right away\nso if I do a p flight\nrun uh I need to add D dash dash remote\nto make sure that this runs in the\nremote cluster and I will run the\nexample uh\nworkflow so it's a good way of testing\neverything uh yeah it's the this is a\nwarning certificate verification is\nstrongly advised but it's pointing me to\nthe console now now you can see it's\nrunning and uh hopefully eventually it\nwill run\nsuccessfully right uh the next thing we\ncan uh do here or probably the next\nthing you will be trying to do is try to\nuse a custom image I mentioned so um the\nwith me provides the instructions let me\ntake this out the instructions for using\nuh artifact\nregistry um but in summary we're using a\nprocess that is documented by Google\nwe're not Reinventing the will in this\ncase um so right here what we have let\nme go back to the\ndiagrams and uh prob show\nthis\nhere so we we are\num we have two types of operations here\na push and a pull right for the push we\nhave a Google service account that has\nthe uh registered writer option as as I\nshow in the IM module um let me point\nyou\nhere uh there you go so we have\nthis uh artifact regist\nwriter and the the writer has the um the\nactual permissions where are you\n[Music]\num uh yeah there you go the actual\npermissions to eventually push images to\nthe\nrip your particular username will use\nthe service account will impersonate the\nservice account to get an access token\nand uh with this you will be able to\ngenerate and image pool\nsecret added to the default service\nkubernetes service account and in that\nway and plus the artifact registry\nreader role that was already assigned to\nthe flight workers it will enable the\nflight pods to pull the images right so\nit's a two-step operations first for the\npush you as a user or if it's a CI\nsystem that needs to consume this uh Pro\nprogramma ly it will be the same you\nwill use this Ro their service account\nthe artifact register writer to be able\nto push and your pots will use this R to\nbe able to pull right so the process is\ncovered here uh to actually make use of\nthis in the meantime let me see yeah\nit's all green uh at least at least it\nworks and you can see here just testing\neverything the outputs were stored in\nthe flight gcp data uh Google search\nbucket that the modules created right so\nit's all there and you can see here that\nit is using the default service account\nin each name space you don't need to\nknow corones for this just this is just\ninformational right but the next piece I\nI I I'd like to have a a better way to\ndo this and there are ways to automate\npart of this process but for now I'm\nleveraging what's documented by by\nGoogle cloud and how to connect to\nartifact registry but that's what I will\ndo here the next step will be here's the\nreference to docs but uh the F First\nStep here will be to uh create um create\nan\nactual um\nkey so this is\ndcloud sor\nokay service account keys so we will\ncreate first all that is required for\nthe writer role because that's the first\nstep in the operation to write so uh we\nwill use here uh that DCP registry\nwriter that's the service account or\nGoogle service account we're using and\nuh we will store this as gcp artifact\nrer\nkey right so that's the first pie you\ncan see here that it created this\nkey cool uh The Next Step will be to\nactually activate the service\naccount right so it it uh this is the\nimpersonation kind of\noperation where you will uh provide your\nkey file the one that was just created\nand the GSA the G rule service account\nyou will impersonate from your g-cloud\nsession or from a CI\nsystem right so that's the next piece\ncool activated service account so in\nthat context uh the next piece will be\nto actually authenticate to your uh repo\nCloud out\nlogin oh sorry green access\ntoken doer login Etc and the only thing\nyou need to change here is the location\nor the region for your uh\nregistry um I will show you here before\nrunning this that this is where\nmy\nregistry I have it configuring my\nproject it's running it's configuring\nthe same project where I'm running this\nand there it is it's a Docker type of\nregistry and and it's there right so I\nwill\nlogin and the result should be a login\nsucceed something like\nthat H yeah there you go Lo login\nsucceeded so your gcloud session is\nauthenticated the next piece is that you\nneed to generate an image image pool\nsecret uh to be able to actually uh\nconfigure the PO to pull images right uh\nthis is all cover here uh we will then\nagain create a a keys but in this case\nfor the\nworkers we don't have a separate Google\nservice account for\nreading um or for pulling images we're\nusing the one assigned for all the\nflight\nworkers um to make it even more\n[Music]\ndirect artifact reader ah yeah I need to\ndo this\nuh\nin remember that I use a context with a\nservice account and that now that I'm\nI'm using a different one so I need to\nkind of\nreset uh the identity I'm using to\nperform this operation this one creating\nthe\nkeys cool it created the\nkey uh The Next Step sorry for that the\nnext step will be to create a uh gron\nsecret this is the particular step that\nI'd like to\nautomate but it it's I mean it's heavily\nopinionated if we add this to the\nautomation there's a bunch of content\nthere content there or that's what I see\nso far probably uh in the near future I\num Prov to be wrong but uh uh the the\nthe reason for this is that this secret\nneeds to be available in each and every\nproject domain name space that you have\nin your environment if you create a new\nproject right now we we only have one\nproject if you create more projects you\nwill have three additional name spaces\nor three additional default service\naccounts there are ways to automate this\nfrom flight using cluster resource\ntemplates but for now for the sake of\nthis session I'm using just what is\ndocumented by gcp to make it work for a\nsingle name space right there's a note\nthere in the dog that says yeah kind of\nyou need to do this for every name space\nbut we want to automate this so we will\ncreate a um secret qtl create secret do\nregistry we will collect artifact\nregistry right and it will ask for the\ndo server is the location of your\nrepo uh the email is basically the\nservice account the the email address\nfor the service account that you will be\nusing uh the username is Json key the\ncare password is the the actual key that\nyou just created I think the\nthe file name is\ndifferent um and the name is space\nbecause as you probably know secrets are\nname space resources in cores so um I'm\ndoing this specifically for the default\nflight snacks development\nname right so I'm just creating the\nsecret the secret is there so the next\nthing I need to\ndo uh it's notr I don't love this but um\nit's what it is right now you need to\nedit um you need to edit the default\nservice account and specify image po uh\nC\nsecrets and then you need to say this\nname is act artifact registry the one we\njust\ncreated and if it's all right you edit\nwithout problems and now you can run\nworkloads that use a custom image so I I\nwill use\na a um a workflow it's it's very similar\nto the hello world workflow the only\ndifference is that it uses a custom\nimage I had this\num this one and we use image spec which\nis a a feature that is particular to\nflight where you we you don't need to\nwrite Docker files because it's it's not\nespecially convenient so you just Define\nhere the configuration you need for your\nuh Custom Image and uh it uses an a\nplugin called MVD and it will build the\ndocker image for you it's it's really\ncool I will change the name here just to\ntrigger a new build make sure that that\nat least a new layer will be\npushed and uh yeah let me P flight run\nbut just\nremote image spec is the same workflow\ntraining workflow um that we just use\nthere you go so yeah my my color scheme\nfor my C it's not great but I hope that\nyou see here\nsomething but uh probably by the end\nyeah most of the layers are catch which\nis perfectly normal but there will be at\nleast um One new layer here pendulum\nit's because we instruct the code to\num add pendulum to Custom Image as a\npackage um so he's building wheels and\neventually I mean right now it could\nstill fail if we don't have the correct\npermissions there's one point where we\nwill finally push the image and that's\nwhere where all this theory that I\nshowed will will come to reality pushing\nlayers there you go pushing\nlayers uh and it's done\nand it's done so if it's done I should\nsee something here um something new Let\nme refresh\nthis H there you go flight cool there's\na\nnew image\nthere\ncool okay so it asked me to go to the\nconsole um\nFL next development FL demo FL\nnext development there you go it's\nrunning yep it's running without\nproblems it use a custom image push to\nartifact\nregistry that's\ngreat all right I think that so far\nthat's it in terms of content\nagain for the SSL thing just remain\npatient uh if you you experience a long\nissue with this just hit us up probably\nthat's the go to action here\num join this slack community and head\nover to the flight on gcp channel we\nusually hang on there available to\nanswer questions\num every time we've seen this kind of\nbehavior with SSL is DNS propagation\nthat hasn't finished you don't need to\nreally do anything at this point and you\ncan see that even temporarily you can\nuse it without um SSL\nverification and if you like what you\nsee please go to the flight rep and give\nit uh GitHub star doesn't take much time\nbut it's really meaningful to us you can\nfind flight and Linkedin as that flight\nthere there are multiple flights there\nis even a band called flight but um that\nthat is The Right Flight in on LinkedIn\nLinkedIn and flight or in in the El musk\nsocial media I hope you enjoy this I\nhope it's useful for some of you and\nyeah thank you for watching"
    },
    {
        "title": "Flyte School: Learn Your Codebase - Fine tuning CodeLlama with Flyte",
        "transcript": "hi everyone I'm Neil denen I'm the chief\nml engineer at Union Ai and today um in\nthis edition of Flight School we're\ngoing to go through U how to fine-tune\nyour own large language model and serve\nit on your own infrastructure so in this\njourney I'll have made many opinions\nabout what parts of the tech stack\nspecific tools that I want to use for\nthis\num but hopefully you'll come away with\nthis with a sense of what you'll need um\nmaybe you'll pick your own tools but at\na high level you should be able to\nabstract away some of the specific\nchoices I've\nmade um so before going into the main uh\npart of the talk just a brief\nintroduction on myself so\num as I said earlier I'm at Union AI\ndoing machine learning work mainly\nbuilding\ntools um and apis and you know useful\nthings that will make you a little bit\nmore productive hopefully um but I'm\nalso the open source creator for of\nPandera and Union ml I'm one of the core\nmaintainers of flight and my background\num was coming from biology and public\nhealth and um before I kind of fell in\nlove with programming and data science\nand ml um that was where I was headed\nbut you know now my mission is to build\nopen source tools for data science and\nmachine learning\npractitioners so unless you've been sort\nof living under a rock you will know\nthat llms are really hot right now um\nthey are not used just for language\ntasks but also um like natural language\ntasks but also programming tasks and a\nlot of other very promising applications\nthat are um I'm personally excited about\nmultimodal um\nTransformers but today we're going to\nfocus on the language part of it and\ncoding part of it so um before we get\ninto more technical details I don't want\nto give you kind of this way of\nframework we're thinking about\nwhere say classical machine learning and\nwhat's being called AI these days where\nit all sits in this ecosystem right so\nyou may have seen this blog post um uh\npublished by Laten space and this is how\nthey're framing it right so on the on\nthe left hand side you have data and\nresearch constrained activities um\nthings that uh\ntraditional classical machine learning\nresearcher and engineer\nmight the space they might be in so your\ntraining models and you're evaluating\nthem um and you're\nusing kind of all the traditional\nconventional tools and techniques um for\nassessing you\nknow model performance and uh deployment\nperformance at deployment time um and\nwhere a AI engineering sits is\nessentially using machine learning\nmodels as components to larger software\nsystems and all the way over the right\nwould be a full stack engineer where\nyou're building platforms supporting\nplatforms and products\num so as you move from the left to the\nright you care maybe care less and less\nabout specific architecture you're using\nand the all the datg gritted details of\nhow something was trained um and you're\nmore concerned about how to stitch these\nthings together to build a a llm powered\napplication for example\num obviously it's very important to know\nhave a foundation there but I think the\nskill sets have um evolved such that\nroles can have various different\nconcerns um so one way I want to kind of\nframe this for this particular talk is\nthere's this AIML product stack and the\nvery bottom similar to the the previous\ngraphic is ml research so you care about\nmodels as optimization outputs the next\npart of the stack is uh models is\nweights ml engineering so uh you may not\nbe researching and developing brand new\nsay neural net layers or loss functions\nor things like this but you're taking\nthose components and you're trying your\nbest to find the most optimal set of\nweights for a particular data\nset and then further on top of that is\nwhat I will call a AI engineering which\nis seeing models as functions so it is\nliterally maybe even an API call or a\nfunction call that's abstracted away to\nsuch a degree that you don't even really\nthink about what the underlying\narchitect model architecture is You're\nsimply giving it say text and the\nfunction is doing some interesting you\nknow Matrix multiplies or you processing\nthe data in some way and then you get\nsome text back\nout and as long as the output conforms\nwith you know what your requirements are\nfor your particular appli\nuh you're all you know you're you're\nfine you don't really care about the\ninternals of the\nmodel then finally finally in the top\nlayer is AI product\ndesign and these kind of this is kind of\nT tongue and cheek but kind of treats\nmodels as magic right so llms have\nunlocked a lot of different use cases\nthat were previously not available to us\nbecause now we can just sort of for to\ngreater or lesser degree greater or\nlesser performance you can get an llm to\nsort of take a initial input you know\nchunk it up or like rerender that prompt\num so things like Chain of Thought uh\nallow you the llm to reason about the\noriginal input tokens and output more\ntokens to help it come at an answer for\nexample so this is the way I'm sort of\nlooking at this the landscape of all the\naway from Models at the lowest level to\nthe AI products that we enjoy\ntoday um and I guess the message I want\nto start with and motivate this entire\ntalk is that it's still very early days\nand things are evolving very quickly so\nif you are an AI engineer or an ml\nengineer data scientist I think it's\nstill important\nto um build\nusing these this kind of powerful\narchitecture to build things to sort of\nlearn about them and to build intuition\nabout how they work and how they don't\nwork and uh I'd argue one of the ways to\ndo this is to build developer tools so\nthat you are a user yourself and um in a\nrelated note one way to do this is to\nget your hands dirty and fine-tune your\nown llms on a problem space space you\ncare\nabout you don't always have to fine tune\num but this is the I guess my message\nhere is like to learn and to own maybe\none part of the stack and in this\nparticular case I've chosen fine-tuning\nas opposed to say retrieval augmented\ngeneration then finally just um from a\ndeveloper productivity perspective it's\nvery useful to use tools that support\nspeed reproducibility and um declarative\ninfrastructure and these are points that\nI'll get get into later when we talk\nabout\nflight which is now um literally and I\nwanted to introduce you to flight for\nthose of you who are not super familiar\nwith it or even if you are um flight is\nan open source workflow orchestration\ntool that unifies data ML and\nanalytics and as you can see here in\nthis brief code\nsnippet in Flight you can compose\nworkflows which are sort of more complex\ndata transformation\npipelines um in this case it's fairly\nsimple we're just getting some data and\ntraining a\nmodel um the declarative part of this\nthat I mentioned earlier is that you can\nactually sort of add configuration to\nyour tasks which are the building blocks\nof your\nworkflows and in this case I'm telling\nthe task to render a flight deck for me\nwhich I'll also show you later but this\nallows you to visualize static artifacts\nthat are part of your\ntask\num and then in this next step you can\nsee that I'm actually requesting\nspecific resources for this other task\ntrain model and um I'm asking for two\nCPUs in four gigs and this is something\nyou can do quite naturally in Flight is\ncreate complex heterogenous workflows\nwith different requirements per node in\nyour\ndag once you have your workflow you can\nsimply use py flight run and run\nit um and this will run locally just as\nregular python code or you can add this\nremote flag um and when as this GIF\nCycles through I'll point this out to\nyou all you have to do\nis um add this remote flag and if you've\nconfigured your flight uh config file\ncorrectly it will create an\nexecution which we will see here so this\nis a video of the flight console as you\ncan see there's a list view of all the\nnodes IE tasks that were\nexecuted um you can inspect their inputs\nand\noutputs\n[Music]\num as you can see here it's using S3 as\nthe intermediate\nstore um we have a graph use here and a\ntimeline view to help you sort of\nprofile your workflows and see which\nones are the longest running find out\nthe bottleneck in your workflow so\nhere's a flight deck that was that I\ntold flight to render as you can see\nit's just a simple table but you can\nembed any arbitrary\nHTML so that's a quick rundown of flight\nessentially you write code locally you\nrun it locally and then you can ship it\nto to a flight cluster super\neasily um and you want to do this for\nmany workloads including deep learning\nwhere you require a lot of memory GPU\nmemory and and\ncompute so the thing we're gonna quickly\ngo through today in this um the next 30\n40 minutes is I will share with you a\njourney that I've been going through uh\nbuilding a flight\ncode flight specialized code assistant\ncalled flight co-pilot um so this is the\nkind of AI product\num at the AI product level where we're\nlooking to build something at at\napplication Level to help in this case\nflight flight\nusers so we're looking to build\nsomething that generates text based on\nsome prompt so this is the simp simplest\nsort of application you can build with\nan llms which is just uh which is just\ntoken\ncompletion um and this give gives you a\nsense of how this\nworks um so I'll be doing a demo and\nyou'll be kind of seeing this a similar\noutput here\nlater the underlying model that will be\nbe powering this system um I've called\nflight llama since it's built on code\nllama and we're going to fine-tune it on\nthe flight code base and documentation\nof which there are a handful of repos we\ncan use as our data\nset um in the\nnext section of this talk I just wanted\nto briefly go through and justify and\nmotivate why do we fine tune and then\nshow you some of the techniques that are\nuh heavily used in the fine tuning\nspace so we have llm it takes an input\nand produces an output\nwe can do prompt engineering and more\nbroadly AI engineering which is to\nmanipulate the input space um this is\nall rag is right we can reduce it to um\nit's a little bit more complex you need\na vector database perhaps and you need\nto find a similarity of uh uh items in\nyour vector database based on your\nprompt um so I'm not going to go into\ndetail there but essentially you're\nmanipulating the input space in order to\nget the desired\nBehavior whereas in fine-tuning you are\nusing a a custom data set that you want\nto specialize\non um to further update the actual\nunderlying model weights of the model of\nneural net\nitself um some of the pros and cons of\nprompt engineering versus fine-tuning\nare essentially around um they're sort\nof correlated with these decisions right\nbut if you're prompt engineer and you're\nmost likely using a proprietary model\nsay coh here or open AI or something\nlike this um the nice thing about this\nis you're essentially using this these\npowerful models as a function and you\ncan sort of control your costs and and\nhow you're interacting with it um more\ngranularly whereas with fine-tuning you\ndo have full access to model weights but\nthat incurs the cost of having to create\na high quality data set um\nand meting the gpus and the resources\nand\nexpertise for actually doing this\nfine-tuning here's a my little decision\nchart here that's also sort of a joke\nbut um you know we start here at the top\nleft and if you haven't tried prompt\nengineering yet um as a application\ndeveloper right you're not a researcher\nresearcher you can do whatever you want\nbut uh with if you're trying to build an\napplication it's probably a good idea to\nstart with prompt\nengineering and also if you have say out\nof distribution data that's super\nspecialized that probably doesn't exist\non the internet for example then it\nmight be worth it to fine-tune um but\nit's always good to start off with\nprompt engineering since there are you\ndo have access to much more powerful\nmodels but if you have a fairly bounded\napplication and problem area then it may\nbe may make sense to fine\ntune so there's\nessentially kind of three ways of\nfine-tuning or three different ways of\nthinking about it is Contin continued\npre-training was simply\nuh what it sounds like right so you just\nhave a pile of tokens and you're just\nuh optimizing your chosen loss function\num against the the token\ndistribution with supervis fine tuning\nyou need a higher quality data set of\nprompt and\nresponses and with rhf you have uh in\naddition to the kind of like leveraging\nRL for this you do need a data a very\nhigh quality data set of prompt with\nmultiple responses that are are\nranked um we're going to focus in this\nflight school session on continued\npre-training it's the simplest setting\num and yeah we'll get our feet wet\num going to briefly go through some\nfine-tuning techniques just to give you\na sense of you know what people are\ndoing to make training more parameter\nefficient and um to be able to fine-tune\nvery large\nmodels on smaller data sets so you you\ndon't necessarily have to do full\nparameter\nfinetuning so just to appreciate the\nthis right if if I have a model with\nparameters Theta here um assuming I'm\nusing the atom Optimizer I'm going to\nneed five times the GPU memory to\nactually do training itself since you\nhave activations gradients and Optimizer\nStates for each of those parameters\nright um so one way to do to make your\nyour training more um memory efficient\nis to do to leverage quantization now\nthere's a notebook here you can play\naround with I if I had more time I would\nstep through this but essentially what\nyou're doing is you are representing\nfloating Point numbers which are the\nbread and butter of data type of neural\nNets and you can represent them in less\nwith less Precision so you have 32\nversus 16bit float floating Point\nnumbers this is an extreme example where\nyou're uh representing a floating point\nin terms of an integer and and you know\nyou're saving 4X the amount of\nmemory um the this is just a chart to\nshow you you know if you quantize and\nyou do it right you actually don't lose\nany uh\nperformance and um the next technique I\nthe last one I just wanted to briefly\nmention is Laura which is turning out to\nbe quite a Workforce in terms of fine\ntuning um in regular fine tuning right\nyou have your weight\nyou get an update and you need apply\nthat weight update directly to the\noriginal pre pre- chained\nweights but with Laura you actually keep\nthose two separate right and the way\nthis becomes more efficient is when you\nrefactor this Delta W um block here into\ntwo lower rank matrices so if you think\nabout how to line up the different M\nmatrices if the original pre-trained\nweights have D * D um\ndimensionality as long as the Matrix\nmultiply of A and\nB is the same as the original\npre-trained weights then you're all good\nso what's happening with Laura actually\nis you're approximating the gradient\nupdates with these two smaller matrices\nand so this makes um your model size the\ntrainable parameters in your training\nset up much much\nsmaller um and R would be that the hyper\nparameter that you can dial up or down\nthe the memory um requirements of the\nadapter\nweights here's just to show that in some\non some data sets on some problems Laura\nactually does equivalently or if not\nslightly better than full fly fine\ntuning and there's some theories around\nthis that I I won't get\ninto lastly the um I'll point out Cura\nwhich is a kind of a development\nextension of Laura which is you do use\nquantized weights here so as opposed to\nfull fine tuning and as opposed to Laura\nQ Laura uses 4bit floating Point uh\nrepresentation for everything and it has\nsome other things um like performing\ndouble quantization which I won't get\ninto and um paging Optimizer sa to CPU\nto handle the spikiness of GPU memory\nuse during\ntraining um okay so we're almost ready I\nthink to hop over to my terminal Editor\nto uh walk you through the fine-tuning\nprocess\num but first here's the kind of Bird's\ney view of the flight co-pilot vzer\narchitecture um we're going to First\ngrab some data from GitHub um to grab\nthe flight repo source code and\ndocumentation\ncreate our data set here we're going to\nthen fine tune with Cur using uh some\ndependencies I'm calling out here that\nare pretty critical for this going to\nuse weights and biases for experiment\ntracking um then we're going to publish\nthe model to the hugging face model\nregistry and we're going to use this as\nsort of our yeah our model registry and\num for the serving layer we're going to\nuse mosac\num which is a model inference framework\nthat's open source and then we're going\nto deploy it to Model Z which is a a\nhosted serving platform that uh is\nbehind the mosac project as well and\nthen we're going to see as I showed you\nin that gif earlier we're gonna have a\nclient that can um essentially stream\nthe llm outputs based on a\nprompt\nawesome so just G to check real quick on\nthis okay on\nquestions all right let's move on then\nso G to hop over here to my terminal um\nwe're g to kind of go through this\nentire journey of creating a data set\nfine-tuning model serving and then\ninteracting with a client\nCI set myself up\nhere\nokay so I'm just going to orient you\nhere first um just get rid of\nthese and we are in the flight the llm\nfine-tuning repo this is actually an\nopen source repo that you can access and\nif you have a flight cluster or Union\ncluster you can actually uh run the\nworkflows in this repo um on on your\ncluster so here we\nare\num first thing we I'll do is I'll just\nshow you what this repo looks\nlike um the important part here is\nflight llama this flight llama package\nit contains all of the source files\nwe'll need to fine-tune deploy and um\npublish and then deploy our\nmodel server um we have some config\nfiles here that I'll give you a quick\ntour of in a\nsecond um requirements files obviously\nfor the virtual environment I'm using\nright now and some Docker files for\npackaging up our application for\nserving\n[Music]\num the first thing we'll do is we\nwill create the data set\nso\nin this data set\nfile this is pure python by the way so I\njust want to get everyone's feet wet and\nI'm not going to show any flight code\nquite yet so all this does is it takes a\nlist of URLs and these are all the\nflight repos containing source code and\ndocumentation it's going to iterate\nthrough each of those\nURLs grab the documents and dump them\ninto a\nthe common\ndirectory so here's we're iterating\nthrough the URLs and then here we're\niterating through each file in the in\nthe repo and then writing them all just\nto this common directory\nso I'm going to do is run flight llama\ndata set with this output\npath so I'll take a few more seconds oh\nthat's done\nso our data set is not super big\nright um just to give you a sense of\nwhat what it looks like there are some\nbash incantations to just go through\nhere so this just counts the number of\nfiles per repo that we pulled in and as\nyou can see flight has the most number\nof files um this contains a lot of the\nmain source code uh components um flight\nkit is the flight SDK that you might\nwrite in Python so that from a user\nperspective that's like a big Focus\nflight snacks is our documentation web\nuh\nrepo and um so that's the composition of\nthe different repos in terms of\ndifferent file formats you can see a lot\nof them are Json and DL um these are\nthis is because flight is built on top\nof gretes and has a lot of configuration\nso a lot of those\nfiles um it's why we have a lot of Json\nand yl\nfiles\num flight many components of flight are\nbuilt on top of or built in go go\nline th explaining\nthis and um yeah a lot we also have a\nlot of python files so hopefully this\ngives you a sense of the high level kind\nof description of the data\nset um but it's always is useful to look\nat the data itself in in the rawest form\npossible and it's useful\nbecause uh you're you're never going to\nsee the entire data set especially\ninternet scale data um but it is a good\nway at least in this case to just get a\nget a sense of what it looks like what\nwhat we're optimizing for so I'm gonna\nimport some useful things here that\nwe're going to use to inspect our data\num do some\nimports going to use this get data set\nfunction\num and basically this is using the\nhugging face data sets Library um and\nhere's the first example in this data\nset so it's code and you can see\num roughly speaking this all of this\nwill be turned into tokens and so\nimagine this is like the context that\nthe model sees and it'll have to predict\nsay I don't know this if this is a one\nof the um tokens in the\ntokenizer so to get a better sense of\nwhat that looks like let's load the\ntokenizer we'll be using Code llama 7\nbillion and let's tokenize that first\nexample and if you look at the input\ninput IDs this is literally what the um\ntoken representation or the feature\nrepresentation is that the the model C's\num each of these represents an index\num in the vocabulary of the\ntokenizer okay\nawesome um yeah if we had more time we\nwould look at more examples but as you\ncan tell a lot of these will be code a\nlot of them will be config and\ndocumentation will be probably the least\nout of um all the files we\nsaw okay so next step is I will run this\nlocally and hopefully this the demo Gods\nwill\nbe um happy today\nokay uh oh yes I have to do\nthis so I'm actually gonna train this\nlocally so um actually I'll cancel this\nand I'll break down what this is doing\nso I'm using pight run so this pight is\nthe CLI that ships with flight kit and I\nam pointing it to this workflows dopy\nfile and I'm calling this train task so\nlet me hop over and just show you what\nthat looks\nlike so in my workflows file I've\ndefined a bunch of tasks in workflows\nand the workflow I'll be F I'll be\nrunning just now is the training\nworkflow so as you saw earlier if you\nrecall in the intro to flight the\ntask allows you to uh declare your the\ninfrastructure that you need and some\nother metadata that you need for any\nparticular task so in this case I'm\nusing uh the caching feature such that\nuh I don't have to recompute and retrain\nthe model if I give it the same\nconfiguration um in the\ninputs um I'm also asking\nfor the resources of uh any you know\nCloud instance that has h gpus\num I know I'm using j4 DM uh large\ninstances here in this case um but here\nI'm I'm in a kind of agnostic way asking\nfor 120 gigs of memory 44 CPUs\nEtc could also ask for Secrets um so I'm\ngoing to be using my weights and biases\nAPI key and hugging face Hub API key\nhere as\nSecrets um the training code itself is\nkind of a wrapper\naround the python code that I developed\nkind of independently so a bunch of this\nis flight specific code to get secrets\nand um prepare the configuration the\ntraining code itself is very\nsimple\num there Fair am amount of lines on it\nbut all it's doing is we're loganing the\ntokenizer um preparing the configuration\nfor 4bit Q Laura um so this allows you\nto load your model in forbit\nrepresentation we then load our model\nhere um we then ask our training\nfunction to use qora which essentially\nallows you to\num add another bit of configuration here\nwith the p library\nand load your model such that you freeze\nyour weights as I showed you in that\nearlier slide and then you create your\nLaura adapters that are much smaller\nmatrices that approximate the the\ngradient update to those\nweights\num we grab our data set we tokenize\nit and we wrap everything up in in this\ntraining argument that uh hugging face\nTransformers uses and then we call\ntrainer. train here at the bottom so\nmultiple steps again all this code is\nopen source so you can um stare at it\nand read it for for much longer but I'm\ngoing to go ahead and run pipelight run\nlocally\nand I'm actually using the\nlocal\num you see here I'm using this local\nJson um configuration so if we take a\nlook at\nthat where using um here's a lot of the\noptions right but we're using a 70\nmillion python model here just locally\nso I don't blow up my computer but we\nload in the data set and as you can see\num this is just a useful way of\ndebugging and just making sure your code\nworks this is one of the the big things\nin Flight as an orchestrator is you\ndon't have to ship your code and run it\non your flight cluster can actually run\nwith few exceptions all of the code that\nyou'll ever write in flight you can run\nlocally\num so great so what happened here is we\ngave a data set we created just\nseparately we fine-tuned our model and\nyou can see here that it spits out this\ndirectory this actually is a local file\nor directory that contains the different\num\nartifacts um backing the Laura adapters\num so this is the low adapter\nweights um and some other configuration\nthat that hugging face dumps\nout um so to show you how simple it is\nto scale this so we ran this locally\nwe're happy with the code and now we're\ngoing to run this remotely and literally\nall I needed to do was add this little\nremote\nflag I added this copy all flag also\nwhich actually will copy all the files\nin your working Direct\num in case you've changed any of like\nyour uh custom your your local python\nsource files that you that you want to\nuse with your workflows um\nso pretty much the same except for this\nother config file which I'll show you in\na second but here we're actually going\nto be using the code llama 7 billion\nmodel and if we run\nthis\ntake a few\nseconds what's going to happen actually\nis it's going to spit out a URL but let\nme just take you through what happened\nhere which is flight actually um allows\nyou to specify your container\ndependencies in a way that's I think\nfairly ergonomic and nice so if I hop\nback to my workflows\nfile you we see that in the container a\nimage argument we're passing an image\nspec and this image spec is\nessentially a container specification\nfor um your task right so you can\npotentially have different image specs\nper task but in this case we will just\nuse a common\none and here we can point to\nrequirements. text file that contains\nall the dependencies uh python\ndependencies um you can add app packages\nand specify Cuda version and a few other\narguments here like environment\nvariables um but I've actually run this\nbefore so it's um notice that there's\nalready an a image in this registry by\nwith this version so\num oops Yeah all it all it did was it\nskipped the build and it immediately ran\nthe\nworkflow on the configured cluster\nso here it ran quickly because the step\nwas is already cached right\num let's see if I can find one\nthat's this is actually yeah this is the\noriginal workflow that was cached and\nhopefully we'll get some metrics here\nbut if not yeah\nso just to orient you here um this is a\nunion\nCloud uh this is Union Cloud product and\nso this is essentially a managed flight\ncluster with additional features that\nare are nice mainly geared towards\nEnterprise and\num some bells and whistles for\nobservability and monitoring but as you\ncan see here we had two steps in our\nworkflow um here's the graph\nview here's a timeline\nview but we can inspect you know the\ninputs this this was all the\nconfiguration that went into our\ntraining workflow this is an output so\nthis is a S3 p to all the model\nartifacts that we saw\nlocally um and if I show you this few\nutilization tab which is this this is a\nunion specific feature where you can\nactually see GPU memory allocated and\nGPU utilization um also metrics on CPUs\nas well and some logs that uh now that\nthis task is completed the logs are kind\nof wiped um but this is super useful for\ndebugging\num\nso that ran very quickly because it was\ncached um for the purposes of this talk\nwe we not actually run one that that\ntrains um but you can see the different\ntools that you have available to you to\nkind of make sure that you're fully\nutilizing the um resources you're asking\nfor the next thing we'll do is we'll\npublish this model and so what we'll do\nhere\nis we are going to actually go back to\nour\ndashboard we're going to look at the\noutput here so we want to publish this\nspecific model to hugging face so I\nactually have a\nworkflow down here called publish\nmodel and all it'll do is download that\ndirectory from\nS3 um this is all\nabstracted uh by flight by the way so\nyou don't have to reason about S3 or\nanything like this and we have this\npublished to hugging face um function\nand all it's doing is it's following the\nhugging face Hub API to upload files and\nfolders to our hugging face repo um so\nlet me just run this real quick and I'm\ngonna swap out this S3 path for the path\nhere\nso we're going to run this\nworkflow and spits out the URL to the\nexecution again this has been cached as\nwell um so this is this is nice for this\npurpose just to show you uh completed\npublication flow and I've had I have the\nworkflow outputting the repo URL in\nhugging facee so hopefully you can see\nthis um this is a not the best model\ncard but um this is the published um\nfine tune model and so here here are all\nthe artifacts that we're going to use\nand in the last I do want to save some\ntime for questions but in the last 10 15\nminutes I I'll take you through the\ndeployment and serving of this\nmodel so the use case I wanted was a CLI\ntool just for this stage of the project\nand so I wanted a streaming kind of ux\nwhere I don't have to wait for the\nentire um you know if I request say a\nthousand new tokens in the the to\ncomplete the prompt I don't have to wait\nfor the entire sequence to be generated\nI wanted the server to um basically send\nevents um for when some par particular\nnumber of tokens maybe every 10 tokens\nit'll uh\nstream so that's kind of the main\nrequirement for our\nserver and and what we're going to do\nhere is I'm going to hop over to this\nother\nterminal\nand I'm just going to run this locally\nso hopefully\nagain export some Secrets here so you\ndon't um see\nit and we're\ngonna look\nat a mosac server so mosac as I said\nearlier is a framework for model\ninference model serving um it's fairly\nsimilar to a lot of Frameworks out there\nit's pretty easy to use I would say um\nbut the main thing construct in mosac is\na\nworker and a worker is initialized with\nsome state in this case we're loading\nthe uh language model\npipeline um and then a forward method\nwhich basically takes in data which\nyou're going to send through the client\nand it's going to you know pipe it\nthrough\num the language model and produce an\noutput so I'm actually going to swap out\num this model for the smaller one that I\nused\nearlier and I'm going to say no\nadapter and I'm gonna start my\nserver um cool so it's going to load up\nthe model now it's\nrunning and I'm going to\nhere again this is Pia 70 million that's\nnot fine-tuned and um so I'm G to now\nsend flight is\na to that server and I'm GNA have to\nwait for some\ntime and as you can see the the model's\nnot great it's probably never seen these\ntokens before and it just loves talking\nabout favorite movies and it just kind\nof repeats itself um so not great output\nbut um you get a sense of you know you\nstart a server locally and and you can\ndebug stuff with mosac quite\nnicely um the next thing thing we'll do\nis I\nwill run this Docker\nbuild and this Docker file\ncontains a different server file it's\nnot the one that we just saw here\num right it's a server that does\nsupports um serers side events and so\nthis is what's going to enable the the\nstreaming kind of ux and the the server\ncode is very similar the only difference\nis we're using a special we're\nsubclassing SS worker instead of\nworker and we\nare our forward method looks slightly\ndifferent we're basically going to\niterate through some end turns and send\nthe server stream event\nuh which is over here incrementally as\nwe go through the end\nturns\num so hopefully that's that all makes\nsense um and now we are going to I'm not\ngoing to run this locally but\num just take you through kind of the\ndocker build process um so this will\nbuild it we're not going to wait for\nthis all to build because I I already\nhave one um that's already\nbaked\num so then what I'm going to do is going\nto use a deploy\nscript and let me just run\nthis so This deploy\nscript is a simple little script that\nhits the model Z API and Model Z again\nis the posted model surveying\nplatform and um I won't belor you the\ndetails here you can check this out uh\nin the open source repo but all we're\ndoing is we're taking that image that we\nbuilt with the\num the server ssse file as the entry\npoint and we're going to deploy that\naccording to um model these kind of\nconventions and how you specify these\nthings so you give it an image you\nprovide your API key you tell it your\nstartup duration for the service and a\nfew other things\nand this is the script I just ran and it\nspits out the deploy key um so this\ndeployment is linked\nto my model the account which is here\nit's a fairly simple kind of clean\nproduct um and you can see here I have a\nwarm flight llama ssse server here\nrunning but this is the one that we just\ndeployed has zero invocation so far um\nso what I'm going to do actually is just\nrun the one that's already warm just so\nwe don't um have to wait for too\nlong and we're going to do this with the\nclient ssse script and if you haven't\nseen uh what these look\nlike they're also fairly simple um we're\nusing htpx here um to make those call s\nand the main thing here is we're\nbasically creating a Model Z client to\nget the inference\nURL and we're creating a server side\nevent connection here this is again a\nhttpx um\nssse uh convenience\nfunction and this allows us to create a\ncontext context manager with the Event\nSource that we can iterate through right\nso we're going to iterate through this\nSource until we see a stop token and\nwe're going to\nbreak\nso here we\nhave the Cod snippet below shows a basic\nflight\nworkflow\nand if we hop back over to\nour model Z\ndeployment we can see um there's one\nthere's one request that's in Flight\nright\nnow\nyou can check out some of the metrics\nhere and it's generating our flight code\nand as you can see it's kind of fallen\ninto like a hallucinatory pattern here\nrepetition um but the main caveat here\nis that we didn't really train this\nflight llama for too too long right um\nand the actually the initial data set\nwas a smaller subset of the the full set\nof repos and docs um so I suspect at\nleast from a token completion\nperspective this will become better with\ntime um let's give it maybe a more\nconceptual thing a\nwhite\ntask has a\nfew fundamental\nproperties let's see what it does with\nthis okay it has a name inputs and\noutputs command the command actually is\na lower level thing that um isn't\nusually exposed to to end users um but\nyou know so you see here like the main\npoint that I'm showing you this demo is\nwe have now a kind of complete pipeline\nthat takes us through data set creation\nwhich is arguably the most important\npart um I guess the exception to that\nwould be you know the product thinking\nbehind actually figuring out what you\nwant to build which is very hard\noftentimes but from a technical persp\nperspective you know we could create a\ndata set we use all sorts of tools at\nour disposal now including PFT bits and\nbytes that allow you to fit larger\nmodels fine-tune larger models on\ncheaper infrastructure cheaper\ngpus we then um deployed this to hugging\nface here and we used Model Z um well to\ntake a step back we um used Union which\nis a fully hosted flight with extra\nstuff uh to to train and publish a model\nand we then we use model Z\nto\num serve our model in a streaming\nfashion so we can get that sort of a\nstreaming type ux out of our\nsystem okay\nso let me hop back onto slides just to\nwrap everything\nup\num there are a few optimizations here\nthat I'd just like to call out um\nobviously this system that we saw just\nhere is um kind of slow not many tokens\nper second um but there are a few ways\nof squeezing more efficiency and\nperformance out of this which is uh one\nthing would might be to merge back the Q\nLaura adapter weights into the main\nmodel um Laura model the Laura model\nclass actually has a method to do this\nand all this will do may not make a big\ndifference actually but it will take\nthat those two smaller matrices that are\npart of the adapter weights and multiply\nthem and add that them back into the\noriginal\nmodel um we can use faster inference\nflame framework so L VM is one of them\nthere are a handful of them um The Flash\nattention is another technique to speed\nup\ninference um can always expound our data\nset\nuh an example would be to look on GitHub\nfor repos that have flight kit as a\ndependency this may capture actually\nmore of how flight users write flight\nkit code in the\nwild and finally there's always the\nnever ending Loop of experimentation\nusing a larger model using different\nhyperparameters um so this you know that\nthat for any practitioners out there um\ncan be a rabbit\nhole the road map for this specific\nproject is still sort of um not set in\nstone yet but there are two directions\nthat I'm playing around with going into\nwhich is uh creating a vs code flight C\npilot extension um hugging phase\nactually has created a nice framework to\ndo this um there are a few other things\nthat we need to do to actually support\nthis like fill in the middle completion\num there's the paper here for those of\nyou who are\ncurious um and a slightly more involved\napplication would be a flight attendant\nslackbot\num you know we're just going crazy here\nwith the uh the puns uh\nbut yeah the flight attendant would\nbasically be a flight a slack bot that\nwould would help answer people's\nquestions on our slack um obviously this\nwill require much higher quality data\nthan just Auto token completing kind of\nthe the text distribution of the\nrepos um and yeah I just wanted to send\nyou off with some of these points that I\nmentioned earlier which is that you know\nif you're in the AI ml space um it might\nbe tempting to\nstay kind of in the\nAI engineering stack and above you know\nso you know uh this area um but I guess\nspeaking from personal experience it's\nit's it's really all about what you want\nto focus on and what you want to learn\num but it is similar to how it's useful\nto know some of the math and\noptimization lower level optimization\ntechniques as an ml engineer who never\nhas to really touch those things still\nuseful for AI engineering folks to learn\nabout how model weights work how to\nchange them with data how to use the\npackages and tool sets available to you\num that's uh more in the concern of the\nthe ml engineering and research\nfolks and the best way to do this I\nthink is through projects through things\nthat you empathize with um through\nproblems that you want to see fixed\nyourself um and with that here are some\nresources and my contact info on\nLinkedIn and GitHub and Twitter or\nX um and here it's the links to this\nslides the slides the slides themselves\nand the notebooks that I teased earlier\num here's the fine tuning\nrepo and um I just want to end with uh\nkind of cult to action if you are in\nthis space uh it's developing very\nquickly be it you know computer vision\nor language models or audio and um we\nhave a bunch of people who are using\nflight for various use cases in in all\nthese areas not just in ml but you know\nbioinformatics and\num Logistics and different areas like\nthis um so if you're curious about how\nflight can help you sort of make your\nworkflows more reliable and reproducible\non the left hand side here is the open\nsource path um you can try it out at uh\nthis QR code just points to sandbox.\nunion. this will point you to a flight\nvery lightweight prototype flight\ncluster that you can start playing\naround with um you can learn more on\nfight.org\num but if you don't want to manage your\nown infrastructure uh you can talk to us\nuh reach out to us on unioni demo we're\nhappy to talk to you and figure out um\nyou your use cases and how Union and\nflight can help all right so let's go\nahead and wrap up um thank you everyone\nfor joining and I look forward to seeing\nyou in the next flight school cheers"
    },
    {
        "title": "Flyte School: Enrich your AI pipelines - A deep dive into Flyte plugins",
        "transcript": "all right welcome everyone to flight\nschool this is episode I believe it's\nnumber\nsix and a special one because we have a\nfantastic guest today no other than Mr\nKevin sue a core maintainer for flight\nand Kevin will be leading a deep dive\nsession on all things flight\nplugins as usual please uh drop your\nquestions in the chat we'll we'll try to\nanswer them as we go and uh yeah let's\nstart with just share with us in the\nchat where are you tuning in from where\nare you based right\nnow uh where are you joining us\nfrom um\nright Co cool so I think we can get\nstarted for now um can just share the\nscreen\nKevin okay sure thank you de uh let me\nshare my\nscreen all right uh hello everyone uh\nwelcome to fight school and then today\nwe're going to De dive into the flight\npluging and then we're going to explod\nwhat uh exting uh flight pluging what\nwhat kind of f pluging we have and then\nwe're going to teach you how to\nimplement and then went to why do why do\nwe need to use\nplugins first uh I'm I'm Kevin uh I'm\nserver engineer and Union at dii and we\nare Bas sh and then uh open source team\nand my daily job is to maintain and\ndevelop our open source project fly and\nthen I love to contribute I contribute\nto open source I have contribute many\nother open SS like hotu and submarine\nand then in my spare time I like to play\nbasketball go hiking so if you in sh as\nwell let me know can play some again\nyeah so today agenda we are going to\ntalk about the bacon plugging in FL we\nwe have two kind of plugging one is\nbacon pluging one is flood plugging and\nthen we're going to De dive into this\ntwo kind of plugging and then I will go\nthrough the code and tell you how to\nimplement this kind of plugin and then\nuh I I will give you a demo and then in\nthe next session we'll we'll give you\nsome time to ask some uh\nquestion okay uh before we go into uh\ntalk about the plugging uh let me let me\ntalk about the union claw so Union Claw\nis a managed version of fly that over\nadvanc OBS ability and then robust a\nsupport so which mean you if you want to\nuse fly and then you don't want want to\nworry about the deployment you can use\nUnion Cloud so you just register your\nworkflow to Union cloud and then\neverything is around Union Cloud you\ndon't need to worry about the uh\ndeployment and then you don't worry\nabout maintain your own kuet culture so\nuh if you uh interested in the union\nCloud you can scan the Q CLE and then uh\nthis CLE like uh we have a hosted fly s\nbox and then this same box will be\nrunning on our Union uh our Cloud so uh\nyou can register any wlow to this St box\nand try try try to run anything on on\nthe stand box and try try try to any\nfature in fly and then you if you want\nto see some demo you can scan the right\nuh QR code and then we will SK schedule\nsometime and then give you a demo\nyeah\nso uh let's talk about the uh let's talk\nabout the simple workflow and what why\nis the WL Trader so first of all there\nare lots of WL Trader in the world and\nthen what people usually do on is that\nthey Define your data pipeline ml\nPipeline and then they use wle trator to\nuh will help you allocate a resource um\nsome dist system like connetics or like\nuh uh ads ECS or something else and then\nyou will Traverse your deck in the\nworkflow Traverse your workflow Traverse\nyour deck and then find specific no to\nrun and then each no will be run on\nseparate container or separate process\nor a part part and then you the wflx\nShad will help you save the output to\nthe Bluster this is a very uh simple\nworkflow and many W many WL oor can do\nthat but the problem is that when you\nhave more and more data and then you\nwill run into many problem and you will\nhave you see a lot of challenge in the\ndata pipeline is that how do you move\ndata faster um some sometimes uh you\nyour data will in the different will uh\nstore in different region\nand sometimes maybe you stor in\ndifferent cloud\nprovider and\nthen you run to any issue like uh how\nhow do we move data from like in the\neast coast to West Coast how to move it\nfaster and then this this the main\nproblem problem in the data Pipeline and\nthen uh another problem is that uh when\nin your dat data pipeline you not only\nwant to run the python code sometimes\nyou want to run a SQ cryy in in your pod\nor like you want to run B cry or snowf\nflare in your to apply and then and then\nwhen you have and then another problem\nis that you want to save the money so\nyou want to run some tax on the Spy\nmachine or you want to run some tax on\nsome Cloud surface like Lambda or a\nspatch and then if you have more data\nyou have you must use uh spot cluster\nbefore and then the problem is that um\nmany of user some of our user um uh we\nhear we some feedback from our user and\nthen not many company have a internal\nSpar cluster and then they don't have\ncapability to maintain this Spar cluster\nthey want to run a data Pipeline and\nthen they want to run The Spar job uh\nand then they want to use it only when\nwhen when K off this TX so which mean\nlike we we they want to delete this\ncluster after the Tex uh is complete and\nthen they don't want to maintain\nit and\nthen after data processing you will run\ninto any another powerment in Machining\nPipeline and then I'll just\nchallenge so first is that how to\nmonitor the model matric and then\nparameter um uh yourl shared UI yeah and\nthen how to do distribu training um a\nlot of a lot of warl trator can uh can\nrun python code but they cannot run this\nreturning and then not how to manage the\nPyon dependency yeah so if you R want to\nround the tax and kinetics the first\nproblem is that you\nneed to build the doer file and it and\nthen or if you want to run uh your your\ntax your workflow on a distri system you\ndon't figure out how to install this\npython dependency on this dist distri\nsystem and then for now Ray has become\nvery popular like a lot of people want\nto use Ray but the same same problem\nlike a lot of people don't want to\nmaintain a ray\ncluster internally yeah yeah so these\nare the um problem in the machine\nlearning and the data pipeline so this\nwhy we be fly so fly uh let's take a\nlook at the fly first and then let's\ngive you an overview uh so fly is built\non top of kuber netics so everything is\nbuil on top of kuber netics so first uh\nfirst the first problem when you run Tex\non you need to write a doc file so\nthat's why we build a a plugin called IM\nback here it's it's\na uh it's a plugin it's a\num image definition you can write in\nyour python workflow you tell us what's\nthe uh python dependency you need what\nkind environment variable you want to\nadd and then we will help you build the\nimage for you when you register to the\nworkflow so you don't need to write any\ndo file and then will will automatically\npush your image to The Container\nregistry and then after you deploy the\nworkflow you want to run your workflow\non\nconnetics um we have a component called\nflight propeller it's a connetic\noperator and then you will help you\nmanage your WL life cycle what what this\npropeller can do is that uh the F the\nfirst thing it can do is that it help\nyou schedule and monitor your workflow\nuh you can you can like launch a part to\non the kubernets and then second is that\nif you want to run\nthe uh distending or\ndistribute uh data processing it can\nhelp you create a cluster here uh on the\nkinetics or if you don't want to run uh\nany tax in the communities we can uh the\npropal can talk to the other system like\nsubmit a job to the batch or like\nsend the rec to the uh estone plugging\nsystems and\nthen another uh\nyeah uh let me talk about uh uh first\nplugin is called image back and then you\nallow you to build build the image with\na dock file and then you can help you\nbuild MTI Arc\nimage and\nthen here uh we use the open open source\nproject called\nmty uh um it allow you it help you to\nbuild a smaller image and then you you\ncan build the image much faster\nthan using Docker Docker and then one of\nthe cool thing in the image back is that\nwhen you try to build an image and then\ntry to push the image we will hash your\nimage\nspe and then your uh image image n or\nyour image URL will become like fly AR\nSL flly key slash with push to The\nContainer reg uh so every image in your\ncontainer reg uh has this hash so FL you\nwill know why what what this image what\nwhat what what kind of dependency have\nin this image so first for for example\nlike um if data team use image\nback and then they push the image to the\naccount they Reg wlow and then FL will\nhelp you push the image to container\nregister and then The Ting if they use\nthe Send image back uh flighty will go\nto the uh container to search this image\nand import it so you will not rebuild it\nagain so uh the the the problem when you\nwhen the problem in when you use doer\nfile is that people keep building\nbuilding the same imager again because\nthey forgot which image they push before\nuh so that's why we we we build image\nback and\nthen uh in the uh in the flight key we\nhave a plugin called IM plugin and then\nit contain a builder uh it have a uh\nimage Builder engine so which allow you\nto uh create another image back Builder\nyou can we use uh by default we use uh\nopens MD to build image but you can also\nuse uh any other tool you want to build\nimage back uh image spe high level just\na high level uh image definition in flly\nkey\nyeah so another another feature another\npluging is called flight dech um and in\nin our flight console in our UI um we\nhave\nfeature uh you can uh we have a popup\nwindow and then you can see uh you can\nyou can you can explore your data or see\nthe model matric on the\nUI uh and then in the pluging in the\nflight dat plugin we have bunch of the\nrender you can use for uh you can use in\nyour text for example if you want to uh\nwant to explore and want to see the data\npenda data frame on on this UI um you\ncan use the table render or uh we have\nanother render like table profile render\nwhich which allow you to profile your\nPanda data for you will it will show you\nthe distribution of your data um data\ndata frame yeah and then we have a\ndecorator you can uh we integrate M\nmlflow we have a mlflow auto decorator\nyou can use and then you will\nautomatically lock your model matric and\nthen show this matric on the flight\nconsole yeah um when you run the TX and\nthen another pluging is datais plugin so\nwhen you run the Tex uh in the Pod or\nyou want to run the TX in the claw um by\ndefault we use the open source another\nopen source project called\nfsp uh uh to download upload the data\nwhen you try to when when the Tex is\ncomplete you will P push the data to E3\nbucket or GCS bucket and then when the\ntax start before we run the text\nfunction we will help you uh download\nthe download the data to to the part\nyeah but uh but but we use that respect\nbut um the implementation of the aspect\nis very slow so one of the contributor u\nin flight they add a r data pluging you\ncan see this chart um is much more\nfaster than um fsp so so if you you want\nto like like have much better\nperformance you can reimplement your own\nplugin in C plus or Ross yeah and\nthen\nuh sometimes like people want to save\ntheir they don't they don't want they\ndon't want they don't use clo they want\nto save the data to their uh unbrand\nstorage like hdu or any other slow\nstorage in their own protocol they can\nimp they can implement the database\npluging internally so they can uh\nregister the this plugin in fly key so\nfly will know how to write this data to\nyour own\nstorage and then another cross project\nis skyp plan so if you have a data\nacross across different region or\ndifferent cloud provider you can use SK\nplan you can you can move the data and\nthen you uh much faster and then much\ncheaper so uh let's talk about a another\nuh big and\nplugging\nso um when you when you have more and\nmore data you uh you may want to use\nbe\nso um\nbut the problem is that well you don't\nwant to run\na uh b t you you can B is running on uh\nGC gcp and then you don't want to run a\npart to submit this request to the G gcp\nso what we does is that um in the\npropeller we instead of creating the\npart propeller can send it request you\nwill create a go routine and then send\nit request to the B without launching\nthe partt and then when you uh after\nyour after your draws\ncomplete B cry will save a res\ntable uh in res sa uh sa resource in\nyour resource table and\nthen and then you can create a Str data\nset Str data set is a virtual data set\nvirtual data frame buil on top of B\nStone flag and then pocket arrow and\nmany other data frame on top build on\ntop of it so sh you can me it's a\nreverence of the data frame and then\ncontain in this sh will contain some\ninformation like format the\nURI and then the\nschema and then after you run your B\nquy fly will create a Str for you and\nthen when you pass this Str to the next\ntax for example you you um you pass a\nStr asset to your Channing tax you can\nyou can use the building Str uh data\nTransformer um we call the encoder and\ndecoder you can decode your Str set to\nthe penda data FR or Arrow data FR or\nspot data FR we help you to do that you\ndon't have you don't you don't need to\nfigure out how to download the B data uh\nhow to transform this table to the data\nframe you can you can decode it in one\nline and then after after you do\nafter you do the data processing uh by\nusing Spar or like any other Tool uh you\ncan you can you can use encoder to\nencode your this data back to the bqu or\nyou want to save this to snowfl I will\nshow it more detail in the demat\nyeah\num another another plugin is called uh\nspot pluging so\nso uh people people the people don't\nwant to maintain uh the The Spar cluster\nso we Leverage The Spar operator in the\nconnetics so which uh which can help you\ndynamically create a spell cluster and\nthen you can round data processing on\nmuting KN and\nthen uh the only thing you need to do is\nthat you create a uh pent tax or flight\ntax and then you write your ppot code in\nyour function and then one thing you\nneed to add is the spot config here uh\nthe spot config uh Define like how many\ndriver know and then how\nmany uh uh executor you want to deploy\non the\nconnetics once you uh once you define\nyour Spar Fe once you register your\nWarlow\num we um flight uh we will send this\ninformation to propeller and propeller\nwill know how\nmany uh driver and worker they you want\nto you will create create it for you and\nthen run this Spa on uh on this driver\nand then work\nknow so sorry sorry so it's very simple\nto start it you don't you don't need to\nuh you don't need to maintain or you\ndon't need to create a spot on your own\nyou just Define uh Define your cluster\nlike in esod and then in in your Warlow\nfile\num\nso why why do we implement the bacon\nplugging um because when you run when\nyou when you get more and more data your\nuse case get more and more complex uh\nyou want to\nrun uh man you not only want to the tax\nin the part you you want to run many Ser\nyou want to uh run many service external\nservice a service GCB service like it s\nmaker or snowl in your workflow so you\nwill you will you may need to Implement\na bacon pluging to give uh Propel\nability to talk to other\nservice and then another use case is\nthat if you run to R like distribute\nchaining and then like streaming\nprocessing you uh you you can you need\nto implement another bacon plugin to\ncreate a spell culture or like create a\na m part around the\ndistribution and then sometimes uh\nanother interesting use case is that uh\nsome of our user uh they build they they\ndeploy fly on their own but they have a\nvery small CTIC closure but they they\ndon't run any\npart uh any tax in kinetics they use our\nUh current ad spatch plugin they run\neverything like ad spatch so they don't\nthey just have just they just need to\nhave a very small community culture and\nand then the the benefit of running on\nEd bch is that um um you uh\nB uh you you they they they don't run\nthere they don't have too many too many\nWarlow they need they need to run every\nsingle every single over yeah so\nthey they can they can run they can\nsubmit a job to the OS batch like uh\nlike spa and then use the spa instant to\nsave their their\nmoney and then another another use case\nis that you can implement the uh another\nbacon plugin to run the your tax on the\nad lown\nokay uh let's jump into\ndemo\nSC\n[Music]\none\nsecond\nokay\nso the first the first example is that\nuh this is a very uh simple\nworkflow uh you\ncan you can use P WR\nthis example this very simple hello\nexample you can run this\nlocally and\nthen if you want to register this\nworkflow to our cluster you can\n[Music]\nuse and then you\nwill uh you will reg the W FL to the\ncluster kin can you zoom a little bit\noh sorry\nyes\nokay how on now baby\nit's better yeah\nokay okay so you can register your\nworkflow to the rem remote\n[Music]\nculture\nuh Oh by by the way uh I upload all the\nexample to this GitHub\nreple uh Pink suw fly hop and then fly\nscr directory uh this directory contain\nall the example we're going to demo\ntoday okay I will add it to the comments\nfor sure thank you okay thank\nyou\nuh all right so\nhere you can see the wflow St um in this\nexample we use\nthe default flag key image which\nis fly all/ flly key 3.10\nL yeah it's good but the problem is that\nif you want to use tensor flow in\nyour\ntext and then if you\nreg this work\nagain\nsorry\nuh\nrest\nokay let's\nsee\nyes uh I research around closer yeah uh\nokay if we go to\nthe fly\nconsole and then it's the tax fil the\nreason is that in the default flag image\nthe only dependency is flly kick uh so\nif will try to use any other package you\nwill show the arrow no module so what\ncan we do like uh we can use the image\nSt and then we can create a\nimage the first thing you need to\nspecify is the\nregistry uh it's a uh your container rry\nlike for example I use T Hub so I use\nthe uh Docker my Docker account\nhere and then\nyou can specify okay thanks the\ncompiler uh you can\nspecify what kind of dependency you\nneed for this text that's it uh in thism\nwe only install tensor Flor so you can\nspecify\nyour container image and use image spec\nhere so for\nnow let's\nregister theflow\nagain you will uh you will start\nbuilding your\nimage and then in this case you find\nthat we already build this image before\nso you find a the image on on my do hop\nand then use this image net and replace\nthis for replace this with the this\nimage\nnet and then we go to\nthe slide console start\ninitialize and then start\nrunning and then you can see\nthe you can see yes let see\nyeah because for now we using the\ncontainer which\nhas t flow dependency yeah so this is a\nun spec so if you add another dependency\nrandom dependency here and then you\nregister this\nagain use\n[Music]\nyeah so this time you say image is not\nfound so we use empty to build the image\ncan see here start building the image\nyeah so this why it's this why we use IM\nspe you don't need you don't you don't\nyou don't need to um use any do file\nDefine your image your code and then how\nhelp you fy help you build and\ndeploy the image for you\nokay\nuh another example is flag deck so let's\nsay you have a panda data frame and\nthen here we use we we import\nthe two different render in the flagy\nplugin called Flame profiling render and\ntable render so let's say you have a met\ndata frame you want to provide it you\ncan\nuse uh this render and then use this\nrender to generate the HTML string and\nthen you can add it to the flight\ntag and then this flight tag and then if\nyou want\nto uh use your you you want to use your\nown render you can create a\nclass uh called image render and then\nthe only function you need to implement\nis uh to\nHTML and then you uh the in in this case\nthe input is the p P image and then you\nyou tell flight you what you need to do\nis that uh you need to override this fun\nfunction and then convert this image to\na HTML stream and\nthen if you register this render then\nflacky will know how\nto render this image un like that and\nthen for example like we I create\nanother text this text Will download\nthis image and\nthen we use your our Custom Image render\nto render this image and then add it to\nthe FL T that's it uh you can you can\nImplement you can add your render in\nyour warl file or you can add it to the\nanother plugin you can create your own\nplugin\nwhich has bunch of uh render or you can\ncontribute to fly\nkey FL key and okay let me let me rest\nthis\nfirst\nokay you can also use uh IM SP which one\nhere we use flight key plugin deck and\nthen P here okay you\nregister okay start\nrunning so let me show\nyou here uh in the flag key\nhere we a here's the flighty deck plugin\nyou can see here we have a bunch of the\nrender in this plugin you can use like\nmodel you can give it a model file and\nthen we can render the model on FL\ndeck and then or we have an image render\nhere so you don't need to implement by\nyourself yeah so you can use this render\nor if you like to contribute like you\ncan add as many render as you want to\nthis FL FL\nplugin\nokay okay so you see this text see so\nyou can clear this text and then this\nhere's a button C like that you can open\nit\nhere uh in flighty we have uh in on\nflight de we have three\ndifferent uh tab one is timeline one is\ninput and output in Timeline you will\nknow uh how how long you take to\ndownload your data and upload your data\nyour tax input and tex output and then\nhow long it takes to have\nyour workfl code here see your timeline\nand then you just create just use\nthe uh frame profiling render so you can\nclick here you can see uh number of\nvariable in your data frame number\nmissing sales zero is okay and then you\ncan see the distribution of your data\nframe\nhere yes\nthis automatically Cate by fling\nyeah and then if you go to the another\nexample we create our own um\nplug-in so we can see\nhere yeah it downloaded our image the\nspace Med and then showed it on the flag\nyeah and then this\nexample\nwhich you can see your your a data here\neverything and like it how many column\nwe have like anything else yes so it's\nFlight Deck you allow to easily to\nexplore and visualize your data on\nyou uh\nokay uh another another another plugin\nis ml\nFL uh uh Amo pluging we integrate FL in\nflight flight key so we create a\ndecorator here\ncalled M\nautolock uh you write your chaining\nchaining text in your function and then\nyou just need to uh write add this\ndecorator we will help you automatically\nit will help you automatically lower the\nmatric and the model\nparameter and then you will show show\nyou will show this matric on your uh\nlet\nokay uh let me talk about it so here we\nhave another plugin am plugin so here we\nImplement a decorator here so we use the\nuh autolock method in M\nfloor and\nthen\nwe\nuh we\nhere here we run your\ndispline we run your text function here\nand then after Tex complete we get the\nmatric and then get the\nparameter and then we render this mat\nparameter on the flight\nde uh so you can Implement other plugin\nyou use any other tool to render this\nplugin you can create another decorator\nand then use your use other\npackage to get the matri to log the\nmatric or to L the\nparameter\nokay here okay uh I have another example\nhere can like\nthat your timeline model matric here we\nhave the accuracy and the loss matri you\ncan see we have like\n39 uh\n93%\naccuracy and then you can see your\nparameter your badge\nsize yes this the uh ml\npluging\nand\nthen another another plugin another\nplugin is is the red pluging so people a\nlot of people want to run the r remote\nfunction on top of uh on top use rem\nremote function and then run this remote\nfunction on R cluster so you here I\ncreated a two Ray remote function and\nthen I use it in this Tex and then how\ndo we create the r cluster we have\nspecify R\nconfy uh\n[Music]\nno and then use config here and then\nhere what we have is that in the red\nconfig we defined we have a two hand no\nconfig and worker no config which Define\nyour closer and then that's it you give\nit to the\nthe TX decorator and then we will help\n[Music]\nyou okay yeah uh\nhere let's go\n[Music]\nto yeah here you can see the closer is\ncreating is the first FL cable uh fly\npropeller will create a closer first and\nthen you will submit this rar to this PR\nclure so here you can see uh if you want\nto run any other\nuh uh ra uh\nplugin uh or tax uh distribute tax or\ndistri Shing under C cluster uh the\nfirst thing you need to do in this case\nwe defined\na uh a new text called rare function\ntext so people can use this in their\nwarl code and then in the rare function\ntext you have you may need to Define\nyour uh cluster config in this case it's\nthe r r drop config uh you how many the\nthe config for the work and the config\nfor the Handel and then you need to\noverwrite these three\nmethod so in this case preq is as is SQ\nbefore we run your R remote function\nwhich which is we call r. which is try\nto con down to your red cluster and then\npost EXC after your R red text uh is\ncomplete you will shut down your red\ncluster and then this function get C is\nused to confer your config\nto uh to the uh portal so when you\nregister your workflow we will serialize\nyour tax and workflow to a portal buff\nso we you will serialize this config to\nthe port bar and send it to the uh to to\nthe fly server and then in fly in fly we\nwill a marshal this tax uh this config\nand then propeller will will know will\nget this propeller will uh download this\nportal and then get this information and\nthen use this information to create a\nred cluster for you so in the Bon side\nin the propeller side we have a plugin\nin the this directory we call the ks\nplugin uh\nthe some of the function you need to\noverwrite is this\nso we need to implement build resource\nbuild resource you need to create a uh\nconnetic crd so you may import a uh R\njob uh import R uh R client or like SP\noperator client and then you create your\nuh R drop crd here and then return it so\nfly will know how to create this closer\nfor you create this uh crd for you and\nthen the get test f is there in the CD\nyou will see the status of your cluster\nor your job or your spot cluster or spot\njop so you need to convert this uh uh\nrace status or spot status to the fly\nstatus so uh fly because fly doesn't\nrecognize the status of your\nuh your CD so you need to convert these\nstatus to the fly status so uh fly will\nknow this TX is sued or fail or\nrunning and then if you go to the\nrighted pluging here the build resource\nhere we create\na we try to create a r jop spe R drop\nhere and then return it this the view\nresource function fun and then\nanother function is get test F so this\nstatus is Define in the CU so we convert\nthis status to the fight status here and\nthen fly will know uh fly will know then\nfly will know the status of your\nTR\nokay\nuh another example is B BB\nplugging uh\nhere one of the use case is that I\ncreate a panda data FR I don't want to\nsave it locally I can use Str data set\nand then I can specify the the your it's\na b you want to save this data to B and\nthen this the project data set and\ntable and then when when you return it\nFY will help you upload this data\nto um to the\nbqu and then think Downstream tag you\ncan run a query for your penda data\nfriend and then\nor when you want want to run like model\ntraining or another data\nprocessing you can convert the biger\ntable\nhere we we have a function callede open\nyou can use pen. pen uh\narrow. table to convert a bqu t table to\nthis Arrow table or you can use penda\ndata print you can use a lot data print\nhere okay so in this\nexample I try to\nrun this\nlocally\nyou're\nokay here you can see uh let's go to the\nGCS\nconsole here you can see it just created\na t for me\nyour Contender name AG colum and then\nyou\ncan profile you can see your data here\nand\n[Music]\nthen you write this data to the pr and\nthen I can and then I print it in the\narot table format here you can see this\nArrow table Yeah so it's very\nuseful in your data pipeline you don't\nneed to worry about how to save your\ndata to like beire no FL or like how to\nsave data you sometimes you may want to\nsave these data into a pocket file and\nthen show has it help help you\nautomatically and decode it or encode it\nto the uh to the other format and then\noutload it to the S3 or bqu\nand then if you want to use it you just\ncode open and\nthen uh Str will automatically decode it\nand download this data for\nyou\nyeah\nuh the last example I want to show\nis p me pluging so some some data\nscientists really like to use used The\nNotebook so we integrate the pap which\nallows you to parameter parameterize\nyour your notbook\nTex so here I write a notebook and then\nEP is my parameter here and then you can\ncreate a noble text and then in this no\nTex you specify your pass of your\nnotebook and then you define your in put\ninput which is in\nOT and then once you\nregister this is the local pass but when\nyou res this\nworkflow flight key will help you\nautomatically upload this data this\nNoble file to the E bucket you don't\nneed to e credential in locally we use\ndata processing help you to upload the\ndata here okay so I register\n[Music]\nokay yeah so I already run it can open\nit and then you can uh we have a fe\ncalled we have a argument called render\ndeck which allow you to render this\nnotbook on of FL deck so we can see what\nkind of you can see your code and then\nyou can see your what kind of what code\nyou just run and then you can see the\noutput\nhere yes\nso\nokay\nso um I just introduce some of the\nplugin uh in fly and but there's a lot\nof bunch of plugin in fly you can check\nout our website\nso to so the tech is that is impossible\nto build an orchestration platform that\nis 100 ft so people you always want to\nextend decorate or contribute to um\nor flux stader so that's why we be fly\nit's highly flexible and then it's flly\ncan be uh complete customized so if you\nwant to run your job um uh other\nplatform or create a cluster you you\nwant to flly create cluster for you you\ncan create a bacon plugin so if you want\nto visualize your data and then you want\nto make moving data faster or you want\nyou want to uh see the magic and\nparameter on the flight deck you can\nimplement the flighty\nplugin okay uh so if you if you you want\nto know more about uh the flag check out\nour website if you want to uh know our\nproduct manager flight cluster go to\nUnion Union website and then if you like\nthis demos Please Subscribe our YouTube\nchannel and then give us start uh\nG yes that's what I\nhave awesome yeah there's a couple of\nquestions uh that was great Kevin uh\nfirst one comes from ner BOS I hope I'm\npronouncing your name correctly so\nrenderers are those only for profiling\nor you can use it for other visual\nthings too it's it's not it's not be\nit's not only for\nprofiling but uh most of time people use\nProf um uh profile profile profile\nrender uh because it's very useful but\nyou can Implement other you can also\nImplement many other render uh uh for\nexample I just showed the image render\nbut like you can also some other\ncontributor add a use mapol they they r\na bunch of render which use M and then\nyou can allow you to show the chart on\nthe flight deck so it's not only limit\nto the profile profile or only limit to\nthe data frame you can render anything\nlike which\ncan which you just need to create a your\nrender just need to create a HTML stream\nand then flagy will know how to render\nyour data or any other else\nyes\nawesome and another question from mer\nalso uh are these the computations are\nthese running locally or in the\ncloud yeah\nso both like so the the benefit of the\nflight is that\num people usually iterate their workflow\nlocally so to iterate your workflow\nquickly we support local execution so\nwhen you devel develop your workflow uh\nyou can run everything locally so it\nwill use your local machine resource and\nthen I just show and then if you run if\nyou want to run the B you need a b\ncredential locally so which and then you\ncan you can run it uh we you will submit\na b from your local laptop desktop to\nthe G gcp but you can R when you\nregister this workflow everything is\nround CL so the Pyon tax is running in\nthe PO and then uh the cryy tax uh is\ncreated by the propeller propeller will\nstart go and send the request to the gcp\nyes\nthank you well we are at time uh thank\nyou again Kevin that was\ngreat plugin yeah thank you Dey yeah I\nthink plugins is one of the main aspects\nthat makes flight a\nplatform and uh if there there's still a\nlot of to cover so\nhave sessions here for the time being\nthank you all for joining and I hope to\nsee you in next\nright"
    },
    {
        "title": "Flyte School: Flyte Deployment - A Live Walkthrough",
        "transcript": "all right welcome everyone to episode\nnumber\nfive I think it is for Flight School\nI'm Davis your host today and I'm really\nglad to be here if you're watching us\nfrom live stream\nuh feel free to leave a comment where\nyou\nare tuning in from\nuh I typically do this because yeah it's\ninteresting and probably a conversation\nstarter\num\nI'm joining from Pereira Colombia from\nSunny South America\nand uh yeah feel free to leave your\ncomments right there today will be\num\nfocusing more on Hands-On content let me\nshare my screen real quick here\nuh yeah entire screen why not\ncool\num so yeah today we will be going\nthrough\num a live deployment or a deployment of\nflight and two different environments\nand uh we will do it from scratch\nso yeah\nI'm I'm kind of paying my bows to the\ndemo gods for this to work as expected\nbut if it fails you will be also an\nopportunity to to learn collectively\nfeel free to ask questions I'll try to\num handle them at the end or if\nsomething really important arises uh we\nwill cover it right away\nuh but this is your session so feel free\nto ask questions\ncool uh so yeah we'll take a look at the\ndifferent deployment paths right now for\nflight\nand and uh we'll go through the plant of\nthe plan deploying flight on a cell\nhosted kubernetes environment\nself-hosted I mean it's a cluster\nrunning on-prem no Cloud no managed\nservice\nand um and also flight on AWS\nright in some of the next steps\nuh after deployment cool so I'll start\nby mentioning uh Union Union AI who's\nnot so not only in sponsoring uh this uh\nlive stream and the episodes flight\nschool but the union also offers the\nmanaged service the managed flight\nservice\nuh which is very interesting because you\ndon't have to deal with anything of what\nwe will see here\num right there in on Union cloud and uh\nit's it's the same core\nflight server plus other features like\nuh contextual I would say or task level\nmonitoring observability\nand role-based role basic Access Control\nso it relieves you from the all the\nburdens of infrastructure management so\nfeel free to check it out request the\ndemo it's available now\ncool so in terms of deployment paths uh\nI would say that the first step in\nprobably many of you are already\nfamiliar with this option is what we\ncall the sandbox\nand there's a little there's some\nconfusion out there because there's\nstill a command flight CTL sandbox start\nbut that command is kind of old it\nprobably will be deprecated soon\nso right now if you want to invoke the\nsandbox you should just issue a flight\nCTL demo start\nthe only thing you need is well flight\nCTL installed in your system and Docker\nDocker demon running in your laptop or\nyour system right it packages all the\nflight components that Remembered in the\nprevious session and if if you are not\nable to join previous session I'll\nrecommend that you check it out in the\nunion AI YouTube channel episode number\nfour it was a exploration the flight\ncomponents in the architecture of the\nsystem all of those components are\npackaged in a Docker container\nwhen you use sandbox so it's good for\nfor a rapid iterations for testing for\ndemos it's great uh the thing is that\nprobably doesn't scale really well you\ncannot scale out sandbox instance for\nexample for for uh to be able to run\nmultiple workflows or large workflows is\nnot the best option\nand uh you cannot really it's it's hard\nto productionize it I mean you can you\ncannot add some of the features we will\nsee in a little bit right but for\ntesting demons is is totally fine\nor by validating if you're just becoming\naware of lighting you want to validate\nsome of the assumptions or the ideas you\nfeel about flight you can totally\nvalidate that validated with the sandbox\nversioning everything is there\ncool uh\nnow when you make some progress towards\na more robust deployment this the next\nstep is the single cluster deployment\nwith cluster here we mean kubernetes\ncluster right right now the the\nsubstrate for flight is kubernetes\nand when when we mention single cluster\nwe are talking about a single kubernetes\nuh cluster in this case typically we use\na Helm chart for for anything that is\nnot sandbox even the sandbox is\npackaging on a Helm chart\num so for single cluster you you will\nuse a flight binary Helm chart and as\nthe name implies all the components are\npackaged in the single binary AKA a\nsingle path right if you if you run the\nbinary and by the end of this episode\nprobably hopefully you will see the\nbinary running\non a live environment and you will see\nonly one pot right all the components\nare right there\nokay\num if you need to scale out all the if\nyou take a look at the documentation\nthere is a page in the documentation\nthat is optimizing performance\nit has\num some of the ways you can take the\ncore scalability of Light which is which\nis very important uh to uh to a Next\nLevel and uh all of that you can do it\nwith the fly binary no worries of\nrunning everything in a single path\nso it's good for most of the use cases\nand it runs on I would say any\nkubernetes environments I I haven't\ntested all the all the environments all\nthe providers no that would be a lie but\nI don't find any reason why why wouldn't\nwhy it wouldn't run on on different\nenvironments for the most part it just\nneeds access to the kubernetes API\nprobably when it gets tricky is when you\nneed to interact with for example a\nparticular blob storage option or\nparticular\noath Provider by the core flight\nfunctionality you can run on any\ncoordinates environment\ncool so that's single cluster and the\nnext step will be what the flight\ndocumentation treats as a production\nenvironment is the same Helm chart the\nsame flight binary but you will have\nIngress and oath\num\nyeah the text I don't know what happened\nwith this text but you can even scale\nout if you need\num yeah so Ingress is basically a\nkubernetes native resource that will let\nyou expose the services in a way that is\nmore scalable and probably cost\neffective than just using Services\nkubernetes services and using load\nbalancers for each service you can\ncondense everything in a single instance\nin single resource which is the Ingress\nand uh yeah it's I mean production can\nvary depending on your organization\ndefinition of production environment but\nwe typically find that that users\ncustomers uh consider production\nsomething that has scalable networking\nand it has\nauthorization and authentication\nembedded right otherwise probably is not\nexactly a production environment but it\ncan be it can the definition can be\neverything you know on your policies so\nyeah it's the same uh binary chart like\nbinary chart but you will add some\nadditional\num resources what we will do here in the\ndemo is deploy a production single\ncluster environment from scratch\nall right and the next piece probably\nthe currently most advanced uh or mature\nor robust form of deployment for flight\nis multi-cluster again cluster here here\nis kubernetes cluster so for example\nwith single cluster you can have\nmultiple nodes because there was some\nconfusion and a couple of weeks ago I\nhad a conversation with a user and there\nwas some confusion that that single\ncluster was only for one note were from\nfor one physical machine now you can\nhave multiple nodes uh and in it's still\nconsidering single cluster for\nmulti-cluster we have different\nkubernetes environments and for this you\nwill need to use the flight core chart\nand this is restrictive\num flight binary won't work for\nmulti-cluster with fly core all the\ncomponents run on separate binaries\nseparate paths you will see it in a bit\nI also have a multi-cluster environment\navailable\nand uh the idea is to separate the\ncontrol plane from the data plane\nand this is optional you can also you\ntreat the control plane as data plane\ntoo uh by this I mean and probably you\nwe need to go back to the previous\nsession where we explore the components\non the control plane which is mainly\nflight admin and the scheduler uh all\nthe logic the idea lives there\nand the data plane is where the actual\nworkflows are reconciled into kubernetes\nresource right so\num there was also a question this week\nduring community meeting someone asked\nif they could run the control plane for\nexample on eks in the data plane and gcp\nfor example in gke\num again I haven't tried myself but but\nI didn't see any restriction why not\nbesides the obvious\num authentication authorization issues\nof handling different environments\num for Flight Sales I mean the the\ncontrol plane will request executions to\nthe different data planes depending on\nsome rules uh that we will see in a bit\nright so this is right now uh the main\nuh scenario for deploying flight\nobviously probably the next step will be\nthe managed service you you don't if you\ndon't want to deal with anything of this\nthere's the managed service\nright so those are the deployment paths\num let me see there's is there any\ncommon learning question right here no I\ndon't think so cool so the next piece\nwill be demo right\nuh so I'll start by the documentation uh\nthese guides are about to be updated\nthere's a PR right now in the works you\ncan find it in the repos and draft State\nand it will be updating all these\nsections but you can see here different\npaths\nand uh yeah the flight the sandbox is\nthe simplest form uh if I use antique\nalready deployed a sandbox\nand you can see here uh that Christ\nthat old uh services are right there\nthe sandbox even chips with a Docker\nregistry which is very interesting right\nif you didn't have a container registry\navailable the sandbox got you covered it\nhas all the dependencies right there you\ncan see here that it includes a postgres\nSQL instance and a menu instance for\nthere but if you take a look it's\neverything belongs in a single container\nright it's a single container everything\nis there and it's fine for uh testing\nand and whatnot\nuh the next thing is a single cluster\nsimple Cloud well well simple is\num\nrelative depends on what we consider\nsimple but I will say single cluster\ndeployment and there are some prereqs\nhere and some general instructions\nbut if you want to take a closer look at\nthis Rex and the process of preparing\nyour infrastructure for uh for single\ncluster deployment we have a couple of\nresources very different one of them is\nthis uh family of tutorials fly the hard\nway\nall of these was ah sorry that's great\nall of this was built uh\nuh with the community with many users\nout there trying testing and\nI'm helping us collecting learnings and\nthese guides so for example the flight\non AWS the hard way\ngoes through the process of doing\neverything almost manually mostly\nmanually right probably the only thing\nthat is not manual is the helm chart\ndeployment of course but the rest is\nit's mostly manual we have we use a\ncouple of scripts for the aqueous\ncluster for example the ETS CTL\nuh but for the most part everything is\nthere so it's fine if you want to have\nsome control in if you're be it your\norganization requires a viewer the type\nof person that learns by knowing how\nstuff works at the kind of deeper level\nuh I think that this is the correct\nresource it's it goes through the entire\nprocess explaining everything uh you\nneed for a successful production flight\nenvironment on eks\nuh up to oath so yeah completely\nproduction grave deployment in this case\nis in Octa as a provider\nuh I won't use this today because it\ntakes a lot of time\nso the next resource we have available\nfor a single cluster deployment is uh\nthis was released like a couple weeks\nago it's a set of terraform modules that\nhelp you prepare\nbasically do all this stuff\nautomatically\nand leaves your infrastructure ready to\ndeploy flight right\nso the agenda uh\nyeah according to the agenda\nbefore will be the self-hosted option\nbut considering the time\nsome of this process will take I will\nstart the um\nthe deployment using terraform and once\nit's running in parallel we will go back\nto the on-prem deployment which is not\nvery automated right now okay\nso\num let's get started and let me do this\num\nand I'm sharing my screen so probably\ndoesn't make much sense so let's get\nstarted with the AWS uh deployment so I\nwill\num\nI'll start from scratch\nlet me see I have here um an empty\num\nan empty uh folder and I will clone\nuh the repo here\num\nright everything is there so we go to\nenvironment AWS\nuh and here are the let me show you here\nthe the DF um\nfiles so the next thing we'll do is we\nwill follow the tutorial\nline by line\nand uh yeah we'll do it together so the\nfirst thing that it says is go to\nprovide OTF and replace profile with\nyour AWS CLI profile\nuh it doesn't have exact instructions on\nhow you set your AWS CLI profile because\nit varies depending on how your\norganization handles authentication\nauthorization uh through AWS so we'll do\nit in a way that it's helpful for me\nuh my organization uses a single sign-on\nso I verify the code and authorized the\nterraform client\nso we'll use my super powers\nuh I'll be deploying everything on EOS\nWest one\nand I'll be using\num yeah and AWS CLI\nprofile so that's the first thing so I\nneed to probably I need to open code\nhere\nso I need to go to provider PF and\nchange the name of my profile\nright\nuh\nvideos\ntier two I think so yeah cool and the\nregion probably you're using a different\nregion but you need to change it here\ncool and yeah save your changes\nnext step is create a S3 bucket to store\nterraform State long story short\nterraform uses state every time you need\nto apply a change it Compares everything\nto the estate that is stored in this\ncase on an S3 bucket so probably uh in\nthe through the entire process I will go\na couple of times\nto the Management console\nbut it won't be too much I mean\nthe idea is not to use the console\num\nalmost at all right so I will\num\nthis is a prereq so I'm not using\nterraform for this so I will use flight\nschool ex state\nand there you go\nthe pretty simple the idea is that if\nyou have some life cycle policies or\nobjects here really concerned disabling\nthem or using a more relaxed life cycle\npolicy for TF stay because terraform\ndoesn't react really well\num\nif if the state is rotated deleted Etc\nokay so flight school TF state\nand I go here\nand I change the bucket name\num\nthat's cool if you have state\nfull uh this is the region where the\nactual bucket lives\nyes rest one and that's it I'll save\nchanges the next thing will be to go to\nDNS TF and indicate a indicate that DNS\nmanaged Zone\nuh the one that you will be using\nright\num assuming you have uh\nyou have something like that available\nso in this case I will use\num manage some\nit's available\nright uh the next thing will be you will\nneed to define the fully qualified\ndomain name that you will use to connect\nto flight\num and uh put the first part of such a\nname here so we'll use here Flight\nSchool\nright and the entire thing you need to\nchange it here and I'm just using the\nsteps in the tutorial\ntrying not to context switch so much\nso the entire thing will be a new demo\nthat run in this case right\nuh there's a record here that you'll\nneed to change but the only way the only\nway to know what this record is all\nabout is once flight is completely\ninstalled\nonce the helm chart installed the ALB\nwill create a lot of load balancer\ninstance and you will retrieve that\naddress and you will be able to update\nthis DNS or cname record right but right\nnow the only thing you need to do is to\ndefine the manage DNS Zone and the the\nfqdn you will be using to connect the\nflight which is Flight School\nall right uh okay what else uh yeah okay\nyeah go to locals this is important\nbecause all of the objects that this\nmodules create will use a naming scheme\nthat starts with project Dash\nenvironment\nuh so I will\nI will use flat School TF\nright\nsave and uh finally I'll go to the RDS\nmodule again as you probably remember or\nyou already know\nuh by now flight uses a relational\ndatabase and this is the current config\nto deploy the RDS instance\nso if you don't change anything what\nwill happen is that the terraform module\nwill generate a random password\nfor the database and it will use the\nflight admin and for the database name\nand also for the username but if you\nneed something else if your organization\nneeds something else you can change the\nnames here\nyou will need to change them also in the\nhome chart\nwe will go there in a bit and also if\nyou want to define a master password you\nwill comment this line and uncomment\nthis line and Define password here right\nso you have control\nthere\ncool\nall right\nnext thing we'll just perform in it\num\neverything should work\nthat's going through the modules that\nare the declare the providers they are\ndeclaring the files\nand oh yeah\nuh there's something else I need\nto do\nsorry\nfor my actual CLI to work\nand uh\nso there you go\nmm-hmm\nin the meantime you can share in the\nchat if uh whether you plan to install\nflight uh is AWS your current\ninfrastructure or cloud provider do you\nplan to run it on gcp or Azure Oracle\nCloud on-prem just let us know in the\nchat\nwe are building resources like this to\nhelp you with deployment on other\nenvironments not not on the AWS when\nyour feedback is as usual I appreciated\nall right so\ninitialization worked so the next thing\nis that we will do a tour home plan\nand uh I will save this enough\nwell\nuh just to make sure that when I run a\nterraform apply\num I'm invoking the same plan that was\ngenerated\nso it takes some seconds cool\nso now we'll do a curve for apply\nuh the slide plan\nand all right\ncool\nso let's I will leave this executing\ntypically in my environment takes about\n12 minutes to run so in the meantime I\nwill go through the\num on-prem installation\nso where this instructions well it's not\nin the docs it is again a community 19\ntutorial because especially for on-prem\nyou can only imagine the how how diverse\nare the options for everything and for\nthe coronary distribution that you may\nbe using for the blob storage for\nnetworking for ETC for for example\nstorage providers Etc\nso\num this little guide uses basically k3s\num which basically deploys virtual\nmachines and uh we install\num well we'll use multi-pass will we\ninstall their k3s which is a compact\nversion of kubernetes and we use this to\na install the dependencies in the\ninstall flight itself right so\num\nit covers a single cluster a single node\nright it uh I will show you in a bit how\nit looks like if you need to add\nmultiple notes\neither feel free to contribute or we are\nworking on on testing and expanding the\nguide to include multi machines\nand also to make it even more automated\nbut right now I will basically launch\nan instance\nk3s Master for gigs of memory for git\ngigs of disk\nand once this thing is up I'll get the\ninfo what I am interested in I I I'm\ninterested on the ipv4 address\nI'll show you why in a little bit\num\nright it's going through the process\nthis one is creating the eks cluster\ncool\ngreat so multi pass\num\ninfo I think it's info yeah increase\nmaster\nright so here's the ipv4 address up and\nrunning that's cool uh next thing that\nI'll do is basically\nexecute this script inside the k3s\nMastery instance it will install k3s\nthere\num inside the VM it will install k3s\nthere to be exact\nand that's where the funds start I know\nthere are tools out there to manage\nmultiple contexts multiple kubernetes\nclusters again this repo is called a\nhard way for a reason\nwe tend to do most of the things\nmanually here so you can see step by\nstep and because the thing with\nautomation is is that it's great\nuh but it's highly opinionated I mean\nthat includes these modules\nthese terraform modules have many\ndecisions made up front\nyou can change many of them but if you\nhave to tweak this completely to your\nenvironment probably will take you more\ntime\nah so that's the idea with the hardware\ntutorials to give you full visibility on\nthe process even when\nit doesn't look sexy at the beginning uh\ncool\nright so I will retrieve the info from\nthe k3s cluster okay so I got this here\nthe uh the ca for the cluster and the\nout info for the actual user and the\ncontext so what I will do is to\nbasically edit my um\ncubeconfig file to so I can connect to\nthe cluster and do everything else right\nso\num let me do this\nuh home\nCube and coffee\nuh yeah I trust this why not\nlater cool\nso the first thing that it recommends is\nthat you will create a new entry\nright a new cluster entry\nuh right here\nbut you will change the IP from uh\nlocalhost to be the IP address of the\nactual instance\nso this is this one\nokay\ncool the next thing that the guide\nmentioned is that they need to create a\ncontext right uh\nyou will basically you need to assign a\nname\nuh to the cluster there's a name here\nthat is default but it's there to use a\ndifferent name and more information I\nwill use here atreus master\nFS from by school\nso I will go now down here to the\ncontext section\nand\nI will add a new context\nso yeah the guide mentions that you\nshould just\nuse the same cluster name and give you\ncan use a different name for the context\nbut in this case I will use not only the\nsame stuff\num so you I will create a new context\nuh cluster\nin this case is going to be k3s Master\nFS\nuser I will use the same k3s Master FS\nand the name for the context\nI would use basically the same\nthere's an indentation problem here\nadrius\nFS\nokay\nthat's the context the cluster the user\nand the the last thing is to actually\nadd the user\nso there are multiple users here\nuh we'll create a new one\nwith the information that is here\nright the only thing that we'll need to\nchange is again the name of the user\nuh because default is not especially\nuseful\nso this name needs to match what we\nDefine in the context so it's k3s Master\nFS\nand that's that's it right\ncool cool so if everything is\nuse context\nI would say uh yeah Flight School\ns\nuh\nyeah why not use context\num\nyeah uh let me see\nTetris Master effect yeah what's\nhappening with my memory okay Chris\nMaster FS\nboom it's there which will have a single\nnote\nthat's it so we're inside the uh let me\nclear this we're inside a corneris\ncluster\na single now poor Knight is cluster that\nis running on a VM on our machine\nin parallel the reform is still working\nand uh we will deploy the postgres and\nmenial dependencies here and then we\nwill install flight here right why why\nbother why not just run in the sandbox\nif it's running on my laptop well\nsometimes again for for experiments for\nexample for testing Etc\nthe sandbox is not enough and uh this is\njust an um an example of how the process\nof deploying on on-prem probably not\nyour laptop let's say you have an\non-prem environment how the process will\nlook like right so you will induce\nprobably k3s you will have a Q ADM\nbootstrap cluster or some other\ndistribution\nso up to now we have in Dutch flight so\nnext thing will be to actually deploy\nthe\nthe actual dependencies\nso there's a manifest here that\nbasically it's prepared uh\nuh to help you with the dependencies\nuh let me see where are you\num no\nyeah that's a good reason uh flight\nschool episode five\ncity code sorry into flight school\nepisode five here let's do the curl\nthere you go\num what I should be\nand then close that\ncool let me show you real quick here\nuh all right the lock the local flight\nresources\nso these uh manifest\nI don't know\nuh it's again highly opinionated right\nit has it uses the local path\nprovisioner that's probably the main\nuh the main\ndifference between\nor the main point where you will need to\ncustomize this for different\nenvironments it uses local pad meaning\nthat it will consume\nstorage from the node where it runs but\nwhat happens if you have multiple nodes\nyou will need some form of shared\nstorage\nthis manifest right now is not prepared\nfor that\nbecause again for share storage and\nthese share storage will will be needed\nfor example for the database\nand and for hosting the actual for\nexample the actual menu instance\nand in in share storage so it could be\nNFS it could be another form of block\nstorage so there are there are many\noptions there and you will have to use\nthe proper storage class for that\nokay so again in the near future I plan\nto expand the tutorial to include\nmultiple machines using NFS storage\num but right now it's it's local per\nnode okay uh the rest is it's kind of\nstandard this is it uses a persistent\nvolume\nso the uh the the actual data in your\ndatabase will outlive the path the path\ncan even be destroyed but the the data\nwill still be there\nuh but the if the cluster is destroyed\nwell\nyeah the volumes will be destroyed right\num okay and\num it has a service for exposing\npostgres and uh it's opinionated in the\nimage it uses in in the authentication\nmethod it uses for postgres it also runs\na menu instance uh for blob storage yeah\nan upgrade it has the credentials right\nthere\num but this is a way\nnot the best way but this is a way to\nactually\ndeploy the the dependencies for flight\nso uh okay so we will do and apply on\nthe local right and resources thing\nuh yeah this is in the guide no problem\nyou need to create the flight names\nplease\nright yeah there will be a one resource\nthat is unchanged because the storage\nclass is not name space\nuh the rest of the things are there so\nif you take a look at the parts in the\nflight name in space it's creating uh\npostgres in Nino and it should be\nrunning before you attempt to create to\ndeployed light\nhey it's it's there\nuh so let's go back to the tutorial yeah\nyou need to create the flight name space\nuh okay so yeah there's an option here\nwhere is it's not really it doesn't make\nmuch sense now because with flight with\nthe fly binary chart you can you don't\nneed to have the you need to put the\ndatabase password in plain text right\nthere in your Helm chart\nwe will use this feature for the\nproduction deployment\nin some minutes I hope so I hope we have\ntime and uh but for the on-prem\ndeployment uh as you as you saw the uh\nthe Manifest is very opinion the\npostgres authentication so he won't use\nthis secret but we will change this\nin the short term\nto make it even better all right so now\nwe will download the hand values file\nuh let me see okay now minion postgres\nand running\ncool\nagain the local values\nuh this is a Helm values file that it\nhas everything by default I mean the the\nservice name the credentials the\nmetadata data container Etc\neverything's right there so if you just\ninstall install let me yeah\ninstall the fly binary\nthe only difference is that you will use\nlocal values\num it should work\nright out of the box\nso they need container it's working the\nthe\nfirst init container in the flight\nbinary chart is the one that connects to\nthe database\nand validates that the the connection is\nsuccessful before trying to run\nlight\nuh cool and yeah in parallel and in the\nmeantime\nyeah platform is done so it's configured\nto Output uh the information you will\nneed\nor the actual Helm installation or the\nactual flight installation using the\nhelm so if I\nright if I go back to the tutorial\nright we are at this point so we will\nmove to the we will switch the context\nuh\nuh still works we will switch the\ncontext to the the new cluster\nit should we use Flight School\nuh\nmessage project environments what what\ndid I use for this\num\nthis is locals yeah ah TF sorry\nbye School dot PF\ncool it's there so\nit has already nodes deployed\nuh but it even has some nice thing in my\nopinion\num\num she's for example the ALB the AWS\nload balancer controller is already in\nplace\nso when I when I try to install flight\nis in Ingress it should reconcile and\ncreate the load balancer automatically\nso the next thing that the that the um\ntutorial will ask you to do is to create\na secret for the database password\nuh and we will do this so if you take a\nlook the master password of the database\nis there but it's sensitive\num field so we will sensitive variable\nwe'll have output this here\num directly and we will use it to\nbasically create a secret\n[Music]\num\num\nwhere are you\nah database secret.dm it's it's right\nthere\num\nthere you go\nso here I will set the\ndatabase password that\nit's in the output\nright here\nwithout the quotes\nand Save\nand next thing will be to basically\napply I prefer create for new\nresources\nand\num\nthe ah Christ again yeah this is again a\nnew environment the flight school TF\nright on eks doesn't have the\num\nthe namespace flight\num true I would have made this\npoof the secret is there\nso the next thing that we should do is\nuh to download add the repo already add\nthis and download the values file\nuh which is here\nthe instructions are there so again as\npromised I will do the entire I will run\nto the entire process with you\nhere so first things\num everything is in the guide but you\nneed to have username like admin because\nby default the chart uses postgres\nand remember in the terraform module we\nDefine the username with flight admin\nyou can remove the password field\nbecause we will use a secret for this\nfor the host\nsuppose is the writer instance for the\nRDS cluster and it's here the cluster\nendpoint output\nuh it's right there the database name\nremember that it will Define as flight\nadmin\nuh the metadata container\nyou can use the one that was these\nmodules just create a single\nbucket right you may very well have your\ndata running\nliving somewhere else you will Define\nhere the\num that bucket but right now I'm using\nthe same bucket for for both metadata\nand the actual data to be processed\nuh the region where this bucket is\nleaving\nuh cool I will disable oath right now I\ndon't need it\nand um\nyeah the instructions probably a a good\nPR will be to make this chart remember\nthese values file less opinionated\nbecause it enables Spark by default and\nmany times you don't need it right away\nat least\num there are other things that you need\nto add everything again is in the\ntutorial but for example you need to add\nthis do this section\nuh the purpose of this is that for for\nthe proper IM role annotations to land\non the pots when Flight execute tasks\nright those tasks will need access to\nthis Amazon resources they will need\nrole annotations and this piece\nbasically\num helps automating that I forgot one\nimportant field under configuration\nwhich is the mainline secret\ngraph right this is the reference to the\nuh secret again for the\ndatabase\nand this is the one we just created\num\n[Music]\nso yeah\nokay cool\nall right what else uh yeah the Ingress\nagain the values file it has some\nannotations for nginx but in this case\nwe'll we will use\num\nwe will use\num the Amazon load balancer controller\nso basically I'm replacing the whole\nsection with ALB annotations\nand there are some things you need to\nchange for example your certificate\nAaron there's one thing with the\ncertificate that I still need to check\nbecause it requires us to go to the\nconsole and I don't like this but that's\nthe current status\nso you're better off knowing this so you\nneed to go to the certificate manager in\nthe correct region and you can find your\nrecently turn from creative certificate\npending validation\nright and the only thing you need to do\nis to hit create records in round 53\nhe will create the records and I don't\nknow one two minutes after the\ncertificate will be issued\nI need to automate this\num so you don't have to do it but\nfor now I mean I I can take the Arn and\nadd it to the value file\num\nand the other thing you need to change\nis the uh the fqa and you will use to\nconnect to fly in this case remember\nthat it was flight school that Union\ndemo that run\nright\nuh cool\nand I'm finally here\nfor the service account you will use\nhere the\nthe binary role Arn which is here\nokay\nthere's one another more thing that\nprobably I missed here\nand uh that honestly I don't see here I\ndon't know why\nis the basically the annotations for the\nactual\num for the actual Bots\nthat will run there\num\nlet me take a look at this\nuh probably yeah\nthis is an inline\num\nand I don't know why it's not there but\nuh yeah let me\nwait they are at these two sections two\nof them are covered in the tutorial\nagain but I'm not sure why they are not\nin the actual file\nso yeah\n[Music]\num\noh yeah there is\noh\n[Music]\nthat's resources and custom resources\num yeah\nlet me see here cool\nso the first section is Task resources\ncompletely optional but if you want to\nDefine platform y defaults for how much\nresources can flight task request and\nthe limits especially the limits for\nrequests you can Define it here\nright I'm using this just to prevent out\nof memory situations when when a task or\neven an example tax probably May crash\nI've seen this in the past so I tried to\nuse this as a way to prevent\nthat behavior and uh\nvery quick uh flight can help you manage\nyour code using a kind of a\ngithub's approach approach with\ndifferent environments development\nstaging production\nand each one of them will be a different\nname space in kubernetes\nand for each namespace depending on the\nenvironment you will need to have access\nto different resources on your\ninfrastructure you so you could use\ndifferent IM roles in this case I will\nuse the same\nuh for for the three environments but\nthis is where you set\nthe role that will be automatically\nused by any\nthoughts that results from a workflow\nexecution\nokay\ncool all right so I think that's it so\nfar I'll say let me check back on the\nstatus of the certificate\n[Music]\num oh great\nI love you too\num yeah so the DNS records are there uh\nyou can see here that the first one was\ncreated automatically\num\nhere you can see that\nglad School\nuh it's right there\num these two\nright\nso certificate through work\num yeah\nthe thing is that if I install this\nright now uh the Ingress won't work I\nmean everything else will work and we\nwill we will need if we run out of time\nI will deploy this without Ingress and\nwe will do a a simple pull forward\nthat's it\nbut um\nand yeah basically I will\nissue a fly binary installation and\nthat's it\nin the meantime let's go back to the\num\nlocal yeah I think\nI need to\num sorry\nuh grenades use a config use context\nMaster okay\nlet's pause all right great we need yeah\nnot how much times you run this\nbeforehand\nit can fail unable and unmanned bones\nand attach volumes time it out waiting\nfor the conditions yeah not not sure if\nyou have PVC\nuh\nuh pieces are bound\nuh so not sure what's happening here\nuh what change on the on the actual\nlocal values\num\nyeah I don't think that\nanything changed pretty simple\nso yeah it's weird\nokay uh but yeah the next piece is\nbasically running the health install\nflight binary uh to this environment and\nthat's it and it's it's right there in\nthe I mean the flyer Hardware tutorial\nthe next thing will be to run a insult\nflight burner\nand uh we will need to run because this\nversion of the on-prem deployment\ndoesn't include Ingress you will need to\nrun pre-port for forward sessions one\nfor menu\nbecause when you when you register and\nrun a workflow from your CLI it needs\ncommunication to your blob storage\nso you will need Community a portfolio\nsession from your laptop to menu\nand one for grpc which is the backend\nAPI where you connect to compile\nworkflows and HTTP for the UI and this\nis the Hello World\num workflow and it works yeah this is\nproof that it works and\nlocalhost right\num let's see what happens here\nah great thank you much love so let me\ninstall on on\nright now on AWS\nlet's go here\num\nright so Helm install flight binary\nbut we will use here the eks production\num EPS production yaml file\nhere we go\nwhat's happening right now is that the\nhelm is rendering all the\nall the templates and validating this\nwith the kubernetes API and if it is\ndeployed it is because at least the yaml\nwas\ncorrect and if you take a look at the\nparts in flight\nuh you will see a single binary\nuh\nrunning or trying to run\nI like this\num\ncool I will follow\nads\nit's getting started\ncool\nyeah\nready so right now fly Banner is ready\nprobably we don't have any Ingress\naddress right there's no address because\nas I mentioned or some strange reason\ncertificate is is failing to be issued\nthis is totally a non-flight thing\nand the many times I try this the only\nthing that was needed was create the\nC name\nmissing cname record there but yeah I\ndon't know but we can\nget the services right now it has two\nServices one for grpc one for http\nuh yeah thank you\ntaking some time but in the meantime is\nthere any question I I can't see\nuh there's a comment or primary cloud is\nactually with the hope of migrating to\nAWS in the future thank you Chris for\nsharing yeah right now I mean we have um\nthere are some users in the community\nsorry who are running flight on AWS\nuh big and small implementation and AWS\nusing within Azure blob storage using\nAzure ID\nuh and we have right now a working group\nthere's a channel on the flight flag WG\nDash azure\num and uh there's some experience folks\ntrying to improve the the way flight\nruns there and so it's it's kind of\neasier\nuh and uh the next thing we will be\ndoing is try to again do the same thing\nwe did for AWS with these tutorials\nwhich is basically Gathering the\nknowledge from the community it will do\nthe same for sure\num so there are folks out there who have\ngone through the process and we're happy\nto help right so I will do a pour\nforward session right now uh with the\nfly binary HTTP example\nand uh right if I go now to localhost\nI will find their\nuh the console live in kicking in right\nand uh\nyeah if I run for example uh or a binary\ngrpc\nI think well I I could expose grpc\num\nAPI but workflow execution probably\nwon't work well because it will find a\nmismatch between the certificate which\nwhich is created at least for flight\nschool you know then around that run and\nlocalhost\nright so this is prepare for for\nbasically for production\nuh right now oh yeah the certificate is\nthere\nso\num\nI think we need to roll out the ALB\ndeployment Ingress\noh no address is there so yeah we're\nrunning out of time I will do this real\nquick so here you will get everything is\nin the tutorial here you will get the\naddress from the ALB you can update\nremember that we created this cname\nbut it's not pointing to anything useful\nbecause we didn't know the actual\naddress so you can now update you can go\nback to dns.tf uh\nchange this value\nright\nand Save\nand you can do a terraform refresh and\nthey're on terraform apply\nyeah I can just go there to the Route 53\nconsole and just copy paste right uh I\nI'm just trying to avoid access to the\nManagement console at all\nand uh just show you that everything can\nbe updated using terraform\nand um so once yeah in the terraform\napply\nright\nand uh\nonce the actual DNS entry is updated\nbecause right now it's pointing to\ninjury\nthe example.org\nslides cool\nuh there you go\nentry\num yes please\nchange me to the actual value that we\nneed\nand once that is done the only thing we\nneed to do is to\num update or local flight config file to\nmake it point to the actual fqdn\nand that's it so let me see do we have a\nan actual\nor forward session running now\ntake some time because again it takes\nsome time for for the change to be\napplied\nuh\nright the value is there\nand it's been updated cool so I will go\nto\nhome a config and I will change the the\nend point\nwe apply it's cool the DNA demo\ninsecure false because we're using a\nproper certificate and that's it\nright\nso uh I will go right away to a place\nwhere I know\nI have a\nexample the same example workflow\nthat is in the tutorial I will run it\nagainst\num the newly created instance and if it\nworks yeah it will point you to the uh\nit will tell you yeah you don't need to\nuse this now you can use\nuh Union demo you can use now your\nyour actual URL oh sorry it's slash\ncontrol\num go to the development\num\ndomain and you can see this workflow has\nthree tasks and it started running and\nhopefully will run without any issue\nall right I think we ran out of time\nI think we covered a lot\nuh again the the on-prem deployment as\nmany things again to to be improved but\nthat's kind of the general idea there\nare some dependencies there are many\nthings to that there are many ways to\nconfigure data persistency and uh one\nzero dependencies are in place which is\nbasically postgres in some form of blob\nstorage you can\ninstall flight binary we have users\nright now consuming flight core I mean\nrunning multi-cluster on-prem uh so now\nyou should let me show you real quick\nuh yeah well I have a multi-cluster\ndeployment but I'll better off\nuh show you\nthe docs which are about to be\nupdated\nand uh\nyeah probably the the best thing would\nbe to show you\nthe actual uh updates to be applied\nI hope you're still with us if not you\ncan watch the recording\num\nyeah let me\nshow you\nright away\nuh\nuh the changes to be applied in multi\nmultiple kubernetes cluster is basically\nthis you will have a control plane\ncluster\nwhich could be also data playing cluster\nif you need right you could also run\nworkflows there and you can have one or\nn data plane clusters\nkind of headless clusters with no\ncontrol plane uh running that's\nexclusive and um users administrators\ncan Define using basically labels they\nthey can Define what the behavior they\nwhat they hear the behavior they want on\nthe platform for example I want team a\nrun workflows only on cluster one but\nother teams could run workflows in\ndifferent clusters you can even Define\nweights or clusters that are prepared\nabove others\nand uh\nthat's from the administration point of\nview and users can just use labels and\ntheir executions and the control plane\nwill uh will run the actual workflows on\nthe data planes depending on those rules\nright so the whole process is here this\nis kind of yeah the most advanced\num\nway of deployment right now probably\nwill need a follow-up session but\nmulti-cluster and again for deployment\non other clouds\nall right so I will stop sharing my\nscreen is there any question out there\num\nis there anybody anybody out there\nyet um\nno questions\nand you can see\ncan you talk about how to think about\neks managed note groups on how they\ninterface with flight task workflows\nyeah good question here uh the the\nactual modules the actual terraform\nmodules have some decisions let me stop\nsharing my screen have some decisions\nmade up front in terms of note groups\nfor different\num different executions you can use for\nexample pod templates and this is a way\nto basically map different node groups\nfor example for spot instances for for\ngpus Etc\nuh in in with this with their match for\nthis labels executions will land on\ndifferent note groups\num the the again the terraform modules\nare prepared in the actual prepare and\ncreate different note groups let me show\nyou real quick\nand from the user point of view you\ncould use for example plot templates to\ncontrol placement on on different uh\nin those groups\nyeah it has different groups for spot\ngpus on demand Etc\nand you can control this from the user\npoint of views\nI don't know if that answers your\nquestion but we can\nsync on the on this lag and continue a\nconversation if not\ncool\num yeah anything else any other question\nno\nall right uh and yeah it's my pleasure\ninto our eyes pleasure everything\nsucceeded it's working it was a that was\na live deployment in less than an hour\non AWS so I hope all of this was useful\nto you\nagain there are many ways of doing these\nthings but we are trying to create\nresources that that help you focus on\nwhat matters I understand how how data\nscientists couldn't care less about\ninfrastructure with so we're trying to\nbuild resources to automate the entire\nprocess and let you focus on what\nmatters to you\nall right thank you all for joining and\nhope to see you in the next episode\ngoodbye"
    },
    {
        "title": "Flyte School: Flyte Architecture Deep Dive",
        "transcript": "welcome again to flight school episode\nnumber four\num David I will be your host today\nwe'll be\num going through a deep dive or more an\nexploration a technical exploration the\nflight architecture flight the the open\nsource workflow orchestrator for ML and\ndata\num so yeah you are welcome to ask your\nquestions we will have a section at the\nend uh to handle questions and answers\nif you can wait uh until that point that\nwould be awesome uh but feel free to\ndrop any other comment or question in\nthe chat whenever you have it\num great\na bit about myself if if you were here\nin the last episode sorry this is kind\nof repeat information but for folk folks\nyou were just joining uh my professional\nbackground is mostly on the\ninfrastructure side all the things\ninfrastructure here means uh compute\nsince the virtualization days uh to\ninfrastructure as a service Automation\nand more recently the cloud native\necosystem coordinates Etc\nuh yeah I probably I one of the things I\nfeel most proud of is being a former\nCornelius release team member\nand especially because of the human\nbeings that make this community the\nkubernetes community make this project a\nreality you wouldn't believe how how\nsmall the team is uh and how big are the\nexpectations so it's it's incredible\nright I'm not a proud father of two boys\nI enjoy bike cycling running\nand uh yeah music is is very important\nfor me I listen classical music since I\nwas a kid and heavy metal also and some\nother generous well in general\nI enjoy learning about how music is\nwritten all right\ncool so the agenda will cover uh flight\ncomponents and we'll cover this from the\nperspective of a workflow kind of a day\nin the life of a workflow in flight at\nprototypical workflow it won't cover all\nthe possible uh\num customizations and difference for\nexample we won't cover Dynamic workflows\nsuper workflows Etc it's a simple\nworkflow in how it traverses the entire\nflight system and how all the components\nplay a role there\num and also how the system brings some\nguarantees in terms of scalability and\nreliability which is still important\neven on the AI era\ncool so um I will start off by showing\nappreciation to the union\num theme I'm part of Union\nwho sponsors this episodes this entire\nshow let's say and uh kind of one of the\nmain\noutcomes or products of Union is Union\nCloud the uh under three folks waiting\nuh cool uh the union cloud is the flight\nmanaged service so in here you basically\njust have your data somewhere else and\nthat's it you don't have to deal with\nkubernetes infrastructure anything like\nthat you will have flight as a core\nengine everything that you will see here\nyou will be able to use it and you're in\nCloud\nplus\nsome contextual observability\nand also the\nvery soak after feature of role-based\nAccess Control right\nuh so yeah feel free to check it out\nrequest demo and also use it in a\nsandbox environment for free which is\ngreat\ncool\ngreat all right yeah welcome everyone\nand feel free to mute yourself unless\nyou need to say something\num cool so we'll start the exploration\nof the components using these planes\nwhich is pretty much used for for many\nsystems out there it will start with\nwhat's closer to you the user the user\nplaying components\nso the first thing that you do is to run\npipe install flight kit right this is\nthe SDK\nuh right now available for python\nJava and Scala and you use this to\nauthor workflows task launch plans and\nmany other objects right this is this is\nthe in the main interface you will use\nto author workflows\nthat's flight kit uh flat CTL think of\nthis like the general purpose CLI that\nyou will use to interact with the\ncontrol plane\nand uh you can perform the yeah growth\noperations for workflow executions\nand you will use it to interact with\nsystem entities this kind of network\naware it's not code aware it doesn't\nunderstand your code it you you use it\nto interact with the system mainly\nin the executions with pi flight is\ndifferent\nuh if you if you probably take a look at\nthe getting started guide or you if you\nare already familiar with flight\nprobably you already use by flight\nespecially at the beginning this is the\ncode aware CLI because it not only is\nnot only interacting with the system but\nit's responsible for\ncompiling your actual workflow and task\ndefinitions into product of messages\nusing a component that you that we will\nsee in a little bit\nand so we will parse your code and it\nwill understand everything and submit\nthis to we compile right so it even\nincludes an option where it will package\neverything and will submit it to\nexecution in a single command which is\nby flight run you will see it in the\ntutorials they all use\num by flight run\nright so that's what's closer to the\nuser uh that's those are the user\nplaying components in the control plane\nthe main component probably what what uh\nfills the entire space in the control\npane is flight admin\nright and um\nflat admin exposes the workflow API uh\nit processes requests to create entities\nbut it's not in the execution path\nflight admin doesn't execute workflows\nit receives your request\nfor execution and create some entities\nwe will see in a little bit what kind of\nentities\nand um yeah well it's there it's\nbasically a custom resource definitions\non coordinaries right now the the\nplatform for flight or the main backend\nis kubernetes so flight admin will ask\nthe kubernetes API to create a crd what\nkind of crd for what type of flight\nobject we will see this in a little bit\nuh the other component in the control\nplane is the scheduler\num this is\npretty cool in my opinion and we'll\nexplore it in a little bit it basically\nimplements one of the um core objects in\nFlight which is the launch plan the\nlaunch plan is basically a a definition\nof the inputs for uh or a set of\nworkflows and also includes a definition\nif you need of a schedule for especially\nfor for batch workflows or workloads\nthat need some kind of cadence and\nrepeated executions you can include in\nyou can include sorry in the scheduling\nconfig inside a launch plan right and\nthe native scheduler plays a major role\nin scalability and we will see this in a\nbit\nand the other component that sits in the\ncontrol plane is data catalog\nuh did I catalog in my opinion it's\nbecoming one of my favorites\nprobably it will change in the future\num\nand take a look at the artifacts RFC\nthat it's out there in the repo it's\nright now in draft state that probably\nwill transform how uh this role is is\ncompleted by flight but right now even\nwithout that data catalog is very\ninteresting in my opinion\nit basically uh\nimplements a special form of caching\nwhich is memorization if you're in the\nin the ml space you know what is this\nbut it will basically\num help the data plane to store not only\nthe outputs of a specific task but a\nhash of the inputs so every time there's\na new execution request with the same\ninputs it will be a catch hit basically\nand uh you will the system will remember\nthe outputs without having to execute\nthe workflow again\nit creates stacks and\num you you will be able to retrieve a\nspecific specific executions having\nsomething which is very important and uh\nit's it's interesting to see it missing\nmany times in ml which is\nreproducibility having the chance to go\nback to specific execution and and being\nable to reproduce the same outputs and\nalso the traceability or provenance in\nin our context the provenance terms is\nused being able to track uh okay what\ncombination of inputs produce this\noutput which is very important is table\nstakes in software engineering it's not\non ML and this is by default and like\nit's another thing that I love if you\nyou just need to in your code to say\ncatching through to enable caching in\ndata catalog we'll do the rest\nwhich is awesome\nokay\num we'll explore all this in more detail\nin this session and finally in the data\nplane\nit's the flight propeller component and\nthis is basically a kubernetes\ncontroller that follows the operator\npattern\nagain this is not a kubernetes session\nand you don't need to understand this\nbut basically the operator pattern is a\ncontrol loop again I'm using a term from\nelectrical engineering but basically a\nthermostat\nright if you know how to how that this\nthermostat works that's exactly the\noperator pattern in this case flight\npropeller will watch uh for new crds and\nwill understand what the user wants to\ndo and will\num kind of reconcile all this logic in\nthe data plane we will explore this in a\nlittle bit right and also interestingly\nit can be a scale out using sharding uh\nuh that's for special cases when when\nprobably even before hitting scaling\nlimits scale limits in Flight preparer\nyou will find scale limits and\nkubernetes itself on the corners API and\non the Clusters themselves but even so\nflight propellers right propeller can be\nscaled out if you need\nand finally the plugins and the multiple\nIntegrations the logos here are just an\nexcerpt of the multiple Integrations\navailable right now\num there's a new mechanism right now\nwhich is the flight agents\num to be able to write and test locally\nplugins kind of making much easier to\nwrite plugins for flight\nsome of them are native meaning that\nflight itself can handle the Logics some\nof them rely on external services like\nintegration with search maker or data\nbreaks or others right so\nthese two components sit in the data\nplane\nif you just need to invoke a Plugin or\nrequest a plugin your code for example\nif you need to to do parallel processing\nusing gray\num\nyou will just invoke this in the code\nand in this case flight propeller will\ncall out the external system that is\nneeded and complete the execution\ncool uh yeah no problem so we'll now\nexplore a day in the life of a simple\nworkflow right and we will see how all\nthese elements play a role here uh\nkind of that this is the most expensive\nslide that I've done in a while in terms\nof times I hope it's clear for some of\nyou out there if not please let me know\nhappy to improve stuff it needs some\nimprovement so everything starts with\nyou\nthe user and you install flight kit\nagain and you use it to Define tasks and\nworkflows if you're not familiar task in\nFlight is the fundamental unit of\nexecution here you can even\nrun a task without a workflow I don't\nknow if if you knew that but I learned\nthis recently I've been engaged with\nwith flight for\nfor the past eight months not not a lot\nof time but um I just learned recently\nthat you don't even need a workflow to\nrun a task so really the fundamental\nunit of execution here is the task and\nthe task is each one of these granular\nsteps in your ml pipeline\nand you can write this in on python or\nany other language we will see how it\nworks in a bit\nright so you ended up authoring your\nworkflow everything is great so you will\nuse either by flight\nto again\ntake this code compile it and ship it to\nthe control plane for execution or you\nwill use flight CTL\nthis is for the special case where you\nhave uh a custom image for example you\nneed to have not a default Docker image\nbut you have a custom medium image and\nyou made some changes to it uh you will\nneed to do a two-step process\num to submit a workflow for execution\nyou will use fly CTL for that but in any\ncase\nThe Next Step what happens\num behind the curtains is a packaging\nprocess packaging is basically\nsubmitting this\nto be compiled\nand the component here the first\ncomponent that it touches is flight IDL\nthis is interesting because this is part\nof\num of the control plane that you won't\nsee it in a deployment if you install\nflight today you won't find flight IDL\nanywhere right this is the interface\ndefinition language fly I flight IDL is\nwhere the definition for the core API\nlives including the backend grpc API and\nit's where the compilation happens\nwhat's the output of this the output\nhere will be a set of product of\nmessages the definition of these\nmessages are part of flag ideal\nand this protograph will include your\nexecution plans the input and output\ntypes that remember that in-flight tasks\nare strongly typed and flat is able to\ncatch\nmismatches on on types between outputs\nand inputs in tasks on at compile time\nhelping you write you know code that is\nmore production grade\nand all of this lives input above it\nit's saved in Portable Files\nso if you just package your code you the\nthe end result will be that you will\nhave proof of files\nand that's it\nthe next thing that needs to happen it's\nthat in parallel the user will either\nuse image spec which is a recent\naddition to flight it's it's very\ninteresting I explained this like a like\nthe build packs for ML it's kind of\nsimilar image expect will basically scan\nyour code it will build the docker image\nfor you\nright it's it's having an image without\nthe Pains of writing a Docker file\num you can use image spec or you can use\na regular Docker build but in the end\nyou will have a an image leaving on a\nregistry right and that's where that's\nwhat will be used in the end for\nexecution but we will see this in a bit\nright\nso great you have an image you have a\nprotocol files again this is kind of the\nhard way\nright in this kind of their hard way and\nprobably the first time you use this\ngoing through the hard way right because\nthe next thing that happens is that you\nwill register you will register uh these\nfiles this compilation\nto the control plane in this case flight\nadmin in all the cases flight admin\nright what happens with pi flight run\nfor example is that it does everything\nin a single command when you do python\nrun uh it does the packaging\num the compilation register in execution\nall in a single command but for\ndifferent use cases for example if you\nhave multiple flight admins\nuh for you probably will be better to\nhave some control in the process\nand use for example flight CTL package\nand use these protocol files and\nregister this on multiple flight admins\nfor example that's a use case\ncool so now flight admin has a workflow\nregistered but nothing is being executed\nright now nothing is happening right now\nso uh once you for example you can go to\nthe UI and click on launch workflow\nthat's when the execution happened or\nyou can use the CLI to trigger an\nexecution\nand next thing that will happen is that\nflight admin will instruct the\nkubernetes API server to create a crd\ncrd is a custom resource definition\nand by custom I mean that kubernetes\ndoesn't understand this I mean it just\nit just stores the crd on hcd\nEtsy is the key Value Store where\nkubernetes stores the definition for\nevery resource under under its\nmanagement\nuh but kubernetes doesn't understand\nthis I mean the the definition of a\nworkflow the kubernetes doesn't\nunderstand is there's an operator right\nthere in the field which is flight\npropeller uh that will constantly watch\nat city for newsy artists and um Fly\npropeller will understand this and this\nis the DAC kind of the DAC structure\nright the uh the workflow structure in\nFlight proper will reconcile that\ndefinition into the actual\num resources that they need to execute\nthe tasks so for example you have three\ntasks in your workflow you will have\nthree parts right one per task if you\nneed to consume Secrets if you need to\nconsume IM roles if you're in the cloud\netc etc all of that will be reconciled\nin the data plane by flight propeller\nand also at the same time flight prepare\npropeller will send notifications back\nto flight admin these notifications are\nguaranteed delivery meaning that the\nworkflow won't progress unless the\nnotifications arrive\num and flight admin will persist this\nevents on the relational database that's\npart of the reason flight needs uh\nrelational database right now pause\nGrace uh but hopefully soon it will be\narbitrary back-end databases but right\nnow it's mainly a postgresland right and\nthat's the place where the these events\nwill be persisted\nright so what happens once a task is\nexecuted once a task runs it will\ngenerate outputs right so the um the\nmetadata uh of the task will be stored\nby flight propeller on blob storage\nthat's why flight needs blob storage and\nby blob storage I I want to be very\nclear that it doesn't have to be Amazon\nS3 the product right uh it can be very\nwell GCS or it can be very well any\nother form or blob storage that is S3\ncompliant\nwhat it in in terms of S3 because I I\nhad a conversation recently with a user\nuh who was like I don't want to use this\nmanaged S3 service well you don't need\nit just need you can even use Minion or\nany other thing that exposes an S3 API\nthat's what flight propeller needs right\nto store task metadata\nand if you enable catching\num that's what uh\ninbox the data catalog in data catalog\nas we said in summary it will generate a\nhash of the inputs and alongside with\nthe outputs in a data set and it will\nstore this in Blob storage that's the\nmemorization\nportion of the process giving you\nreproducibility\num right away okay\num\nyeah uh yeah that's a good question can\nwe use Azure storage yeah you can use it\nI mean we have users out there\nwe don't have probably the best guides\nright now to help you with the process\nbut but now with with a recent addition\nof the fs spec\num back-end supported by flight uh we\nhave users using Azure blob storage\nsuccessfully\num so yeah you can you can use it that\nmy my\nremark was specifically to avoid this\nconfusion between S3 the product and 3D\nAPI but in general yeah you can use this\na boss is a complying file system\ninstead of blush storage that's a good\nquestion I uh I'll ask you to either\nask this on on the Aztec Community\nChannel or otherwise I can take this\nquestion and I will I can get back to\nyou in the end I will show some ways we\ncan keep connected after the session\nit's okay and\num yeah we can we can cover because\nthat's a fair question\num cool\n[Music]\nyeah let me let me move on and I will\ncover some of these awesome questions uh\nin at the end cool so that's that's kind\nof the process for for a simple workflow\nwith uh catching enabled and\num\nyeah that's kind of the process so far\nso we will see in some details some of\nthese elements I will yeah just uh in\nthe end if the user if you want to know\nwhat's the status you probably you can\nuse again the the CLI or you can use\nflight console which is the UI and the\nUI will basically retrieve status from\nthe control plane all right so these are\nall the components uh the next session\nin the flight school series will be live\nbuild we will deploy live here and a\nflight environment from scratch so you\nwill be able to see all these elements\ncome to life\ngreat so how this helps how um all these\narchitecture\nhelps with scalability well there are\nmany strategies out there and uh in past\nsession we covered a bit the definitions\nof what makes a system scalable which is\nbasically at which stand you can apply\nlocals strategies to help the system to\nhandle more flows\nand a system is more scalable depending\non on how much you can extend the the\nability to perform well at a low cost\nand\num this is great I start with go\nroutines because this is a zero course\nfor the user serial code strategy that\nis enabled by default that's what that\nwhat it means zero cost in this content\nyou don't have to do anything with this\nyou don't have to even remember this\nprobably you will forget this tomorrow\nand it doesn't matter flight still will\nuse gold routines\nuh governments are basically\num a native go like mechanism that\nflight is written in Gold length a gold\nmechanisms mechanism to handle\nconcurrency right and it's uh basically\nexecutes majority of destructions inside\nthe go executable\num user space let's say without the\nusually expensive context switching of\ngoing to to the operating system kernel\num performance Benchmark showed that\nit's about 500 x cheaper than operating\nsystem threats\nuh that's how fast it is and uh flight\npropeller and other components in Flight\nuse this to handle concurrent workflow\nexecutions so that's the first thing\nagain you don't need to do anything with\nthis you you don't need to enable this\nit's enabled by default right so\num concurrency it's written\ninside flight\nthe other scalability it's in the in as\nI mentioned at the beginning ml is not\nmy background but I had the chance to\nspeak with many of you daily ml\nEngineers data scientists and I find\nconsistently that in well in many cases\nthe scheduler in a workflow tends to be\num a bottleneck in terms of performance\nespecially when you start to grow\num so the flight native scheduler brings\nsome ideas to have to handle concurrency\num with with a strong guarantee here is\nthat no schedule is missed\num\nyeah yesterday I was talking to\num\none of the creators of light\num because I had some questions even for\nfor this session and uh yeah the the\nconclusion of the session was that\nflight was designed with correctness\nover performance uh as a core design\ntenant and yeah there are some\ntrade-offs that that were made after\nthat decision but correctness versus\nperformance seems to be even more\nimportant I mean correctness seems to be\nmore important even than performance for\nML and data workloads so one of the\nreflection or the implications of that\ndecision is that for example in the\nscheduler not schedule is missed\nand also all this every schedule in\nFlight is versioned like like also like\nthe workflow executions everything is\nversioned\nso you can have this this again this\ntable stack features uh there is stable\nstakes in software engineering you can\nhave it on ML and have the\nthese are reproducibly\nI want I wanted to animate this but for\nsome reason uh Google diagrams was not\ncollaborating so I will have to use this\nthingy here the laser pointer the old\nschool laser pointer so what happens\nwhat's the kind of the life cycle of\nscheduling and flight well first thing\nthat we need to say is that probably by\ndefault you if if you don't have batch\nin France or this type of workloads\nprobably you don't need scheduling but\nif you need it uh it's typically defined\ninside a launch plan let me show you\num where are you buddy\num yeah it's here great so let me show\nyou real quick here\nin docs you can see that inside the code\nyou will Define the launch plan and you\nwill Define the schedule it accepts\nregular Chrome syntax\nuh here for yeah Chrome based jobs or\nfixed rate jobs\num\nand uh yeah that's that's the first\nthing you will you will Define your\nschedule inside a launch plan and you\nwill activate the launch plan\nthat's that's all you need to do\nflight admin will read this and uh that\nwill activate the schedule management\nsystem\nand uh flight admin will ask the system\nto create a schedule\nthe next piece is that there is a\nspecial implementation built for flight\nfor the open Chrome library for go\nwe call it the gokron wrapper so it uses\nand extends this library and it\nbasically will read these schedules be\nit fixed rate or Chrome based and the\nnext thing it will do is that it will\ncreate a go routine per schedule again\ngo through things are here and you\nalready saw how\ncheap they are in terms of resources\nright so yeah it will create an\nindependent goal routine per schedules\nwill you you can have a massive uh\nnumber of schedule schedules work\nrunning concurrently\nThe Next Step will be that the executor\nwhich is another component the scheduler\nwill generate an execution ID that is\npartially based on the timestamp\num timestamp when the actual schedule\nwas created and it will submit this uh\nto flight admin the idea of using\ngenerating a timestamp is to ensure the\nmost accurate execution of of the jobs\nuh not only when the when it was\nrequested when by the user but actually\nwhen the schedule was created\nand um yeah the idea is again\ncorrectness in in this case in the\nscheduling\nanother step that happens in parallel is\nthat there's a component that is called\nthe snapshotter again you don't you\ndon't have to deal with anything on this\nI'm just showing you what happens behind\nthe scenes right and the snapshotter\nwill read the timestamp from the last\nexecution and\nperiodically will persist a compact\nversion of the timestamps in the\npostgres database in the relational\ndatabase right\nwhy is that well again what happens if\nthe scheduler goes down for example is\nnot available right that will be the\ncase when a schedule is missed right\nwell what happens is that uh once is is\noff and uh in one well yeah the\nscheduler goes down but once it's back\nuh they catch up all subsystem will re\nread the last timestamp from the\ndatabase and we'll compare this with\ntime.now I mean\nhow many executions we missed until\nright now\nand the next thing is that it will\nsubmit the missing schedules for\nexecution so no schedule is missed\nthat's one thing and the other thing is\nthat schedules are even potent in a way\nmeaning that new executions won't affect\nx16 executions I mean every every new\nschedule execution is a different goal\nroutine in this case it won't it won't\ninterfere with existing executions right\nagain ensuring correctness and ensuring\nthat no schedule is missed\nright again\nwhat do you need to do with this nothing\nyou just need to Define your schedule\nand and that's it the system will handle\nthe rest and that's what make\nconceptually a system more scalable in\nhaving\nstrategies to handle concurrency that\nare low cost for users\ncool\nyeah keep the questions coming the chat\nwe will address\nall of them and if I don't have answers\nI will get back to you with answers\nand yeah finally reliability which is\nstill important in the mail I covered\nthis in the last session how how many\num\nobservations in the field show that in\nsome cases failures in ml systems come\nfrom from outside the ml system and more\ntypically are they are distributed\nsystems failures and reliability matters\nso the first thing is that uh in the\ndata plane there is a cloud native\ncontroller that I briefly described\nwhich is flight propeller uh in the left\nhand side here you can see kind of the\ndefinition all the logic of flight\npreparer is this in summary it's a\nreconciler that will watch for the\ndesired State what's the desire State\nyour workflow definition\nyeah it was compiled it was transformed\nwhy it said why flat uses protocol for\nserialization for compiling where protov\nis Pro buff is very portable\num so it means that you can write your\nworkflows in in any language\nit's not only that is containerized what\nmakes it language independent but also\nthe the way flight serializes it makes\nit also language independent you can you\ncan write your workflows in python or\nany other language right now\nand that's your desired State it's\ntransforming profiles it's submitted in\nas a crd kubernetes and that's what the\nflight propeller uh will observe will\nwatch\nand it will work 24 7 relentlessly to\nmake the actual State match the desired\nstate if there's any deviation if there\nis any mutation in the infrastructure in\nthe data plane you as a user you don't\nhave to deal with this\nright this is a way we fulfill the\npromise of abstracting away the the\ncomplexity of the of an infrastructure\nlike kubernetes you don't have to deal\nwith if a pod goes down if a note goes\ndown if a service goes down Etc this is\nhandled by the reconciler uh that again\nis a thermostat if the if the\ntemperature goes up it will it will just\nkick off the fence and do what it needs\nto uh uh make the actual room\ntemperature match what you declare as\nyour desired state\nthat's it that's the core uh design\nproposition or value proposition of\nflight propeller in the right hand side\nyou see a kind of a\nmore detailed diagram it's it's in the\ndocs in the flight documentation of what\nflight propeller does uh as you can see\nyeah it tracks crdis stored on Etc and\nit will handle a queue\nworkers executors he will recurse\nthrough the DAC flight popular\nunderstands that and and this is\nimportant because not all the\norchestration or all not all the\norchestrators are born equal I mean\num\nyou can use a general purpose\norchestrator to run\nML workloads and that includes\nkubernetes but the problem with this is\nthat that many of these general purpose\norchestrators will\num will render\nsomething like this in serial task\num you know one after the other but in\nan ml pipeline that that's fine for\nsoftware engineering but in an ml\npipeline there are tasks that need to be\na loop that need to be repetitive and\nthe best way to represent this is the\nDAC and um\nyou can bolt on this Logic on a general\npurpose Chris Trader but it's totally\ndifferent if this is the by default\nbehavior and if the orchestrator was\nwritten for this was designed for ML and\nthat's the case for for flight\ncool and yeah again flight paper will\ninvoke the plugins if it's needed if you\ndeclare declare that in the code and\nagain it will report events to flight\nadmin so those are the reliability\nguarantees on the data plane it\neven despite failures it will arrive to\nyour declared or to your desired state\nright\ncool uh yeah before demo I will cover a\nbit of this\nquestions uh\nyeah the post six one I will I will\nhandle this after the call are both\ninputs and outputs of the task hashed\nand stored as part of caching uh inputs\nare hashed yes and they are stored with\noutputs as part of the catching\nmechanism they are stored in Blob\nstorage\nso the system if if the if the hatch\nmatches a combination of inputs the\nsystem will remember\nthe outputs and it will retrieve them\nfrom\nfrom blob storage\nuh and let me know if that answers your\nquestion can we use any FS spec\ncompliance file system I will say so\nI will say yes but to be 100 sure again\nI encourage you to join this like if you\nare not there otherwise uh with your\nname I will reach out or feel free to\nreach out to me and\num I will get back to you with an answer\nI mean I can see the I can say this\nconfidently because I always get back\nwith an answer even if it takes some\ntime but I know who to ask and\num I will get back to you and answer\nI hope that's helpful\nuh always letting you know cool uh yeah\nreal quick here\num\nthat's a\ndimple execution I will use here by\nflight run remember that uh this command\nwill do the packaging\nuh register and it will submit this for\nexecution it will launch the workflow\num it's a it's a very simple uh workflow\nand I'm using remote because the the\nnice thing with flight is you can you\ncan test locally\nand you can then launch the same codes\nuh on your flight environment which is I\ndon't know if your production\nenvironment you just need to remove that\nstash remote\nuh you can use pipeline run and invoke\nyour workflow and you will be running\nlocally in your python environment\nuh in this case I'm using remote because\nI will I will trigger this in a\nuh I will trigger this and and\non a flight cluster which is running\nhere locally on my laptop\nuh again we will cover deployment and\nstuff we will cover this in the next\nsession\nuh so here is uh it's running right now\nthis is flight console\num kind of the basic version you can see\nhere login\nuh not found which is not particularly\nhelpful uh but you will see here if\nyou're using oath for production\nenvironments you will be connecting this\neither with the embedded authorization\nserver or with the external service like\nOCTA key clock Etc\nand uh you will need to log in to flight\nright\nokay so you can see here the executions\nand this one is running you can see that\nthis one has three tasks and um\nI can show you\nright away sorry you'll get bots flat\nsnacks\noh white not needed you can see here\nthat this one was started a minute ago\nand it's running\nit's the note zero of the deck and zero\nmeans node zero\nright so it will be a part per task\num you don't need to go to kubernetes to\nsee this but again I'm showing you what\nhappens uh behind the scenes\nand you can see here workflows all\nworkflows that I register here I'm\nrunning this one but here I have a hello\nworld that I can launch here\nright or I can launch from from the CLI\nas I described previously here are the\ntasks\num\nyeah some information about desk and if\nyou see for example here the execution\nof this simple task you can hear the\ninputs and the outputs pretty basic\nuh this one the one that is running\nright now\nalready run yesterday and uh yeah\ntraining the model it will generate\noutputs that are right now in my S3\nbucket this is a menu bucket\nso um\nyeah the the inputs are also\nrecorded here and I'm not using catching\non this task\nokay uh as per how how much time which\nis a great question how long for how\nlong the catch result is retained in\nBlob storage\num\num I'm sure this this one can be\ncontrolled but I will get back to you\nhim uh that's a a great question and\nagain uh as you see as you see here\nfirst names\nuh if you register using Eventbrite\nprobably you can\nreach out to you otherwise feel free to\njoin flight slack I will show the link\nin the end\nand uh so we can keep the discussion\ngoing\num\nand I can connect you with folks who\nknow better\nthat portion of the system\ncool\num and yeah lunch plans here here's the\nlaunch from Adventure I don't have a\nschedule for this uh but this is kind of\nthe the the whole plan that is created\nby default with the workflow\nand uh the nice thing is that you can\nhave multiple projects and multiple\ndomains per project right this one is\ndevelopment but you could you could have\nstaging or production so helping you\nfollow a kind of a git Ops approach on\non handling uh domains and and projects\nand it has a a match on on kubernetes\nfor example you can see that the\nnamespace is the project Dash domain\nif you move to production it will be\nproject dash production\nand um yeah you can see here it's still\nrunning it's a bit\nslow on my system\num cool so yeah that that's kind of the\nsummary\num\nso so far is there any other question\nout there\nas you can see I don't have all the\nanswers and that's fine because uh it's\nit's nearly impossible so um but I'll\nmake sure that you have your answers\nsoon\nokay see some familiar names here\nany other question no\nokay\ncool so yeah I I will just finish by by\nasking you to join uh flight community\nin this case\nthis lag instance is where the majority\ncontroller runs by crons sorry a\nquestion coming we can trigger runs\nbackgrounds but can we run a workflow\nwith another one finishes uh yes that's\npossible there's a mechanism for that I\ncan forward you the docs and uh but yeah\nthat's that's entirely possible on\nflight\nand uh yeah feel free to connect on\nslack so\nthat the the the conversation persists\nand and everyone can see the link after\nthe fact uh yes sorry\nfeel free to reach out cool\nyeah again join the slack communities\nwhere we spend most of our time and uh\nalso feel free also to start discussions\non the flight repo\nuh I started repo uh both for issues\num again we're happy to interact this is\na fairly active community\nand we are proud of it and uh we're\nhappy to help on LinkedIn we are as\nflight you will find many other flights\nright uh flight company uh some other\nkind of consultancy job I may maybe\num but uh yeah find the flight that has\nthis logo the or flight right and in\nthis social network formerly known as\nTwitter we are as flight org\nuh you can reach out to me directly also\non LinkedIn and David Espejo\nand you'll find like picture there\nand I'm also on Twitter but not very\nactive these days to be honest cool\nanother question from Alex is it\nsensible\nto pull an external service in a\nworkflow I read it's currently an entire\npattern but not sure why\nyeah\num I'm not sure also why Alex sorry uh\nbeing completely honest but again we can\nwe can continue his conversation on his\nlag there is where uh there's the people\nwho can uh cover better why this is an\nanti-partner\num\nbecause yeah it's been a kind of a\nrecurring conversation here and there\nand um\nyeah feel free to reach our Walter Alex\nsorry or otherwise we will reach out\nah yeah I read that visual about side\neffects within tasks and workflows uh\nyeah I don't know Juan if you want to\nand and mute and you could elaborate\nbetter\nuh what do you mean by side effects in\nthis context\nso I guess it's something I read uh in\nthe flight dogs themselves that we\nshould not uh read from any other data\nsource within the execution of a task or\nour workflow we should only do that\nbetween stages between tasks inside a\nworkflow\nokay thank you that helps\num\nyeah I mean we need to dive a little\ndeep on this and uh I know that you are\nalready in this black community one\nwhich is great and and thank you for\nyour contributions yeah I'll make sure\nthat we discuss this there as a\nfollow-up for sure\ncool\nany other question for my backlog\nanything else\nno\nawesome\nokay I hope this was useful to some of\nyou how this this was helpful since\nyesterday I knew this one and I was\ntelling the the team today we need a\nfollow-up for this\nthis session won't be enough and I know\nand there will be a follow-up for the\nsession where we can we can start really\nfrom first principles from a task and\nabove\nand probably we can bring some false to\nhandle this uh deep dive really deep\nlive questions and but besides that I\nreally enjoyed this I enjoyed your\nengagement your questions thank you all\nfor joining and um\nyeah we'll see you in the next one again\nfeel free to reach out on slack on\nsocial media we're happy to help"
    },
    {
        "title": "Flyte School: Kubernetes for ML and Data - An Introduction",
        "transcript": "right let's let's get started it's great\nto see you all here uh today I plan to\ncover kind of an introduction uh to\nkubernetes for data scientists and ml\nengineers\nand as the title implies there are a lot\nof assumptions uh there are a lot of\nassumptions on the level of familiarity\nuh data scientists and animal engineer\nthey have on infrastructure\num jargon and also assumptions on what\nwe consider or what I consider in this\ncase relevant for this audience uh in\nterms of of kubernetes\narchitecture objects and Etc so\num\nbear in mind that that yeah probably we\nare not covering everything and uh I'll\ntry to provide an overview and again\nhope this is useful for you so we'll\ncover I'll start with why why coronary\nis in this context what is it from\ndifferent perspectives a bit of how it\nworks\nand a short laugh and uh feel free to\nask any question during the session\nwe'll try to address them at the end\nall right a bit about myself I've been\nworking for the past 14 plus years on\ninfrastructure so my my background is\npure infrastructure from virtual no\nbefore virtualization from migrating as\nSEO Unix\num machines to Virtual\nLinux\ninfrastructure and then proper\nvirtualization and then cloud computing\nand more recently kubernetes and and the\ncloud native echo system\nuh yeah I hold the 35 kubernetes\nadministrator research and I'm proud to\nbe a former member of the coordinators\nrelease team not because it\nimplies some knowledge really competency\nand kubernetes sale but because it it's\nuh it's a demonstration of what humans\ncan achieve it's incredible to see how\nevery quarter every three months a group\nof voluntaries throughout the world join\nforces and spend time to produce a new\nversion of kubernetes it's amazing\nall right I'm a proud father of two boys\nI love bike and I love probably running\nmore than cycling and yeah for for some\nreason there's always a song in my mind\nall right let's get started but before I\nwant to\num say thanks to Union AI for sponsoring\nuh all the flight school sessions\nunionized the company behind Union Cloud\na platform that provides a managed\nversion of flight\nwhere you get\num some of the most\ndemanded features in terms of Enterprise\nReadiness you can call it that way like\ntask level observability or monitoring\nrole-based Access Control among others\nand you certainly don't have to deal\nwith kubernetes there but if you're here\nI'm glad you're here and and you want to\nknow how things work\nso right kudos to Union AI\nall right so why kubernetes well a\ncouple of reasons first there was a\nstudy\num polish a couple of years ago on the\nmetadata of failures from one of the\nbiggest and oldest ml pipelines inside\nGoogle it's been running for more than\n15 years with some failures and the\nstudy found that basically more than 60\npercent of failures came from were not\nml failures and most of them had a\nstrong distribution distributed systems\nnature so classic failures like\ndependencies\npure downtime latency\nEtc not really male failures but the\nclassical let's say distributed system\nthere so it's important probably it's\nit's the result of a of a simple uh or\nof a single system not a simple one a\nsingle system but it could be uh\nyeah it could be uh interesting to infer\nthese results for for the whole ml\ndesign of systems that are reliable\nand the the other thing is that well\nit's it's a matter of fact that ml\nsystems tend to grow like many other\nsystems there are different dimensions\nuh on which an ml system can grow if you\nfind other dimensions or or you have\nanything to contribute just chime in the\nchat remember that ml is not my\nbackground but these are my observations\nand also what I found on on academic\nresearch so first of all model count you\nstart probably with a small number of\nmodels and and you will be adding more\nand more\nand also complexity\num\nyour models will grow will become more\nsophisticated\nand that ends up adding complexity to\nthe system and finally if probably if\nyou are successful and depending on the\nuse case you'll get more and more\ntraffic volume\num trying to retrieve predictions or\nresults from the ml system\nso why this well because scalability\nmatters and it's been a\num poorly defined term uh through many\nyears in technology only recently I\nfound it's old but only recently I found\nproper academic document on on topic of\nscalability\nand it's basically a a system is\nscalable if it has ways to improve the\nsystem capacity to handle more workloads\nand if the administrators of the system\ncan apply these strategies or these\ncontrols uh multiple ways at a low cost\nright if you can tweak the system just\none time and the next time it won't\nhandle more load probably it's not very\nscalable\nso there are not systems that are not\nscalable or infinitely scalable it's\nit's range right and uh\nthe sooner a system uh runs out of\nstrategies to handle load uh probably\nmakes it less scalable\nso in the example here in the graph you\nsee that the system administrators were\nable to apply strategies or controls\ntwice to keep the response metric under\nan acceptable level by the third time it\nwas harder and the fourth time it was\njust not feasible the cost was too high\nand the respawn metric was rendered the\nsystem unusable so that will be the\nlimit of scalability in the system\nso\num\nwhat does it have to do with ML well\nhere I'll say that for for Valley\ngeneration considering that probably you\nwere aware of of this metric that\nmajority of the ml uh projects never get\nto production and there are many\nnon-technical reasons for this but\nprobably one aspect that could help\nimproving this metric will be building\nan ml platform that is highly reliable\nand scalable there are many trade-offs\nthat you need to\nmake to to achieve both\nbut\num my goal is here to kind of position\nkubernetes to be this kind of substrate\nwhere you can build a platform with\nthese features\nall right so what is kubernetes probably\nthis is a first introduction on why we\nneed it on ML and for data intensive\napps but what is it what is kubernetes\nwhere there are there are different ways\nto Define it probably if you if you\nGoogle you will find a multiple\ndefinitions it's an orchestrator it's an\nAPI whatever but for me uh Cornelius is\nlike a thermostat\nthis is an analogy that is even used in\ndocs in the kubernetes docs and I I\nhaven't found a better way to explain\nwhat is kubernetes so far so if this is\nthe only thing you remember from this\nsession it's great kubernetes is like a\nthermostat and probably all of us here\nknow how to use it so you set the\ndesired temperature of the room\nand you forget about it right you sit\nand forget there the the\num the actual thermostat will work 24 7\nto make the actual room temperature\nmatch the desired room temperature so if\nI don't know if the temperature goes\nabove these are Level it will kick off\nthe fans and if it's the opposite it\nwill I don't know turn off the fans or\nstart some healer Etc but besides what\nwhat what what the thermostat ends up\ndoing the thing is that for the user is\nvery simple it's a simple interface you\njust Define what you want to achieve in\nthis case I want the uh I don't know 72\nFahrenheit in in the room and that's it\nyou set and forget and this is exactly\nwhat kubernetes does\nkubernetes provides the user a simple\ninterface where you declare the a sire\nstate\nright and you have a controller that\nwill work 24 7 to actually make the\nactual state of the infrastructure match\nwhat's what's in the desired State this\nis the reconciler pattern it will\nreconcile the desired state with the\nactual state\nevery time there is a deviation there is\na change unintended change on the actual\nstate for example some\ndowntime some node is offline Etc it\nwill do whatever it takes to make the\nentire State match the actual state so\nin this pattern in my opinion it's where\nthe actual power of kubernetes lies in\nits unparalleled reliability\nthis is what you probably have heard\nabout it the declarative infrastructure\napproach where you declare the desired\nstate of your infrastructure and you\nforget about it you probably have used\nsome other options in the past like\ninfrastructure as code or terraform\nansible Etc in those options you declare\nthe desired state of your infrastructure\nright and you submit this to a\ncontroller but this is not a loop right\nso every time there is a deviation in\nthe actual State probably you will need\nto run some process some batch process\nwhatever to detect The Exchange and then\napply again the desired state so so the\nmain difference is that kubernetes is a\ncontrol Loop it is a big array of\ncontrollers that all all of them follow\nthis control Loop pattern so at all time\nis watching that the actual State\nmatches the research\nthat's true next\nso if we dive a little deep you'll see\nthat there are three planes and this is\nusually\num use to describe systems the\nmanagement plane the control Pane and\nthe data plane right so starting from\nthe from from bottom up the data plane\nis where the actual workloads run your\nflight workloads your whatever you're\ndoing with data it works there it read\nruns there so in this case it could be\nthese rectangles could be I don't know\nphysical servers or instances in the\ncloud Etc and if we you what you have\nrunning in there are your workloads we\nwill give it we will give them a proper\nname and come in slides and probably\nprobably for example you have some notes\nwith access to gpus for inference for\nsome other use cases but but this is the\nkind of the data plane this is where the\nactual workloads run and probably where\nyour data lives also right uh no\ndifference there the difference is in\nthe control plane where the majority of\nthe logic of kubernetes lives as I\nmentioned there are multiple controllers\nand every controller\num\nmanage a single type of resource there\nare different type of resources we will\nwe will give an overview about some of\nthe mass common types of resources but\nall of them have a dedicated controller\nnot for every instance but for every\ntype of resource type there is a\ncontroller who will do exactly what we\nsaw in the previous slide right to\nactually reconcile the desired state\nwith the actual estate\nand the users or the systems that will\ninteract with kubernetes will use the\nAPI server as the gateway\nto either Define the desired state or\njust run queries and know what the\ncurrent status and what's happening in\nthe cluster but the main point of the of\nthis interface or the or the connection\nwith the user or the cicd systems or any\nother platform that is that is using\nkubernetes is basically to declare this\nother state and that's it\nright\nagain if you have any questions just\ndrop in the chat I'm really happy to\naddress them at the end\nso the other point here important with\nkubernetes uh is yeah it's an\norchestrator yeah I I haven't used the\nterm uh only now it's an it's a\ncontainer orchestrator yes and as you\nmay know one of the main expectations on\nan orchestrator is that it schedules\nworkloads right not only in terms of\ntime or I don't know like a Cron job\nat a specific frequency running\nsomething but we expect more than this\non kubernetes we expect that the\nscheduler also is aware of where is the\nbest place to run a workload not just\nrun it every hour and that's it just I\nwill let you know if it fails now it\nwill also decide where is the best place\nto run this workload that's one of the\nmain uh differences in one of the\noutstanding features of the kubernetes\nscheduler is that it will detect every\nrequest to create a new workload to or\nto run our new workload and it will\ninform the API server about where where\nwhat is the best place to run it\nit takes into account multiple criteria\nbut one of the criteria is the actual\nresource you realization in the notes in\nthe actual\ninstances where your workloads are\nrunning\nin the um in the kubernetes jargon this\nis the worker now right where the actual\ncontainers are running right so the\nscheduler will watch for resource\nvisualization it will make decisions to\ntry to balance utilization uh throughout\nthe cluster right\nanother important feature here or or\nyeah something that it stands out in the\nkubernetes scheduler is how extensible\nit is you can you can tweak even from\nfrom the most basic stuff you can for\nexample Define hey I want this group of\nworkflows to to run only on the notes we\nhave gpus\nfor example right\nand if there is no GPU available don't\nschedule\nit's it's very easy to say this and you\ncan also\num\ntweak the logic and adjust kind of the\nrules on how the scheduler decides where\nto run workloads this is not needed in\n80 of the times you don't need to tweak\nthe logic of the scheduler you just need\nto Define this kind of rules hard rules\nof I don't know hey I will group these\nnotes with gpus and a specific wordload\n200 gpus will run there\nthat's more common the the point here is\nthat scheduling is is scalable uh in\nprobably right now you have a better\nidea of what what a scalable means here\nit has multiple control multiple\nstrategies you can apply to enable the\nscheduler to handle more and more\nrequests parallel requests at the same\ntime\nand also it has some mechanisms like\nAuto scaling Auto scaling can also be\nautomated uh you can Define rules and\nthe scheduler can act automatically to\neven expand the cluster or shrink down\nthe cluster if it's needed it's it's\nvery powerful you can also write your\nown schedule\nall right\nso\num in terms of resources now let's\ndescribe a bit about the most common\nresources in coronaries and now we'll\nstart with name spaces naming spaces are\nkind of the substrate where every other\nresource lives most of the resources\nlive on a name space on kubernetes and\nit's a way to isolate resources one\nthing and they work in tandem with a\nfeature that probably Linux folks here\nin the audience will remember the c\ngroups\nso namespaces allow you to isolate\nresources and manage resources for\nspecific themes or applications Etc\nbut and and also c groups will make sure\nthat uh Team a for example if you\nestablish a limit or a quota for\nresources uh c groups will make sure\nthat uh Team a cannot cannot exceed the\nlimits right they are kind of contained\nnot only in terms of resource ownership\nbut also in how they use the resources\nright so this is very important\nespecially as as you start scaling uh\ngrowing uh the system and if you in the\nml context you are either running\nworkflows or yeah developing models and\nrunning models for different uh teams\ndifferent applications naming spaces are\nkind of the refactor way to manage\nresources for different apps\nokay so even the the system chips with a\ndefault namespace\nso the first thing and the fundamental\nunit of execution in kubernetes is the\nplot\nI will explain what the container is\nprobably it's not the the scores of the\ntalk but but just briefly container is a\nway to package the application and\ndependencies in a in a lightweight\nfashion much more lightweight than many\nothers uh and typically on kubernetes\nyou schedule a single container per pod\nthe Pod is again a logical construct uh\nand is the is the fundamental is the\natom yeah well atom is not fundamental\nanymore but yes it's the fundamental\nunit of execution in kubernetes right\nuh probably out of the main things you\ncan say of a path\num I will I will say a couple of things\nfirst labels are everything\nlabels are everything not only for Bots\nbut for coronaries all the resources and\nkubernetes can use labels and we'll see\na little later on uh\nwhy labels are important but it's the\nsystem is designed to also organize the\nresources by using labels and even\ncontrol uh how to group those resources\nusing labels and then selectors right\nokay and the second thing that I want to\nsay about thoughts is that it's probably\nthe only resource in kubernetes that can\nthat actually consumes\ninfrastructure resources like CPU RAM\nstorage the policy the only resource\nthat does this most of the other\nresources unless you create a custom\nresource but most of the other\num\nlet's say native resources are logical\nconstructs that you you won't see them\nconsuming resources exactly only the\npods run workloads right so in the\nworker notes that I just describe a\ncouple of slides ago well you'll see\nrunning mainly is pots right in in\neverything ends up being executed by a\npath\nall right\num yeah we'll touch a little deep on\ndata storage because here we are trying\nto give this maintain this focus on data\nby default uh what happens here is that\nif we start from from left hand side\nuh the own container or the yeah this\nstorage the file system in a container\nby default is FML means that whenever\nthe container crashes or it's get it\nstops or gets deleted and whenever that\nhappens the data that was written by\nthat container will also disappear right\nso it's the life cycle of the data is\ntied to a life cycle of the container or\nthe pod in this case right that's the\ndefault behavior and that's fine for for\nmost of the workloads what we call the\nstateless\nworkloads that doesn't need does not\nneed to store anything\num but probably you have different needs\nin your application so there are\ndifferent options for data storage next\nnext up is the config map\nit's it's interesting because the config\nmap is not really storage\nit's a cornery's resource used only to\nmount configuration data to a path so\nfor example if a pod needs to access a\ndatabase\nuh how does the path where is the\ndatabase where the credentials or where\nthe secrets to use to connect Etc\ntypically that's in storing a config map\nthat the Pod mounts reads and the\napplication inside the container will\nwill use to for example connect to the\ndatabase right so it's not exactly\nstorage because you cannot right there\nfrom the path it's a it's a resource\nsitting in the kubernetes cluster that\nyou mount uh to a pod\nuh next up is the empty layer volume tab\nis a it's again a volume that as the\nname implies it's it's it starts empty\nuh and exists if the path is running so\nno matter if the even if the container\ncrashes\nuh the um the data and the anterior is\nis safe but if you or the system or for\nsome reason the Pod gets deleted uh also\ndata in the interior gets deleted right\num but there are some use cases that\nprobably make sense for for ML for\nexample for for check checkpoints for\nlong executions empty here is a good way\nto handle this because once the\nexecution is over probably uh the\nimportant data the the output of this\nexecution has been already transferred\nto the next step in the pipeline and\nprobably you're using a nice workflow\norchestrator for this but in any case\nthe output has been\n[Music]\nyeah communicate it to the next step in\nthe pipeline and uh so the container can\nbe the Pod can be destroyed and the data\nand the interior can also be destroyed\nso it's it has some use cases where it\ncould be interesting it's very simple to\nset up\nuh next up is a host pad is where you\nmount a folder or a disk not a clear\nfolder in your machine to the path it\nhas some security concerns because\nhostpad can be and has have been used in\nthe past uh to expose\ncredentials from kubernetes components\nand run containerscape attacks and other\nforms of privilege escalation so not\ngreat if you ever need to do this\nmounted as read only\nit's a recommendation\nbut if you're in a situation where you\nneed to support uh an ml system on local\nkubernetes environments not in the cloud\nfor example just just servers with local\nstorage it's possible you can use the\nlocal volume type\nit doesn't have the implications of the\nor the problems or the of the host pad\nthe only\nproblem is that obviously is tied to a\nnose if this node goes off well the the\nvolume will be unaccessible but you you\ncan share it on through different parts\ndifferent hosts so\nit's kind of a midterm solution\nbut the the most coordinate is native\nand widely used option is the persistent\nvolume claim right here in the India in\nthe right hand side it's a way to again\nclaim durable storage it's completely\ndeclarative I mean it's it's part of the\ndefinition or the\nintention of the application you define\nhey this app needs I don't know\n100 gigs of storage with these\ncharacteristics that's it this is mostly\nfor Block storage but uh the important\npiece is that it uses an API that hides\nthe implementation details and makes it\nreally simple for the other person to\nclaim storage and do it even dynamically\nwithout the intervention from anyone on\nthe infrastructure team\nso a bit more details in that the API is\ncalled CSI not not that CSI no that TV\nshow but the container storage interface\nis the API and it's a way for for\nstorage vendors providers especially\nblog storage\num to expose standard\num\na standard interface for pods to claim\nstorage so you will have a piece of\nblock storage a slice of it is a\npersistent volume that ends up being\nclaimed if it matches for example the\ncapacity uh the policy storage class\nname if if there is a match it will be\ncleaned by the Pod or the pots that that\nneeded and will be mounted right that's\nfor Block storage for object or blob\nstorage you will typically use the S3\nAPI you don't need to exactly Mount\nanything\nuh it's just API calls to either write\nor read\nokay\ncool all right so the main question here\nand a really question for you all is is\nthis enough\nand yeah probably not right probably\nthis is not enough\num\nyou can have different ways to\num Mount volumes to write data to read\ndata\num but probably this is not enough\nespecially if your ml system interacts\nwith other systems in the organization\nwhich is typically a case like a cicd\npipeline like all the clients or the\napplications that consume the actual\noutputs of the ml system so running\neverything on a path mounting a volume\nvolume probably is not enough you need\nmore than that\nso grenade is as you call it the next\nthing is the deployment deployment is a\nresource native resource in kubernetes\nand with the deployment you define you\ndeclare again the desired state of the\nwhole application so for example in this\ncase I will say\num\nacronym that is I need this application\nto have four replicas at any point in\ntime for replicas in this case\nremember the thermostat so I need for\nreplicas what happens if some some of\nthe pots crash immediately what\nkubernetes will do is to kick off the\ncreation of A New Path to replace the\nold one right\num because it again it will work\nrelentlessly to make the actual state of\ninfrastructure match your desired State\nagain if you if the changes are top down\nlet's say if you say no I don't need\nforward because I need six replicas\nimmediately when you apply this change\nagain the kubernetes API server will\ntrigger the creation there's a control\nfor that that the deployment controller\nwill trigger the creation of the\nadditional uh two parts in in this very\nsimple example right\nso\num\nand and the other thing is that well\nthat that makes the deployment\nself-healing in a sense right because\nit's protected against failures and in\npots and it has controllers for for\neverything for the storage connection\nfor connectivity Etc they are\ncontrollers everywhere in kubernetes\nthat will make sure that it will\nreconcile the actual state with the star\nState how does the API server the\ncontroller in this case knows what parts\nare part of this application if you have\nthousands of applications in the same\ncluster how how can we identify well we\nuse the the tags or the labels that I\ndescribed at the beginning and there's a\nselector\nthat in this case we'll use the label\nthat we applied at the beginning to say\nhey every path that matches this\nselector that has this label this key\nvalue pair\nI assume it's part of this application\nand I will I will handle\nuh the rest right so that's the\ndeployment resource it's fine but again\nI don't see a user here\nand you see parts\nconnecting to data so uh yeah again what\nhappens if something fails will again\nit will kick off the creation of any\npart right awesome great uh animation\nright so the next the next bit probably\nuh moving a bit up in the hierarchy of\nof resources in kubernetes is this\nservice\nand the service is a way to expose a\ndeployment a group of parts and\nresources there uh with a specific and a\nsingle endpoint where now a user or\nexternal system will use this endpoint\nto connect to the application\nit's like a load balancer if if probably\nif you were\nfamiliar with this term the load\nbalancer is this Central component and\nyou interact with this component but you\ndon't you are not sure what's behind it\nthere's a component in the middle that\nit's the the front end for for the users\nin this case it's not really that it\nload balances and it's very basic in\nterms of load balancing it just\ndistributes incoming requests\nand um for sure if if there is a pod\nthat is down it wants and request there\nand probably\nyeah even before they had the deployment\ncontroller will will create a new path\nand we'll add it to the service\nright so um this is the service there\nthere are other resources above the\nservice like for example Ingress if you\nthis is for a single app but what what\nhappens if you have multiple apps you\nwill need a service per app more and\nmore services and if you're running the\ncloud and you using the load balancer in\nthe cloud it implies more and more cost\nfor every load balancer you will need to\npay so probably it can become cost\nprohibitive so the Ingress is like a\nmultiplexer it's a way to define hey\ndepending on on these rules let's uh\nredirect request to this service or that\nservice or that other service right so I\nwon't touch on Ingress too much but I I\nsee that for most Vox is a bit confusing\nand again if if you want to see a\nsession for example just Ingress I'll be\nmore than happy to do this and just let\nus know\num\nall right\nEarnest\num\nit's it's okay for you to wait a little\nbit for the question\nor does it need to be now\njust let it know\nI just need to ask right now yes right\nnow like uh we just test the back flight\nyeah well\num yeah let's let's complete the lab\nit's not that it's not a Long Lap and\nthen we can go over all the questions\nyou don't mind\nso yeah there are many ways to run\nkubernetes there are many ways to deploy\na kubernetes cluster even locally uh you\ncan you can start with just using\nminicube it's a package it's maintained\nby the kubernetes community it's it's\nfine it's it's good uh probably it has\nmore\nfeatures enabled than you may need you\ncan use k3d or k3s from Rancher it's\npretty cool lightweight it has most of\nthe stuff you need\nuh you can start in the cloud or you can\njust create your own on the hard way I\nmean using Kelsey High Towers like the\num chronette is the hard way yeah I\ntried that in the past and it's it's\nit's incredibly long an incredibly long\nprocess uh that the conclusion after\nthat is yeah I I need something\ndifferent or a manager this this is\ncrazy uh but from how we will use uh the\num\nplayground\nsponsored by Docker it's right there I\nwill I will also put the link here in\nthe chat\num if you already have a kubernetes\ncluster that's also fine\nand probably you will find these two\nbasic but this is an interactive\nplayground that will give you I think\nit's four hours\nof access to a currently is cluster not\nonly featured but a coordinates cluster\nwhere you get to see some of the\nbehavior of resources so next phase is\nuh the new instance so remember that I\nmentioned that there are worker notes\nthat run your workloads uh\nbut you probably are wondering where\nwhere the control plane runs where they\nare Master notes in a in a in this case\nwe will use a single node to both for\nboth the control plane and and also as a\nworker node and if you use mini Cube or\nK3 or this kind of options that's the\ncase by default it will deploy a single\nnode and that single node will be at the\nsame time master and worker but in\nproduction that's that's not a good idea\nright in production you need to have at\nleast one master ideally a cluster of\nMaster servers\nwhere you cannot run workloads and then\na group of worker nodes where you run\nworkloads and not the control plane\nright\num again this is this session for data\nscientists I I didn't want to die to\nleave on the Chronos architecture but\nprobably this is all for introducing the\ncommands here uh the first option is in\nEnglish at the first step initialize the\ncluster Master now\nwrite a master node that will be where\nthe where the API server runs basically\nso you can just copy and paste and run\nthis\nthere's a whole process behind this and\nthere's it it could be pretty long but\nit's basically\nbootstrapping all the elements for a\nkubernetes cluster to actually be able\nto respond to queries create resources\nall the stuff that we just saw uh this\nis doing it and it will store metadata\non an Etsy cluster\nand you don't need to touch that\nI hope\nuh initialized cluster networking\notherwise\nit won't work and next piece is in this\ncase we will\nobey\nI I wanted to run a different type of\nworkload uh in in view are running on a\nproper kubernetes environment uh you can\ntry for example this one I will put it\nin this Zoom chat\nand uh\nyeah it's it's also here in the flight\nthe hard way repo\num it's a manifest basically to deploy\neverything we just saw\na storage class a persistent volume\ncleans persistent volumes and deploy a\ncouple of stateful applications and\npostgres database in a menu object\nstorage that by the way is used to\ndeploy flight but it's a good way to um\nsee how all the pieces fit together I\ndon't I don't deploy it here on on the\nplayground because\neverything will remain pending I mean it\ndoesn't have the ability to to consume\nlocal storage whatever so even for\nexample if you see here\nsorry oh yeah my number one uh\nproductivity\ntip in kubernetes environments is the\nuse of alias\nuh k equals Cube curl or Cub CTL Cube\ncontrol whatever and I'm so used to it\nbut just get bots\nah there you go it's spending right and\nyou and you if you describe sorry when\nyou describe in grenade you need to tell\nthe API what kind of resource in this\ncase it's a path\nand uh yeah I mean the these no there's\na control plane node and cannot run\nworkloads there's a way to change this\nbut it's beyond the scope of this talk\nand even if the Pod is running it makes\nnot not a great difference\num\nI just want to point to a couple of\nthings first the path\nit's scheduled on the default name space\nright everything or most of the\nresources especially Bots run and name\nis space\nright and uh it it will have a node\nwhy doesn't have a node because there is\na rule that prevents the scheduler it\nfailed is scheduling prevents the\nscheduler to actually run workloads in a\nin a control plane now which is a good\nidea for production so you can see the\nscheduler here intercepting a request\nhey you cannot run this\num in this node right you can see here\nthe use of labels\nright it has a couple of labels and I I\nbelieve this thing also creates a\nservice right\nand if you describe the service\num also you can see that there are some\nabbreviations for resources I can say\nfor example service or SVG\nwhich is uh interesting my nginx SBC\nuh there you go so it has a selector\nthat says everything that matches this\nlabel app equals nginx\nI will handle this right it's it's part\nof this service I will suppose this\nright and you can see here that\nit will be a match because this spot has\nthe label right\nuh the status is pending\nthere's a lot of content out there on\npod status but right now pending is\nbasically the scheduler is is preventing\nyou to run this right from running this\nand inside a path as I mentioned\ntypically it runs a single container in\nthis case uh you can you you have only a\nsingle container which one an nginx\nserver exposing the 80\n4 DCP protocol\nand uh yeah there are some other things\nthere that we don't need operations we\ndon't need that right so if you explore\nfor example storage class\nuh I don't think there's anything\nuh but you get to see these manifests\nyou can see that for example we use\nuh this portion to create a storage\nclass\nuh that uses local path provisioner from\nlauncher Etc and there are some\npersistent volume plans hey I need 20\ngigs\nuh read write\nand for postgres and I need for uh yeah\nI mount this in the deployment and I\nneed something similar for menu yeah\nyaml I didn't promise you that you\nwouldn't see yamu if you want you you\nwant to learn kubernetes\nit all starts with yamo I'm sorry\nI don't know bad news but it's the truth\nyou need to know yeah probably not not\nan expert but it's the uh the actual\ninterface used to both declare resources\nor it's also useful to know to\ntroubleshoot first troubleshooting\npurposes but probably data scientists\nand ml Engineers don't need to\ntroubleshoot kubernetes cluster you have\nother problems\nall right so yeah I think this is a it's\na introduction uh hopefully friendly one\nto kubernetes in the ml context\num so yeah any question any comment you\nmay have so far\nyes I have a question\num you\nyou at the beginning when you mentioned\nlike the namespaces and resources you\nmentioned that the there is some\nlimitation that we make\nso to separate uh like the up services\nEtc and like uh\nwhen you set like these limitations they\napply separately to each uh like\nnamespace or we have like one limit\nthat is General\num\nyeah\nyes so um limits and quotas typically\nthe the thing you use\num you can establish limits for pots for\nexample establish uh the default\nrequest in terms of CPU RAM storage and\nyou can also establish limits right and\nthis part will will always run a\nnamespace naming Space by default don't\nhave any limits nor resource quarters\nanything but you can create those\num\nlet me see if I can find it real quick\num but yeah you can create resource\nquotas and resource quotas are applied\nto name spaces right in this resource\nquota object you say hey this\nI I don't care what the pots will ask\nbut the only thing I will say is that uh\nresource this namespace only can reach\nto a maximum of n number of CPUs RAM and\neven gpus right so um let me point you\nto the docs I will share this right away\nand\num yeah this is the way to\num\nyeah Define limits that apply to in this\ncase to name spaces and you can see here\nthat there are limits in terms of CPU\nmemory\nand for all the resources in this case\nfor gpus you can also establish limits\nand they will be enforced by the by the\nc groups that you don't need to touch\nyou just need to Define this and it's\nit's it's pretty easy uh here well this\nis longer than that needed it\nestablished multiple resource quotas uh\nbut only this it's enough\nuh to establish a resource quota and\nthat's it it will be enforced\nI don't know if that answers your\nquestion yeah yeah thanks\nthanks\nright who else\nany other questions\nnope\nall right well with that uh if you have\nany other question after this session\nfeel free to ping me on on LinkedIn I'll\nsay um David Esperon LinkedIn uh you'll\nprobably find a the same picture that I\nuse for this session you will find it\nthere so it's easy to recognize and uh\nyeah it would be great to connect\num if you still use X or Twitter if\nsomeone still use that I'm there as\nDavid mirror\nand uh yeah it would be great to keep\num\nchatting and the uh there will be next\nsessions there will be follow-up\nsessions\nand um I hope we can see you again\nso if no other question yeah thank you\nall for joining\nand see you in the next one\nthank you"
    },
    {
        "title": "Flyte School: A Practical Introduction to Machine Learning Orchestration",
        "transcript": "all right\num\nthanks everyone for joining the flight\nschool session today my name is samhita\nI'm a developer Advocate at union.ai\nunion.ai is a company that offers\norchestration Solutions we help you get\nvalue out of your data and ml use cases\nand flight is one of the Open Source\nProducts we contribute to and maintain\ntoday I'm going to introduce you all to\nflight and we'll look at why you might\nwant to use flight and how you can get\nstarted with that let me share my screen\nall right\num before I kick this off I just want to\ngive a quick heads up we don't have a\nmoderator for this session because most\nof my colleagues are in the US if you\nhave any questions please uh do post\nthem in the chat and I'll address them\nat the end of the session and if I don't\nhave the time to uh answer all of your\nquestions uh please post them in slot\nand I just shared the relevant links in\nthe chat\nall right I'm sure you'll gain plenty of\nvaluable insights from this session\nplease feel free to interrupt me in case\nyou have any questions\nso first uh all right first I want to\nquickly talk about why you should\nconsider orchestrating your pipelines\nand what orchestration is all about\nall right ml models Thrive because of\ndata and models could deteriorate\nbecause models heavily rely on data and\ndata keeps on changing and so do our\nrequirements so data drift and concept\ndrift could contribute to model\ndegradation it's important that you keep\na track of your data models and the code\nas well your data and ml pipelines have\nto be manageable version reproducible\nand so on and you should be able to\nscale the infrastructure up and down\ndepending on your requirements and this\nhas to be done differently from software\nit just shouldn't be like a bunch of\nEngineers who built some pipelines they\nleave the company and none of the others\nare able to get it to work because if\ndependency conflicts or infrastructure\nrelated problems this was the exact\nproblem the creators of flight faced at\na company they worked at called left\nin music orchestration is the process\nthat ensures a large Ensemble of\nindividual instruments play their parts\non time and in Tempo in software\ndevelopment orchestration keeps software\nprocesses workflow systems and services\nrunning at the right time and in the\nright order orchestration can exist in\nseveral categories like software\ndevelopment machine learning\ninfrastructure data Etc\nhere's an example journey of a data\nscientist pain points diagnosis and\ntreatment let me just read through this\nyou're a data scientist you come across\na customer problem you pull analyze\nvisualize the data\ndevelop a model and run it everything\nseems to work perfectly all of a sudden\na node on Virtual Runner scheduled goes\ndown and the Run stops you add a retrace\nmechanism to retry the job a certain\nnumber of times to solve the issue the\njob runs smoothly now however you Ponder\nthe time that goes to ways to run the\nexact same job again and again so you\ncache the data sadly after a certain\nperiod the Run doesn't stop due to an\nunderlying issue and hence you are time\nout again an infrastructure error causes\nan Abrupt run failure and you lose all\nyour progress to prevent loss of data\nyou or you add a recovery mechanism so\nthe conclusion is you get exhausted to\nbring about all the changes to improve\nthe resiliency of your code and\ninfrastructure now these features\nimplemented by the data scientists are\njust the tip of the iceberg you also\nneed to take care of team collaboration\nresource allocation\nEtc\nuh so why orchestration matters\norchestration is helpful to handle\nrepetitive tasks to schedule workflows\nto enable team collaboration for data\nprovenance or lineage and to scale\nwithout worrying about the underlying\ninfrastructure there can be a lot more\nreasons which you can get which you\ncould get a hold of as you progress\nthrough orchestration\nnow orchestration is critical to ensure\nthat your workflows are streamlined they\nrun the way you want are reliable and\neasily manageable meet flight flight is\nthe open source workflow orchestrator\nthat unifies your data ML and analytics\nstats it is orchestration is especially\nuseful if you have like a bunch of\npipelines a handful or more number of\npeople working on them and if you want\nto save the compute costs and flight can\nbe helpful to implement orchestration\nfor your pipelines the overarching goal\nof flight is to enable you to develop\nyour data and ml pipelines with ease run\nthem with ease share them with ease and\nscale them with ease\nnow I hope you've gained an\nunderstanding of what orchestration is\nwhy it matters and what flight is all\nabout now I'm gonna provide you with an\nintroduction to flight a hands of a\nHands-On kind of experience so let me\ngo to The Notebook\nall right\nI hope you can see my screen\num\nthe goal of the session is to make you\naware of the benefits uh that\norchestration offers and of course the\nsession can also help you decide if\norchestration is something that you need\nto incorporate into your Pipelines\nthe outline of the session is as follows\nfirst I'm gonna talk about how you can\nset up your virtual development\nenvironment and then I'm gonna talk\nabout the basics of flight uh including\ntasks workflows launch plans which are\nthe building blocks of flight and then\nPi flight run the pi flight run command\nit is useful to run your pipelines\nlocally or on a flight cluster and then\nI'm going to talk about flight console\nit is the flight user interface and then\nflight remote where you can which you\ncan use to programmatically run your\ntasks and workflows or pipelines uh\nlocally or on the file cluster and then\nscheduling launch plans so I'll talk\nabout how you can schedule your workflow\nso that they run on a particular Cadence\nand then finally I'm going to talk about\nthe flight programming model to tell\nwhere I delve deeper into the flight\nConcepts\nso first prerequisites these are the\nprerequisites uh you know to install\nflight on your system you need to have\npython Docker and you need to install\nflight CTR so flight CTL is the command\nline interface that you can use to spin\nup your cluster tear down your cluster\nand also uh you know connect to your\ncluster and run all sorts of things on\nit\nand if you want to run the code that I'm\ngonna show very soon or you can clone\nthe GitHub repository I shared the link\nuh in the chat you can clone it and then\nyou can create a virtual environment\ninstall all the requirements and just\nrun this command to make sure that\neverything is working fine\nand then in order to start a local\nsandbox you just have to run the command\nflight CTL demo start and that's gonna\nspin up a flight cluster on your local\nmachine\nand yeah that's about it you can also\nrun these tests just to make sure that\neverything is working fine\ncool uh okay GitHub link let me just\nshare it again\nall right\nnow\nhere's the notebook\nI'm gonna go through the notebook block\nby block and we'll just understand uh\nwhat flight is all about and how you can\norchestrate your Pipelines\nso first environment set up so here I am\nimporting a couple of environment\nvariables uh which I'm going to use\nthroughout this Jupiter notebook so\nfirst I'm importing the flight CTL\nconfig environment variable uh this\npoints to be a demo cluster which I have\nalready spun up you can spin it up as\nwell on your local machine so here's the\nconfig file which I have it on my system\nI'm gonna show that to you uh so here's\nthe config it gets automatically\npopulated by flight for the demo cluster\nit basically has uh configuration\noptions like endpoint auth type insecure\nlogger and so on\nand then I'm also importing\nthe image environment variable so image\nhere means Docker image I'm gonna use\nthis Docker image to run all of my\npipelines but you can also use something\ncalled as image spec wherein you need\nnot write a Docker file but you can uh\nyou know ensure that the image is built\nthat's automatically handled by flight\nunder the hood so you can use something\ncalled as image spec for that which I'm\ngoing to show very soon\nand now let's uh now let me talk about\nthe basics of flight\nso first task flight task a flight task\nis the core building block which is uh\nfor which is useful for type safety uh\nState plusness and reproducibility so\nhere's like an image for that task\naccepts inputs and it produces outputs\nit is strongly typed and what that means\nis every input and output has to be\nannotated with a type and that's where\nthe data validates that's how the data\nvalidation happens in flight so every\ninput and output has to be annotated\nwith the type and a task is\ncontainerized meaning it runs in a\ncontainerized environment and it is\nversion it always has a version\nassociated with it\nwe are going to use flightgrid to write\nour tasks so flightcat is the python SDK\nfor flight\nand here's an example of a flight task\nI'm basically importing tasks from the\nflight kit Library so flight kit is\nflights python SDK and then I am\ndefining a python function and I'm just\ndecorating it with the task decorator\nand that's how it's uh\nconverted to a flight house that's it's\nthat it's that simple so you just have\nto write a python function and decorate\nit with the task decorator and I can run\nthis task locally just by calling it\njust like a function you know hello and\nuh the brackets parenthesis and now I'm\ngonna run this yeah it's just friends\nhello world\nthis is this is a flight task it's uh\nthat simple and here's an example of a\nflight task that accepts an input and\nproduces an output here if you look at\nuh the input and the output uh they are\nannotated with the type right so X is a\nfloat and hence we are I'm telling that\nX is a float I'm just specifying the\ntype and I'm also specifying the type of\nthe output as float and here I am send\nI'm calling that function I'm sending\nthe value let me run this as well and it\njust prints that value so here you need\nto notice two things one is flight is\nstrongly typed as I've mentioned before\nand the other stars arguments must be\nkeyword argument so you need to be\nsending keyword arguments to your tasks\nyou just can't say 2.0 you have to say\nthat X is 2.0\nso here's an exercise\num I'm gonna go through them as well try\ncalling square with a different data\ntype\nlet me do that now I'm gonna say x isn't\nit X is up string\nand let me run it\nso flight is going to throw an error\nbecause X is not a string it's an it's a\nfloat\nright and hence it throws an error\nlet me change this to float again and\nwhat's the second one modify the code in\nthe Square function to Output a\ndifferent data type let me do that as\nwell I'm gonna say Bool\nand yeah it's going to throw another\nsaying that 4.0 is a float but not a\nboom and that's how the data validation\nhappens in flight\noh sorry\nuh let me change this back to float\ncool\num now why containerized functions\nflight treats tasks as containerized\nfunctions this means that each task runs\nin its own isolated container and the\nflight cluster now when this is coupled\nwith strongly typed interfaces you can\nassume tasks to be like microservices\nthat can be composed together to form\nworkflows so here are the benefits of\nflight task one is statelessness each\ntask is stateless which means it can be\nrun multiple times without side effects\na task is reproducible which means it\ncan be run multiple times with the same\ninputs and you're going to get the same\noutput it is portable because it is\ncontainerized it is portable as well it\ncan be run on any infrastructure that\nsupports containerization\nand heterogeneity each task in a\nworkflow can be written in any language\nnot just python you can write your task\nin any language beat or Java or any\nlanguage and it can be run with varying\ncompute environments\nnow let's look at workflow so what's a\nworkflow a workflow can be thought of as\na collection of tasks wherein the tasks\nare dependent on each other so if you\nlook at this image over here there are\nfour tasks in this workflow and the\nsecond and the third tasks are dependent\non the first task and the final the\nfourth task is dependent on all the\nthree of them so that's how uh you know\na workflow is composed and the\ndependencies between the tasks are uh\nsorry I\nokay so the dependencies between the\ntasks uh it's handled by flight\nso a workflow's job is to capture the\ndependencies uh and it's also a DSL a\ndomain specific language it's not a\npython function but it's a DSL that\nbuilds an execution graph that uses\ntasks as the building blocks for more\ncomplex pipelines so unlike tasks a\nworkflow is a DSL and flight you just\nspecify the tasks that you want to\nexecute in a workflow so here's an\nexample of a workflow\nfirst I am defining three tasks and in\nthe end I'm defining the workflow\nwherein I'm calling the uh three tasks\nso first I'm calling the error task and\nI am sending the output of the error\ntask to the squares task and so on and\nfinally I'm returning the output of the\nfinal task and this is a float and the\ninputs of a workflow are list of floats\nand then I'm calling the workflow just\nlike a python function wherein I'm\nsending X and Y list of floats I'm going\nto run this now and it's going to print\nfive that's about it now let me look at\nthe exercise questions in the sum of\nsquares workflow function print out the\nsquared variable let me do that\nit's a promise uh so what's a promise A\npromise is something that can only be\nmaterialized in a task so the value\ncannot be deciphered in a workflow it\ncan only be deciphered in a task so\nSquare when sent to a task over here\nsquared doesn't have any meaning in the\nworkflow meaning it does have meaning\nbut you cannot understand or uh yeah you\ncannot understand the value right but if\nyou send it to a different task like sum\nunderscore task when I'm sending that\nvalue to that task what happens is it uh\ndeciphers the task deciphers the value\nand it's going to use that value\nso that's a promise and then try\ninvoking some of squares with different\ndata types so what's going to happen is\nit's going to throw an error because we\nare signing it we are annotating the\ninput or the output with a different\ntype\nand yeah now let me go to the next\nsection\nlet's take a look at the example intro\nscript\nhere it is\nso here's an example uh workflow which\ntrains up logistic regression model\nusing the Palmer Penguins data set\nso first I am defining all the tasks the\nfirst task is to fetch the Penguin's\ndata it returns a data frame\nthe second task is to split the data\ninto train and test subsets and the\nthree model task is to\ntrain the logistic regression model\nand the final task is to evaluate the\nmodel and return the accuracy score and\nfinally I am defining a workflow which\naccepts hyper parameters this is a data\nclass test size and random State and it\nbasically calls all the tasks that I\ndefined before so it's calling uh I'm\ncalling the uh get data task first and\nthen I'm calling the split data task\nwhich is dependent on the get data task\nand so on\nand in the end I'm also defining a\nlaunch plan\na launch plan can be thought of as a\nplan that you can use to launch your\nworkflow so over here I am specifying\nthe name of the workflow uh the name of\nthe workflow this is the actual workflow\nand then I'm also setting the schedule\nthis is the schedule uh which I'm you\nknow which I want the workflow to use so\nthis has to run every two minutes and\nthen I'm also setting the default inputs\nthat the workflow needs to use and\nfinally I am including this so because\nthis can be run locally just like any\npython script and that's about it I also\nwanted to quickly talk about image spec\nso image spec lets you build images\nwithout a Docker file you just have to\nimport image spec from the flight kit\nlibrary and then you can initialize it\nby sending name is an optional argument\nso you can send the name of the image\nthe registry where you want to store\nyour image the packages that you want to\ninstall in your image and so on so what\nhappens under the hood is flight Builds\nan image for you when you run the script\nflight Builds an image for you it pushes\nit to the registry and while executing\nthe pipeline uh it fetches the image and\nSpins up a container so all that is\nautomatically handled by flight and you\nneed not write any Docker files so this\nis for those uh who don't want to write\nDocker files\nand yeah let me get back to the notebook\nyeah let me just run this just like a\npython script you can just say Python\nand the name of the Python file and it's\ngoing to return the output\nnow how do you iterate on your workflows\nyou can use pi-flytron command so this\nis useful to run your workflows or tasks\nlocally or on the flight cluster here's\nan example you just have to say Pi\nflight run\nthe name of the Python file the name of\nthe task or the workflow and the input\nnow if you specify default inputs\nalready you need not send any inputs to\nthis uh particular command but if you if\nyou haven't specified the inputs you\nneed to send the inputs now let me read\non this as well so this basically runs\nthe code locally and prints the output\nand if you want to run this code on the\nflight cluster\nyou just have to add this particular\nflag to your command which is remote\nso here I am also sending config this is\nthe path to the config file this is not\nnecessary if you have already exported\nthe environment variable\nand then I'm also sending an image uh\nimage where I have all my dependencies\nlike scikit-learn Palm Palma penguins\nand so on\nthis is an optional parameter again and\nit isn't necessary if you are using\nimage spec\nand then this is this Remains the Same\nlet me run this\nand now let me go to the console\nhere it is so yeah if you look at this\nuh\nso this is still running this is going\nto take some time I'm gonna get back to\nthis so let me just go back to the\nnotebook\num\nall right\nso here's like an example uh\nuh the console just a tour of the\nconsole so you should be able to view\nthe uh okay let me just\ngo to that\nyeah\nso you should be able to view the\nworkflows all the workflows that you\nhave registered tasks and launch plans\nhere I have registered one workflow\nwhich has four tasks so you can view all\nthe four tasks and there's also a launch\nplan uh this launch plan is\nautomatically created it's the default\nlaunch plan that gets associated with\nthe workflow but if you want to create a\nseparate launch plan you can do that as\nwell uh\nyeah and let me go to the workflow\nthis is still running let me go to the\ngraph view so here's the graph wherein\nit shows the dependencies between the\ntasks so split data is dependent on get\ndata evaluate is dependent on all all of\nthem all the three and so on\num let me go to get data inputs is empty\nbut outputs is a parquet file this is uh\npandas data frame and uh it's converted\nto a parquet file I'll tell you why\nlater and then split data\nit's accepting the parquet file and then\nit's returning to pandas data frames\nagain and so on so you should be able to\nview the inputs and outputs uh of every\ntask in the workflow and you should\nalso be able to view the time uh the\ntime that the task is consuming so\nhere's like a breakdown of the uh steps\nright so here uh the tasks that setup\ntook about 50 seconds the task runtime\ntook about 13 seconds and so on should\nbe able to view all those uh values over\nhere and the timeline view so there's\ngraph view timeline view and notes View\nand yeah you can also relaunch this\nworkflow if you want to you can specify\nthe version\nthe launch plan that you want to use and\nall the inputs and you should also be\nable to rerun this task if you just want\nto run one task now the whole workflow\nyou should be able to do that as well\nyou just have to you know click plus\nbutton\nnow let me go back\nto The Notebook yeah\nall right flight remote\nflight remote enables you to\nprogrammatically run your tasks and\nworkflows this is useful for running\nyour tasks and workflows within a\nJupiter notebook as a micro service and\nit is also helpful to integrate flight\ninto your CI CD Pipelines\nso here's the code for flight remote and\nfirst initializing uh the remote\nthe definition is in this file uh so I'm\nbasically sending the config the config\nis the one that I've shown earlier the\ndefault project that needs to be used\nand the default domain so here project\nis uh in Flight it's a group of tasks\nand workflows and domain provides an\nabstraction to isolate resources and\nfeature configuration so an example for\nproject can be uh machine learning and\nthere can be a different project called\nData engineering and so on and\ndevelopment can be developed sorry\ndomain can be development or staging or\nproduction\nall right\num\nlet me go back to the notebook so here\nfirst I am initializing the remote\nobject I'm creating it and then I am\nexecuting the workflow by sending the\nworkflow and also the inputs\nand uh then I am generating the console\nURL this is the console URL and then let\nme wait for the execution to complete so\nthis uh method what it does is it\nwait for the execution to complete and\nthen it's also I have also said sync\nnotes as true right so what it means is\nit syncs the notes meaning I will be\nable to retrieve the inputs and outputs\nof every node or you can assume node to\nbe a task in the workflow so it will be\nretrieving all the inputs and outputs of\nevery task in that workflow\nthis is going to take some time and uh\nyeah here here's here uh here's an\nexample output so I'm printing the\noutputs and here are three outputs one\nis a float value the other is uh uh\nmodel and the other one is a float value\nagain so I am loading the model here I\nam um oh this is going to take some time\nokay uh let me so I'm basically loading\nthe model and then I am also loading the\ndata and predicting uh\ngenerating predictions using that the\nmodel that I trained and the data that I\nhave fetched over here\nokay cool let me run this let me run\nthis first and this\nyeah so this what what it's doing is I\nit is fetching the model\nfrom the flight cluster right from the\nblob storage wherever it is stored it's\nfetching the model and then I am\ngenerating predictions using that model\nand the features that I've loaded over\nhere\nand yeah uh next scheduling launch plans\nuh as I've mentioned before a launch\nplan is the one that you can use to\nlaunch your workflow wherein you can\nspecify the schedule default inputs\nnotifications and so on\nso for that to understand that better\nhere I have yeah I've included an\nexample so let me just go through that\nuh first I am\nsetting the version environment variable\nand then which I'm going to use and then\nI am registering the workflow so here\nI'm for that I'm using the command Pi\nflight register uh and I'm also sending\nthe image the package not just the\npython file but I'm sending the package\nwherein I can have multiple workflows\nand tasks and then I'm also specifying\nthe version the difference between pi\nflight run and Pi flight register is\nthat Pi flight run is quite simple to\nget started you can use it to uh run say\na single workflow you can also use it to\nuh run multiple workflows but yeah it is\nspecifically helpful to run a single\nworkflow or a task now if you want to\nregister if you just want to register\nand you want to trigger the workflows or\ntasks later you can use by flight\nregister\nand yeah here I am registering the\nworkflow first and then I am\nactivating the launch plan so since I\nhave registered let me go to the console\num\nright\nokay\nso here's the launch plan\nlet me\nokay\nso I am activating this launch plan this\nparticular launch plan activating\nmeaning it will now run the workflow on\na specific Cadence so let me run this\ncommand\noh sorry\nuh for some reason this isn't working\nlet me just\num\nsorry\nokay\noh\num yeah\nso this activates the launch plan as\nI've mentioned before wherein I am\nsending the project the name of the\nproject domain uh the activate flag and\nthe name of the workflow and then I am\nusing the flight and then I'm using\nflight remote to\num yeah you can also use flight remote\nto activate the launch plan so here's an\nexample uh first I am initializing the\nremote object and then I am fetching the\nlaunch plan and then I'm uh setting the\nstatus of the launch plan to act up so\nthis you can either activate the launch\nplan using pi flight or flight remote\nyou can also activate the launch plan\nusing flight serial which is the command\nline interface and then uh using flight\nremote I'm fetching the recent execution\nthe most recent execution\nlet me run this so yeah it got triggered\nand it uh\nyeah it it fetched the most recent\nexecution\nand now let me deactivate the schedule\nwith pi flight I just have to send the\ndeactivate flag to the pi Flight launch\nplan command\nand then I can also deactivate the\nlaunch plan using flight remote not just\nby flight but I can also use flight\nremote\nlet me run this so it basically\ndeactivated the schedule meaning the\nschedule isn't active anymore and it's\nnot gonna run it on a specific Cadence\nwhichever I have set\nand that's about it now let me\nyeah uh talk about the flight\nprogramming model\nthis is like the it this image provides\nan overview of what's happening when you\nwrite your workflow and when you trigger\nit so first you write your task and\nworkflow and then you package and\nregister it what happens is it generates\na compiled workflow and then uh there's\nalso a container associated with it\nwherein you have all your dependencies\nand configuration and when you run it\nwhat happens is it schedules it on a\nkubernetes cluster wherein every task\nruns in a kubernetes spot\nand they are yeah it just runs and then\nuh after the output is generated it's\nshown back it's shown to the user\nnah uh to give you a sense of what's\nhappening when we do PI flight run let's\nrun the get data task from the uh\nexample intro file if you remember we\nget data tasks here it is\nokay so let me run it\nand this is going to generate a console\nURL and\ngo to the logs link okay now let me go\nto the console\nokay\nproject dashboard yeah\nhere you should be able to look at the\nkubernetes logs as well here it is empty\nbecause uh I'm not you know printing\nanything but yeah\nyeah so\nyou should be able to view kubernetes\nlocks let me just go to the workflow\nokay that's all empty all right yeah uh\nand uh you should also be able to view\nthe task details so here's like the task\ndetails wherein it shows the command\nthat is used to run this uh workflow on\nflight if you look at this command it's\ncalled Pi Fly Fast execute because we\nhave fast registered fast registered\nmeaning uh\nthe code is zipped up and it's stored in\na blob storage or object stored like S3\nand yeah that's fast registration\nwherein the code is stored in a blob\nstorage and the image doesn't have your\ncode so whenever you wanna say iterate\non your workflows fast registration is\npretty useful because even after you if\nyou update your code you need not\nrebuild your image but in the case of\nregistration unlike fast registration\nyou need to rebuild your image and that\nis advantageous in your production\nsetups because then you can ensure that\nyour code is reproducible because your\ncode is present in your Docker image\nwhen you have all your dependencies\nso here we are fast register string by\ndefault by flight run or Pi flight\nregister or fast registers and yeah this\nis the path where the code is stored and\nyeah it also shows the image that I am\nusing the project\nversion and\ninputs and outputs and so on\nlet me go back to the notebook\nto your spouse life fast execute so it\ndownloads a compress code distribution\nspecified by additional distribution and\nthen calls the underlying task execute\ncommand for the updated code\nnow let me talk about the type system so\nthe flight type system is responsible\nfor a lot of flights production grade\nqualities it's helpful for runtime type\nchecking serialization and\ndeserialization of inputs and outputs\nbetween tasks type checking of workflows\nat compile time this type system is\nlanguage agnostic and it is implemented\nin the flight ideal protobuf format this\nmeans that flight sdks can be returned\nin any language currently flight kit is\nthe python implementation but there's\nalso Java Scala and JavaScript sdks\nand yeah you can also write your\nworkflow in r or Julia or whatever\nlanguage your prefer using container\ntasks\nhow data Flows In Flight so if tasks run\nin their own containers inside a flight\ncluster how is data passed between them\nso since a task runs on its own part\nright it's trans It's associated with a\nkubernetes spot it runs an in a pod so\nhow does data flow between the tasks now\nflight kit needs to convert all python\ntypes into something called as a flight\nideal protoba which I've specified\nbefore so here's like an image\num\nso\ntasks are associated with or run in\ndifferent pods as I've mentioned before\nand flight enables you to run your\nworkflows in a distributed setup meaning\nyour tasks can be present in different\nsystems and yet they need to communicate\nseamlessly right right so how does that\nhappen if you look at the social over\nhere metadata is stored is used to store\nuh the inputs and outputs like ends\nfloats lists and so on and draw data\nstores where files or data frames are\noffloaded to and retrieved by flight\nhere's an example\nworkflow just to understand how this\nworks so first I am generally I'm\nexecuting the workflow and then I am\nwaiting for the execution to complete\nretrieving the first node output which\nis the get data output not the output\nexactly but the URI where the output is\nstored so this is where the output is\nstored and then I am retrieving the data\nthat's that's present in the file and\nthen I am retrieving the input of the\nnext node which is dependent on the get\ndata node and that's the split data node\nand then I'm retrieving the input\nuh this is the input URI and if I try to\ncheck you know validate if the output\nURI of the get data task and the input\nURI of the split data tasks are the same\nit's the same which means uh the data is\nbeing passed right so the same URI is\nbeing used by the two tasks by the\noutput and the input and that's how the\ndata is passed\nand yeah next is life cycle of a\nworkflow\num when you run a workflow locally\nflight kit just runs the tasks in a\npython runtime however when you run the\nworkflow on a flight cluster a lot of\nthings are happening under the hood so\nflight propeller is the core engine in\nthe flight stack that orchestrates the\nexecution of tasks in a particular\nsequence it manages the data\ndependencies between the tasks the\ncompute infrastructure needed to run the\ntask and so on so this is a very deep\ntopic that we don't have time to cover\nbut here's like a high level overview of\nwhat happens if you have say a spark\ntask and also array task and if you also\nwant to run pytorch it's all possible\nwithin flight you can have all the three\nof them in a single workflow as well and\nwhat happens is when you write a spark\ntask flight will automatically spin up\nan ephemeral cluster and tears it down\nafter the execution is complete so if\nyou specify the spark config and if you\nexecute it on the flight cluster\nflight handles the setup and the\nteardown the same applies uh the same\napplies to Ray and a bunch of other\nplugins like Cube flow operators and\npytorch elastic and so on\nand yeah so all the setup and tear down\nis handled by flight you just have to\nyou just can focus on your code\nand here's a development lifecycle\noverview\nso typically working with flight as a\ndata scientist looks like a looks like\nthis first you create your tasks and\nworkflows locally with pi flight run and\nthen you build container for your tasks\nand workflows you can avoid this if you\ninitialize image spec in your code and\nthen you iterate on your flight cluster\nwith the pi flight from remote command\nand then you deploy to production using\nthe pi flight register non-fast command\nso this is to register but not to fast\nregister and this is helpful to ensure\nyour the reproducibility of your code\nand yes uh let's look at by flight\nregister now so Pi flight register zips\nup all of your source code and uh yeah\nhere's an example of the pipeline\nregister command wherein I am specifying\nthe image project domain version and the\npackage the name of the package now\nversion is an optional argument you have\nto specify it if you are specifying the\nnon-fast flag if that isn't the case you\nneed not specify version because it is\nautomatically computed by flight\nand this registers the workflows and\nyeah you can also use Pi flight run\nto register your workflow wherein I'm\nspecifying all the relevant parameters\nif you look at the pi flight run command\nyou should be able to look at the\nadditional distribution flag which I've\nshown earlier wherein the code is stored\nand if you specify non-fast it's going\nto be different\nright so let me run this code now\nokay so it registered the entities and\nthe version is prod v0 let me go to the\nconsole\num\nokay\nlet me register it again\nlet me stop image\nV1\num after that it seems to be another but\nI'll try to debug it later all right\nuh and yeah that's about it you should\nbe able to specify the version and that\nversion actually has to be shown on your\nflight cluster and when you uh you\nshould be able to trigger that\nand yes uh you have made it through the\nend of the session in our nutshell\nhere's what you learned why\norchestration is something you need to\nthink about when you have like a bunch\nof data or ml or any other pipelines\nthat you want to manage uh and what\norchestration is all about uh the basic\nconcepts of flight how you can go about\norchestrating your pipelines and lastly\nthe flight programming model I also\nwanted to give you a quick glimpse of\nUnion Cloud Union cloud is a managed\nversion of flight that offers Advanced\nobservability streamlined workflow\nexecution and robust our back support if\nyou wish to avoid the hassle of\nestablishing and maintaining the flight\ninfrastructure Union Cloud as a viable\noption\nand yes thank you so much for sticking\naround until the end of the session uh\nwe'll be hosting many more sessions as\npart of Flight School delving into\ninfrastructure aspects features\nIntegrations production grade\ndeployments and so on so stay tuned\nnow let me take a look at the questions\nright\nyou can also unmute yourself and ask\nquestions if you have any\nis it possible to handle different\ndependencies for each task as it is\npossible you can specify\nan image for each task so here is here's\nan example I'm going to show that to you\nnow so when I initialize the image spec\nright here's a custom image so I'm gonna\nuse that image for the star so I'm gonna\njust say container image Custom Image\nand what happens when you execute s this\ntask is going to use the Custom Image\nyou've specified so yes it is possible\nto specify different dependencies for\nevery task of yours\ndo we need to set any AWS credentials to\nwrite data from local to S3 bucket\nyes you need to uh you need to set\ncredentials you can specify you you'll\nhave to specify environment variables in\nyour configuration uh in your propeller\nconfiguration I believe and yeah that\nshould do it that should work\nwhat is the difference between a launch\nplan and flight remote launch plan and\nflight promotes a flight remote as the\npython API that you can use to trigger\nyour workflows you can do much more than\nlaunch plans and flight remote you can\nregister a task for a workflow or a\nlaunch plan you can also execute them a\nlaunch plan is the one that you can use\nto trigger your workflow it's just to\nlaunch your workflow right and you can\nalso specify some argument some\nparameters like uh as I've mentioned\nbefore the schedule uh notifications and\nso on so they both are like completely\ndifferent\nhow many launch plans can we have in\nscheduler can we have multiple launch\nplans at the same time or different\nscheduler times\num you can have multiple launch plans\nbut\nI think you can only have one that is\nactive\nyou can only have one active launch plan\nfor a workflow uh that's what I know\nyeah\ndo we need to have a Docker image if\ncode is stored on S3\ndo we need to have a Docker image\nif code is stored on S3 I haven't\nunderstood your question Chandra shaker\nif you could uh uh yeah so the question\nI heard was like uh you showed that like\num\nuh the with the fast register or\nsomething the code remains on uh some\nblob storage\nand that code gets pulled from that blob\nstorage right yeah so uh\nin that case\ndo we uh in the code itself for the\nthere was some Docker you know some\nimage related thing was also mentioned\nthat's why I had asked that question\nlike if the code\nis residing uh in S3 bucket itself like\ndo we need to bundle that entire code in\nthe docker image\nif you\nso if you specify image\nimage is uh it's gonna build the image\nand it's also going to store the code in\nthat image\nuh but if you want to fast register\nwithout image spec you can specify the\nimage and you need not have your code in\nyour Docker image that's not a necessity\nit's actually not required\njust can use your image without the code\nokay okay thank you thank you\nand next uh can we give some\nuser-friendly name to workflow version\nyeah it should be possible to set your\nown version so if you if you look at\nthis command or the one I've shown\nearlier yeah you should be able to set\nthe version\nthank you for the good explanation okay\ndon't we need to activate the launch\nplan after register yes you need to\nactivate the launch plan and for that\nyou need to specify the activate flag\nwhat is it\nright yeah you need to specify the\nactivate flag\nin your Pi Flight launch plan command\nand that's when it gets activated\nyeah in this notebook yeah sorry but I\nwrite those comments because I think you\nare not getting the latest version in\nyour console when you are running the\nupdating the versions\nis it uh yeah it actually has to I'm not\nsure why that's happening so when\nlet me just so when you do when you do\nthis thing to five flight register and\nyou went to the console and then you\nwere not able to see the latest version\nB1 right so it does it not required to\nyeah but does it not require to activate\nthe launch plan after once you register\nit then the version will coming in your\nconsole\nbut I am not I'm also registering my\nworkflows right so the workflows\nversions uh it has to get reflected on\nthe UI okay okay yeah yeah yeah let me\ndestroy it from the CLI give me a second\nI'll show what's Happening\nso\napplied\nI had some workflows let me go to work\n[Music]\nI fly treasure\num example intro version video\nI\nokay this is gonna pull damage I'm gonna\nlet you know\num will this notebook be available\nyeah uh it's already available on GitHub\nuh\nI I've also shared the link in the chat\nlet me know if you can access that\nyou should be able to use DCP and Azure\nas well\nuh it's not just AWS\nwe also uh have I I don't think we have\nlike dedicated deployment guides in our\ndocumentation but yes you should be able\nto deploy because we have lots of uh\nusers who have already deployed flight\non gcp and azure\nyeah we are planning to upload this\nsession to YouTube we're planning to\nmake that make it available on YouTube\nwe'll share the recording with you after\nit is available\nand yeah any more questions\ndo we have anything uh\nsome more details about your cloud\ncloud infrastructure this Union cloud\nwhere we can look into it like how much\nuh what are the pain points or what are\nthe how can we register ourselves and\nuse the to use this uh facility or\nyeah there's this website union.ai\nwherein you where you should be able to\nfind uh all the relevant details so if\nyou wanna take a look at the\ninfrastructure side of things you should\nbe able to look at this page wherein we\ntalk about all\nuh the relevant features that are\navailable and how you can get started\nyeah yeah\num yeah\nthanks\nyeah not a problem I'm trying to\nregister if there are any more questions\nplease feel free to unmute yourself or\nyou can also ask them in the chat\nI'm gonna stay for five more minutes\nso I just uh together yeah I just\nregistered uh the workflow from the CLI\nand it's working not sure why it's not\nworking for the Jupiter notebook I'll\nhave to debug but yeah I just registered\nyeah I just ran the by flight register\nuh the name of the Python file and I\nspecified the version\nuh hi uh is this session going to be\navailable on YouTube or like how when\nhow can we get notification or something\nonce you're making yeah it will be\navailable on YouTube will probably\nokay okay sure then I can share this uh\nsession with my colleagues as well okay\nyeah\nall right then since there are no more\nquestions uh thank you so much for\nattending the session uh so stay tuned\nfor the upcoming ones as well thank you"
    },
    {
        "title": "Union Mood 4",
        "transcript": "it doesn't take billions and dollars or\nAdvanced infrastructure\nit doesn't take a team or an army of\nthousands it takes just one stray\nthought how can we\nwe cannot ignore\nwhat if\nwe must because the spark of inspiration\nand an Unstoppable Drive\ncan change the world\n[Music]"
    },
    {
        "title": "Flyte Testimonials",
        "transcript": "[Music]\nforeign\n[Music]\nfirst off what we like about flight I\nthink the biggest thing is the community\nthank you so much I've worked with light\nfor it's been almost two years and we've\ngotten a lot back we've gotten so much\nsupport\nwe got a lot of reusable workflows and\nit makes it fairly easy to\nshare a complex machine learning and\ndifferent dependencies between teams\nwithout actually having to put all the\ndependencies into one one container\n[Music]\nthere's a lot of interesting support for\nlarge dags natively letting the\nscheduler reach in and understand and\nhave a large dag it can be really nice\nbecause it can start to do smart things\nabout handling errors and passing data\naround better so that was one of the\nsort of more unique things that it\nwasn't just flight did better it kind of\njust did differently than a lot of the\nschedulers we were used to\nso some of our core science tools that\nyou use repeatedly imputation is one of\nthem for folks who do\nbioinformatics work you can just say\nlike hey give me imputation and it'll go\nlaunch 40 spot instances which are\ncheaper than your on-demand instance\nthat you're using for your notebook\num and then it will return the results\nback in memory which is really neat\ndepending on what the results are you\npeople don't even have to understand\nwhat was going on on the flight cluster\nand how the data got back\num they'll just get back the outputs\nobject and they can do\noutputs.getwhatever\n[Music]\nI think ultimately what convinced me is\nthat uh you know I think fly has a very\nclean abstractions it had very thorough\ndocumentation so it was very easy to\nkind of you know just pick it up and\nthen finally I think the the community\nwas very reachable\num was very responsive\nanother advantage of light that I really\nlike that it has like a really really a\nclean UI I think that's the you know\nthat I think that's a light bulb moment\n[Music]\nit's like flight actually\num is used in every single one of these\ndifferent components so it's like across\nour entire data engineering and data\nscience Stacks\num and so it's not an understatement to\nsay that flight is really a Workhorse at\nFreedom\nit has an extensible plug-in system this\nis like this was really important for us\num you know we're running all of our\nmost of our workloads on kubernetes\nright now but flight is really execution\nengine agnostic and so like if you\nwanted to run spark workloads if you\nwanted to run pytorch workloads uh or\nwhatever else like you know AWS batch or\nGoogle AI platform or something like\nthat like we can basically write a\nplug-in uh that would work and you know\nsome most of these plugins already exist\nI just like having to write a python\nfunction to run like workloads on flight\nis really really nice for us and the\nbeauty of this is that you know you can\nrun it locally so it's just like running\na python function and then just ship\nthat remotely and run that on at scale\non the cluster and that's really nice as\nwell\nuh most importantly one of the biggest\nreasons we picked flight was it is\nideologically kind of aligned with what\nwe think all mlop systems should have\nand that's strong lineage guarantees\n[Music]\nour contribution velocity and narrator\nwhich we're retributing is a reflection\nof like I really are confidence in play\nlong term as the defect overflow\norchestration engine I'm not just saying\nthis I really think flight has got the\nthe model correct absolute correct with\nrespect to how you're thinking about\narchitecting and deployment posts as\nboth code first and leveraging the\ncommunity scheduler to abstract the way\nthe scheduling on a per task brand new I\nreally think that's the absolute Right\nWay think about it and we want to help\nsupport and grow this project and make\nsure it sticks around for a really long\ntime\n[Music]\nso if you imagine that you have\nsatellite imagery for the whole world\nthat's about two and a half petabytes of\ndata and processing that takes quite\nsome some computation power\nthe reason we're we're mainly using it\nis it's Cloud native capabilities we do\neverything in the cloud we also don't\nwant to be limited to a single cloud\nprovider so having the ability to run\neverything through kubernetes is is\namazing for us\nand uh yeah it's also pretty robust we\nhaven't really been able to to break it\ntoo much though we will be trying a bit\nharder\nforeign\nmodels and basically we're moving away\nfrom airflow to flight for everything\nrelated to machine learning\nthere are a lot of things that we don't\nlike about Basics even but it's it's\nlike also like you know the different\ndomains that you have in Flight already\nin the first place which is which is\ngood so that's the first thing and then\nyeah all the versioning all the caching\nuh also that that you have uh you know\nyou don't really need to think about it\nusually you have it yourself so those\nare like key decisions in the first\nplace foreign\n[Music]\nI personally really love Argo workflows\nthat to me is like the the king of\nworkflow executors but I think flight\ntakes the best of that and makes it\nusable from someone who prefers to write\nworkflows in Python in a really really\nneat way\n[Music]\nit's also fast and scalable so during\nour evaluation stage we did some stress\ntests to understand whether our flight\ncan satisfy our\nrequirements and it was providing us\nwith a good result\n[Music]\nwhy did we pick flight this is actually\nwritten by our data science teams um but\nwe think it's really cool because as\nEngineers a lot of this might be kind of\ntable sticks for us but for the data\nscientists being able to get up and\nrunning on flight and getting all of\nthis stuff for free has been a really\nbig win for them and they've been really\nexcited about it the ability to share\nmodels with each other and compose\nthings easily out of the box parallelism\nagain may not seem like the biggest deal\nbut for them when you're writing Python\nscripts and everything runs and takes a\ncertain amount of time whereas now for\nfree we get parallelism across tasks I\nthink I think that's really cool the\ncaching just connectivity with GCS all\nof this stuff\num they've been really excited about it\n[Music]"
    },
    {
        "title": "UnionML v0.2 Harmony Release",
        "transcript": "I'm going to head over to Nielsen Niels\nhas a fantastic update Union ml version\n0.2 Harmony release the stages years\nthank you Martin uh hi everyone I'm\nNiels bentelin the chief ml engineer at\nUnion Ai and\num yeah I wanted to go through this very\nquick update of uh\nUnion ml zero to zero Harmony\nso I want to highlight three things the\nfirst major update in Union ml is the\nBento ml integration I actually demoed\nthis last week before the release\nso please check that video out if you\nwant to get like a deeper sense of how\nit all works but\nreally a high level\num\non the slide here you can see what it\ntakes to\nuse the Bento ml integration you\nbasically have your union ml app\ndata set and model\nand how use Bento ml is you define this\nmental mail service that takes in this\nmodel argument\nand then from there you can save the\nmodel to a Bento model model store and\nthis the second code block\nand then finally you define a service\nfile that basically automatically\ncreates a Bento ml service for you using\nall of the Union ml functional\ncomponents and then you can serve it\nwith Bento mail serve\nthis actually opens up a lot of kind of\nprediction endpoint type services for\nUnion ml folks including serverless web\nendpoints and even eventually streaming\nwhich which isn't yet supported in Union\nml\num the next major feature that we\nintroduced is native scheduling so you\ncan\nbasically Define a Time aware data\nreader the reader is how you get data\ninto your union ml app\nso you can schedule a training job say\num every 12 a.m on 12 a.m every day with\na specific set of hyper hyper parameters\nso if you're doing a frequent or less\nfrequent model retraining this is a nice\nlittle tool\nand for scheduled prediction jobs you\nalso then have to kind of refactor your\ndata import to your data reader\num to be label aware in the sense that\nyou know since Union ml super flexible\nyou can get your data from anywhere in\nthis case we're using an S3 path\num and so you basically have to factor\nyour reader function to take in a\nlabeled or whatever kind of argument you\nwant to represent there but in this case\nyou know we have this labeled Boolean\nargument that if set to false will just\nget features for you\num and so this is how you basically\nschedule a prediction job that takes in\na model object\nthere are multiple ways of doing this\nbut you can give it a model object that\nit then uses to generate batch\npredictions all of this is Deployable to\nflight so unin ml uses flight as the\nunderlying orchestrator\nand so here we're just Library\nleveraging a lot of flights built-in\nfeatures then finally\num prediction callbacks was something\nthat that was implemented implemented by\nZev eisert\num so thanks to him again for for\nimplementing this basically we're going\nto implement these for other components\nbut for now only a prediction\num the predictor component supports\ncallbacks basically the use case here is\nfor logging right but it's fairly\num\nuh agnostic to the use case basically\nthe prediction callback is called after\nthe predictor function is is invoked and\nit automatically is past the model\nobject features and predictions and so\nthis will eventually hook into you know\ntheir revela API but you could use it\nfor whatever other logging third-party\nlogging package you want\num and to wrap up\nthe roadmap for zero three zero Fusion\nwe're eventually going to run out with\nuh kind of thesaurus like synonyms of\nUnion here uh so Fusion is going to be\nthe release that will include hyper\nparameter tuning uh which is a big\nmissing piece currently model\ncheckpoints and fine tuning\nuh experiment tracking with ML flow and\nmodel monitoring with revella so we're\nslowly but surely adding more and more\ncomponents and more and more\nIntegrations that makes Union ml this\nend to end sort of uh developer first\nAPI for building ml apps if you want to\nlearn more there's a blog post on Union\nAI about the Bento ml integration uh you\ncan join the slack\nthrough this short link\ngo.union.ai uniml Slack\nand then the similarly the roadmap is in\nUnion ml roadmap and the OSS planning\nmeeting which is actually happening to\nhappening tomorrow\nis uh\nI think 12 p.m EST and you can go to\nthis link to subscribe to that event\nand with that uh thank you"
    },
    {
        "title": "UnionML + BentoML Integration Demo",
        "transcript": "with that uh let's see if Neil's always\nlike uh the Twitter handle Cosmic b-boy\nso Twitter that's at some point you got\nto tell us the story behind that Niels\nbut uh let's focus on the union ML and\nbetter ml the integration yields the\nstage is yours if you have to present uh\ngo ahead I don't know if I have a slide\nsale for you but yeah I do sure thanks\nMartin I have a few slides um and then\nI'll hop over to the demo\num so\nyeah hi everyone I'm Niels bentelin uh\nChief animal engineer at Union Ai and\num yeah really excited to show you this\nuh new integration that'll be coming out\nin the 0.2.0 release of Union ml\num which should be coming out later\ntoday\nso just to give you all a little\nrefresher on what union ml is all about\nit's a framework for ML Ops and it aims\nto reduce\nthe boilerplate and solve various other\nproblems that we've come across kind of\ncollectively as a team\num so\nreally what union ml is about is framing\nmachine learning applications machine\nlearning systems as applications and not\nnecessarily pipelines so as you can see\nhere you have a data set and a model\nand these objects expose decorators that\nare basically these functional\ncomponents that you you as a user\nimplement\nand once you've done that\num\nyou can use it locally or you can\nactually launch deploy it to various\ndifferent Cloud infrastructure so if you\ngo to the next slide I'll show you a\nkind of a picture of what I mean\nSo currently right before this\nintegration\nyou would deploy training jobs to a\nflight cluster in a cloud agnostic way\nobviously you have to set up flight you\nknow currently\nyourself and\num it Bridges the gap between this kind\nof like offline compute data intensive\nworkloads like model tuning and training\nand very different contexts where you're\nserving creating predictions and serving\nyour model in some kind of serverless or\nweb endpoint right so today you can\ncreate a fast API endpoint a serverless\nendpoint through AWS Lambda and so that\nkind of fulfills some some part of like\nthe quote unquote full stack like\nmachine learning application and if you\ngo to the next slide basically what\nBento ml gives us\nis you know it's it's we're basically\nleveraging all of the uh surveying uh\ncapabilities that Bento ml gives us so\nyou know to take a step back Bento ml is\na is a model serving tool\num and I've worked with it a little bit\nwith this integration and it has some\nreally nice abstractions\num that I'll save for maybe a different\nuh presentation but you know the the\nlong and short of it is basically you\ncan train your models on a flight\ncluster\nand uh and also locally and once you're\nuh ready to serve that model\nyou can do so with Bento Bento ML and it\ngives you it unlocks This Cloud agnostic\nserving story where you can use web\nendpoints through AWS Google cloud and\nAzure as well as serverless endpoints uh\nin these equivalent kind of uh Cloud\nvendor products\nso\num\nnext slide is just the present demo\nslide so let me\nswitch gears\nover to my\nscreen here so\nokay hopefully you're seeing my terminal\nand my editor\nso quick a quick demo here I've kind of\npre-built a bunch of stuff so we don't\nhave to sit around and wait for a Docker\nto build but you know this is the uh\nvery simple digits classifier app I\nmight have shown this in earlier\npresentations but it's basically just\ntaking sklearn's digits data set and\num\nwriting a union ml app out of it so a\nlot of this might look familiar to you\nthe new pieces here is I'm importing\nthis Bento ml service\num and I'm basically binding it to uh\nthe model\nand what that allows us to do is like\nautomatically create a bunch of stuff\nthat uh as a Bento ml user you would\nhave to hand write not going into that\nbut basically you have to Define uh what\nthey call a runner model and a service\nand all I'm doing here is is defining it\nI'm not doing anything else\nand at the bottom here I'm saving the\nmodel to a Bento store and so what that\nmeans is if I run this real quick\nit will train the model and save the\nmodel this is a Bento ml model to this\nlocal uh store so if I list them out\nhere you can see that I've created two\nother ones in the past\nso\nThe Next Step here is once I've saved my\nmodel I need to kind of wrap it up in a\nservice\nand so what that means in Bento ml terms\nis\num\nI have to load the model from the local\nstore it could be a remote store as well\nbut we're working locally right here\nand this this kind of like\num\nI'll say just one liner it's just a\nsingle call to this configure method in\nthe Bento ml service\nuh basically creates a service a Bento\nmail service that\num you know\nI can show you the diff right but uh\nbasically it's it would be maybe 50 to\n70 lines of code\num defining the API what the input types\nand output types are and I I won't uh\nshow that here but basically this is it\nfor Union ml the union ml integration I\ncan say certain things about what that\nservice looks like so is it async uh\ndoes it support CPUs and or gpus does it\nsupport multi-threading and does the\num what the Bento ml world will call a\nrunnable which is basically like a\nprediction kind of service a prediction\nclass whether whether the inputs are\nbatchable so there's a lot there's a lot\nto unpack there with Bento ml so\nI won't dive too deeply this is uh kind\nof like the default settings that are\nforwarded to the Bento ml abstractions\nso once I have the service I can say\nBento ml serve\nand in the Proto prototyping phase I can\njust\num reference this service and it exposes\nthis dot SVC that's kind of a Bento ml\nconvention that points to the actual\nBento file service\nand I can serve my endpoint locally here\nso\num I have this request oops\nyeah I have this request helper file and\nI'm going to hit that endpoint and boom\nso so this is this is cool for uh\nfor local development but once you're\nready to put it into some kind of more\nproduction grade Cloud infrastructure\num you basically have to build what they\ncall a Bento and a Bento is just a you\ncan think of it as a directory\ncontaining all of the source code other\nartifacts config and a bunch of other\nstuff that Bento has standardized into\nwhat they call a Bento package\num\nand so in order to build this this\nartifact uh I have to create this Bento\nfile Bento file.yaml\nfile and among other things it says you\nknow it points to the module and the\nvariable that an attribute that contains\nthe Bento ml service along with a few\nother things like the requirements of\nthe of the service\nso if I just say Bento ml build\num it's going to be going to build this\nartifact for me and so the way I can\nlist these out is bent over my list so\nthis is\nyou know uh like just to show you a\nlittle bit under the hood this is\nliterally like a local directory with a\nbunch of stuff that Bento Mel has\ncreated for us\nso once we have a Bento\nthe next thing we want to do is we want\nto\nuse this other tool that uh the Bento ml\nteam has created called Bento CTL\nand what this does is essentially\ncreates a bunch of terraform files from\nterraform templates that allow us to\ndeploy this Bento to some Target uh\nCloud infrastructure so that in this in\nthis case we're going to use AWS Lambda\num so\nI will say Bento CTL init\nand this will allow us to to\ngo into this interactive mode where\num\nI can fill in a few things\num so I already actually have a\ndeployment config file that it's going\nto spit out after this command so I'll\njust show you what that looks like so\nyou can version the API it has a name\nwe're using the AWS Lambda operator you\ncan use again you know Azure functions\nGoogle Cloud run\num and a few other things\nso from here\nI'm going to say Bento CTL build\num\nsecond\njust reminding myself of the command I'm\ngoing to grab the\nthe identifier here so I say Bento CTL\nbuild\nuh with the the tag of the Bento\nand then a pointer to the config file\num so I'm going to skip all of this\nright so this is It's\nuh gonna build the docker image and I'll\nput a bunch of uh other terraform files\nactually while we're waiting here uh are\nthere any questions so far about this uh\njust about Bento ml or you need ml in\ngeneral\nyeah I do actually have one but let's\nsee if all those come up here too\num so so are there any resources is\nthere any uh upcoming blog posts or any\nany other information that you can share\nup front\nyeah of course so\nI'm working on a blog post this week\nbut the\nthe latest so like kind of like the\ndevelopment version of the docs has\num a surfing with Bento ml page and it\nbasically takes you through this entire\ntutorial and It'll point you to\num you know Bento ml team\ndocs that uh will give you some more\ndetails if you if you want to customize\nthings this is just really scratching\nthe surface\num so it's in the deployment guides\nserving with Bento ml\nfantastic\nso we are almost done here\nso this is actually pushing the docker\nimage to AWS account\nso it's gonna add a bunch of useful\nstuff\num basically we're going to use this\nBento CTL apply command which under the\nhood uses terraform to initialize a\nbunch of templates and apply those\ntemplates\nforeign so nothing actually much has\nchanged and because I've already created\nthis um this function let me\nhead over here so this is the union ml\nBento ml demo zero function that we've\njust created\nwe can see that it was just Modified by\nthe command that I invoked\nand you know I tested this but of course\nthere's always\nthe demo Gods\num oh right I need to update the the\nrequest endpoint here so after calling\nBento CTL apply you'll you'll see a\ncouple of useful pieces of metadata this\nis the function name the image\nURI and I'm just going to swap this out\nhere\nand\nthe first time you invoke this Lambda\nfunction it always takes a little bit of\ntime\nbut after the first invocation it'll\nit should\nbe fairly quick\nbut you know this this gives you a sense\nof you know\nbasically Union ml\noh awesome okay and then just to prove\nto you\nI can give send it some other slice of\nthe data and we'll give you some other\npredictions\num so I'm really excited about this\nintegration because basically you know a\nunion ml's mission is to provide like\nthis\nbasically full stack\num framework for ML Ops\num\nand so there's a bunch of work to do\nthere's model tuning that's currently\nnot supported but this is a huge step\ntowards\nproviding a very easy way of deploying\nyour models via Bento ml so you know\nshout out to that to that project and\nthat team for creating this uh really\nnice set of abstractions\nand yeah that's it for my demo\nyeah that's fantastic uh Neils uh so\nmaybe a question obviously now Union ml\ngoes all the way to serving with the\nband to ml integration of the endpoints\num and you you talked a little bit about\nthe road members anything else you can\nshare about the roadmap or where you\nwould actually request the help and\nsupport from the community\nyeah so actually with this Bento ml\nintegration Union ml is introducing this\nnew abstraction called a service\num and so this is\nthis is a like fairly new territory for\nus here but basically uh we would love\ncommunity help with other model serving\ntools like k-serv and and Selden\num and as far as I can tell doing some\num initial sort of like\num research on the ground just like\nlooking at their documentation it seems\nlike\nuh it's fairly common to\nto come up with these abstractions of\nlike a service and an API and like ways\nof loading and saving models\num so so we'd love help there in terms\nof feature Integrations uh the the next\nbig step in Union ml is model training\nand experiment tracking\nso those those will be the focus for the\ninternal team uh in the next let's say a\nfew months\num and so those those will leverage you\nknow the things in Flight world right so\nRay tune for hyper parameter tuning\num we'll probably have a native flight\ntuner that's more like grid searchy\num but then if you you want something\nfancier uh we'll just leverage you know\nthat and sort of\nyeah it's it's been a fun project to\nwork on because it's you have to\ndetermine what abstractions to expose to\nthe users versus what to to uh\neither\nDeli or sort of like have reasonable\ndefaults and at least abstractions to\nhide uh from end users\num so that'll apply for the model tuning\nAPI as well and experiment tracking will\nbe a another big one\num where we'll probably\nagain leverage flight Integrations like\nml flow\nthat's fantastic all right Niels thank\nyou so much what's another question\nuh yeah sorry I had a question comment I\nthink for Niels is shown is just sorry\noh my name's ketan by the way sorry\num and uh\nI think I've been working a little bit\nwith Niels as he's been doing the Bento\nml integration and\nit's really turned out awesome but we\nhave plans to bring went to a sort of\nintegration into as a as an example\nintegration into flight for serving\nuh it will leverage whatever we've done\nwith Union ml kind of interplay but the\nother cool part is I think we our goal\nwith flight has been to be the pipe that\nyou take your data transform them into\nmodels and finally deploy them and\ndeploy them that matches your production\nschedule so if people are interested in\nhow we are thinking of doing that it\ninvolves using human in the loop stuff\nthat's actually complete now I'm going\nto be available soon and went to ml and\nuh flight then yeah let us know we would\nlove to have a chat and share RFC soon\nand I think\nthere is a small prototype that Niels\nalready has and we're just waiting to\nfinalize some other bits before we write\nthat let's see so cool yeah\nthanks Jason\nawesome great comment thanks Caitlin\nit's lovely to see this project moving\nforward and actually really getting the\nthe bigger Vision that case in you and\nand also news you had about going really\ninto end and having this abstraction on\ntop that makes it so much easier to use\nan orchestrator in an ml setting so I\nalways feel like this is really built\nfrom the ground up versus you know the\ntop-down approach where you have a\nreally solid scalable orchestrator under\nthe hood and you can do really\nabstracted with little code and little\neffort really cool things so great job\nhere"
    },
    {
        "title": "Union.ai at KubeCon + CloudNativeCon NA 2022: Recap",
        "transcript": "um so then\num we went uh not we but you and a few\nother team members went to kubecon uh\nlast week in Detroit uh it's uh cookecon\nCloud native North America I think\nofficially it was about six to seven\nthousand people uh who were there in\nperson and probably the same amount of\npeople who were there virtually we had a\nbooth there flights you can see the\nphoto here at amateur and I feel like\nthat was the first time flight to to be\nrepresented at such a show uh it's a\nlittle bit of a stretch because it's a\ncomplete infress show and uh not very\nstrong on the data I'm outside or\nactually in the ml side but I you know\nwe felt like it was you know probably\ngood good learning experience to go\nthere anyway so\num then please uh introduce yourself\nreal quick and tell us about uh your\nimpressions at kubecon\nabsolutely um so I'm my name is Dan for\nthose of you who don't know me I'm a\nback-end engineer with the Union team\num I was one of four team members that\nwent to kubecon this last week from the\nunion side\num just start out by saying it was was\nan absolutely great time we saw a lot of\nexisting users um of flight and it's\nnice how he's put a face to a name you\nknow we work on slack all the time and\nsome have pictures some have don't\num but but it's always great to kind of\nsee people in person get to talk through\nthings and we got to talk to a lot of\nother people to it and kind of discuss\nhow flight may fit into their workflow\nand that was just a great experience\num the small recap two kind of just\noverarching things that we noted\nthroughout the the conference first off\nwe saw a central theme that that there\nis a need for flight\nwe had a chance to attend a number of\ndifferent talks spoke with lots of\ndifferent attendees and kind of saw this\nincreasing importance of cloud native\ndata orchestration specifically like Ai\nand mo workloads at massive scale and I\nthink at the union side and working on\nflight day to day we kind of lose sight\nof a lot of different things one we've\nbeen working on this problem for for a\nvery long time but you go to a\nconference like this and you see that\nthere are a lot of organizations a lot\nof companies where where this really is\nin its infancy and they're kind of just\ntesting tools and and there's a lot of\nuncertainty there in tooling and best\npractices\nEtc but we feel that there's a lot to\ngain here there's a lot of Crossroads\nthat that flight can make it's\nDeployable on a lot of different scale\nand has utility in a lot of different\nenvironments\num secondly it was great to hear people\nkind of Express their opinions that\nflight does a lot of things right flight\ntends to be opinionated about certain\nthings for example versioning workflows\nand tasks to track data lineages and\nensuring reproducibility of executions\nhaving strongly typed input and output\nvariables\num and we're talking to people they seem\nto like these features they've tried\nother tooling there are significant pain\npoints that they ran into and still kind\nof this feedback Beyond flattering our\negos of course\num it's great to hear you know\nvalidation and reassurance that that\nflight is doing a lot of different\nthings correctly and seems to help a lot\nof people\num so just to summarize kubecon great\nexperience we're thankful for all the\ngreat people we got to talk to and look\nforward to do kind of more of this stuff\nin the future"
    },
    {
        "title": "Union KubeCon + CloudNativeCon 2022 Booth Video",
        "transcript": "[Music]\nforeign\n[Music]\nthank you\n[Music]\nthank you\n[Music]\nforeign\n[Music]"
    },
    {
        "title": "Kubernetes-Native Data & ML Workflows in BioTech - with Flyte",
        "transcript": "my name is jeev I work at a company\ncalled freenom we are a very\ncross-functional team that's essentially\nbuilding Next Generation diagnostics for\nearly cancer detection within freenom I\nwork in the research platform team this\nis a large part of the Innovation engine\nat freedom and we build we operate and\nwe support different components like the\ncompute platform data platform and\nautomation platforms and our role is to\nessentially like Empower our users and\nour users span like you know multiple\nteams across Phenom including molecular\nresearch computational biology machine\nlearning and so on so forth and we want\nto empower them to like basically build\nbetter products faster we're here today\nbecause you know flight is a very large\npart of our research platform it's a\nvery key component of it and that's just\nlike one of the places in Freedom that\nwe're using flight we need a very\nflexible tool to like support both our\nresearch use cases and our production\nuse cases and and um that's essentially\nlike one of the main things that we get\nfrom flight and that allows us to like\nbasically you know go from research\nresearch into production like fairly\nquickly otherwise you're ending up like\ndoing a lot of like rewriting or like\nmodifying you know large pieces of your\ncode base as you're transitioning or\npromoting it from Research into\nproduction so all of those reasons aside\nlike I think that was uh that was like\none of the like very important pieces\nfor us to uh very important like reasons\nwhy we went on a flight so we started I\nthink like around 2020 or so you know\nafter like flight had been open source\nwe were very early in our adoption our\nEngineers were like you know very like\none of the first ones to like start\nadopting or moving on production\npipelines to flight really and so like\nthat that happened fairly quickly the\nresearch side moved a little bit slower\nI think researchers are generally more\nresistant to change like because you\nknow they don't want to focus on like\nthe ux and like all this stuff they just\nwant to like do their research in like\nan environment that is like most\ncomfortable and familiar to them so the\nway that we started there was to start\nonboarding like like specific teams and\nlike working very closely with them to\nreally Empower them to be successful on\nthis platform and then as we started\nseeing like more success and people\nstarted getting excited about it like\nthere's a lot of Word of Mouth going\naround like within the research\norganization and then other teams\nstarted picking that up and then we\nstarted supporting those teams as well\nit was a little bit slower albeit a\nlittle bit more like very mindful\nintentional on the research side on the\nengineering side it was more of a\nno-brainer because of like all the great\nfeatures that like life provides us so\nat Freedom we have this Mantra that like\nyou know we don't want to have a lot of\ntools in-house like we want to have a\nfew things that work really well that we\nreally understand that we can you know\nwe're very confident about operating\nkeeping up debugging fixing and so on so\nforth and also like in the case of\nflight we've we've built I think a\npretty good relationship with the team\nand like we feel very confident like\ntaking this supporting this tool like in\nour production stuff I think open source\nis like a big important thing in biotech\num specifically because like you know we\nare trying to build this like idea\nregulated product right and so the more\nthings that we have to build in-house\nyou know the more proof the evidence we\nare sure they have data like you know\nthis is the right way to do things I'd\nrather invest that effort in you know\nlike a really solid like open source\nproject like flight and then you know\ngrow that product nurture that product\nfor biotech and you know for a future\nlike FDA regulated products together\nwith a much larger like open source\ncommunity so like having one really nice\nstandard\num that you know a regulatory body can\njust look and be like oh yeah there's\nother companies in the space that are\nlike using that have been successful\nusing this so it makes sense right it\nwould be a much better like uh selling\npoint"
    },
    {
        "title": "UnionML Patch Deployment Demo",
        "transcript": "let's get started\nso yeah we don't have anyone new here\ntoday so we'll just um\nget started welcome everyone this is\num\nthe open source Union ml planning\nmeetings it's uh September 28th 2022.\num so the agenda for today is\n[Music]\num\nbasically I want to go through a\nsomewhat revised roadmap a lot of the\nmain Milestones are still the same as\nyou've seen before but I've basically\ncut out\num\nI've moved to the backlog certain things\nlike the case serve integration so like\nbasically focusing on like one class of\na certain type of feature so\num we're keeping Dental ml here as the\nfirst integration\num and then you know in future\nreleases will prioritize additional\nIntegrations of that same class if that\nmakes sense\num yeah Caitlin\nhas pushed in a patch deployment uh\nfeature so I'll be doing doubling that\nat the end of this\num\nso go through the roadmap second item is\nto remind everyone that we're\nparticipating in hacktoberfest this year\nso\nI'll go through some of the logistics\nand um a few things over there and then\nlastly just go through the patch\ndeploying the demo\num so all this stuff is done for the\n0.1.4 milestone\num\nso I'll be cutting a release for that\nprobably by the end of the week\nso that's great this this will kind of\nlike get us out of\nuh a phase and I I would say the project\nwhere things you know are not super\nstreamlined I feel like after zero one\nfour\num you know we'll start stabilizing the\nAPI and there there's still a few things\nthat we that might change but I think\nmoving forward we're going to double\ndown on Integrations and making it work\nwork really nicely with\num\nother things in the ecosystem\nso just to go through these really\nquickly the zero to zero Harmony release\nwill feature a scheduling API\num\nthe bentylml integration\nuh additional template so Union ml ships\nwith various project templates so the\nidea here is just to add a template for\nI'm supporting\nstreaming databases so TBD which one\nwill support\nfor this this one but Kinesis sqs\nwhichever one\nmakes more most sense here\num\nyeah so the in last and last uh the last\nOSS planning I demoed the S3 event stuff\nso this is kind of going along that\nthread of supporting reactive sort of\nevent-based predictions\num and then the last one here that's\nslated for this release I've assigned\nthis to Zev\num I think yeah I've chatted with him\nhe's aware uh so we'll get to this when\nwhen he frees up\nbut the idea here is to add a\nprediction hook\nto the predictor decorator\num\nthe idea there is\nyeah I have some pseudo code here\nyou can kind of add a custom callback\nhere that will be called after\nthis function body and basically it uh\nit'll pass in\nthings that you might need for logging\nso this will lead up directly to\num revela integration and other\nIntegrations of that nature where you\nwant to\nsay blogs and predictions the features\nmaybe the models that generated them and\nthen you can do whatever you want in\nhere\nright so\neventually we'll be able to support\nmeans about that so the API is always\ngoing to be the same right model feature\nthe prediction\nso that they can\nprobably buy it like the client or\nsomething uh or you know other\nconfiguration for let's say calling\nnovella\noh yeah so like\nlet's see how that works so here you\nyou're thinking like adding like some\nkind of configuration\nlike passing configuration I was just I\nwould say thinking that we should just\nrecommend you with public start partial\nuh essentially to create the partial\nfunction\nso that but then that also means that\nyour API has to be very\nconsistent one one way to avoid that\nthing might be is that in the my custom\ncallback instead of sending the three\nparameters exploded\nyou could send them as one at as one\nclass or something oh I see I see what\nyou're saying so that okay you know you\ncan keep on adding things to it yeah yep\nyep that makes sense\nyeah I mean this this hits up some kind\nof class that you know has a Dunder call\nmethod associated with it and then\nwhatever this signature is\num we'll just get past that thing\nyeah things yeah he's not on right now\nbut I was paying him to see if you can\njump in at some point over the next\nlittle bit here see how he has some\nthoughts there as well\nyeah I know he's been he's been thinking\nabout it he hasn't written any code yet\nbut um I know he's been on his mind and\nwe talk about it periodically so he's\ngot some ideas\num cool but uh\ncool nice um yeah he thoughts through\nhim there I'll just hang around and just\nsee if he's I know he's having meetings\nright now he's gonna be able to jump in\nhere\nyeah so just take a note of this so like\nsome kind of I don't know let's just say\nuh\ncall back inputs\nsomething like this\num and then the idea there would be\nyou'd have access to the model\nNature's\ndirections\nthan whatever else\nyeah I mean I can imagine\nthere's a way to\nsomehow feed it resolved labels if\nthat's if that makes sense for the\napplication\nlike you'd be able to actually compute\nmetrics\nlike uh like model performance metrics\nyeah okay cool\ngreat and then I did a similar\nreprioritization\num here so I pulled out a few I merged\nvarious issues and pulled out a few into\nthe backlog\num just\nyou know\nto\nwe don't really have a release Cadence\nright now\num I would like to eventually do some\nkind of like monthly or quarterly thing\num do you have any thoughts there\ncaptain\num I think let's stick with flight like\nquarterly Maybe\nuh for 2.0 we might pull other things\ninto two polarized I think the ammo flow\nexperiment tracking for example we do\nhave some work done\nalready in flight so maybe we'll pull\nthat in\nuh but yeah but I think besides that we\nshould\nlet's stick to the quarterly Cadence\nyeah okay\nawesome\num yeah and then I won't go through each\nof these will will go through them when\nwe kind of get there but yeah uh three\nzero zero three zero is about like\nexperimentation model tuning just making\nthat experience a lot better\n[Music]\num\nyeah but isn't the the checkpoint thing\nand so on much simpler than some other\nstuff in here like\nyeah yeah for sure\num we haven't got into estimating these\nvery well\num\ndo you want to do that now or\num\nyour choice I think yeah I'm fine doing\nit now we can take it offline and you\nknow talk about them\nbut like for me like support model\ncheckpoint from previous trading around\nscenes\npretty simple to be honest\nand then model checkpoint save and load\nmodel weights recover from modulation\nwhat is the difference between 48 and 52\noh interesting yeah I I probably these\ncan be merged I think this would be the\nfirst if we were to break it down like\nthis would be the first thing\num and then this would be\nyeah yeah let me\num\nconvert these so yeah let me just close\nthis this is basically the same as 63.\nalso in this case if I understand\ncorrectly you want to use the checkpoint\nfrom a previous training run\nbut should that not be an output of the\nmodel then I guess that's what you're\ndoing it will be an output\nand then yeah you use an existing bottle\nyeah we basically use flight remote to\ngrab that and then get into the next\ncall yeah perfect yeah that that's very\ngood awesome\num and then there's this\nso yeah I agree this is kind of a small\nthing this will also be small right\nmlflow experiment trackings and it\nshould be pretty flat yeah depending on\nwe're not\nyeah\ncool I think this the idea here is to\nSimply use the\num\nthe flight kit\nagain for now because you know\nwe could scope this small like that or\nlarger where it actually uses a ml flow\nserver\num yeah as a model registry like I don't\nthink see I think this plus one is even\nsaying like well you know flight is\nalready a model registry so\nand we just we kind of have to like\nimprove the ux around that\num as opposed to using the ml flow model\nregistry\nyeah yeah unless that's like something\nreal some people really want\nyeah like let's talk about that because\nthat would be very good to figure out\nwhat is the difference over there and\nhow can we kind of make it simpler to\npeople\num for the mfro experiment tracking I\nthink that sounds simple to be honest\nyep\nso\nuh the beginning one would take some\nwork I think that'll be a kind of a\nlarge one\nyeah so I would actually break it apart\ninto\nuh you know\nthe tuning to be a separate thing of its\nown really like\nI I don't know maybe now that we are you\nknow in the midst of things it feels\nthat tuning might be more work than\nlet's say everything else combined so it\ncould be simple if you use a or\nsomething it just gives the real plugin\nin the beginning\nuh yeah\nyeah so I I don't know the right answer\nover there but I feel that the other\nthree are much simpler here\nyeah maybe then the quarterly Cadence is\ntoo slow here yeah\nso yeah I mean I have I have a few\nthoughts here where\nI mean I feel like it would be nice to\njust have a flight like a um\nyeah like the many use cases you know\nthere there is a use case where you're\ndoing like\nBayesian\nhyper parameter optimization and you\nkind of want things to be super\nefficient where you know\nthat's where you use Bray tune\nyeah because you have some n workers and\nyou always want those workers to be\ncompletely utilized during the entire\nthing whereas here you could like do a\nmap task fan out and that'll do do fine\nright since each run is completely\nindependent of the others so\nactually I didn't I didn't notice you\nhave gel grid search and random search\nactually the yeah this might be simpler\nbut we should type it out then yeah yeah\nbecause these would be very easy you\nlike you kind of with a grid search you\njust exhaustively do everything\num your random search you like sample\nfrom some set of distributions so\nand can we can we not do that like can\nwe do good search and random search\nusing default light and if you want to\ndo any other type of thing you can use\nRay plug-in and somehow the API is very\nyes um\nso that uh using plugins like Ray\ncan you see\ncorrect or something switch Yeah\nokay\nand then this uh incremental training on\nthe schedule\num yeah we already have that scheduling\nRFC there's already a code example for\nscheduling a training job so this will\njust kind of be that idea\num so this would couple with 63 here\nwhere you know this would just support\nlike Manual invocations of remote train\num so it would have some like from\nprevious argument or something like that\nwhere you just um reference the the\nprevious\nmodel training run yeah this one will\njust be that on a schedule so yeah and I\nthink you can filter it by the launch\nplan essentially because the schedule\nhas the large plan so you should be able\nto absolutely get the previous one it's\npretty cool okay I think all of these\nthen if you Scope it this way are pretty\nsimple uh\nobviously different than that but are\nsimpler than we think\num so quarter seems too high as a\nCadence for this\nyeah\nokay\num let's skip over the the future\nreleases since these are it's like yeah\nyou know next next year at this point\num one thing I did want to\nask about here is\n[Music]\num\nwhat is this so\nI made this ticket\nlet's see\nI this is sort of like a concern I have\nabout this whole feature loading stuff\nand this came out of the schedule\ndesigning the scheduling the the uh the\nprediction schedule API for Union ml\nI'm a little concerned that\nthe methods are starting to become\nredundant\nyeah\num\nso I want to see if it's possible to\nget rid of these feature star methods\nand like\nthe the problem of the trade-off here is\nwe have this data set object right and\nit defines a bunch of these methods that\nare then composed together in the\ndifferent like\nwhat we're calling microservices right\nlike training prediction et cetera Etc\num\nthe problem\nis that\nif we were to not distinguish the\nproblem is this it's like you have to\ndistinguish between unlabeled data and\nlabeled data so they're like these two\nexecution paths where you're grabbing\ndata and for training you need that data\nto have some labels associated with it\num\nso you can simplify the all of the cases\nby just saying here's your data here are\nthe features and this is the Target\nField and if the Target Field is none\nthen you can't use that for training\nwell you know you have to filter those\nout\num you can use that data for prediction\nthough because all you need for\nprediction is features right\nbut how do you multiply that at runtime\nyeah that's kind of the challenge where\num\nthe training flow is\nis like well defined here so you first\nread in the data\num say you output a parquet file\nright then you need a loader to like\nload that data into memory\nbut if flight understands how to do that\nthen then you don't need a loader\nbut let's just say for the case in this\ncase we're like output putting this\nparquet flight file\nthen you know\nunder the hood the reader is a different\nflight task than the training job right\nso that's why there's this separation\nand that that separation makes sense\nsince\nreading data is you know you kind of\nwant that partition because\nyeah typically like that job will be\nheavy like it might be a spark job it\nmight be you know some kind of compute\nintensive job you pass it over to the\ntrainer you know you yeah you want to\nblow that into memory\nso\nso then you know you load that into\nmemory as a data frame for example the\nsplitter will partition it into training\nand tests and validation sets and then\nthe parser is the thing that splits the\ndata set into features and targets\num\nso that's that's all well and good\nnow when you get to the prediction\nsetting\nwe've created these feature loader and\nfeature Transformer methods and those\nare purely for when I'm passing in\num\nlike raw data say I have a list of\nnumbers and those are to be features\nlike the feature loader is meant to\neither handle those raw features right\nso this could be like say a list of\ndictionaries and then I'm just passing\nthem into the data frame\nor it could be a path\nin which case I need to parse that\nfirst and then pass it into the data\nframe so like this is what we came up\nwith or like\nwhen I hit a fast API endpoint and I'm\ngiving it Json data and then this needs\nto be called to like prepare the data\nfor feeding into the model\nTransformers it's like another\nadditional thing where if you want to\nseparate the concerns there of like\nloading into memory and transforming it\ninto like the final form\num this is there although you could in\ntheory move this code into the loader so\nso this is like where it's not it's\nstarting to sort of proliferate in terms\nof what you need to implement\num\nbut the transfer is required for trains\nalso right for training too or no no\nactually no why not so it's weird yeah\nit's it's kind of a mess right now so I\nthink\nlike we kind of need to\nlike either have a loader and a\nTransformer and those\nwork in all cases right but then you see\nwhere I'm going here if\nif we're\nif we have a reader loader Transformer\nsplitter parser and they're supposed to\noperate on both the training and the\nprediction settings yeah\nthat the function body could become\nfairly complex there you're gonna have\nto do like if if instance or you know\nwhatever\num to handle all the different cases\nbecause you might have a Target variable\nin there yeah exactly so\nso this is It's kind of not ideal\num\nit's it's a design challenge that's not\nstraightforward\num solve I think\nyeah and I think this is actually a very\nbig challenge for most teams so yeah\nthis is why we started solving these\nproblems I think we need to sit down and\nI think if I had to solve one problem\nthis would be the problem to solve\nyep\num and I think it's worth to get right\nyeah yeah\nlike on one hand if\nif you had just one set of functions\nthat you had to worry about for training\nand prediction\nlike\nit\nit decreases the API surface of this\nthis you know of Union ml\num in terms of like functions I I need\nto worry about\nbut if I conflate the true then\nyou know the the function bodies of\nthose smaller like handful of functions\nI need to worry about those will become\nmore complex\num\nso it's it's not\nobvious to me what the solution is like\nright now we're slated to create a\nfeature reader right to like completely\nisolate the two\num\nyeah\nso that's that's the current thinking\nbut I did want to just flag it here and\nrecord it so that it's uh\npeople are aware of it\nyeah let's go and I think I would love\nto participate in that I'm just like\nyeah think about it I think there are\nthere's something that we can do\nI don't think it's like in place the\ncleanest always the possible solution\njust because of the dichotomy and that\nthe way the data exists\nbut it can be clean\nyeah\nyeah I mean you can kind of see this\nproblem in the feature stores right like\nwhen I saw feature stores\nand they have a notion of targets as\nwell I was like wait\nwhat if your feet a feature store\nimplies that you're only dealing with\nfeatures but now you have targets so\nlike okay then you're like data set\nstore then or I don't know\nI don't know how they think about it\nexactly but yeah in feast or in tecton\nyou see\nyou see this sort of thing where they\nthey have a notion of targets\num so that's interesting is that\noptional in some cases then for them\nyeah I think it's I think it is optional\nI I believe it is yeah because I think\nthe way we were signed within the past\nthere was a feature there were topics in\nthere now that you tell me there was a\nfeature definition but there was also a\nTarget definition\nweird but I was then\num and the feature definition would\nactually just decide on the columns and\nessentially create a view\nwhen you ask for a feature but\nbut not get the targets uh\nand when you call the training data set\nI think that's what it called they would\nget everything\nand a training data set could compose of\na feature definition and uh\nand the target I think\nyeah\nlike one potential resolution I see here\nthis is the last point I'll make here we\ncan move on\nis\nlike the parser handles\nthe default implementation of Union ml\nfor this is like\nif the target column doesn't exist in\nthe data set then return features and\nnone\nlike a tuple of features a none and this\nthe parser only makes sense\nonly matters in the in the the training\nsetting\num\nbut because of that default\nimplementation\nit could be used in prediction right you\nhave your data set uh and then pass it\nthrough the parser and returns just the\nfeatures you don't have to care about\nthe targets at all why is the target's\nnot optional here\nso the opinion here is that Target is\noptional and features are\nand the features are none\nthen just assume that all the columns\nbeside the target column are features\nso that's if not features\nyeah this should be features is not\nyeah that means it features is none then\nfeatures are in targets no no then the\nfeatures are the columns that are not\nthat are these are column names oh yeah\nyeah\num\nso like if a user were dealing with\nnon-data Frame data they'd have to like\naccount for all of this stuff so that\nthat's what kind of makes it feel a\nlittle dirty to me where it's like\nyou know they have to they have to\naccount for all these cases so we either\ndo that and document it well or just\nredesign this so that it's a little\ncleaner\norders you just always use data frames\nnowadays there's so many different types\nof datability during that so it's yeah\num so\nokay all right cool\nwe'll punt on that\num next thing is Oktoberfest so\nuh we have a issue here tag tactical\nprofess intro Eduardo wrote a little bit\nof um\nsome prompts here uh there's some useful\nstuff like the country contribution\nguide\nall the issues labeled with Oktoberfest\num I'll go through the rest of the\nbacklog to see if there's anything else\nthere\nand of course the blog post\num so depending on watching this\nrecording is interested you should check\nthese out and you know you stand to win\nsome swag\nand an O'Reilly\nannual subscription so if you're into\nthat check it out\num all right\nI'll pause here I guess\nMike do you have any are there any any\nitems that you want to chat about or\nseems like right now we're just kind of\nblocked on certain things\nyeah I think um\nit's pretty clear the products move over\nhere so I think we're it's just a matter\nof us getting our kind of like\nresourcing in order\num and getting on with the issue\num so I don't think anything else that\nwe need to discuss per se\nokay\nawesome uh so last part of this uh\nmeeting I'll just demo stuff so you know\nwe might we might do without this once\nthe media gets larger and we're actually\nyou know\num\nin the swing of things but for now\nI'll show you the patch deployment flow\nso this is um\nactually worth just showing this\nso here's my\nuh what should I call it\ndemo\nso I'm gonna initialize my union ml\nproject\nyou see I have a patch demo\nproject here\nvery lightweight\napp as you can see\num so I'm just going to go ahead and\nstart a demo\nflight cluster this will take a few\nminutes\num and in the meantime\njust give you a little co uh tour of the\napp\nI think we've all seen this one\num digital classification so this is\nlike the happy it's like the hello world\nexample\num\nlet's get some digits data I'm gonna get\na sklearn model on it\num evaluated for accuracy\num\nthis is the configuration for the flight\ncluster hey Niels can we drop the config\nfile from there in the default example\nbecause yeah actually because it's gonna\njust do the auto thing right yeah\nyeah\nmakes sense also Docker file because\nthat this is the default\nyeah you anyway yeah\num cool\nso the thing I'll show off today is\nin the union ml deploy CLI command and\nalso the remote deploy\num\nmethod so you can actually do the same\nthing by doing calling remote deploy\num\noh\ndo I have the right environment I don't\nthink so\nthere's this new dash dash patch option\nand this will bypass the docker build\nprocess this is you know effectively\nfast registration we just kind of re\nrebranded it here\num and Union ml so that you know\nlike\nit kind of matches the intent better and\nkind of you know doesn't\nprevents users for having from having to\nworry about flight specific con uh\nConcepts\nokay so I have my cluster\nrunning\nI am going to do Union ml deploy uh\nmodel and this is just this will build\nthe actual Docker image in the um\nthe flight clusterity so we're seeing\nthe logs here it's building\num\nit doesn't take a bit but this is the\nfirst time\nyeah actually yeah while we're waiting\nthis is actually a contribution that's\nadded here which is allow uncommitted so\nby default Union ml will complain\nuh if you have any uncommitted changes\nbecause it's going to try to build the\ndocker image so\nyou can\npass this flag in to Asylum set that\nerror\nforeign\n[Music]\nreal quick\nyou can see that uh unit ml created this\npatch demo project for us automatically\nand actually I'm hoping that this issue\nthat that Dan is debugging doesn't come\nup here there's a pair there appears to\nbe\nlike a lag between project creation and\nnamespace creation\num he's taking a look at this so usually\nit doesn't come up I think because like\nwhen you spin up a flight cluster flight\nsnacks is already there as a name space\nso typically people will just\num\nregister will close against that but\napparently if you create a project\non a fresh cluster it'll it's\nprobably still doing some stuff so it's\nit's when you create it the namespace\nwon't be immediately created so\nhopefully that there is a thing period\nit might take a little bit\nso if you just wait out it should happen\nyep\nokay\nall right so everything's being built\nright now\ncool so our workflows have been\num registered\nyou can see here now\nprotection workflow prediction from\nfeatures and the training workflow\nI'm going to call this command line\nargument to kick off the training job\nthis is where we might see that bug\nthere you go so the namespace hasn't\nbeen created yet\nthis might be just my computer kind of\nthing because I think Dan wasn't really\nable to\nreproduce this\num\nif you do\ngive get\nParts I might see if it's running uh\nit's okay I can help you people because\nI know the problem\nokay that's it\nyep still happening\num all right it's like\nyeah so while we wait for the namespace\nto get built\num let me make a change here so I'm just\ngonna print out a print statement\nlater\nhere\nand I'm going to redeploy\nfor the app with the patch argument\nas you can see\nyou got some logs here so I'm deploying\nan app version patches\nbasically use the git Shaw\nplus a little uh suffix here with patch\nand some random ID\num\nand looks like it got deployed\nif we look at the console you can see\nthis patch version is is out\num so calling this again will actually\nby default\nuse the app version so\num just to show everyone here\nthe arguments here is the app version\nwhich is\num you so you can explicitly pass that\nin but if I if I don't provide anything\nit'll just uh execute the latest\nwhatever the latest is\nokay great so\nlooks like the namespace is up this will\ntake a few minutes\nbut we'll see uh in the logs that there\nis a print statement with the change so\nthat's basically the patch argument\nthere\num as I said earlier you can pass this\nto the the model remote deploy method\nlike so you do remote deploy\nand basically you say patch equals true\nthis is you know if you're\nlike in a rebel and you're interactively\ntrying to deploy these things\num\nand you won't really feel the gains of\nthis locally on a local flight demo\ncluster but you know if you have an\nactual cluster up and running this wall\ngive you the same benefits as fast\nregistration in flight\nI I wanted to add one thing over here so\nwhen when you're doing model.serve it's\nusing your local code right but now\nlet's see if you have a containerized\nenvironment and you deploy the model\nwith the patch release it won't use your\nlatest code\nright\noh\nmodel service and like a fast API server\nyeah\nnow if you go back to your the previous\nscreen\nlike in here you have the model that\nserved right that's\nthat does not prefetch\nthe patch\noh um let me actually double check that\num because\nI had snuck in a few changes oh you did\nthe code base\nso yeah interestingly you can\npass in an app version and a model\nversion\nand what this will do is it will\nat app startup assuming you have your\nflight remote\num set up correctly\nit is going to fetch it's going to load\nthat model\num\nhere\nusing the flag remote\nyeah but let's say if you\nchange the predictor method then\nlike the signature or like maybe change\nthe prediction logic\noh yeah I mean I guess I guess if you're\nlike in a development workflow you and\nyou have um like the reload option and\nyour your fast API\nor in unicorn when you call it\num it should happen I think locally it\nwill work locally it'll work because you\nhave yeah yeah but I'm just saying\ndeployment I think what I'm also saying\nthat I don't think we should support it\noh no yeah yeah you should probably just\nlike error saying that hey you're doing\npatch registration that is not\nrecommended for production deployments\nplease\nbuild a container\nyep yep exactly\nuh yeah somehow I can't find the logs on\nthese but anyway\nour training run completed\nand uh yeah unfortunately I won't be\nable to pull up the print statement\num\num but anyway is it\noh you are in the wrong name space\nconnection\nyou should have linked correctly but if\nnot you can fix that that's\noh interesting yeah I think you have two\nhyphens is that the problem\nno\nvlogs\nthe first uh yeah that one ah there it\nis\num\nso in our first execution\nlike if we were to Union ml\nI I should add a thank you for getting\nthe app versions\num but let me just\nand grab it from\nconsole\nso I can do something like this and I\nsay app version\nthis older one\nthis completes it it won't have that\nprint statement\nso that's that's basically the new\nfeature\num I'll be cutting a release to add this\nprobably by the end of the week so\nlook\nbe on the lookout for it\num all right so with that I think we can\nend this planning meeting thanks for\nattending and um\nif you're watching a recording of This\nfeel free to please join us on Slack\num\nwhere is that let me find the um the\nlink to that\nif you go to the contributing guide\noh\nI guess we don't have oh no yeah it's up\nhere in the banner so\nyou can join us on slack if you have any\nquestions\num and yeah thanks for watching\nawesome all right bye everyone\nthank you bye"
    },
    {
        "title": "UnionML AWS S3 event demo",
        "transcript": "okie dokie let's get started\nokay\noh I lost\nsomething you can people see my screen\nyep\nawesome\ncool so\nuh again thanks everyone for joining the\nopen source unit on our planning meeting\num\nthe agenda for today is just to go\nthrough some introductions\num\nso typically you know there will be new\npeople in these\num so just wanted to get a sense of\nyou know who you are and what makes you\ninterested in uni ml\nand I'll go into the zero one four\nroadmap that\num\nthat we're working on right now\num\nI'll go through\nlike a change in the roadmap structure\num hey Dan\nI'll go through the changing the roadmap\nstructure around documentation issues so\num I'll go a little into detail there\nbut basically we want to kind of\nseparate like\ndocumentation certain kinds of\ndocumentation changes from like features\nobviously there's some overlap there\nand since we do have 45 minutes I don't\nthink we'll spend the most all the time\non these three first points so I did\nwant to show off a little bit of um the\nitems I'm working on in the zero one\nfour roadmap which is the um\nS3 event demo so basically it's a kind\nof it's a mode of model prediction uh\nmodel deployment where you know you you\nkick off a prediction job based on\ndumping an S3 the file into S3 so I'll\ngo into that a little bit\nand then finally go into the model\nserving RSC see if any of you folks have\nany feedback or comments around how\nwe're thinking about model serving and\nhow to integrate with other other\nplatforms\num so first off introductions I'm Niels\nventilin\num Chief ml engineer at Union AI I'm\nheading up the union ml project which is\nbasically uh\nkind of a framework on top of flight\nthat is machine learning specific\nand um\nin terms of design philosophy\nit\nkind of is a successor of application\nFrameworks in Python and that it's very\nmuch functional it's it's like you\ndefine\num these core objects data set and a\nmodel\nand then each of these expose\ndecorators where you implement certain\npieces of the machine learning pipeline\nand then from there\nit sort of combines and composes those\nfunctions in ways that are abstracted\naway from the user\nso for example you can call model.train\nand what happens under the hood is it's\nconstructing a dag from all the\nfunctions that you've created\num\nyou can run this locally as python you\ncan run it on a flight backend\nand once you have a trained model\nartifact you can use fast API right now\nto serve model predictions\nso that's a quick the quick intro to\nUnion ml\num\nmaybe we can go around quickly just to\nintroduce ourselves\num sorry for the the union AI folks\nyou're gonna have to reintroduce\nyourself\num so let's start with samita\nhey everyone\num my name is Sam hetha I'm a software\nengineer and developer Advocate at Union\nAi and I'm here to contribute and learn\nwhat's on the roadmap so yeah\ndone\nwell my work focuses mainly on back-end\ntype stuff but I was interested in\njoining these types of meetings here\nit's Kevin\nyeah yeah I'm Kevin yeah and uh I'm\nsuper engineer at Union also and I'm\nmainly working on the weekend pop like\nif I keep talking or else like vlogging\nanything bacon porch and come here to\nenjoy the union house meeting to learn\nsomething any other day about the union\nabout Union male ml\num and Ali but sure so hi I'm Ali I\npreviously used flight in one of my\nprojects and just want to like quickly\nsee how how the roadmap for Union ml\nlooks like I'll be participating in a\nhackathon in near future so just want\nyou to see what the roadmap is like and\nwhat is the tool like capable of so just\nwant to quickly drop by and see how\nthings are looking for it so yeah\nawesome\nthanks Ellie uh it looks like Eden\ndropped out but no worries\nokay so\num here's the roadmap you can find this\non the documentation\num it's under the union\nOSS projects\n[Music]\num\nso for the zero one four release\nwe're working on a couple of issues\num basically these two 39 and 40 are for\nadding integration tests\num for various things that are currently\nnot covered\nso remote execution of Union ML\nworkflows on a flight back end\num uh welcome back Eden\num testing out certain methods that are\ncurrently not covered in the model\nobject\n41 is to add support for S3 event\nservings\num\nin the demo I'll I'll show shortly\nand then the last one here is supporting\nfast registration of unit ml workflows\nso\nso these are the other four items I'm\nnot sure I need to touch base with\nEduardo on where things are happening\nhere\num\nbut\nlike based on where I put my\nunderstanding so far I feel like he's\nworking on other stuff so\num\nyeah I'll I'll Circle back with them to\nsee if this is still well these are\nstill open\num\nI think in past meetings I I've gone\nthrough the other\nMilestones here but I think I'll just\nstick with zero one four\nare there any questions about any of\nthese\nuh what are you planning to release zero\none four\nyeah that's a good question we don't\nhave like a very constant release\nCadence right now I think because it's\nsuper early on in the project and we\ndon't have like too many hands on deck\nhere\num working on issues\nso\nbasically the answer to your question is\nwe'll release it when all these four\nthings are done\num that is TBD\ncool okay\nso as I mentioned earlier\num about the docs issue Point\num\nover\nis that yeah over here\num\nI separated out documentation that uh\nare related to things like examples\nso there are a few holes in the\ndocumentation today that need to be\nlike either clarified so for example\nthis one\nis\num relating to the existing page serving\nwith AWS Lambda and there's like view\ncode Snippets missing\num\nunit tests and examples for these two ml\nFrameworks X3 boost and tensorflow\nand then some other like examples that\nare not really tied to specific like\ncode changes in the union ml code base\nbut um\nwould be more like lightweight you know\nlightweight examples of how to use like\nthe flight MPI operator in a training\njob and the sagemaker in a training job\nso these again I guess the thinking is\nwith any of these docs issues like\npeople can pick them up work on them and\nthey'll just be merged into whatever\nrelease happens to be\num\nin the running at the time\nokay cool\num\nyeah I know you know we're we're sort of\na little\nspread thin on resources here but is\nanyone\noh should they just left okay\num yeah would anyone be interested in\neither of these two\nso these would be fairly\nlike\nI actually don't know what the estimate\nhere for these would be I feel like\nthere's some\nflight configuration that's uh\nthat would be\nyou know a little a little more to learn\nfor a union ml user who you know we\nassume doesn't really know much about\nflight\nI can try uh the MPI issue is it okay if\nuh yeah I might not be able to get it\ndone anytime soon so is that okay\nyeah yeah that's totally fine\nI'll just you know ping you once once a\nweek or or so just to see if you have\ncapacity or you're working on it\nokay\nOkay cool so\nyeah I'm excited to show this I\nhopefully it's working right now\num I've got it to sort of work\nbut um to give you a tour of what what\nthis actually looks like\nlet me\nshow you this template so I created this\nproject\nby doing Union ml in it\noops\nyeah you know now init template\nbasic AWS Lambda S3 and then I gave it\nsome app name\nright so the resulting the resulting\nfolder is here\nthis is still kind of a work in progress\nbut it should be\nshould be working\nfor all intents and purposes\num\nso let me start with the with the Union\namount app\nso if you're not familiar with this\nsyntax I'm just doing a bunch of imports\nfor like the various tools I'll be using\num and I'm using the digits data set\nthat ships with sklearn\nso\nagain to zoom out the the whole point of\nthis piece of this feature is\nyou know they're different modes of\nserving there's there's batch prediction\nthere's kind of like API based like\nfunction calls\num a prediction where you you hit an\nendpoint with some like payload\nand that payload can say contains\nfeatures and then you get predictions\nback\nand then like I guess the broad kind of\nlast bucket here that you could maybe\nfurther break down is sort of like\nevent-based predictions so these could\nbe events\ndepending on whatever kind of system\nyou're using right so they could be S3\num\nput object events or they could be like\nthe analog of whatever streaming\ntechnology you're using like Apache\nFlink or beam or something\nor sqs\nso this third type of\nserving modality I'll call it\nis\num can get fairly complex but we're\ntrying to keep it simple and the first\nthing we're doing is supporting\num predictions when\nsome other you know external process\ndumps a file into S3\nokay\nso all of this is kind of like Union ml\nboilerplate it's like this is my\ntraining data set how do I get it into\nyou know\nhow do I load it from an external Source\nhow do I train my model generate\npredictions and evaluate data\num this is additional configuration for\na flight back end so this currently is\npointing to a sandbox\na flight sandbox\nand this last function here is\nis not really you know Union ml specific\nstuff right this is just a Lambda\nHandler\nfor AWS Lambda\nso to unpack this a little bit I'm\nbasically loading my model from an\nenvironment variable\num and Union ml has like this hard-coded\nthing\nwhere\nit can load\nfrom some environment variable that\npoints to some model path and that model\npath\num\ncontains a model object that unit ml can\nthen load up into memory\ninto this this model artifact\nso when I say load from n basically\nI'm assuming that\nthe runtime the container that is\nrunning my Lambda Handler\nhas this model object already in there\nright so I'll show you what that looks\nlike but assuming that we have our Union\nml model loaded up with its artifact\nI can iterate through the events records\nso if I upload just one file this event\nwill just contain one element\num I get the bucket and key metadata\nfrom it\nI get the features from S3\nwhich I pass into the model.predict\nmethod\nuh producing some predictions\nand then I have to implement you know\nhow do I upload that back into some some\nTarget okay\nin this case we're using the exact same\nbucket that um the features are in but\nwe're just like uploading it into a\ndifferent prefix\nso this is a lot and any reactions\nyeah so I just had a question so in like\nuh could we probably in the future make\nthis kind of a\nI guess vendor uh\nvendor free function so something like a\nserverless Handler where yeah instead of\nlike the S3 client will provide the gcp\nbucket client uh we just have to pass\nthat\nand\nyes yes so that's a perfect Segway to\nthe text section which is that model\nserving RFC so this is like we're still\nwe want to be really careful about this\nbecause you know\num there are a lot of trade-offs to\nhaving this abstracted kind of Handler\ntype convention\num but I think we can strike the right\nbalance so um that's a great a great\ncomment question\num so what we're doing here right now is\njust providing templates so when you do\nUnion ml init\nand then you give it a specific template\nlike there's a sort of\num\nit gives you a union mail project\nwith these this kind of implementation\nthat's like not you need a mouse\nspecific it's more specific to like the\nthe platform that you want to use for\nfor serving\num this is a lot lighter weight but you\nknow potentially more verbose\num\nand that verbosity comes in because you\nknow this is the we're using Sam AWS Sam\nCLI to handle all the provisioning of\nthe different resources right so I'm not\ngoing to go through all of this but\num\nlike this is the union ml function\num actually this code isn't needed but\nthis is the file upload event right so\nlike\nI'm going to create this Union ml app\nbucket resource which is referenced down\nhere\num I'm going to react to all kind of\nobject creation things so this actually\nshould be put but let's leave it here\nfor now\nand then it's going to react to\num events in the with these conditions\nof it has to have this feature's prefect\nand it has to have a Json suffix\num and then this bucket name is uh\nprovided here so the bucket that will be\ncreated for this example is the union ml\nDash example Dash you know et cetera Etc\nso like this puts kind of the burden on\nthe the user to like figure Sam out and\nyou know it'll ship with this template\nbut you'll have to know how to modify it\nto like\ndo anything else other than what the\ntemplate is doing\num\nand then finally uh this template ships\nwith a dockerfile.aws Lambda so this is\nthe thing that loads up the model object\nright so I'll show you like the full\nworkflow\nyou know right so you you initialize the\napp\num in this simple case I'm just going to\nrun the app.pi file\nand as you'll notice at the bottom here\nI'm just running I'm actually training a\nmodel right so I'm dumping this\nmodelobject.joblib\num in this in this folder right here\nso when I build the docker file\num\nfor the AWS Lambda Handler\nyou know I'm\ncopying the app you know I'm installing\na bunch of requirements copying the app\nscript in here copying the in the model\nobject into the Union ml model path so\nit knows where to find it and then this\nis just a convention for for AWS Lambda\nyou have to point it to the\nthe app and the the variable in the\nmodule that um\nis the Lambda Handler\nso\nthe process here is I do Sam build\nand I've already built this and I do Sam\ndeploy\nand\nthere might be a few things I need to\napprove here\nbut it's um\noh\nokay\nyeah I think I deployed this I ran this\nright before so um\nall the change all the changes should be\nhere already\nso here we go\num\nI have this bucket\nhere\nthat um\nis the source both the source and the\ntarget bucket or my features and\npredictions respectively\nand if I go to Ada if I go to Lambda\nthis is the function that was created\num\nand fingers crossed\nthis should work\nso what I'm going to do\nI'm just going to manually do this it's\num\nso I'm going to upload\nthis file into the features prefix\nit's going to take a while a few you\nknow maybe a few seconds to a minute\nso I've uploaded\nthis these sample features to the S3\nbucket\nand if we go to the Lambda function\nthis will show up\nas I said in a few minutes\num and fingers crossed there are no\nthere are no bugs but there could be\nstill still a work in progress but the\nidea is\nyou know after this\nLambda function runs there should be\nanother predictions\ndirectory here containing the\npredictions to that file\num any questions here\ncool\nso while we're waiting for that\num I did want to show a little sneak\npeek of this RFC that I've been\nreferencing\num there's more to it than than the the\nkind of like Chauvet what you were\nalluding to with like a Handler\nso I'll jump just jump straight to it\nwhere\nyou can\nso there's this high level API right is\nyou\nDefine this AWS Lambda service and you\ngive it the model you need ml model\nand produces a service object\nand the service object exposes more\ndecorators right so this would be kind\nof the syntax\num for specifying like the event type\nso I'm saying like we I want this\nyou know I want Union ml to react to\nsome kind of you know this S3 event\nbased on a bucket and based on these\ncriteria\nand you know\nI want to also specify some kind of\nTarget resource\nagain this could be anything right this\ncould be\nlike\nin the in the future case this could be\nlike gcp or any other type of resource\nand Union ml would be responsible for\nspinning those up so this adds like a\nconsiderable bit of complexity to like\nwhat needs to happen under the hood\num but you kind of get the idea where\nbecause Union ml already implements all\nthe functions right like how to load\nfeatures how to transform them how to\ngenerate predictions like the prediction\nHandler\nas proposed here\njust gets the predictions and then\nwhatever other necessary metadata you\nneed to like handle where those\npredictions go and how exactly they're\nwritten to the the target resource\num and then from there you do like some\nkind of launch command or deploy command\nand that's or actually no build this\nthis should be build\nand this this builds like all the\ntemplates I just showed you\nand then from there the idea is that\num you\nuse whatever\nCLI or conventions that that Target\ninfrastructure\num requires a view so in this case you\ncall Sam deploy\num\nsince we don't want to like deal with\nlike we like the interface for Union ml\nis like all the artifacts needed for\ndeployment based on like our opinions on\nhow these things should be composed\ntogether\num but at that point you then we then\ndelegate to like whatever conventions\nthat um that tool needs for actually\ndeploying the thing so this this will\nbecome a lot clearer when we fleshed out\nlike you know we're not sure if this set\nof abstractions really will work for you\nknow many of the cases we want to\nsupport right so for bento ml they have\ntheir own conventions for how to set up\nthese\num handlers and like config files like\nyaml config files seldman has this like\ndifferent set of conventions so\num the idea though\nchevey as you as you propose like\nis we want to kind of like settle on a\nreally nicely balanced API here for\nthese types of services\nthat um\nthat can be abstracted and generalized\nacross different\ndifferent Integrations\nokay\nso hopping back here\nso okay fingers crossed this actually\ndid what we wanted\num\nnope okay there's still a bug\nlet's check out what the logs say\num\ndumps okay\noh\nthere you go okay but you get the idea\nright like uh I was dumping to a string\ninstead of a file\num I'm not going to go through this\nentire workflow again but uh\nwhat we'd expect to see is a predictions\nprefect uh prefix and in that would be\nthe files corresponding to\nyou know so imagine like a another job\nthat just like dumps features in here\nperiodically with some like unique key\num and so that should show up in the\npredictions prefix\num under a similar key you know so by\ndefault you could just dump it into the\nsame key but you could like format that\nkey in various ways as you need\ncool\num any any questions here any like uh\nlike proposals on how to like make this\nI don't know a little neater\nany thoughts\nso I just I had like one question uh so\nis there a possibility when we are\ndefining this uh Lambda Handler so\nsometimes we might see that if you are\nloading the model uh\nwe might not get a good prediction so if\nyou want to update the model on the Fly\nand probably retrain and load the model\nagain uh\nso it could happen right that if our\nmodel performance Is Not Great uh we\nmight have other pipelines that will\nretrain our model with uh changing of\nthe parameters and that will delete the\nmodel that we have currently saved in\nour HD bucket and uploaded again so\ncan we run this like so what I'm just\ntrying to visualize is that uh can we\nrun this instead of a pipeline where we\nare constantly updating our S3 bucket\nand then just fetching the latest model\nthat is there\nkind of like even iterated yeah\nyep I get I get what you're saying\nyeah this is a this is a great Point\num this is also a larger conversation on\nlike\nthe design of Union ml\num\nwe've sort of defaulted to be a little\nmore conservative when it comes to these\nprediction endpoints like we don't want\nthe model version to be in the hot path\nof this Handler for various reasons\nright like once I've deployed this model\nto Lambda like it's\nthere's a benefit to it being immutable\nlike the model doesn't change and to for\nyou to change that model effectively\nyou'd have to redeploy another Lambda or\nupdate\nit's like you you would take advantage\nof whatever\nlike versioning or rollback mechanisms\nthat the platform offers\num so you know\nin the workflow you mentioned right if I\nwanted to retrain a model\nlike currently you sort of have to do\nthat manually and\nI I you know there is I think value in\nthis more iterative workflow\num I think we just have to be really\nclear about what\nwhat modes of deployment would support\nthe more iterative workflow like you\nknow in real life\nright because if you introduce if like\nif you start at the default behavior of\nlike being able to switch off the model\nin the hot path of this Handler like you\nintroduce all sorts of\npossibilities of uh weird unexpected\nBehavior whereas if you're\nat any given point in time your Handler\nyou know is uh immutable then you you\njust get you know a nice like safety\nfeature there\nso that's a good point\num\nyeah and and I guess maybe to hint at\nlike\nwhat these Services could mean in\nservice of that point of like you know\nautomatically retraining and\nautomatically redeploying\nyou know these these Services don't have\nto be just for prediction right so in\ntheory you could have like\ntrain on event\nand this this event\nsystem would have to be made a little\nbit more sophisticated right you don't\nwant to retrain just on like one batch\nof features right but if you make it\nsuch that this event dumps like a whole\ndata set into the the thing and then\nbased on that you retrain\nlike\nthe thing that you're talking about\ncould be made possible here\num but again I think we're\nslowly trying to figure out what the\ndifferent use cases are and where you\nknow there's always this conversation of\nshould it be in flight or should it be\nin unit ml\num\nso\nyeah a lot of a lot of nuances there to\nhash out\nyou know there's also the signaling API\nwhich is coming up so that's going to\nopen up a lot of things and yeah we'll\nhave to like figure out\nwhere that fits into this\nyou know um Union ml design\nokay cool\num\nany other questions or comments\nif not we can\nend the planning meeting thanks everyone\nI think as the project matures will run\nthis more of like an actual planning\nmeeting you know right now I think it's\nbuilding some momentum up so\num\nI think moving forward this will be like\nshort recap of like existing\ntasks in the mile in the current\nMilestone and then we'll maybe do some\nof these kind of demos to show off uh\nexisting work\num yesterday\nI I just one final question so when it\ncomes to the RFC that you're showing uh\nlike do we maintain the rfcs on the\nunion ml OSS click on GitHub or\nuh no uh this is currently on my\npersonal or my like Union AI\nnotion account\nI think I'll yeah maybe I can like once\nit's finalized copy pasted into some\nset of artifacts that are more\npersistent I don't know\nmaybe GitHub discussions\num actually I personally prefer Google\nDocs I know that's contentious but uh\nthe depends on how much commenting\nyou're looking for if you want to\nuh if it's going to be actively\ncommented on I think that's easiest if\nit's like if the emphasis is on like a\ngood code syntax highlighting\nIntegrations ensure a notion of faculty\nYeah my two cents\nanyways\nokay\ncool\nawesome\num thanks everyone\nbye"
    },
    {
        "title": "UnionML Review",
        "transcript": "meals Nielsen's product Panera that he\nhas finally built was mentioned by Fabio\nearlier today we talk about a different\none we talk about Union ml Union ml is\nan open source system built on flights\nit's flight ecosystem project and I\nwould love to introduce Niels uh talk a\nlittle bit about more just to give\npeople who have not heard about it kind\nof like the high level uh explanation\nand where to find material and so on\nbecause please go ahead\nyeah thanks Martin\num hi everyone I'm Nielsen\num I do a lot of ml related stuff in\nUnion Ai and um yeah just a real quick\num review of Union ml we've spoken about\nit in the past but really the the\nproblem it tries to solve\nis atrocity unit by the ecosystem of all\nthe different tools that you might want\nto use when building and deploying\nmachine Learning Systems into production\nand the production\nterm there is operative here and it's\nnot you know we're trying to bridge the\ngap here between flight as a you know\nvery fully featured production system\nand the prototyping r d workflow that\nyou know\napplied might not be super optimized\ntowards so you know Union ml is trying\nto bridge that gap between\num prototyping quick iteration\num maybe in a Jupiter notebook more on\nthat later\num and it really tries to bring\nadditional things that might be missing\nin the flight ecosystem like model\nserving for example and it tries to you\nknow tie tie a little bow on the entire\nend-to-end workflow you can see all the\nlinks here for the roadmap the website\nand documentation\num and if we go to the next slide we did\nwant to feature\num Zeb Zeb is the co-founder and\nEngineering lead at LaBella systems\nrubella is a model and data\nobservability and monitoring\num tool and just wanted to feature his\ncontribution here\nthe pr you can see in the link but\nreally what what this does it's it's a\nsmall but not a small but important\ncontribution here to allow you to deploy\nyour union ml app\num using maybe an uncommitted code and\nget so by default now it'll complain if\nyou have some uncommitted code in your\nunion ml app\num but P you contributed a PR where you\ncan\num say allow uncommitted was true and\nthis just allows you to specify a flag\nif you're doing quick quick and dirty\niteration\num to redeploy your app so thank you Zev\nand we're we're actually working on\nanother integration with with rubella\nitself\num and this is part of the model\nobservability\nof features and plugins that were\num gonna introduce into Union ml over\nthe next few months\num we do host OSS planning meetings for\nUnion ml they happen every other\nWednesday the last one was last week so\nthe next one will be next week\num you can see on the link there the\nzoom invite you can also add it to your\ncalendar if you want to\nattend these regularly\num and I believe that's it for our unit\nml updates\nI think that's it yeah all right Neil's\nfantastic work so I hope that um the\ncommunity will join the planning meeting\nI always think there's there's so much\nthat could be done on the abstraction\nlater because I would have you know a\nlot of requests for certain areas I feel\nlike that's a good good opportunity for\nthe community to actually really Thrive\nwhere this whole project is going so if\nyou're a member of the flight Community\nplease join the union ml uh OSS planning\nmeeting with Nielsen and I'm sure this\nthis will be very enjoyable thank you so\nmuch for your effort yeah oh oh sorry\nI'm sorry I just wanted to uh wet\npeople's appetites here just in case you\nhaven't really seen what you know Union\nml does but\num I just wanted to point you to the\ndocumentation if you want to get started\nwith kind of zero setup we do have\nguides here in the tutorial section so\nif you go to the docs you need to\nmail.weatherdocs.io there are two\nexamples here that do have Google collab\num links so you can go out this is this\nis the one I just opened\num and you can you know go through the\ntutorial check it out run the code\num and get a sense of how it all works\ngreat that's fantastic thanks for\nsharing that super helpful"
    },
    {
        "title": "UnionML OSS Planning 002",
        "transcript": "awesome\num\ngreat hi everyone uh my name is niels\nventilan i'm in the ml team at union ai\num thanks for joining us here today let\nme\nshare my screen\num to give everyone some context\ni'll do this for every meeting since it\nseems like you know\nthere are different folks coming by\nevery so often um\nlike my screen go\nthere it is okay cool can everyone see\nthis roadmap\nit'll be bigger\nokay\ncool yeah so so give to give you some\ncontext is it on this meeting\nthis is an open source planning meeting\nso\nuh no presentations or slide decks we're\nyou know basically going to do\nessentially what is sprint planning in\nthe open source\nfor the union ml project\nmaybe raise your hands or\nwrite in chat like if people know or\ndon't know\nraise your hand if you don't know what\nyou need email is\nokay\ncool so it seems like\nat least\noh brandon okay\nso um\nmaybe at some point i'll stop giving\nthis high level of spiel on renewable\nbut\nessentially\nunion ml i'm guessing you know about\nflight since you're in the flight slack\nbut union ml is a thin wrapper around\nflight\nthat helps you write\nmachine learning apps as we're trying to\ncall it so\nit's it's designed\nvery similarly to web frameworks like\nflask or fast api\num but\nessentially you define these app objects\ni'll just call them so model and data\nset\nand then they expose various\ndecorators where you implement certain\npieces of a machine learning system\nlike where you get your data how you\ntrain your model\npredicting from that model and\nevaluating that model\nand what union ml\nhelps facilitate is putting those pieces\ntogether in in boiler in ways that get\nrid of a lot of boilerplate you know so\nwith these four functions\nyou can basically train a model\ngenerate predictions on it\nin python so in a python runtime\nit integrates with pass api so you can\ncreate a online prediction service out\nof it\num it also works with aws lambda if you\nwant a serverless infrastructure\nand so this is this is like the\nthe core\nbeginnings of the project where\nit provides a kind of this high level\nsyntax for defining\nkind of\nyou you kind of uh declaratively define\nlike these these entry points and then\nyou\nimplement the actual function body of\nthose entry points\num\nso\nyou know i'm calling estimator.fit here\nbut you could really import any\nlibrary so this is using sklearn but\nyou know this is a pie torch equivalent\nof the same\napp\nthis is the keras version of this app\nand so that this is sort of the design\nof this this framework\nis\nthis is like\nthe surface level api\nand\nthe internals are a little bit uh\nspecialized i want to say so you kind of\nreally have to know what you're doing\nwith flight to to develop but we're\ntrying to you know make this a layered\napi where you can go underneath and\ncustomize things\nhowever\nwe're at the stage now where we are\ncleaning things up where we're trying to\nget some certain core integrations\nuh built in\nuh before\ngoing deeper in the\nthe onion so to speak\nso hopefully that if anyone has any\nquestions uh who's not familiar with ml\num\nfeel free to ask it now or type it in\nthe chat\nokay\nso brandon\n[Music]\nokay cool so yeah again like\nthis is just flight right it's just a\nnice little um\nwrapper around it\nyeah i think i wanted to add one thing i\nthink the one of the reasons why we\ncreated it also was to make it\nmake the serving portion really seamless\num\nand maybe eventually also add like\nsurveying\nuh like\nstreaming serve and\npatch predictions like batch when i say\nmultiple different\nparallelized batch prediction automatic\nbecause that api it's possible to do it\nlike knowing that this is your predict\napi right knowing that that knowledge is\nwhat you need you don't need a lot of\ncode like flight would know it\nbut\nthe problem would be then that we have\nto create something special and like\ncalled predict which didn't make sense\nyeah yeah exactly\num\nso yeah just to tie it all together like\nthe design of this framework is you\ndefine a bunch of methods in code\nuninml like will automatically\ncompile or bundle them together together\ninto services\neach of these correspond to some\npiece of infrastructure so the training\nphoto microservice is a flight\nworkflow literally\nso same with batch prediction\nonline prediction you know we we offload\nthat off to fast api or whatever other\nyou know integration we come up with in\nthe future\nuh we have a few planned and so i'm\ngoing to go that into that in a second\num but these these three dots you know\nimply\nthat there could be an explanation\nmicroservice or a you know\num\ntuning model tuning microservice\nso zev and mike here on the call\nit's not exactly a microservice what\nthey're working on is revela which is a\nmonitoring tool model and i think also\ndata quality monitoring tool so that'll\njust kind of be interleaved\nin\nthey'll be they'll sort of be a\nconfigurable piece that you can add to\ncertain microservices as they make sense\num\nokay cool\nif there are no other questions let's\njump in um\nso at a high level this is what the\nroadmap looks like we're organizing it\nby release\nversion\nwe're following semantic versioning\num\njust a meta point about this roadmap is\nyou know\ni'll go i'll i'll prob i'll i'll only go\nkind of like\ntwo releases through the items um\nbut we you know we're we're flexible\nhere we're we're like if i go down to\nsay zero four zero\nand you see something like this like oh\nwe're using snowflake i want a snowflake\nsupported data set\num\nthen you know we can fast track these\nany of these issues you know down below\num\nbut\nat a high level\nzero one four is just\ncatching up a little bit on testing\ninfra and um not condo packaging that's\nalready done\nbut a few other\nthings that we wanted to get done before\nthe first\nopen source release but we couldn't so\nwe're just kind of catching up now\nzero two zero is all about serving\num\nso you can see here\na bento ml integration\num\na\nmore seamless scheduling api\nwhere you can schedule batch predictions\num an extra little thing with a fast api\nmodel server supporting image uploads\num\nand a few others related to aws lambda\nstreaming support and a better\ninteroperability with flight\num\nzero three zero is about\nmodel tuning and experimentation so\nuh you know one thing to call out here\nas high priority is mlo ml flow\nexperiment tracking integration\num\nby the way we also prioritize this based\non like you know some just feeling the\nwinds and you know based on user\nfeedback and requests so\na lot of people have talked about this\nabsolutely\nbut this is high\nand then looking forward into the future\nzero four zero is just focuses on a\nbetter interoperability with flight\nthere are probably a few items here that\nwe're gonna add\num but the idea is\nif you have a full flight kind of model\ntraining and tuning system that's like\nfairly custom and\ncomplicated\nthe idea here is as long as one of those\ntasks or workflows produces a model\nartifact\nthat you would be able to kind of like\ncreate a minimal union ml app that\ndoesn't even have any of the training\nlike trainer or reader it just\nimplements a predictor\nand somehow points to that\nuh\nworkflow\nand grabs the artifact you know so we\ndon't we don't know what the details are\nyet but basically we want this to really\nwork as\nsort of like the bridge between flight\nand\nsurfing\nwhich currently flight\nuh doesn't really do\num and the last piece here\nis observability and data quality\num i sneaked in jupiter support here um\nbut this is far enough into the feature\nwhere we might have\num\nsome more\nfirst-class uh support for jupiter\num so you know uh zev and mike so these\nthese issues um\ni think i think we can kind of fast\ntrack a few items here\nthat will unblock you with at least sort\nof a\nuh folk of concept integration with\nrubella i think\ni've spoken to both of you about it i\ncan't seem to find the\nissue prediction\nuh prediction hook for this\nright so here for some context um\nso it's easy to find\nso like the idea here is that you know\nthe predictor\ndecorator exposes some callback\nwhere that callback\nhas access to certain things like the\nmodel the features and the prediction\num\nand\nanything can happen in this function\nbody\nso this is where ravello might come in\num for logging those predictions\num i think for now we're not really\nthinking about labels right um\nfrom my understanding\nyou have you guys have some basic uh\nlike\nthings that don't\nuh depend on\nlike\nlabels\nor ground truths\nyeah is ever you listening on this one\nthat's probably a good question yeah i'm\njust struggling with my video i'm trying\nto turn on but i did yeah you just found\nit i did comment in here a little while\nback about some thoughts but um yeah i\nthink we can just start with like\nmodel input and output and we'll worry\nabout like labels later\num just to sort of\nyeah put the the framework and grow like\nbasic pieces in and then we can figure\nout what we want to do after\nawesome\nyeah\nis this part like this callback part is\nthis something that you are able to work\non or do you have capacity for this at\nall yeah um we're just going through\nsome of our internal releasing stuff\nover this week but uh pretty soon here\ni'll be able to hop in there and i'll\nself-assign that issue and\nuh i think that callback stuff should be\npretty straightforward\noh okay looks like i can assign you now\nyeah go for it sure\num okay so\nthis i i kind of want to estimate this\nas\ni don't know t-shirt sizes uh\nmedium\nbecause it's yeah it's like a single\nargument and predictor\nwith some i'm sure there are like some\nother things that need to change\nbut um\ni think it would be reasonable\nreasonable to put this in the zero to\nzero release\nif i have questions on that is it\nappropriate to just tag you\nin comments on the issue oh yeah totally\nyep\num\nyeah also the uh the union ml select\nchannel\nyeah right\nokay unfortunately\nwait it should be here\nokay\nuh well i'll touch base with\nabdul and chevey about these two\ni think\neduardo as well i'm not sure if he's\nstill working on these\ni'm currently working on this uh\naws lambda template\nso um\nyeah maybe to dig into this a little bit\ni don't want to\ngo into too much detail but\nunion ml has these templates so you can\ndo uh union ml on the command line init\nand um\nan argument to one of these templates\nand this will\nyou know create this basic aws lambda\ntemplate app for you\nso this one will support you know\nprovisioning of an s3 bucket and then\nreacting to events upload you know\ndumping files into that s3 that event is\nsent sent to the predictor endpoint\num\nor\nit's not technically streaming i suppose\nbut it's it's more reactive predictions\num\nokay um needs i had a question so i see\nevans around and evan did a fantastic\nexample of running spark with union ml\ncan we add that as an example as well\noh yeah totally um\nyeah\nfeel free to assign me to something like\nthat because i\ni i have more time to uh\nto work on this so\nokay awesome\nyeah i would love um\nlet's see\nyeah let's let's chat offline but yeah i\nwould love to have that as an example\nand even\nso there's this pattern and data set\nfrom\nunderscore star right so\nlike if you have an existing sql flight\nsql like task\nyou can actually\ncall\ndataset.from sqlitetask and it'll create\na dataset\nunionml dataset object for you\nwhere this\nthis is uh the tests will be a little\nmore clear about this\n[Music]\nright so um\nnice so this this basically replaces the\nreader this is the reader\num\nand then now you have a data set you can\nlike play around you can do all the\nother stuff you can define on a data set\num but if you have a spark\ntask\nthere is currently\na problem flight kit test that's not\nthat's private\num\nand yeah it would be a matter of adding\nsome tests for a spark task\nright so yes i think i like the api but\ni don't like the\ncardinality of this api\nlike from sqlite from sql alchemy from\nsnowflake from bigquery is that the idea\nor\nyeah that that's\nlike\nthe initially i was going to end with\nfrom flightcheck task\nyeah i forget why\nexactly i had added special ones\num\nso you know totally open to make it a\nlot simpler\nuh\ni feel like this will work because i\nmean both of these are just following\nfrom flagship task right um there's\nnothing special happening in these\nexactly\nso\ni guess they're mostly decorators is it\njust like a simplification or facade\nyeah exactly so you know we can rethink\nthis api\num maybe first of all making this this\none public\nand\nand then seeing if it generalizes well\nso so um\nit should right if the if the task it's\nonly a flight kit task then that should\ngeneralize almost\ncompletely because my kit task has a\nconfig object like it has its own type\nthing\nyep yep you'd work\nand then that way if it's equal light\nthat is\nexplicit within the flight kit task\ndeclaration right so it becomes nicely\ninteroperable actually\nyeah um and i think you know\nif we can generalize\nso there are things that as you can see\nhere like\ncertain things are being injected into\nthe dataset object\nthat are derived from the to the task\nlike the outputs and inputs and stuff\num\nso if this generalizes across all tasks\nmaybe there are some basic checks that\ncan happen here that you know like\nit has to be some recognizable data\nobject i mean i don't know\nso so um yeah certainly we can do that\nbut yeah but we know the output types so\nwe can actually\nenforce this that like compiled our\ndeclaration time right\nyep\nso maybe\nlet's add a saying here\num\n[Music]\nexpose data\nright\npublic\ni also had one weird question over here\nmaybe i want to get people starts over\nhere like so\ndata set currently is one is to one to a\ntask\nright\nwhat if you need like a entire\nlike an etl\nkind of you know or multi-step workflow\nto achieve that\neventually we could support that right\nlike from flycat\ntask for a workflow\nyeah the way the way dataset works\nis\nlike the there's you know from a flight\nperspective there's uh the reader which\nis its own task\nyeah\nand then all these other methods loader\nsplitter parser that actually is called\nin the training during the training work\nmode\nyeah the reason being\nreading\ncan be like an expensive step\num\nbut loading the data splitting and\nparsing it into features and\ntargets that can happen in the same pod\nas the training of the model\num most of the time\nso that's kind of like the separation\nwe could have a reader workflow right so\nwe could have like a from flight that\nworkflow\nand that'll be the reader\nyeah\nand that'll be a sub workflow in the\nbroader workflow things\num\nthat could be a feature engineering\nthing and eventually i think we would\nlove to have data set from\nlike a feature\nfeature store feature\nexactly\num but sorry evan so going back to the\nthe spark example i think\ni think you know uh\nuh a d uh um\nlike converting the the example that you\ncame up with and obviously removing work\nrelated stuff\num\nas an example would be useful here\num we can figure out where exactly\nthat'll live in the docs but would you\nbe open to\ncreating a page for that um yeah i think\ndefinitely i would come up with an\nexample\noutside of the hbo world\nand then like the first step is just\ncreate a file with\nthe data set and\nyou need a bell thing and then we can go\nfrom there\nawesome\nis so in i remember i sort of remember\nin your your use case\nyou're using\nyou're using spark for data\num data cleaning and stuff like that\nbut also for\nlike as a pipeline right you made that\npope's login right\nyeah yeah because we're running like the\nspark ml\nuh library to to\nuse our models or that's what we're\nusing for our models so\num okay awesome yeah end-to-end spark\nawesome\nsorry to interject but like evan did you\ntry to\nserve the\nspark pipeline model\nnot yet i would do that as part of this\ntask\nthat'll be like amazing to see let's see\nyeah i want to try it\num\nyeah so i think depending on the nature\nof the problem\nwe're kind of\norganizing tutorials around uh\napplication areas\nso depending on like\nthe problem you've uh\nyou've picked for this or we can\nhave like a okay\na broader category and then like spark\nand sparkle yeah something like that\ncool i'll find a good data set\nand i also see chive here\noh yeah\num also\nas a side note if you\ntype in in a comment and when any of the\nissues\nhashtag self dash assign it'll um assign\nyou to that\neven if you don't have like permissions\nto the in the repo\num so evan if you can you can do that\nyou know at your own time\nsounds good\n[Music]\nyeah depending on how quickly you can\nget to this i'll just add this to the\nnext release but you know you can push\nthis off depending on your capacity\num oh yeah chavez\nuh do you have any questions on this\nintegration test and examples for\ntensorflow um how's that going\nsure are your own\n[Music]\nyeah\nso\nfor\nso i think uh chris and arvin and\nactually also brandon i think you're\nyou're new to this meeting do you mind\nuh would you be open to just like\nintroducing yourself and um saying a\nlittle bit about why you're interested\nin union now\nuh brandon if you can go first\nyeah for sure um so i am a analytics\nengineer on uh\nhere at spotify\ni've been migrating our teams to fly and\nseeing this in the\nuh flight slack channel and uh working\nactually on the\nuh ml platform team\num\nat spotify i thought that it would be uh\nkind of an interesting experience to see\nwhat it is that you guys were building\nand\num\nif a similar patterns might work\nfor our team\nawesome\nwell thanks\num chris how's it going\nalso maybe away from keyboard uh arvind\nhi uh i was the one that texts you a\nweek ago in twitter\ni mean\nso right now i am understanding the code\nwest of union amen\ni will start contributing soon\noh awesome\nthat there is anybody created to azure\ncloud\nhi yeah this is chris yeah\nsorry um\nyeah so\ni'm a solutions architect at uh\ndatabrick\nlooking at some flight integration\nuh and you know how a union ml uh plays\nin that\nyeah it should like uh chris i think it\nshould just automatically work\nthat's the example that i was gonna add\nlike spark usage and hopefully\nyeah you know what\nthat's true um i'm curious to see you so\nuh yeah maybe i'll i'll connect with you\nafter\nyeah it'll be awesome to actually uh\nhang out and you know just talk yes\nwe've never had a chat\nlast last time we missed it\nuh is there were there any uh particular\nthings you wanted to contribute to or um\ni want to contribute in the\nin anything related to azure cloud\njavascript or\nlike\npython\nokay yeah feel free to create a an issue\n[Music]\num\nin the uni ml issues uh section\nuh you can see something that like the\nazure support\net cetera uh i feel like azure support\nwould probably be more on the flight\nside by captain\nyeah and i think that there are a couple\nteams that are running flight on azure\nthey have not upstreamed their changes\nuh but\nfrom what i heard the biggest problem\nthat they found was in my facebook\na adfs or a bff driver um\nso they have patched it uh\nif you\nwhoever is impressive i think you are so\nuh i can connect it to those folks\nand we can\nurge them to please contribute of stream\nuh because i think there have been some\npeople who have been wanting azure\nsupport and\ncompletely community contributed\nwell\nyeah there's um\nwhen it comes to pie torch\nit's not exactly pie torch but there is\nuh\na issue here for quite watch lightning i\ndon't know if you want to\ntake this task on\nuh i will look into that\nokay\nunfortunately\nthere's no easy way to like add people\nhere but yeah if you\nlike i said before\nyou can self-assign as well\nokay cool\num\nso i think\nsamita\nthe last that\nyou know you know we have we have a\nbunch of issues here for this next\nrelease\ni think for the next um zero to zero we\nshould start thinking about the k-serve\nintegration um\ndocument that you came up with\ni'm not sure what it is anymore but\num let's see if i can just capture that\nhere so all of this will be\num\nrelated to\nmodel serving so\nyeah i presume yara slow is working on\nthe back end part i'm not sure but i'll\nhave to follow up\nwho's working on the back end part of it\nuh paris love\noh\nokay yeah i'm not sure i'll have to\nfollow up let me just do that\ni don't think that's happening so yeah\nlet's actually come up with the plan\nover here um\nwe should\nwe i want to see like from the relative\npriority within union\nwhere we can support this part but i\nwould love to\nhave\nuh\none integration and i think bento folks\nhave been interested so we should reach\nout to them and say that hey we are\nready\nsure\num\ni must say\nand bento might not need a lot of work\non flight side uh\nhello\nhello\ni just wanted to know that essentially\nlike added for case of i wanted to know\nuh do we also plan for uh any\nintegration with selden\nyeah that was also the idea i think uh\nfolks from selden and door dash world\nhave been interested in this again it's\nall for us it's relative priority i\ndon't think we will get to it i think\nthey would be interested in contributing\nlater in the year so\nyeah so it's all uh\nyou know\nlike a set of puzzle pieces that we're\ntrying to bring together\nyeah\nso we should get ahead of this both of\nyou so the idea in uni ml is that\nideally there will be some kind of\nstandard api for\nsome built-in\nmodel servers and then like a base class\nfor people to inherit from\nuh\nto extend that to whatever future\nmodel server we want to support\num so\ni think we've decided k-serve and bento\nml are like\nthe first two we want to start off with\num\nand so i think that'll take a little bit\nof planning\nbut samitha do you have any capacity for\ntaking on\nstarting on one of these\nuh\none of these ones like so like the\nscheduling api we can discuss but that's\nbasically\nyeah maybe i'll take the time to\nactually write these out\num okay currently\nminimal supports\nbatch predictions\nmethod\nwhich\nexecutes\na work prediction\nworkflow\non some\npre-configured\nflight back-end\nhowever\nthere's no way of scheduling\nthis\num you know\nonly way of scheduling\nthis today\nwas\nby\nrolling out the flight\n[Music]\nprediction workflow\nand using the flight\nschedule\ncome up with\nsome interface\npull this in\nrename them\nso you know like just a sketch\ni don't know what this would look like\nbut\nso right now we have certain a few verbs\nso like we have a deploy verb for\ndeploying\num\nyour workflows to a flight backend\nwe have model.serve\nor\nlike serving a class api app\nwe could have a schedule\nthing\nwith some\nset of arguments i'm not sure\nyeah the idea here would be to create a\nnew launch plan essentially within the\nflight world\nyep yep exactly\nand i guess this could be called\nmultiple times\nmultiple different kind of things with\nthe new name i guess i think that\ncalling we should probably take a name\nin there\num\nyes\na launch pad\nyeah i had a question uh\nneil's that have we have we\never tried an example with where the\ntraining itself is done on\nuh\nso this is a separate\nso these would be like examples of how\nto do that\nyeah at least have one example uh just\nto because i think\na cool thing that can happen over there\nis you know even sagemakers we have some\nfolks who have been interested in union\nml with stage maker and that would be\nand they could write that example right\nyes you should tell them oh this is how\nyou do it go for it\nyeah so i guess we can write this as\nseparate tasks but um you mentioned what\nray\nyou're training on let's say mpi\noperator the simplest one we already\nhave the example in\nflight just like creating unity\num but yeah going back to this uh yes\namita we can we can talk about the\nspecifics of this\num\nyeah there's like a ton of examples that\nneed to be written here yeah in the docs\nlike as you can see\nthere's like\ntwo tutorials for\nlike computer vision applications\nbut\nthere's a lot of like flight features\nthat\nare exposed on union ml that aren't\ncurrently documented so\nthere's just a ton of them\nyeah\ni don't think we can ever document\neverything it's like\nuh as in examples what we should\ndefinitely\ndocument the mechanics\nspeaking of sagemaker\nthe sagemaker endpoints prediction\nendpoints\nfall under\nyeah\nthat server\ninteraction\num\nokay\nuh chavez are you still there\nlike he's a bot and then\nyou're here\nyeah there's like an eight-minute delay\num cool so i think\nthis is good for now\num i think the biggest\nthe biggest pieces here are these these\nintegrations\num\nthere are these four ones that i think\nare lighter weight tasks so um\njust looking forward to zero to zero\nthis is like a fairly small task\nthis would be medium\nthese are all these are both large\num this i'd say is a medium\n[Music]\nslight user with a model artifact\nproduced i think maybe this is a medium\nso\nthis these three i think we can um\nmaybe prioritize next time\ndepending on uh\nwho has the capacity\nbut um i think yeah we can\nopen it up to questions and or end the\nmeeting\ndepending on where i get on 51 i've got\nmy eyes on that fast api image one as\nwell\ni'll uh i'll self assign\nwhen i'm done there in case someone else\nwants to pick that up before i'm done\nwith the one i've got already\nawesome\nlet's see\nawesome thank you\nthanks dog yeah all\nright on the the community edition of\nravello that's that's awesome news\nyeah we'll see we'll see where it takes\nus\noh oh there is a community edition now\noh congrats i did not know that oh we\nshould definitely\nthanks japan yeah it's uh\nit's\nyeah it's just come\nfree to free to sign up free to try it\nthere's still lots of stuff that we are\nworking on building but uh the sort of\nbasic api is there so\nyeah\nyeah we\na couple weeks ago\nin the flight community thing\ny logs presented\nuh i don't know if you guys had a chance\nto see it uh if you guys think\na similar pattern might work for some of\nyour work uh we would love to figure it\ni think you didn't email us the way but\nyou know\nuh eventually you didn't remember flight\ntogether time would love to\ni think we should talk about that even\nif i can do this\nand have even wider audience so\num\nthanks guys\nwe got a blog post i'm just gonna get\nthat on linkedin i was gonna fire it\nover to maybe neil's there and post it\non the\nin the room there just to kind of get\nsome\nideas from the community on how they\nwould like to see the integration happen\ntoo because it's good for us to hear um\nwhat would work from them from their\nperspective as well so\nabsolutely\nhey chris uh\nif whenever you have a chance let me\nknow if you are on the\nflight slack channel\nuh definitely dm me let's have a chat\nyeah definitely it sounds good\nall right guys um and brandon i think we\nare already in touch with mark seed and\na lot of different people uh\nthere's a lot of integrations happening\nwith flight and spotify and bio platform\nas you know probably i don't want to say\nit in here in this meeting but uh\nyeah if there are certain places that\nyou think it's useful to look at union\nml there is no i know mahan with mahan\nout there see if that's the way things\nsaid\nwas interested in the team\nall right if that's it okay that's all\nthanks everyone\nyou"
    },
    {
        "title": "UnionML OSS Planning 001",
        "transcript": "hi everyone i'm niels ventilan\ni'm heading up the union ml effort in\nthe union ai team\num\njust to give you a quick\nelevator pitch for union ml just in case\nyou're not familiar with it and you just\nfound yourself here\nit's a framework built on top of flight\nand it's our attempt at\nreducing the boilerplate and complexity\nof\nin particular building and deploying\nmachine learning applications\nso\ny'all you may all know flight it's you\nknow\na great\nexecution graph orchestrator pipeline\ntool\nthat\nyou know\ncan be used for machine learning but it\ncan be used for any arbitrary\ncomputational\nproblem\nso\nyou know with union ml we're really\ntrying to\nbuild in certain assumptions about\num\nhow you're\ncreating models um in a kind of industry\nstandard way\nso that's a quick thing um\na little since this is the first\nopen source planning meeting for union\nml i did want to just give everyone some\ncontext here so\nthe plan for this meeting is that it's\nopen to the public\nand we're\nthis is literally going to be like a\nkind of a work meeting that we have\ninternally right but we're going to\ngo through the road map re-prioritize\nthings as they come up\nwe do have our own sort of like top-down\nroadmap that we've\ncreated um\nthat's not to discourage anyone from\nhaving\nstrong opinions about where we should go\num\nfeatures you want to add sooner than\nrather than later\nso\nthat's the purpose of this meeting it's\ngonna happen every two weeks\num you can subscribe\nlet's see\nmaybe i can start sharing my screen now\nyeah you can find this um kind of button\nhere this uh this badge\nand if you find it\nyou can add this event to your calendar\nand\nyeah again since this is the first time\ni wanted to just maybe go around and\nhave everyone introduce themselves\njust name\nsort of what your primary thing is what\nyou do professionally and\nwhy\nyou joined today\nso i'll start i guess i already\nintroduced myself\nuh\nsomething that i i guess\nunion ml is like the tool i wanted to\nbuild\num because\nyeah there is it's sort of like a\ndistillation of all the stuff that i've\nhad to\nbuild myself in various different\ncapacities and in the past um\nand yeah i'm excited to use it myself\nfor my own little side projects\nso an example of a little thing that\ni've been working on\nin my free time is\nmusic generator using wavenet so\nthe project was a little more ambitious\nbefore but now i'm trying to get get\nlike\nhip hop beats different genres of music\nand just train a model to generate that\nmusic\nmaybe conditional under genre\nso i've just been you know using pytorch\nand i'm hoping to try out and really\nlike test out\nunion ml\num\n[Music]\nwith using that project\nmaybe\neduardo do you want to go\nsure yeah so my name is eduardo i i also\nwork on union um i'm a software engineer\nthere and um\nmy primary interest in in unimail really\nis um trying to like bridge the gap\nbetween you know the the core problems\nin frequent infrastructure\nrelated to machine learning applications\nor like around that area and the actual\nlike usability of like you know how how\nhow do you structure machine learning\napplications you know like what is this\nlike the fact that we didn't have a\ntextbook solution for this kind of thing\nreally attracts me to this problem\nbecause i i can foresee like how unit\nmail can become\nsuch an uh\nsuch a standard but yeah that's that's\nme\ncool uh maybe at the end of the\nintroduction you pick the next person\ncool uh\ni don't know zev\nwe don't we don't know each other but\nyou know sure yeah that's fine eduardo\nwe know each other now\num yeah i'm zev eiser i'm a software\nengineer i work at ravello systems we're\na startup ourselves um i ran into niels\ntalk introducing union at mlap's world\ni really like the way the project is set\nup rivela is building\na monitoring api that sort of has the\nsame sort of simple what is the basic\nsolution of how monitoring of ml models\nshould work\nand niels reached out or we reached out\nsort of the same time and said hey you\nknow what it'd be pretty cool if we\ncould work together and put you know a\nsimple monitoring service at the end of\nthis too so i'm happy to hang out here\nand and brainstorm and put some ideas\ninto the pipe so\nyeah that's me um\ni will hand it right over to mike so you\ncan hear enough about a rubella right at\nthe same time because he's with me\nhey guys nice to meet you all and some\nfamiliar familiar faces on here um so\nyeah just basically what zeb said i'm\nless on the technical side i'm more on\nthe kind of business development\nkind of ecosystem side so i'm just\nreally interested in learning about the\ndifferent players\nand how the different mlaps tools can\nwork together and kind of provide a\nbetter\nultimate end solution for you know the\nusers and the\nyou know ml engineers and data\nscientists out there so keen to just\nlearn more and um\nlean on zev for kind of the more\ntechnical side of things so that's me\nuh let's do\nmilan\nyup hey good morning fellas and milan\ni'm in consulting more technical like\ndata science and ml consulting and so\nwhen i see new tools on the market i\nalways want to just listen in a little\nbit more or get to understand\nyou know how we could potentially serve\nup different solutions to clients or\nwhat's the best way because a lot of\ntimes clients like well what's the easy\nway to do this what's a real way to do\nthis or we have to use our enterprise\nsubscription because we're microsoft\ncustomer or something right so\ni just want to understand the tools that\nare out there and and and expand our\ntool shed a little bit\nand you know you never meet not great\npeople at like a ml event like this\nright it's always you meet the\nthe superstars right\nawesome thanks milan yeah if you can\npick the next uh person yeah\ncome on down\nhello hi so i'm eric uh so i'm in\nproduct management and uh machine\nlearning two chains is a new thing for\nme so i'm doing some of my research\ni um\nfound a flight and then i found union ml\ni'm trying to figure out what's the\ndifference so that's why i'm here\nawesome yeah hopefully that'll become\nclear\nyeah i'll pick the next one is uh yeah\ntong\noh hey hey that's me i think um\nhi i'm at union um i'm\nhere mostly to see how\nopen source planning meetings can be run\nbecause we want to do it on the flight\nproject as well\nso look forward to that\nhopefully being open\nby the end of this quarter we have some\nplanning today hopefully sooner but\nwe'll see\nuh see that\ncigar are you there\nokay\ncan you see me\nyeah okay\nhi i'm shigato um hello folks um i'm a\ngraduate student i'm working in my\nphysics phd almost about the finish at\nuniversity of wisconsin milwaukee\nit was solely physics there was no deep\nlearning or ml anywhere\nbut the problem required so so\nonce i learned all of that and made the\nsolution i was reading more about how do\nyou implement this to the real world\nbecause you if you want a solution\nuh from start to finish so people could\njust go and use it the impact cannot be\nfelt without the\nthe infrastructure of ml ops and so i\nstarted looking at different mlr\nframeworks and\nthat's when uh luckily i came across\nthis this post i believe it was uh from\nniels\nand\nso then i started looking at uh union ml\nand it\nseemed quite interesting and\nthen i started working on that and then\nuh thank you for\nuh the the contributor of the month so\nyeah\nso i'm here to learn more and\nsee how else or how better i can use\nunion ml for\nml ops that's it thank you\noh yeah so who's next i think uh yeah\nlet's just go with uh smitty why don't\nyou go ahead\nokay\nhi um yeah\nso i'm a part of union ai um i'm a\nprogrammer writer there i was a machine\nlearning engineer before i'm just here\nto understand more about union ml and\nsee what i can do with it\nand see how open source meetings work of\ncourse here\nthank you um i pick sandra\nhey everyone i'm sandra i'm also with\nunion i'm the marketing manager right\nnow so i'm here to make sure the\nrecording goes smoothly i'll be\num editing the recording and posting it\non the youtube channel for union keep an\neye out for that and i'll be doing um\nthe social media marketing\nso yeah good to have everybody here\nis there anybody left um samito why\ndon't you go let me go yeah yeah sure\nhey everyone my name is samhita i'm a\nsoftware engineer and technical\nevangelist at union ai\nso um i have always wanted to build\nmachine learning models and dive deep\ninto the concepts so yeah i'm here i'm\nlooking forward to learning and\ncontributing\nyeah\nthank you awesome i think uh someone\njoined and then left so you know we'll\njust move on now unless he joins a\nlittle later okay cool so\ngreat let's dive in um\n[Music]\nstart sharing my screen\nnews i think i'm missing one person um\nsorry you you only have your\nwhat i see is your\ns gold better oh yes\ndo you want to go um\ns go dead i'm not sure what the name is\nyou're nude right now\noh okay so\nit's actually steve george's data the\nsds are my initials\notherwise thanks i'm a molecular\nbiologist\nstop upon flight and uh\nstamp upon uh nails work in my first\nintroduction to flight i think that that\nwas about two weeks or two weeks or so\nago so i'm much more interested in\nthe prediction of what we call\ntranscription factors in uh in biology\nbut currently i write workflows for\nbioinformatics but i would be much more\ninterested to see how this impact my\ncareer in\nin a few minutes or so\nthanks\nawesome\nthanks for joining us\num yeah it seems like we have a pretty\ndiverse\nbackground\nhere\nwe're always cool to see\num yeah so let me just start right here\nuh just with this diagram\nso\nyou know we're i'm i'm going to work on\na little bit clearer of a diagram that\nshows the entire picture but this is\nreally it like the whole point of union\nml\nis\nit's designed in such a way that you\nhave these functions that you implement\non the left side here which are called\nmethods here\nand the whole job of union ml\nis it will\ncombine all those functions in ways that\nmake sense to practitioners\nusing flight and so\nunder the hood everything is flight\nworkflows and flight tasks\nbut\nthese functions that they're\nuser-defined\ndefine the\nthey handle the variability the richness\nof all machine learning at least in this\ncase\nright now supervised learning\num\n[Music]\napplications\nso there's a lot of different stuff that\npeople can create\nbut the way that you put them together\nshould be fairly standard at this point\nyou know i think i think we've we've\ncome to\nvarious industries industry standards\nof how you put these pieces together\nand on the right side are microservices\nthat you can deploy in various different\ncontexts\nso today\nthe training microservice exists as a\nwhite flight workflow that you deployed\nto a flight back end\nbatch prediction also goes\nin a flight back end\nonline prediction happens in a fast api\napp\nor um\n[Music]\naws lambda\nservice\nand the dots represent you know\nall the different things that we could\neventually do\nthe thing that is not really pictured\nhere\nis things that um zev and mike are\nworking on things like model monitoring\ndata monitoring\nthese are things that will kind of just\nbe interleaved throughout\nthese various pieces\nwhen it makes sense right so when you\nhave an online prediction service you\ncan imagine having\nsome kind of integration\nand\nwith with the ravella the thing they're\nworking on\num and other such tools that are that\nare you know up and coming in the\necosystem but the whole point\nof union ml is that\nit is this kind of middle layer that\nas i said earlier handles the\nboilerplate and complexity of how do you\nput these things together\nso the\njust to give you sort of a very\nlong-term view of this project\nthis is the long-term roadmap that\nthat's\nthat i described here in this blog\nand these are kind of\nsix main buckets i'm sure there are more\nthere's stuff that i'm missing here but\nthese are the six six main buckets that\nfor the foreseeable future we'll be\nreally doubling down on\nso from experiment tracking data\nannotation\nmodel tuning\nfuture engineering and feature serving\nmodel serving and data quality and\nmonitoring\nso\nthat's the long-term view\ni'm actually curious at this point if\nanyone has any thoughts here already\nthat they want to share\nif not i can go to the into the weeds\nokay awesome\nthis is our top down this is a kind of\nthe top-down view of the milestone we've\nset for ourselves\num\nthe first two bug flick the bug fix like\npatch releases here are more\nkind of\ninternal you can you can hop into\nvarious issues that are in here but\nbasically these\nthese um\nmilestone releases are for\njust cleaning up the stuff that we've\nbuilt so far um so there's\nyou know com\ncoming up against the launch you know we\nwe\nyou know spent the tech credit card we\nhave some debt and we're just going to\ncatch up with that um\ncoming up real you know in the next few\nweeks\nbut the the fun stuff happens after this\nright so\num\nagain this is top down so each of these\nhas a theme but if there are any\nspecific items in any of the future\nreleases that you know someone's just\nreally itching to\ncontribute\nuh more happy to move those up\nand um to earlier releases\nso the first\ntheme that we're working on is um really\nimproving the prediction\num\nservices that that union ml offers so\nright now it's just batch and then\nonline\num online being some kind of endpoint\nthat you hit\nand it's assumed that you have some\nother system that\nhits that endpoint to get predictions\nand batches um you know pretty\nself-explanatory you have like a table\nthat you want to write\nto consistently over a schedule\num batch is the way to go\nbut\nthis this release\nwill add support for streaming\nand scheduling batch predictions\num\n[Music]\nzero three zero\n[Music]\nwill focus on model tuning and\nexperiment tracking\nand\nso the current state of union ml is you\ntrain a single model\nand you might call the model.train\nmethod multiple times\nso just to show you what that looks like\nhere's the basic\ntutorial for this\nso there's a bunch of stuff you do but\nyou know you can call it model.train\nlocally to train models locally or you\ncan call model dot remote train if you\nwant to use a flight remote flight\ncluster in the back\nbut it doesn't it doesn't systematically\nsupport fine tuning of your models um\nand keeping track of the experiments and\nthings like this so\nthis um this will focus on getting some\nbasic\nbuilding blocks for that\nup and running\nzero four zero will focus on union ml\nand flight interoperability\nso\nas i said under the hood right\neverything that\nunion ml does is around\nbasically combining the different\nfunctions that you define\ninto tasks and workloads so\nfor example there's this train task\nmethod and if you call it it literally\ngives you back the\nflight task that unionml has created\nand same goes over the train workflow\nso\n040 will really just help people\npart of it is documentation\npart of it is also um\njust things that we might have missed\nwhere you can pull out the union ml\ntasks use it in your flight workflows\nfor prediction as well\njust to help your existing flight users\nuse these abstractions\nso you can imagine this\ncombining well with the model tuning\npart\nso once you have some kind of grid\nsearch a random search or bayesian\noptimization thing\nyou can imagine that compiling down to a\nflight workflow\nthen you can just plug that into your\nflight\npipelines\nif you have some kind of model tuning\nstuff\nand last but not least zero five zero is\num\ncalled integrity\nand it's to improve the observability\ndata quality and jupiter support\num of union ml\njupiter support here is kind of a little\nout of theme but you know\nuh it is what it is\nthis is where things like uh great\nexpectations pandera\nwhy logs revela come in\num\nthey won't be\nfunctions that you implement most likely\nbut you know they'll they'll somehow\nwill find integration points and\num i guess to zev and mike\nuh i'll i'll dive into the actual the\nroadmap here now\nbut uh if if you guys want to contribute\nwe can definitely you know\ni think the rubella\nyeah there's a task here for integrating\nwith vela it's an empty ticket right now\nbut\num definitely would love to chat with\nyou\neither here or just offline\nto figure out the details of what that\nmight look like\nokay\nany questions so far\none of the folks that joined was asking\nabout you know what's different between\nunion and flight\num\nhow far\ndo you want to take the project with\nstill having sort of a flight back in\nfor most things versus sort of bring\nyour own back end for stuff\nyeah that's a that's a pretty deep\nquestion yeah yeah\num\nin theory we i think we\nlike so i guess i'll speak personally i\ndon't know about union ai as a company\nbut uh personally speaking i don't i'm\nactually\nthat be cool\nbut a very heavy lift to support other\nback ends right like\nthe reason flight\nyou know\ni've drunk the kool-aid but the reason\nwhy i think flight is very powerful is\nit because of its type safety\nreproducibility\num isolation of nodes in the graph\nexecution graph\nthat most other pipeline orchestrators\ndon't really have they they will often\nconflate the ex runtime environment\nuh\nfor many benefits right\nsharing memory you don't have to worry\nabout a lot of stuff\num\nso\nwell you know i guess my short answer to\nthat is\nit would be awesome to support other\nbackends\num and union ml is becomes then kind of\nlike a syntax\nfor expressing um and writing down\nwhat your ml system's behavior should be\num at certain key points\num i think for the foreseeable future we\nwill rely on a flight back end\num\nbut i think\nyou know to answer the question of\nflight what the difference between\nflight and union ml you can think of\nthem as very much complementary like\nflight provides the primitives the\nbuilding blocks\nyou know here are your nodes this is how\nyou connect them this is how you\nprovision resources for them\net cetera et cetera\nunion ml\nabstracts a lot of that away\nhowever it does expose it to super users\nso if people\nfor flight users who are familiar with\nhow everything works you know at the\nflight level\nthe idea is you can access those at that\nabstraction level but you know\nwe're thinking maybe for 90 80 90\npercent of the use cases that union ml\nworks well with\num you won't have to dig in there\num\nbut the va that like you know to to\nto reiterate the value proposition of\nunion ml it really is bridging the gap\nbetween\num\nsort of like offline com compute\nprocesses which are usually like\ntraining right like training is the\nbiggest and data data processing\nthose those you really want like really\nrobust production level\num infrastructure backing it\nand then for prediction online\nprediction you want the same stuff right\nbut flight really isn't built for that\nflight flight's good for batch\nprediction\num today\nbut you can't really do\nyou know low latency predictions with it\nvery well that might change in the\nfuture but you know\nit also is beneficial for\nunion ml to integrate with other such\num serving systems like bento\nsagemaker prediction endpoints things\nlike that\nthat um have a lot of you know useful\nstuff just baked in\nyeah i i don't\nyeah i don't uh\ndisagree with the choice of using fight\nfor sure i'm just curious because you\nknow you see stuff like bento or aws or\nfast api coming in for the serving layer\nright so yeah there are other\nalternatives for the\nthe batch layer right but i think i\nthink that all drives well for sure\ncool\nokay we have um 15 minutes left in this\nplanning meeting so you know\nmoving forward we're not gonna spend 15\nminutes like introducing each other but\num i want to just spend the rest of the\ntime going through\nitems in the in the roadmap\num\ni've tagged a few of these as good first\nissues\nmaybe in the chat if you're interested\nin any one of these you can um\njust put\nraise your hand or like\nyou know otherwise express interest in\nthem and then i can assign them to you\num\nso if that sounds great let me just go\nthrough these\nso for zero one three a bunch of these\nare kind of heavy lifts the only\n[Music]\none that is a kind of a good first issue\nis this one\nso\nbasically the idea is that\nuh\nthere is this method called remote\ndeploy\nmodel.remote deploy and what that does\nis literally it'll deploy your workflows\nto a flight back end\nand currently to version your union ml\napp we use the get sha so\nit assumes that all unit ml projects are\nin a version controlled under git\nand there's this one little problem with\nusing that which is that if your branch\nis dirty like say you've deployed your\napp\nunder some xyz version and then you\nchange\nsome code in that\nin your branch\nthe get shot will still be xyz but your\nbranch will be dirty\num and so the the purpose of this is\njust to error out um if if that is the\ncase\nand\nlet the user know\nthat they either should check in their\nchanges or supply the ignore diff\noption\njust to force the force deploy\ni do not it's not a very it's not a very\nglamorous issue admittedly but if anyone\nis interested in that you can um\nsay so now\nif not\nit will just default oh\nzav i see your cat joined us\nyeah i'm trying to reach for the unmute\nbutton because that's the kind of good\nfirst issue i can hop in and do\noh yeah okay great absolutely\ni actually don't know how to do this\nright now yeah without me on here i\nshould have looked this up but let me\nfind it and comment on it that's easy i\nhave the the backlog open\noh awesome okay\num\nniels uh can i say something yes\nyeah so uh this is chicago um so\nrecently\num i added\na a feature in the repository for union\nml any uh issue if you want to assign\nthat to yourself just go into the issue\nand then type in hash\nself dash assign and it will assign it\nto you\nlike this\nyes\nand uh make a comment and then it will\nassign to you\nthe there's a github uh\naction for that\nall right i remember right yes right\nokay yeah so zev if you if you just do\nthat if you do the yeah hashtag\nself-assign it'll it'll do that for you\neven if you don't have the permissions\nokay\num\nyeah there are these other four i i\ndon't know i kind of hesitate i i i'll\nprobably just get\nmake a pr for all three of these since\nthey're all somewhat related\num\nthe other three good first issues here\nin the next release which is fine we'll\njust make a release branch\nawesome\num\nyeah let me assign myself these\nare unit tested examples for xg boost\nand tensorflow so we currently have um\nexamples and unit tests for i think\nsklearn pi torch and terrace\nso\num so this would in these two tasks\nwould it would involve\nadding those unit tests\nand also\nwe need to figure this out but examples\nof how to you know use them so either\ntabs here in the quick start page\num\nso if anyone's interested in that yeah\nlet me know right now or do the\nself-assign thing\nhere's the um\nwhere's my chat\na road map if anyone's interested\nokay\nanother good first issue is adding a\nhealth check endpoint to the fast api\napp\ni actually haven't looked into this i\nthink caitlyn put in a little example\nhere\num\neduardo do you have context on what what\nthis is for it's for like aws lambda\num\ni\ni think it was specific to to lambda no\nit was just like like\nusually you know um apps that you deploy\nin kubernetes exposed health and points\nso that you can you check beforehand\ni know the kubernetes side of it for\nsure\num yeah eduardo's right it's uh\nan app has a bunch\nhttp is one method but um\nfor a health check endpoint it's\nbasically kubernetes won't start routing\ntraffic until the service self reports\nthat it's okay\nlike ready to receive traffic it might\nhave some like boot up stuff it needs to\ndo before it's ready health check lets\nthat happen\num\ni looked at this one earlier today the\nfast api health library is sort of odd\nbecause you really want like services to\nreport their own health not other\nservices health\nthe fast api health library here like\nreaches out to the database to see if\nit's okay\nbefore it'll say that it's okay whereas\nlike you really want these to be\nindependent of each other like the\ndatabase should have its own health\ncheck or something in that example right\nyep um\npretty straightforward for us like\nbasically you just have to have a model\nand say like yeah we're ready to make\ninferences which kind of happens when\nthings\ninstantiate anyway so your health check\ncan basically just be like returned true\nit's kind of more of a like one little\nroute for it and then a bit of\ndocumentation i think gotcha\nokay\nso that is there like a general contract\nfor this or just returned through is\nthat like usually um\non at least the kubernetes side which is\nagain all i sort of know and\num it might be different for different\nplatforms but um\nbasically it just needs a 200\nokay response right\nyep\nokay awesome\num any takers for this one\nif not that it's totally fine\nokay uh\ngreat\nthis is actually a pretty okay\nit's not\ntoo crazy but\nthis task is to\nadd integration tests\nfor the remote\nunderscore you know train predict\nmethods so currently\num\ni mean it is kind of heavy it probably\nwould make sense more for someone who's\nfamiliar with flight but\nthe purpose of this is\nthat you\nspin up a demo flight cluster locally\nand then you you\ninvoke the remote train and predict\nmethods against that flight cluster and\njust you know\nactually test that everything works as\nexpected\num yeah um i'm gonna assign that one to\nmyself news okay yeah i can do that\nthanks\nyeah this also dovetails with the\nthe\nflight thing\nexactly yeah okay\nand i guess this is related to um i\ndon't know if you you can probably\ntackle these in the same go but\nthis\npulls out model tasks and workflows out\nof the union ml\nobjects and then\nuses them in a flight workflow\nso let me just say yeah that could also\nbe like a sort of integration test as\nwell right\nyep\n[Music]\num you want to tackle this one as well\nor yep okay awesome please\ngreat\nokay\nso we have five minutes left i i kind of\nwant to say i don't want to look too far\ninto the future so\nprobably these two patch releases would\nbe great to first tackle\num\ni guess uh zev on the revela side\ni think we have we have we have a\nseparate meeting statute schedule for\nthat right yeah and i mean a lot of our\nstuff comes into play here\nlonger term once we have a bunch of\nthese earlier releases out right\nmonitoring is this sort of later piece\nso i'm happy to be here and\nhang out and think and you know keep\nguiding our product in a way that's\ngoing to work with union eventually when\nwe get there but um yeah no huge rush\nfrom our side on that\nokay amazing\num\nyeah so i think i kind of want to end\nthis now um the last thing i'll say is\nto people watching the recording of this\num\nwe'll post up all the links uh relevant\nlinks and stuff so if you want to pick\nup any of these\nthese tasks if you're interested\ninterested in them\nplease do\num\nand yeah let's let's maybe end if with\nany questions or feature requests like\nideas basically let's let's\nspend the last five minutes just sort of\ntalking about those\ni do have a few but\nyeah i'd love to get uh other people's\nthoughts here\ncool um\nokay so in that case the last thing i'll\nsay is\nthere's a tutorials tab in the\ndocumentation\nit's pretty sparse right now has\ncomputer vision\num it has like an mnist example and a\nquick draw example\nthis is also another great avenue for\ncontributing which is basically\nlike road testing union ml\nwith different different applications so\num\n[Music]\ni'll go ahead and make a few kind of\nplaceholders for like nlp\ntabular data time series\nany other categories\nif there are any that\nany like data modalities or task types\nthat anyone are interested in\nis there anything that\nis on people top of people's mind right\nnow\nuh hi this is eric i have a quick\nquestion i just noticed that you have a\nbeta version it's not the oss version on\ngithub um your documentation is on the\ntop you said you guys release the beta\nversion is that a hosted uh\nversion or\nwhat's the difference between like uh\nuh\nis it\nis the open source version and the beta\ntheme\noh yeah yeah we're kind of losing we're\nusing beta here kind of loosely\nit's more to set the expectation that uh\nthere will be probably many rough\npatches in the api probably you know a\ncouple of bugs that we didn't or many\nbugs that we didn't account for so\num\nthis is different from like a beta\nrelease\nlike zero one zero beta something this\nis more just\njust to express the fact that uh it's\nstill early days for the package but\nit's all open source\noh great thanks yep\num okay well i'll i'm gonna leave this\nissue open here i'll move it up to the\ntop of the no milestones one\num\nfeel free to add comments here if you\nhave any ideas for tutorials you want to\nmake\num one that might be helpful for sort of\nopen source contributors going forward\nis either\nif it's in a separate file but in the\nreadme just like\nsetting up your dev environment to be\nhacking on union ml oh awesome yeah i'm\nfamiliar with python i can spin up a vm\nand work my way through a setup pi\nthat's fine but oh you've got it here\ncool okay\nso yeah the the developer the\ncontributing guide yeah has a\nas a section on how to set up the dev\nenvironment\nokay awesome uh do you think it's worth\num neil's just a small um suggestion on\nthis um i think it'll be worth also\nmentioning um conda in the in that\nsection\nyeah it's probably yeah\ni think we sh\nso i'll i'll do that just real quick\nhere\nyeah there are probably many condo\nusers add conda\ntoo\nand yeah shout out to shigato for\nfor creating that um\n[Music]\ni'm having a brain for it uh\nyes to contributing guide\nanyone want to take this this this is a\npretty quick one\ni can't do that\nawesome\nconda aficionado oh yeah you can assign\nthat\nto yourself\nokay\nawesome thanks everyone for joining um\nlooking forward to future planning\nmeetings and um as always\nwe're always available on our slack\necosystem unionml\nchannel\nawesome thanks everyone"
    },
    {
        "title": "UnionML Update, Roadmap and Contribution Guide",
        "transcript": "hi everyone I'm Niels bentelin\num and yeah I'll just go through really\nquickly give you some updates on the\nunion ml flight ecosystem project\nso for those of you who don't know what\nunion ml is it's a higher level machine\nlearning specific framework built on top\nof flight\nand it's a way for us to\nprovide you know some nice conveniences\non top of flight that help you to write\nmachine learning models and deploy them\num much more easily it's a little bit\nmore opinionated than flight\num but yeah this is my first hand at the\nflight meme propaganda so please pip\ninstall Union ml try it out\num if you're interested\num\nthere are links down here uh for the\nwebsite the documentation the repo but\njust to give you a little bit of an\noutline on the\num\nthe roadmap\nwe have a few things here that we want\nto\num build out so we're prioritizing the\nthe prediction and serving story uh\nbehind Union ml so adding support for\nstreaming batch prediction and\nadditional serving Integrations\num\nI see major attendant asks is it\navailable in Contour Forge yes I'll get\nto that in a second\num\nthe next the following release will\nfocus on model tuning and experiment\ntracking\nand then\nimproving Opera interoperability with\nflight\ncurrently you can use Union ml with\nflight quite well but um I think there\nis a lot more there we can do to make it\nmore seamless\nand then finally we're going to double\ndown on model observability data quality\nand kind of the ux around Jupiter\num and the next slide should be my last\none cool\nso if you're interested in contributing\nuh if you go to the union ml read me\nread the docs site there's a\ncontributing guide where you can\num you know get started in in various\nways from lightweight to heavyweight\num like improving docs to filing bugs\nand feature requests and pull requests\non the bottom there you can see that\nthere there's a link to the road map and\num we're planning on starting uh OSS\nplanning meeting so it'll be different\nfrom this one and that it'll like\nliterally be a develop developer meeting\nwe'll go through the roadmap by default\nit'll be myself and a few of the folks\nin the union AI team but we're going to\nexperiment with just having an open\nstanding meeting with uh that's open to\nthe public\num so you can participate you can give\nyour feedback you can ask for features\nthere you can interact with us there\num and if you want to Fast Track a\nfeature on the roadmap\nyou can check them out in the issues\nlink on the union ml repo you can join\nus for those planning meetings the first\none is going to be in July 6th\nand then you know as Shia LaBeouf says\nyou can just do it just make a PR if you\nwant to implement something yourself and\nyou're excited about a feature\nuh oh sorry very uh the very last thing\nI'll mention here is a union ml\ncontributor uh sugato array is a PhD\nphysics PhD candidate at the University\nof wisconsin-milwaukee\nhe literally just made condo Forge\npackages for Union ml flight kit and\nflight IDL so shout out to sugato maybe\nwe'll we'll have him on here\num next time if he's available\nabsolutely I think this deserves uh this\nhas been one of the most asked feature\nat least for Flight 2 uh the condo Forge\nand I think for Union ml is fantastic we\nwould definitely want to add him as a\ncontributor of the month so thank you\nsocato"
    },
    {
        "title": "UnionML Intro",
        "transcript": "hi\ni'm niels bentilan and i'm in the\nmachine learning team at union ai\nwe think that creating machine learning\napplications should be easy and\nfrictionless but today it really isn't\nthe cost and complexity of deciding what\ntools to use how to combine them and how\nto maintain them in production requires\na whole team of people who often speak\ndifferent languages and follow different\npractices\none of the core abstractions that many\npeople use to tame this complexity is\nthe data pipeline or execution graph\nwhich is extremely flexible and powerful\nbut we wondered whether there's an even\neasier way to build and deploy ml models\nwe took inspiration from web protocols\nand ask ourselves whether it's possible\nto define a standard set of methods or\nverbs for machine learning that can be\nreused across many different contexts\nlike training experimentation and\nprediction so that data science and\nmachine learning practitioners don't\nhave to worry about how these pieces fit\ntogether\ni'm proud to share union ml a project\nthat's our take on answering this\nquestion\nunion ml is an open source mlaps\nframework for building web native\nmachine learning applications\nit's an abstraction built on top of\nflight an open source orchestrator and\nit aims to unify the rapidly evolving\necosystem of ml and data tools into a\nsingle interface for building and\ndeploying ml micro services\nthe core idea behind union ml is that a\nmachine learning application is composed\nof a data set and a model\nand each of these expose functional\nentry points to some core piece of the\nml puzzle such as where do i get my\ntraining data\nhow do i train my model\nhow do i evaluate it and how do i\ngenerate predictions from it\nthe user simply has to define these\nfunctions and unionml takes care of\nbundling them into microservices that\ncan be deployed to a variety of targets\nincluding web and serverless endpoints\nwhat's more is that since union ml is\nbuilt on top of flight you can\nseamlessly scale your training and batch\nprediction workloads by allocating the\nappropriate resources as you need them\nthis not only helps data scientists\nreduce their cognitive load for\niterating on models and deploying them\nto production\nit also relieves mlaps engineers of the\noverhead of provisioning resources for\ndata science stakeholders\nwe're super early in this journey but if\nyou're a data scientist or machine\nlearning engineer who wants to avoid the\nboilerplate of productionizing your code\nyou can pip install union ml and run\ninit my app to see just how easy it is\nto make ml apps\nwe'd also love your feedback so if you\nhave any thoughts or questions join us\non slack using the link below\nsee you there"
    }
]